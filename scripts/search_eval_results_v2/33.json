[
  {
    "segment_id": "6dd4a359-0a00-43e7-92d0-91b7762df460",
    "episode_id": "96666d70-7554-4299-9fb0-2feba5c20724",
    "episode_number": 33,
    "segment_number": 8,
    "text": "Но есть ещё длинный хвост, который остаётся в 1 проценте, и он может быть довольно длинный. То есть иногда база может отвечать за секунду, за 5, просто очень редко. И у тебя возникает проблема, что сервис в целом ведётся непредсказуемо, потому что пришёл запрос пользователя, и ты его можешь обслужить очень быстро или не очень быстро. Другая проблема, если не рассмотреть базу данных, типичная сборка мусора в Java, у тебя много бэкэндов, и на одном из них началась сборка мусора. И он у тебя работал быстро, а теперь работает вообще не быстро. И в этой статье даётся замечательный совет, как это можно разрешить, а ты просто пошли запрос в два бэкэнда. У тебя один из них ответит быстро. Я считаю это отличный совет. На самом деле была гораздо раньше статья, извини, беспилотник. Была такая статья ещё гораздо раньше, от Google, на самом деле, называлась Tail at Scale. Ну как раз то, о чём я с вами говорил, длинный хвост. Там порядка 8 пунктов было о том, как Google борется с такими проблемами. А тут, я думаю, на неё идёт даже ссылка где-то. Вполне возможно. Очень здоровая статья, если вы пишете сервис, который общается с пользователями, хоть с какими-то другими сервисами, и он у вас устроен так, что у вас ответ собирается по частям с многих машин, а не там, то есть если у вас 5-6 машин, то ещё можно это игнорировать. Если у вас десяток машин, и чтобы ответить пользователю, нужно контактировать сразу с несколькими в любом случае. Или что если у Google, там у них идёт как бы, запрос как бы утопает через слои инфраструктуры, каждый раз он разбивается на всё больше и больше под запрос. Как бы если у вас что-то такое, ну или просто много машин, хотя бы этого достаточно, то вам просто необходимо знать это, иметь это знание у себя в голове. Ну и здесь приводятся некоторые цифры, но я не буду всю статью пересказывать, но краткое содержание такое, что за счёт этой избыточности, вы делаете в 2 раза больше работы, сервис может начать работать в 30 раз быстрее. И вывод делается о том, что оно однозначно того стоит, и плюс к этому есть ещё всякие вариации, например, я послал запрос, подождал 95% времени, и если за это время я не получил ответ, я шлю после этого второй запрос. В результате я делаю не в 2 раза больше работы, а там в среднем по больнице чуть меньше, ну в смысле сильно меньше, но получаю всё равно существенное ускорение. Только здесь есть такой тонкий момент, что твой сервис должен быть готов к таким повторяющимся запросам, и для этого тебе нужно написать определённую логику, либо, вот как ты сказал, что допустим мы делаем какой-то тайм-аут на 95% и потом делаем второй запрос, но в такой ситуации тебе нужно каким-то образом подумать, как же тебе отменить твой первый запрос, так чтобы не нанесли никакой коллизии, чтобы у тебя ничего не разошлось в системе. Я тебя понял. Здесь это тоже довольно накладно может быть, в зависимости от того, какую логику ты реализуешь, если она довольно простая, то здесь не будет проблем, но в принципе тут есть много тонких моментов. Свет, здесь есть... Синя, можно я скажу, кто первый? Давай, говори. Смотри, тут есть такой момент, если ты работаешь по ХДТП, если тебе ХДТП хоть когда-то торчит наружу, ты уже на самом деле просто в соответствии с этому протоколу обязан уметь это переживать, может сделать. Потому что он проксируется, лоад балансируется, у тебя мог клиент тайм-ауту не запросить цианного, то есть твой сервис уже обязан уметь это переживать. Нет, это понятно, если мы говорим про ХДТП, но мы же не привязываемся к ХДТП в принципе. Я хотел другое сказать, что во-первых в статье речь вообще идет исключительно о чтении, насколько я это понял по крайней мере. В случае с записью, как отметил Валера, у тебя действительно могут быть не только в случае с ХДТП, но типичный пример. У меня допустим мобильный клиент, на самом деле не важно мобильный он или не мобильный, но у мобильных чаще рвется связь. Я послал запрос и перед получением ответа у меня связь порвалась. После перед подключения у тебя почти любая реализация повторит последний запрос, и твой сервис должен быть к этому готов. Как он к этому готов? Например, каждый запрос должен содержать айдишник, и БКН должен фильтровать, что я такой айдишник уже обрабатываю где-то. Понимаешь, если у тебя есть клиент, у тебя есть сервер, и ты можешь такое делать, но у тебя может быть сервер-сервер коммуникации, это довольно трудно реализовать. Есть очень тонкие бизнес кейсы для определенных случаев. Но мы же понимаем, сер-ДТ придет и нас всех спасет. Ну сер-ДТ не всегда спасает, в принципе, у сер-ДТ есть теоретические лимиты. Кстати, коллеги, то, что вы описываете, довольно известная проблема для поисковика. Если вы представите себе, что происходит, после того, как вы набираете какой-нибудь запрос в кугле или, допустим, в Кококе, что мне лично более понятно, этот запрос разлетается по десяткам или даже сотням сервисов. Карты, маршруты, картинки, геолокационные сервисы, рекламы, еще что-то. Эту систему в Кококе делал Серега, назовем его так. Серега имел какие-то исконные корни из поисковика Mail.ru, то есть мы импортировали их терминологию. Терминологией Mail.ru это называлось верхний и нижний поиск. Нижний поиск – это сервисы, которые предоставляют данные, они могут быть сделаны разными форматами, отдавать даже в разных форматах могут. А верхний, иногда мы его называли мета-поиск, он получает запрос, шлет на все эти сервисы ответы, причем для некоторых сервисов ты шлешь запрос, и он возвращает тебе ответ, допустим для поиска, а некоторые сервисы могут вернуть промежуточный ответ, допустим математика – сложный сервис. То есть когда шлешь какую-то формулу, он сначала тебе быстро возвращает математический запрос или нет, и только после этого начинает расчет. И исходя из этих ответов, верхний мета-поиск принимает решение, в какой момент ему ответа достаточно, чтобы его зарендерить. Грубо говоря, если мы… То есть как бы, тут речь идет, во-первых, только о запросах, GET-запросах, которые не меняют никакого состояния на сервере. То есть мы набираем Саши Грей, мы знаем, что если картинки ответили, уже можно рендерить что-то клиенту.",
    "result": {
      "query": "tail latency mitigation strategies"
    }
  }
]