[
  {
    "segment_id": "95f265a6-03b5-4330-887a-1cfc399cb2fb",
    "episode_id": "8e5af947-2b26-46d5-a87a-7f1fc5a8af7c",
    "episode_number": 138,
    "segment_number": 3,
    "text": "Но даже его там настраивать, конфигурировать надо как-то сильно. Вот если появится супер хероку, в котором, ты знаешь, какой-то программист у себя пишет код, ему даже кнопку нажимать не надо, и всё уже в продакшене работает, автоматически протестировано, ошибки исправлены, вот тогда может быть. Но до этого нам надо сначала какой-то искусственный интеллект написать. Ну давайте посмотрим. Вот. А следующая тема – это тема из блога, нового блога старого Страйпа. Если я не ошибаюсь, Страйп, ведь это про автоматические платежи, вот это всё, да, в интернете? Да, но не только в интернете, у них есть ещё и хардварные всякие инструменты, что можно было бы платить. И они сделали новый блог. А у них хардварные инструменты? Хардвар – это Square. А у Страйпа только… Нет-нет, у Страйпа, по-моему, только онлайн платежи. Господа, я вообще не в курсе, какие-то там Страйпы, инкременты, вот можно объяснить, что происходит? Ну в общем, компания Страйп, они запустили не блог, а некий техножурнал для айтихников, по сути, получается, но скорее для больших компаний и не очень больших, чтобы продвигать, так сказать, правильные примитивы и некие фреймворки по управлению этими проектами. И у них сразу же вышел первый номер, этот первый номер про Uncall, там где-то 5 статей различных про этот Uncall, я могу вкратце рассказать, что в них написано, они достаточно интересные, они очень большие. В общем, все эти статьи объединяют то, что информация была взята у различных крупных компаний, вроде Facebook, Google и так далее. Соответственно, это не просто какие-то единичные люди, которые хотят свое мнение высказать, это уже устоявшиеся какие-то такие фреймворки работы. И, собственно, первый выпуск был про Uncall, и там утверждается, что Uncall – это практически всегда ручная работа, потому что если что-то уже понятно, как чинить, и это что-то уже происходило, то это все чинится автоматически, и, соответственно, уже оператора здесь не нужен. И у них там первая статья – это, собственно, о том, из чего состоит Uncall, и он состоит из пяти этапов. Первый этап – это, собственно, оценка. Что такое оценка? Это когда у вас пришел алерт, вам нужно понять, сколько, точнее, какое количество пользователей, например, затронуло проблема. Это самое важное. Как только вы поняли, какое количество пользователей затронула проблема и какие сервисы затронула эта проблема, вы можете создать некий сев, в котором описать уровень и описать тему. В зависимости от этого уже на втором этапе в кодации будут подключены те или иные команды, те или иные люди. То есть, первый этап очень-очень важен, потому что без него вы не можете понять, что делать дальше. В различных компаниях это работает по-разному, например, как там описано в том же слайке, они просто смотрят уровень от количества пользователей, на которых, собственно, проблема распространена. У нас, например, в дробоксе есть специальный эсмент, и в этом инструменте, когда человек хочет зафайлить сев, он выбирает различные параметры. Например, он может сказать, что затронуто, например, файл, затронуто нетворкинг, или проблема распространена на больше, чем 5% пользователей, или меньше, чем 5%, от 5% до 2%, и так далее. В зависимости от этого, есть такой небольшой визор, который достаточно быстрый, который можно пройти. В зависимости от того, как человек проходит этот визор, генерируется вот этот сев. Собственно, от уровня этого сева зависит, какого уровня люди к нему подключатся. То есть, если это сев нулевого уровня, то подключится огромное количество людей, которые должны его решить очень быстро. Если это третий уровень, к примеру, то вы сами будете его решать потихоньку, и он особо никого не касается. Потом этот улза создает некие документы, например, task какой-нибудь, и создает каналы в слайке, и так далее, и так далее. Об этом будет дальше. В общем, второй этап – это, как раз, координация. В зависимости от уровня включаются, например, инцидент-менеджеры. Инцидент-менеджер – это такой человек, который позволяет технарям работать над проблемой, чтобы они не отвлекались на других людей. Потому что приходят люди из других сервисов, и говорят, вот это не работает, что это такое. Вот это как раз человек, который будет всё это объяснять. Такая прослойка получается, ну очень важная такая прослойка. Он всем практически управляет. Собственно, координация происходит в разных местах, но судя по статье, получается, что во всех крупных компаниях практически все пользуются Slack и PagerDuty. Как бы очень такие критические сервисы получаются. И Slack тоже пользуется Slack, но, как они пишут, в особо плохие дни они используют Skype. Также во время координации, если это действительно очень какая-то большая проблема, которая затрагивает большое количество пользователей, либо сервисов каких-то, может создаваться так называемый варрум. Это, по сути, все созваниваются, люди, которые в этом участвуют, и в режиме видеоконференции пытаются всё это дело решить. Это сильно ускоряет работу, потому что очень сложно следить в Slack за ситуацией, когда очень много людей пишут в разных каналах. Это второй этап координации. Очень важно определиться, куда идти дальше, каких людей подключить. Третий этап – это, собственно, этап, в котором нам нужно убрать симптомы. То есть, у нас какая-то система поломалась, но мы не знаем, какая система, чтобы узнать, что это за система, узнать, что конкретно за проблема, это занимает некоторое время, поэтому сначала нам нужно это всё дело быстро починить, чтобы не аффектить пользователей. Ну, это чинить простыми методами, вроде мы откатываем что-то назад, мы перезагружаем какую-то систему, которая поломалась и так далее. Это очень важный этап, чтобы технари не искали причину, а сначала симптомы. Потом идёт следующий этап, четвёртый, когда мы устранили симптомы, мы постараемся починить систему и выясняем первую причину, из-за чего, собственно, всё произошло. Это очень сложный этап, он может занимать долгое время, например, это может быть сутки, недели и так далее. У него могут включаться различные люди, различные сервисы и так далее, потому что очень часто бывает, что какой-то один сервис вызвал проблему изначально, но он тут же починился, а у вас пошла как бы волна по другим сервисам, и вы не видите этот сервис изначально, потому что он маленький вклад внёс. Вот как эффект домино пошло, и вся система у вас разрушилась, всё очень, конечно, сложно, и тут такой forensic science включается, что люди пытаются найти, откуда же всё произошло. И, собственно, пятый этап тоже очень важный, это follow-up, Никий, это когда после того, как вы уже нашли первую причину, вам нужно понять, как предотвратить это в будущем, вам нужно понять, правильно ли был процесс построен по тому, как вы чинили эту проблему, и какие-то action items добавить, то есть, например, определить, что нужно сделать с дальнейшим сервисом, чтобы этой проблемой, если даже она и возникнет, то она не положит опять сервис, она как-то быстро починится и так далее. В общем, это тоже очень важный этап. Это была всего лишь первая статья. Вторая статья была о том, кто обычно является онколоном в компаниях. И в разных компаниях всё по-разному. Обычно это те же разработчики, которые пишут сервисы, они являются онколами этих сервисов, потому что обычно они как бы настраиваются дашборды, алёрты и так далее, и они знают, как починить эту систему. Иногда этим занимается отдельная команда, как, например, случай с Google, для его сервисов вроде Gmail, Accent и так далее, которые очень важны для бизнеса. Есть отдельная команда, которая занимается именно онколом для этих сервисов и следит за доступностью этих сервисов. Но часто получается некий микс, например, как у нас. У нас есть люди, которые дежурят, они обычно как бы разрабатывают сервисы. Например, как у меня. Я работаю в команде базы данных, но я не DBA, я про базы данных достаточно мало знаю. То есть какой-то такой уровень не очень высокий. Но я периодически тоже включаю онкол в базах данных, чтобы понять вообще, как это всё работает и какие инструменты, например, ребятам нужны, чтобы всё это чинить или следить и так далее. Есть ещё команды онкол, которые следят за всем сервисом в целом. То есть если сервис состоит из различных, не микросервисов, а просто проекты, состоит из различных сервисов, иногда бывает такая ситуация, что весь сервис в целом испытывает проблемы, но подсистемы не показывают, что у них какие-то есть проблемы. Соответственно, нужна команда, которая следит за всем сервисом в целом и пытается определить подсистемы, которые вызывают эти проблемы. Третья статья была о том, как организовать онколоротацию, потому что быть онколом – это очень сложно. То есть обычно люди, когда онкол, они ещё в том числе что-то делают, разрабатывают скрипты, и любая нотификация, любой аллёрт – это просто отрывает человека от реальности, выводит его из потока минут на 20, на час и так далее. Это всё очень проблематично. Плюс если аллёрт случается в течение ночи, есть у Гугла специальное исследование, что это вызывает какие-то проблемы с головой у людей. То есть это совершенно реально. Поэтому когда онколоротацию устраивают, стараются, например, разгружать людей во время ночных смен, например, следуя за зонами. Например, часть людей, которые живут в Америке, они работают по американскому времени, часть людей, которые живут в Европе – по европейскому времени. Во-вторых, должны быть некие KPI для онкол, чтобы понимать, улучшаем мы онкол или не улучшаем. Потому что не должно быть так, что люди только и занимаются онколом, они же должны ещё работать. И очень важно эта эмпатия среди команды, потому что все должны друг друга поддерживать, все должны входить в ситуацию друг друга, потому что кому-то нужно к врачу, кому-то нужно ребёнка встретить, и здесь нужно просто уважать друг друга. В общем, это очень достаточно сложный процесс онколоротации, есть всякие книжки про то, как это сделать. И пятая статья… нет, есть ещё четвёртая статья про то, что быть прозрачным – это круто, и там приводится пример Гитлаба, как бы смысла обсуждать его нет, потому что мы уже достаточно много про Гитлаб слышали, про то, как они чинили всё в режиме онлайн, но плюсы вот этой открытости в том, что если ваши клиенты – это какие-то разработчики, либо технари и так далее, и они могут понять, какая у вас проблема, и что вы делаете, когда чините, это вызывает у них некую эмпатию и отклик, и они как бы пытаются себя поставить на ваше место, вот это вам большой плюс идёт. Также иногда в случае с Гитлабом, когда они что-то чинили, к ним пришёл какой-то суперспециалист по постгрессу и помог им какие-то вещи зачинить, то есть всё тоже прикольно. Если сравнить это случаем, например, с того же Яху, когда у них угнали базу с пользователями, и мы об этом узнали через 3 года, по-моему, это ужасный случай непрозрачности, который никто практически уже не пользуется, наверное, Яху из-за таких вещей. И пятая статья достаточно крупная была о том, как начать онкол делать, разбиралась по этапам, начиная с очень маленькой команды, например, если команда до 100 человек, то вам нужно начать собирать метрики, логи собирать, нужно писать дашборды, настроить систему алёртов, нотификаций. В этот этап многие просто пропускают, что самое интересное, например, у меня коллега ходил на конференцию, я не помню, как она называлась, суть в том, что там различные доклады были про метрики и так далее, про логи, и он разговаривал с людьми, и они говорят, вот, да, здорово, надо настраивать, у нас практически ничего нет, надо настроить, мы будем делать машин ленинг, который это всё будет изучать. То есть у людей ещё даже нет какого-то сервиса, который вот эти метрики собирает, тегирует, объединяет и так далее, и они уже какой-то машин ленинг хотят подключать. Это как бы ни к чему. Вот, если у вас компания начинает уже расти, там больше 100 человек у вас, то вам уже нужно начинать настраивать ротацию онкол, вам уже нужно, очень сложно каждый лог отдельно просматривать, вам нужны какие-то агрегаторы для логов, вам нужно улучшать работу деплоя и рулбека, потому что чем больше у вас серверов, тем чаще вы будете эту операцию проводить. Также нужно начинать проводить дезастери-каверы тестирования, отключать какие-то системы и смотреть, что у вас всё работает нормально, потому что чем больше у вас компания, тем чаще эти системы будут отключаться. По мере роста от 1000 человек и до 10 000 у вас уже должна появиться некая ротация суперменов, это люди, которые знают архитектуру вашего сервиса как свои пять пальцев, они, возможно, с самого начала вашей компании, и когда случается реально катастрофическая проблема, вы должны подключать эту ротацию. А также нужно проводить различные игровые дни или хаус-дни, как они называются, в которые вы также берёте и тестируете вашу систему, увеличивая нагрузку, выключая какие-то сервисы и так далее. И нужно также для различных сервисов определять, кто ими владеет, потому что чем больше у вас систем появляется, тем сложнее следить, кто поддерживает какой сервис. Просто люди уходят иногда и сервисы остаются без поддержки, это очень плохо. В общем, чем больше дальше у вас компания, тем больше различных проблем у вас будет, например, уже появляются проблемы, как справляться с мультидатацентровостью, например, отказом одного из датацентров, как это решать, какие-то мастер-репликации у вас должны появляться или нет, как вы будете полностью поднимать, будете ли вы держать копию вашего сервиса в другом датацентре, либо это будет работать как-то иначе. Всё это тоже нужно будет решать. И, соответственно, ещё полная автоматизация, потому что количество людей не растёт одновременно с количеством серверов, оно растёт намного медленнее. Вот, вот такая достаточно интересная, это прям реально не статья в блоге, это такая достаточно интересная развертка в реальном каком-то журнале с различными диалогами, с интервью у людей из различных компаний. В общем, мне кажется, очень прикольная инициатива. Я не знаю, будут ли они печатать это в бумаге, было бы здорово. Я добавлю, на самом деле они очень интересный формат выбрали, то есть обычно компания подходит к этому как компания, которая говорит, вот у нас есть такая экспертиза и всё фактически.",
    "result": {
      "query": "Stripe Oncall blog summary"
    }
  }
]