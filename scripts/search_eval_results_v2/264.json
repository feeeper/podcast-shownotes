[
  {
    "segment_id": "99f05af6-e027-44d5-b013-4dc63ecebe99",
    "episode_id": "51c66b80-f91e-46f5-9282-dd50ac2a0872",
    "episode_number": 264,
    "segment_number": 5,
    "text": "У меня есть вопросы про то, расскажите про ваш workflow, потому что это процесс очень оперативный. Вы приходите с какой-то гипотезой, которую вам нужно проверить, то есть у вас какая-то гипотеза, вы проверяете модель, вы смотрите на то, на какие там метрики, которые показывают на целую качественную модель, и потом вы снова что-то подстраиваете, снова проходите этот цикл. Обычно хочется иметь какое-то версионирование для ваших итераций, и хочется иметь какую-то инфраструктуру, с помощью которой можно проверить эксперимент, и если эксперимент удачный, отправить его в production. Мне хочется узнать про этот pipeline, который у вас был, и как это у вас настроено. На самом деле мы используем локально развернутый GitLab. В этом плане смотрим на метрики, которые нас устраивают, подстраивают, соответственно, если получили лучшую метрику, то просто пишем. Наверное, логично ответить даже не на примере того, как с Snap'ом была история, потому что со Snap'ом мы в тот момент получили выдачу, алгоритм сошелся и сказал, что такое-то количество комьюнити мы полистали, смотрели даже не на метрики, а скорее руками. Дальше расскажем, как это происходило. Поковыряли, нас все устроило с нейронками, например. С нейронками, когда мы проверяем, например, какой-нибудь F1 или какой-нибудь accuracy, то в принципе у нас развернуты апи над моделью, соответственно, если появляется более удачный вариант, то на самом деле мы меняем, пушим новый блок с весами, и все это начинает заново работать. Так, я сейчас немножко, возможно, что-то прослушал, я немножко сломался. Мне казалось, что вы делали исследование, которое, не знаю, как связано нашему дорогим пользователю, а теперь появилось какое-то апи. Я просто говорил, как мы гоняем модели. На самом деле, модели мы просто стараемся гонять под API, внутри, под собственным, который разворачиваем, просто для того, чтобы не тащить много моделей в память, не дергать их много раз. Так как серваки не резиновые, и если мы используем какую-нибудь толстую модель, то я хотел сказать, что у нас есть, соответственно, API, через который мы дергаем эту модель через Python, есть возможность заменять эту модель, если какие-то метрики, за которыми мы мониторим, стали лучше, и мы понимаем, что мы хотим данную модель катить в прот, то замена весов происходит достаточно бодро через новый пуш. А расскажите, как вы трекаете версионирование этого? То есть, как вы трекаете версионирование моделей, и как вы можете сказать, что эта модель не сработала? Вот данные эксперимента, как учет происходит? У нас, на самом деле, в принципе, проектная работа, то есть я не могу сказать, что у нас много моделей, которые мы отлаживаем и боремся за какое-то большое улучшение метрики. Но если говорить, например, про какую-нибудь модель, которая улучшается, мы добиваемся улучшения пола или возраста, то выбираем какую-нибудь таргетинговую метрику, например, F1 score, и начинаем полномерно улучшать. Соответственно, мы улучшаем по-разному. Начинаем с перепроверки для дотасета, потому что частично не рост метрики связан с фиговой разметкой, например. Потом переходим к тестированию архитектур. На самом деле, у нас было несколько кейсов, связанных именно с возрастом, когда мы перебирали 3-4 архитектуры, пока нас не устроил результат. А есть ли какой-то процесс ревью моделей между Data Scientists? Наверное, такого процесса у нас нет, просто потому что у нас не очень большой отдел DS. И, в принципе, как я говорил, нас четверо. Таким образом, мы все знаем, когда что-то появляется новое, то каждый прикладывает обычно к этому руку. То есть, если бы у нас было хотя бы человек 6-7, то да, у нас были бы... Понятно, что ревью как таковой, наверное, есть, но еще на процессе написания и тестирования к коду в той или иной степени руку прикладывает каждый из этих четырех человек. Расскажи, что является вашей IDE-код для создания моделей. В чем вы пишете? Это ноутбуки или вы код пишете питоновый где-то и потом его ревьюете? Да, у нас есть Jupyter, в котором мы, собственно, ну, ноутбучки, в которых мы пишем. Обычно все это в какой-то момент плавно переезжает в саблайновские блокнотики, и потом они отчаливают на сервер. Дальше запускается это через различные арги в консоли, мы смотрим, что все работает, дергаем, соответственно, другими скриптами точно так же эти модельки. Мы знаем, что все хорошо работает и вроде ничего не сбоит, и дальше все это оборачивается в API. В API прокидываются, опять же, не знаю, несколько картинок в нейронку, смотрится, что все это работает, дальше стопится, и с этим мы отчаливаем на самом деле к ребятам, которые занимаются бэкэнгом. Вот, модель берется, на самом деле берется endpoint, и этот endpoint пристегивает к всему оставшемуся бэкэнгу. У меня на самом деле вопросов пока что больше нет. А вот сделали вы исследование, сделали вы модель, а кто дальше, то есть что дальше, что происходит с результатами того, что вы сделали? Ну, результаты отчаливают дальше к клиенту, либо к клиенту, может быть, внутреннее подразделение, которое хотело, например, получить какие-то инсайты, либо непосредственно к клиенту, который хотел, например, узнать больше о своем аккаунте в Инстаграме, о своих потребителях и покупателях. Мне кажется, еще можно добавить, что, например, у нас есть уже какие-то исследования, которые поставлены на поток, тогда они перерастают уже в какой-то внутренний тул. Да, то есть для внутренних пользователей, если запросов много, то это перерастает опять же в продукт, собственно, поэтому и присутствует несколько десятков программистов, которые собирают тулы, то есть начиная от планировщиков на нескольких медиа, окончая туш-тулами, к которым мы имеем большие отношения. То есть, например, как мы говорили в начале, есть тоже самый планировщик блогеров, который решает очень простую задачку, отвечает на вопрос, а стоит ли взять одну бузову, или собрать микс из 40 маленьких блогеров и получить тоже большую аудиторию, но там за деньги в два раза меньше, чем стоит оплатить",
    "result": {
      "query": "data science workflow best practices"
    }
  }
]