[
  {
    "segment_id": "f19550ba-340b-48a2-8190-8ad4bf4a1876",
    "episode_id": "7862d7a2-70aa-478c-867d-3f10039628b8",
    "episode_number": 177,
    "segment_number": 7,
    "text": "большому счёту и добавление новой. Ну, это так грубовато. И, соответственно, когда у нас такой процесс для всего вообще, это такой способ имплементации MVCC, когда у нас, ну, при коммитте транзакции просто добавленные строчки помечаются, вот они новые прекрасные добавленные, если у нас транзакция откатывается, то у нас наоборот, новые строчки помечаются как вот невалидные, а старые как вот прекрасные нормальные. И когда у нас такой процесс, у нас мусор случается в куче по любому поводу, то есть коммитим мы транзакции, не коммитим мы транзакции, с лерлбечем мы транзакции, коммитим транзакции, но всё равно мусор. И, соответственно, как бы мы не усовершенствовали наш вакуум, вот такой вот способ имплементации, который очень safe, он всё равно оставляет слишком много мусора, и да, типа вакуум в последние годы становится лучше и лучше, но всё равно, покуда база будет мусорить, нельзя бесконечно улучшить вакуум, поэтому они решили провести такой эксперимент своей командой, это примерно Enterprise DB, и сделать так, чтобы Postgres, потом тому, как делают, не знаю, тот же самый, если не ошибаюсь, MySQL так делает, они переносят старую версию в специально отдельное место, undo log, и дальше, если транзакция коммитится, они чаще всего коммитятся, у вас просто в куче всё нормально, всё же как надо. Если транзакция лерлбечится, то ну тогда нужно уже из undo log переносить обратно в кучу, то есть у вас тогда undo становится дороже, но тогда зато common case, транзакция, которая коммитится, он становится дешевле, потому что он не создаёт лишнего мусора в куче, он создаёт мусор там, где его можно выбросить за раз. Тут есть ещё одно интересное наблюдение, что при использовании undo log у тебя в heap всегда лежат, ну очень грубо говоря, твой текущий snapshot, твои актуальные данные. Если же ты пишешь в heap разные версии tuples, то мало того, что он разрастается, в нём становится там, ну не знаю, банально, конечно, засоряется, тебе приходится со старых версий tuples как-то списать ссылку на новую версию tuples. Это означает, что если ты, например, делал чтение и по индексу пришёл на старый tuples, тебе нужно пройти по цепочке на последнюю версию. Этот механизм называется hotchain. И получается такая странная ситуация, что в Postgres чтение, ну как бы запись дешёвая, а чтение дорогое, ну в смысле дороже, чем могло бы быть. Что на большинстве нагрузок не очевидно, что вы этого хотите. Вот, и там на самом деле ещё такой момент, что получается, что у вас в любом случае мусор, он не только плох, потому что мусор в heap старые версии, не только потому что ты сейчас сказал, но даже просто у вас банальный проход по таблице, он становится дороже, потому что вы ходите по мусору, мусор жрет ваш IOU. И вот если, не знаю, для LTP нагрузок это ещё терпимо, то как только у вас появляется, не знаю, хоть иногда запрос, которому реально нужно шарашиться по таблице, ему становится плоховато. И в принципе там дальше статья, она как бы, тут нет пока деталей, как они это имплементировать собрались, это просто такой наброс по большому счёту. И такой, эй, комьюнити, что вы думаете? Ну я примерно в первую минуту твоего рассказа так и понял, что это такой своего рода наброс, и это нормально. Подожди, спроверивай стерео. Они говорят, что у них есть имплементация, там ещё не до конца поддержка индексов есть, если не для всего, и это понятно, что сильно сложнее и менее safe, чем это было раньше, но они говорят, что у них есть уже рабочий proof of concept, что он у них быстрее, чем то, что сейчас он, например, в тех местах, где они ходят, что он быстрее. Да, что они потом скоро всё выложат, и да, самое главное им было получить фидбэк от комьюнити, что типа, они офигели ли вы нам наш прекрасный фосг испортить поганым андулогам. Ну ты, Саша, как представитель комьюнити, что думаешь? Я хотел закончить мысль, что это в комьюнити нормальная тема, таким образом набрасывать, то есть вот мой коллега Костя Книжник недавно сделал прототип фосгерса, переписанного на треды против поточной модели, и набрасывал в комьюнити, что там, смотрите, всё стало намного быстрее, и там больше одновременных соединений можно поддерживать, чем поддерживается сейчас и так далее. Но это, кстати, правда, то есть треды, они существенно дешевле процессов в современных, ну тех же линуксах. Что до HASS, я вот думаю на тему, есть ли тут какая-то корреляция с тем, что на самом деле Роберт, он сейчас заинтересован в механизме плагабл-сториджей для фосгерса. Вот, может ли это как-то коррелировать в том смысле, что возможно для каких-то сториджей невыгодно иметь вакуум, и хотелось бы иметь андулог, даже если не как механизм, вот, как бы дефолтно используемый для всех сториджей, то как механизм, который вот как бы сбоку подключается. Вот у них он, скорее всего, так и есть, я так понимаю, что судя по статье, он не заменяет полностью вакуум, пока мы в своей текущей ипостасе, он просто делает вакуум, как бы, вот именно что он меньше мусорит, короче, то есть типа мусор всё ещё может быть, но ему будет в разы меньше, потому что обычно он будет в той стопке, которая называется андулог. Если же говорить вот просто про ванильный позгрос и моё отношение к всему этому, я бы не стал вот именно выкидывать вакуум и вставлять на его место андулог, потому что здесь нет правильного решения, то есть ты как ни сделай, у тебя всё равно на какой-то нагрузке один из способов будет лучше, другой хуже. И я не убежал. Согласись, что нагрузка, когда тебе нужно часто роллбетчиться, она такая довольно специфичная, типа тебе нужно 50%, по меньшей мере 50% транзакций, должны роллбетчиться, чтобы тебе текущая реализация была выгодна. Ты думаешь про, это частая такая как бы ментальная ошибка, ты думаешь про задачи, с которыми часто работаешь, ты, скорее всего, про LTP, а теперь представь себе Lab, где у тебя долго живущие… Где Лаба полно. Вот, и представь себе транзакцию, в которой там у тебя тысячи и они очень долго живут и месяцами чего-то считают. И тебе совсем не выгодно вот в этих транзакциях ходить по undo log в данные, которые там 50 транзакций назад были, и ты этот undo log не можешь вычистить. То есть вот на этой нагрузке, например, undo log – это очень хреновая идея. А там, ну, ты понимаешь, там только они обе одинаково хреновые, я тебе скажу честно, они обе абсолютно хреновые, то есть там та же самая фигня начинается с вакуумом, потому что когда у тебя вакуум не может кетчап с тем, что ему нужно, больше всего у тебя то аналитический запрос все больше и больше, а он начинает фигачить. То есть они оба одинаково плохие, там нужно вообще что-то другое уже делать. Ну, может быть. Но посыл мой скорее был даже не про производительность на тех или иных нагрузках, а скорее про то, что не стоит, как мне кажется, тратить усилия сообщества вот прямо сейчас на решение этой конкретной проблемы, типа выкидывания вакуума и переписывания на undo. Не потому что это неприоритетная задача или что-то в этом роде, а потому что я, к сожалению, знаю, как работает сообщество Postgres, и это затянется лет на 5 минимум. То есть если бы моего мнения кто-то спрашивал, я бы, не знаю, что-то другое инвестировал в ресурсы сообщества. Там, например, на Комментфесте есть клевый патч про инклодинг индекса, то есть ты можешь в индексы включить данные, которые этот индекс не индексирует, но ты, когда делаешь индекс-скан, тебе не нужно ходить в heap, ты можешь твои данные взять из индекса. И это очень клевый патч, который переносится уже не первый год, и лучше уж его нормально заревьювить, нормально вмержить, и вот это вот все. И таких патчей много, это я просто пример привел. И это, как пользователь Postgres, я бы намного сильнее радовался, если бы мне приходили такие фичи, чем типа «О, ребята, мы тут в 2023 году выкинули к хренам вообще вагу, мы сделали вам undo log, правда в 2023 году железо поменялось так, что это уже никому особо не нужно, да и вакуумы уже потюнили». Ну то есть вообще пилить такие долгосрочные фичи я не считаю чем-то очень хорошим. Но надеюсь, я достаточно развернуто описал свое субъективное мнение. Саша, а можно вопрос от человека, который ничего не знает про Postgres? Ты упомянул идею добавления pluggable storage, это действительно подъемная идея в случае Postgres или просто кажется, что это просто невозможно сделать? Она абсолютно подъемная совершенно. Там проблема не технологического плана возникает, там сообщество не может договориться, какие задачи они хотят решить. То есть, например, хотим ли мы с помощью этого механизма мочь вкорячить в Postgres колоночный storage, да? То есть кто-то говорит, что вот это прямо единственное, что интересно, все остальное вообще было бы неинтересно. А если взять, например, Хааса, то ему наплевать на колоночный, на кевель, на LSM, ему интересно, ну судя по тому, как я это понял из обсуждения, это не обязательно совпадает с действительностью, ему интересно, вот как оно есть сейчас, то есть революционная база и так далее, но сделать ее немножко по-другому, то есть, например, сделать заголовки у тупов поменьше, сейчас они минимум 24 байта составляют, я напомню, это много, какие-то другие оптимизации, то есть вот с этим поиграться. И, соответственно, он предлагает, что давайте сделаем интерфейс, который примерно такой, как сейчас, и у нас есть тупл-айди, у него есть заголовок и так далее, а ему, человек, который думает про другой кейс, он говорит, что какой тупл-айди, если в моем сторидже может не быть туплов. И вот в таком ключе идет конструктивное обсуждение, не первый год уже, если что. Понятно, спасибо. Ну, я так подозреваю, что тема андолога себя что же исчерпала. Я думаю, да. Хорошо, ну тогда я попробую перейти к следующей теме, я, честно говоря, уже смутно помню, что мы тут хотели поговорить. Началось все с внутреннего доклада у нас в компании, вот те, которые я упоминал некоторое время назад, конференции и всему такому. Речь идет о том, что есть распределенные базы данных, которые позиционируются как отказоустойчивые, масштабируемые, троллевые петабайты данных. И вопрос, который иногда не озвучивается и про который забывают, это про то, как это бэкапить, а скорее про то, как это восстанавливать. То есть, концептуально, начнем с того, что даже сделать, если это транзакционные системы, и у тебя есть транзакции между нодами, то не такая уж там тривиально очевидная проблема, это сделать снэпшот всей твоей системы, вот прям снэпшот, и его вот куда-нибудь забэкапить. Во-первых, потому что сделать вот такой распределенный снэпшот всего это непросто и недешево. Во-вторых, потому что у тебя там твои 10 петабайт еще не очень понятно куда девать, на соседние диски, заливать в Амазон или еще куда-то. И куда больше проблема, это как это потом восстанавливать. То есть, опять же, вот у тебя пришел злобный хакер или был баг в очередной версии софта, он сделал delete from users, как бы все сломалось. И что ты будешь, как это восстанавливать, и чтобы у тебя при этом вся система была консистентной. То есть, тебе нужно по-хорошему выкачать 10 петабайт, как бы разложить их опять по нодам, пока оно раскладывается, пусть весь мир подождет. И вот мне хотелось бы, просто с Валерой мы чуть-чуть пообщались на эту тему, мне интересно, что об этом думает Ваня. Приходилось ли тебе работать с распределенной системой, претендующей на консистентность с транзакциями и так далее, и как вы ее бы капили? Я просто заранее знаю, что приходилось. Слушай, ну мы обошли... Всегда можно попытаться свести одну сложную проблему к двум более простым. У нас это получилось сделать, хотя я понимаю, что в целом это не всегда может удастся. То есть мы смогли, во-первых, разделить наши данные на не пересекающиеся однозначно, скажем, данные разных клиентов, которые не хотели видеть друг дружку, и разложить их по более маленьким базам. Этим мы решили вопрос одной большой базы данных, сделав несколько маленьких. Консистентность внутри одной базы данных мы уже могли поддерживать, а дальше мы просто делали вертикальный рост и оптимизации внутри базы данных. То есть попытка уменьшить индексы, попытка придумать другие альтернативные способы индексирования, ну какие-нибудь такие все штуки. То есть я отвечаю на твой вопрос по-другому. У нас не получалось... Не то, что не получалось, мы максимально пытались избежать задачи сделать большую распределённую консистентную базу данных, которая с нужным уровнем консистентности, наверное, так. Я ответил на твой вопрос? Да, и на самом деле я примерно такого же мнения придерживаюсь, что ни при каких условиях я не хочу выкачивать целиком 10 битабайт и полностью восстанавливать свой кластер, немножко притормозив его. То есть я, скорее всего, пожертвую немного консистентностью, но нарежу свой кластер на какие-то ноды, которые редко взаимодействуют. И в случае, если у меня что-то чуть-чуть разъедется, как-то это подопру либо в самом приложении, либо... ну хз, потому что это зависит от конкретного приложения. Ну да, да. То есть у нас были большие хранилища, у нас было, я не знаю, не помню сколько, 50 нод РИАК, которые держали все наши полностью данные, как key-value хранилища. Но это не было консистентное хранилище а-ля Postgres, где есть взаимные ссылки и так далее, это просто было какой-то key-value, в котором ошибками консистентности можно было с какой-то долей пренебречь. Ну то есть мы могли придумать какие-то правила, мы могли как-то поговорить с клиентами, если что-то разъехалось, внезапно мы это обнаружили. Были какие-то штуки, которые мы решали на коленке, какими-то скриптами, обходами и так далее. Ну это всё... ну то есть на мой взгляд, вот подобную сложную проблему нужно всегда решать тем, что разбивать её на более маленькие, которые мы знаем как решать, чем сделать какое-то одно большое правильное сферическое решение в вакууме. Я только что подумал про то, что вот эта проблема восстановления части данных с бэкапа, она как-то коррелирует с понятием eventual consistency, потому что я беру свой кластер, вливаю туда вот кучу не вполне консистентных данных с тем, что есть, и оно потом должно... Чего оно не вполне консистентное? Мы же это выясняли с тобой в личной беседе даже. Ну вот смотри, я взял одну ноду с 10% пользователей. Ну ты так не делаешь, так не делаются распределённые бэкапы в консистентных системах. Так бэкапится React. Мы только что с Ваней говорим о том, что мы готовы немного поскупиться консистентностью, поимев за это скорость восстановления. Ну понимаешь, я думаю, что нужно делать и такие другие бэкапы, на самом деле. То есть у тебя... Смотри, у тебя есть две истории. Одна история, у тебя как-то крупный аут, отчего нужно... Прости, я тогда свои мысли закончил, потому что она почти закончилась. То есть как я это вижу, мы берём 10% данных нашего кластера, выкидываем их, заливаем из бэкапа, и потом вся система в целом, она должна прийти к чему-то не обязательно строго консистентному, но в сумме к чему-то там правдоподобному. То есть что-то, что пользователь посмотрит и скажет, ну ок. Вот, что я хотел сказать, мы с тобой лично беседить обсуждали, что тут есть два юзкейса. В принципе, у тебя какой-то один пользователь, данные одного или группы пользователей запарились, или у тебя глобальный аут, или же глобальная потеря данных. Или там ещё третий вариант, у тебя, не знаю, конкретная машина вылетела. Ну конкретная машина вылетела, это вообще репликация даже решает. Там не нужны как-то бэкапы. Бэкапы нужны, когда нужно данные на какую-то точку назад вернуть. В принципе, мы даже если мы не планируем использовать бэкапы консистентно, мы всё равно их можем консистентно снимать. Для этого есть вот этот, как его там, лэмпорт, чанг, как я не помню правильную фамилию. Есть вот этот давнишний-давнишний стэпшот-алгоритм. В принципе, там, ну, мы можем до некоторой степени упрощать. Смысл в том, что мы всю систему на очень короткое время говорим, так, ты записи сейчас буферизируешь, а не пропагейдишь. Мы везде делаем на дисковой подсистеме снапшоты. Сейчас почти все они работают на LSM. На LSM это очень просто делается. Мы просто там отбиваем в нужном месте, что у нас вот тут. Данные, которые сейчас будут бэкапиться, мы их пока не компактизируем, не переписываем поверху. Дальше отпускаем вот это состояние. И при этом можно делать, например, хардлинки на файловой системе. И у нас получается такой снапшот а-ля ZFS. Я сейчас не очень на пальцах объясняю, но можно посмотреть статью, например, очень классная статья от создателей HyperDeX. Я думаю, я могу это проговорить, может быть, чуть-чуть, ну, не знаю, сказав по сути то же самое. То есть, что Валера предлагает, что мы условно в каждом сервере добавляем такой же диск или такой же рейд. Подожди, подожди, я даже пока не про это говорю. Я пока говорю чисто про механизм снятия снапшота, даже пока не без копирования данных. А ты упомянул хардлинки, ну окей. Ну, хардлинк у тебя не говорит о том, что тебе нужно непременно место увеличивать. Ты его сделал, у тебя просто место осталось сколько было. Просто когда у тебя система может потом заходить и этот файл удалить, но он у тебя все равно останется. Это другой вопрос. Но ты подразумеваешь, что файл больше никто не пишет после этого? LSM больше никто не пишет. Это же LSM. Но ты как-то очень резко ввел LSM в нашу конструктивную беседу. Я говорю, что у нас сейчас почти все эти распользованные стороны, что бы пока не все, которые есть там как ROG, HyperDex, почти все, что я вообще видел, глаза, они все там ROX или LevelDB. Ну и какая-то вариация на эту тему. Вот, в LSM вот такой снапшот делается просто тривиально, потому что у них старые левелы, они просто всегда редонли. Там у тебя запись происходит в лог, и из этого лога потом делаются молодые поколения, молодые поколения потом мержатся с более старыми, и так далее, и так далее, и так далее. Но все равно при мерже у тебя берутся два поколения, мержатся, пишется новое поколение, а старые два просто удаляются. Соответственно, у тебя в LSM никогда старый файл не будет перезаписываться. Поэтому если ты хочешь снять снапшот, тебе нужно просто с мочи отбить вот это место и в логе пометить, какие данные еще на тот момент уже были записаны, которые тоже должны в снапшот попасть. Все, у тебя, в общем-то, на этом снапшот заканчивается. Это очень быстрая процедура, чисто на одной машине. Соответственно, ты на всем машинам говоришь, так, ребята, встаньте на паузу, снимаешь физический снапшот, отпускаешь паузу. Дальше уже как бы, что делать со снапшотом, есть варианты. Можно, как у Саши сказал, просто вставить второй хард драйв в каждую систему, копируем туда, например, на ней держим типа daily backup, а еще как бы в принципе в каком-то холодном сторожах, они просто более длинную историю, степенно сливаются из локальных дисков. Дальше, если нужно восстановить данные в зависимости от кейса, мы можем либо поднять, как бы если это данные конкретного пользователя, мы можем поднять реплику именно с этими данными, не включенные в обычный кластер, выковырять их и перенести. Это одна история. Если это у нас, не знаю, какой-то глобальный аутедж, то у нас, скорее всего, система уже и так себя не может обслуживать. То есть она и так уже, если там настолько все плохо, что нужно все данные поднимать из бэкапа, вообще все машины, то у вас уже, скорее всего, сервис не работает, вам, скорее всего, нормально машины просто выключить, скопировать данные и поднять. Нет, подожди, я вполне могу рассказать, ну то есть я прям следую пример, который не обязательно все убивает. Я выкатил новую фичу с багом, но включена она на процент пользователей. И данные появились только у этого процента пользователей, не во всей системе. Или другая фича, у нас из кластера реака умерла десятая часть. Ну в смысле, 10% нод просто умерло. И на них были покорабчены каким-то образом данные, которые у нас есть в бэкапе. Это реальная история, нам надо было их восстанавливать, и баша предложила нам способ, с помощью которого нам надо было 3 недели их восстанавливать. Вот я, собственно, говорю про что? Про то, что ты всегда можешь поднять второй кластер рядом, переносить нужные данные. Ну если такой способ, как ты говоришь, да, это прям вообще печально. Там тогда, возможно, уже нужно жертву консистентности, это такая история, неординарная, прямо скажем. А вот если то, что говорит Саша, что мы выкатили включенную процент пользователей, мы теперь как можем откатываем, тут вполне себе работает. Мы поднимаем второй кластер, мы знаем, кто группа пользователей, мы точно знаем их айдишники, мы конкретные их данные пересаживаем. Это вполне себе выполнимая операция, и не какая-то черная магия. Вот то, что там были десятая часть машин, да, это печально, там уже, скорее всего, придется жертву консистентности в коем-то месте. Тут, наверное, стоит заметить, что было бы неплохо, если бы современные СУБТ, они все предлагали такие механизмы из коробки и держали в голове такие кейсы. То есть я не убежден, что в каком-нибудь, не знаю, каучбейзе, я могу вот прям легко понять, где мои 10% пользователей с их айдишниками. Кстати, по-моему, каучбейз же не претендует на глобальную консистентность, то есть каучбейз, если кейвейли только. Ну, то есть, смотри. Если я поверх на ключе перковатера транзакции накрутил, вполне будет консистентно. Это будут твои проблемы, может быть, система для этого не очень предназначена, согласись. То есть, как тот же самый, я помню, Reconsemble, для своего консистентности, то есть у Reaki появился последний характер Reconsemble для консистентного анализера. Я все-таки не согласен, что это будут мои проблемы, потому что они в своей документации, в своей литературе и во всем этом прямо говорят, если вы хотите распределенную транзакцию, делайте ее так. Это документированный способ сделать консистентно и распределенную на каучбейзе. То же самое касается Cassandra, кстати, и Mongo. Они все так говорят делать распределенные транзакции. Ну, как бы, это хорошо, что они так говорят, но я так понимаю, что они это все-таки не признаются, но их еще и база данных, понимаешь, согласись. Собственно, в чем отличие между Kakroch и тем, что ты сейчас назвал. Kakroch, они транзакции встроены в базу, у них и бекап правильный, встроены в базу, правда, только в Enterprise лицензии. А тут ребята, типа, ну, как так и делать. А ты вот уверен, что мне Kakroch даст ответ на вопрос, где мои 10% пользователей? Подожди, в смысле, где мои 10% пользователей? Но ты только что сказал, что это правильная база с правильными бекапами. Смотри, еще раз. Я догадываюсь, опять же, они не описывают, как именно их Enterprise бекап работает, но догадываюсь, что он правильный, иначе бы они его отдельно не продавали. Потому что у них есть не Enterprise бекап, который просто базу тебе сливает. Вот, чтобы из него твои 10% пользователей выдавать, я уже говорю, что ты поднимаешь еще один кластер с бекапа, и ты свои 10% пользователей должен знать. Ты же их как-то на основе какого-то критерия выделил, ты их в втором кластере на основе того же критерия можешь выделить. Я правильно понял, что мне уже нужно 3 кластера? Один хранит бекап, а второй восстанавливает его? Чтобы влить в первый, который я восстанавливаю? Нет, смотри, у тебя обычно нужен один кластер с данными, собственно, где база оперируется. Там тебе нужно куда-то отчислить бекап побыстро, вот просто, не знаю, чтобы снять снапшот, и, грубо говоря, просто нужно место под снапшот. И ты можешь, ты даже не обязан второй диск иметь, просто, что бы, какое-то достаточное место, чтобы держать снапшот хотя бы за вчерашний день. И этот снапшот ты сливаешь в какой-то cold storage, не обязан, это не кластер базовых данных, это сев, это с3, это что-то, какой-то cold storage, может быть, амазон, глейшер. И сколько ли эти 10 кб из амазона? Внутри амазона, я, честно говоря, не знаю, сколько занимает, но я думаю, что глейшер вполне может с такой не тупой фигней справиться. То есть они себе полезны. Ну, может, вопрос за сколько. Слушай, ну, как будто ты задаешь вопрос, на который не знает ответ, потому что я амазона не пользовался. Но у тебя в любом случае, как бы, компания решает задачу переноса данных в cold storage. Ну, вот, как бы, это делают. Особенность данных, особенность backup и снапшоты инкрементальной, я думаю, что это, в принципе, решаемая задача. Вот. То, что снапшоты инкрементальной не ускоряет моё восстановление, скорее наоборот. Вот, и ещё раз. У тебя, скорее всего, самый недавний backup лежит где-то рядышком. Ты на тех же машинах просто со снапшотом можешь развернуть второй кластер, дублирующий, на который продакшн нагрузка не сервится. Ты просто, у тебя на машинах уже какие-то файлы помечены снапшотом, ты там же поднимаешь кластер. Ну, я опять же не говорю про конкретное решение, я тебе говорю про решение, которое бы я придумывал. Поднимаешь второй кластер на этих же прямо машинах, тебе просто физически, ну, не физически, а логически нужно два инстанца на каждой машине держать. Это кластер read-only, ты на нём находишь данные, которые тебя интересуют, и уже какой-то логикой переложения перекладываешь данные. Ну, вот я почти уверен, что ни одна реальная база данных не поднимется на read-only файловой системе, тем более в которую в это время пишет другой инстанс. Я вот ещё раз почти уверен. Снапшот никто писать не может по определению снапшота. Поэтому твой инстанс, скорее всего, реальный инстанс, я как бы про реальную имплементацию, он скорее всего не сможет запуститься на таком. Ну, как бы да, это хороший вопрос, тут нужно, не знаю, думать, смотреть, как-то спрашивать вендоров баз данных, но в теории это существимо. Ну, собственно, вся тема про то, что в теории оно многосуществимо, но я очень не убеждён, что вот оно на практике много где есть. Я не слышал про успешные истории, что там типа мы использовали решение X, там как ручник, как ручник, всё что угодно, и мы, не знаю, выкатили фигню на 10% пользователей, а потом за 15 минут легко восстановились из бэкапа. Ну, смотри. То есть всегда восстановление из бэкапа – это боль такая. За 15 минут ты не восстановишься, но у нас немножко другая история была в УГИ, у нас, в принципе, там был туряк, у нас не было межпользовательских отношений, консистентности, но мы просто хранили бэкапы по профилям, и, в принципе, когда делаешь какой-то токовый кап, ты потом поднимаешь, у нас в истрии это хранилось, но ты из истрии потом идёшь, поднимаешь, у нас не то, чтобы 0,5 раза это случалось, там было неогромное количество пользователей, и, в принципе, это вполне себе делается, но это не 10-15 минут, но, и да, там, конечно, был не такой консистентный бэкап, но всё равно там смысл такой же. Поднимаешь второе место, где профиль есть, и ты из него протаскиваешь. Фактически ты предложил то, о чём мы с Ваней с самого начала говорили. Ну, в смысле, понятно, что пересаживание профилей из второго класса, оно всегда заведомо не суперконсистентно, просто ты это можешь сделать на уровне, ты это можешь сделать с какой-то бизнес-логикой, а не так, что я сейчас подниму маршрут. Понимаешь, в чём разница, да? Пересаживание выделенного профиля по каким-то бизнес-правилам, версус просто поднять одну машину из 10, из вчерашнего бэкапа. Количество того, что может пойти не так, оно очень разное. То есть, грубо говоря, поднимая целиком всю машину из вчерашнего бэкапа, ты рискуешь, даже так, поднимая машину вчерашнего бэкапа, ты просто ничего не добиваешься, потому что, так сказать, у тебя есть реплики, которые просто признают вчерашний и просто на него надаливают новые данные. Поднимая, не знаю, все три машины, на которых конкретный пользователь лежит из вчерашнего бэкапа, ты, скорее всего, делаешь так, что ты каким-то другим пользователем можешь данные откатить. То есть, тебе в любом случае нужно прямо пересаживать данные точечно, скорее всего, до тех пользователей, которые у тебя поломались. Если у тебя именно вопрос того, что у тебя плохая версия приложения, что-то кому-то запороло. Мы в этом контексте забыли про существование отложенной репликации, которая в контексте этих задач может не повредить. Ну, то есть, просто имеешь реплику, которая отстает на сутки. Ну, так сказать, хороший вариант. Собственно, где снапшоты, то же самое дают в этом плане. Просто репликация, отстающая на сутки, там есть интересные последствия, потому что нужно логику долго держать. Ну, давай сойдемся на том, что снапшоты файловой системы в этом плане ничем не лучше. Спорно, потому что у тебя LSM, у тебя там поколение может просто... То есть, если поколение какое-то старое, но его компакт может не трогать очень долго, и тоже на деньок полежит, место на диске у тебя сильно не увеличится. А вот хранение лога, типа на целые сутки, который он мог бы давно уже уехать, непонятно. Я решительно не вижу разницы, что хранить просто старый снапшот или лог. Еще раз, снапшот в случае LSM, ты просто говоришь про какой-то файл, который уже и так был, что он все еще нужен. И стафайл второго поколения, он не меняет, ну, как бы они в принципе не меняются, но они еще при этом чаще всего и не компактизируются очень долгое время, потому что они так же довольно старые, там даже какие-то данные, какие-то пользы пекли же, которые давно не заходили. Новое поколение, как бы они все равно будут писаться отдельно, и они все равно будут писаться, пока компакт не придет. И соответственно, если старое поколение, большую старое поколение попадает в снапшот, оно тебя не увеличивает вместо занимаемой на диске. У тебя просто еще одна ссылка появляется на этот файл, на файловой системе. Я уверен, я понимаю, что ты говоришь про случай, когда база данных использует LSM 3, и случай, когда плюс к этому она еще должна хранить долгий лог. Ну да, да. В этом случае, да нет, в принципе, такой снапшот, построенный на LSM, с логом, он заведомо экономнее, чем хранить лог в любой системе LSM или не LSM. Вот так я скорее хочу сказать. А что происходит в случае, если у меня какая-то не очень активная нода, и в ней не происходит изменений в течение недели? Ты имеешь в виду с логом? Почему тогда у тебя эта нода вообще есть? Я бы тебе такое опрозадал. Почему бы ей не быть? Потому что это специальная нода для забаненных пользователей, которых я подозреваю в спаме, например. Ну подожди, как это современные, вот эти консистентные... То есть опять же, я все-таки говорю не про абстрактную распределенную базу вакуум, я про... пытаюсь прикидывать более-менее на существующее решение, типа Кокровича, типа там еще чего-то, Hyperdex с их варпом поверху. То есть Hyperdex у них есть транзакция глобальная, называется варп, а не платная. Тогда давай вспомним, что в ресинке используется B3, и не будем говорить, что во всех реальных системах только... В ресинке нет распределенных транзакций. Совсем? По-моему, их там не было. Я боюсь соврать, но последний раз, когда я смотрел, их там не было. Там были и по ключу транзакции, там можно было накрутить внутри одной машины, а вот между машинами там была такая же история, если я не ошибаюсь, простите меня, слава, но мне кажется, там распределенных транзакций не было тоже. А если я поднял 5 постграссов, использую 2PC, да, 2PC плохо, но это считается распределенной транзакцией? Ну, смотри, мы сейчас пытаемся говорить про теоретически правильные системы, а 2PC, это считать теоретически правильные системы или это такая практическая система уже? Ну, она вполне себе правильная, если ты как-то делаешь фейловер транзакционного менеджера. Окей, ну на самом деле, опять же, в любом случае применимый вопрос, зачем тебе отдельная специальная нода, а не просто таблица для таких вот странных пользователей, почему ты их на отдельной ноде держишь? Я такого в жизни нигде не видел. Таблица же где-то живет, ну на какой-то ноде она должна жить. Скорее всего, у тебя на той ноде какая-то еще активность происходит, и у тебя, поскольку это все-таки довольно релиционная база, если я правильно помню, там общий лог на базу данных, а не на таблицу. Ну ладно, отойдем от этого примера. Пытаюсь понять, то есть с одной стороны тема совершенно неисчерпаемая, а с другой пытаюсь понять, исчерпала ли она себя. Мне было бы интересно послушать, в точку зрения Ромы, так скажем, людей, которые работают с базами, которые там объемы данных огромные, с другой стороны, там нет такого объема обновлений, как я понимаю, там еще вообще как. Я попробую тогда прям конкретный вопрос сформулировать. У меня есть 10 петабайт данных, в одну машину не влезло, ну да, не влезло, и я сделал распределенную систему, и пытаюсь ее делать относительно консистентной. Как мне это бкапси восстанавливать на практике? Ну вот сегодня, в 2018 году. Нет, я, к сожалению, по этой теме ничего вам не смогу сказать, потому что, если говорить о той системе, с которой я работал, это Druid, это на самом деле не совсем база данных, а это просто такой некий Big Data движок, ну то есть сравнение тому же, по сути дела,",
    "result": {
      "query": "distributed database backup strategies"
    }
  }
]