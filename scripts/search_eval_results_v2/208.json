[
  {
    "segment_id": "12b0259a-2aec-4f6b-b64c-5618d8e1aff5",
    "episode_id": "71da7eff-a496-40a9-b026-e2b6bae5e83e",
    "episode_number": 208,
    "segment_number": 5,
    "text": "Все это вообще довольно тяжело. Вот. И, соответственно, в Improbable компании они решили придумать что-нибудь новенькое свое. Я думаю, что это появилось не мгновенное решение, они постепенно развивали все это дело. И даже есть отдельный документ про тому, как мигрировать с Прометеуса на Thanos, и я так понимаю, что они по этому же пути развития строили. Итак, Thanos — это некий набор сервисов, который будет крутиться рядышком с Прометеусом, и каждый сервис из этого набора служит для какой-то отдельной цели. Я сейчас буду их по очереди перечислять на страничке, где все это подробно рассказывается. Там есть отличные рисунки, которые поясняют структуру, и взгляд на них становится намного понятнее. Я не знаю, как у меня получится все это голосом показать. Во-первых, это Prometheus Sidecar, это маленький сервис, который работает рядышком, можно сказать, внутри Прометеуса, и он служит для того, чтобы фактически управлять текущей реализацией Прометеуса, и все запросы всех остальных сервисов идут не напрямую к Прометеусу, а через этот Sidecar. По сути, это просто как маленький прокси, который дополнительно служит для управления потоком данных. Второе – это сервис, который управляет запросами. Он сделает запросы из всех Sidecars, потом запускает PromQL запрос для всех данных, полученных, и, соответственно, выдает его наружу. То есть, когда вы работаете с Прометеусом, вы работаете с одним инстансом Прометеуса, когда вы работаете с Thanos, вы работаете через Query со всеми инстансами Прометеуса, которые у вас внизу. При этом этот Sidecar, который работает на Прометеусе, он фактически насквозь проксирует, а тот сервис, который работает с запросами, он может понимать, вы работаете с одним Прометеусом или у вас там два в паре стоят, High Available паре, и фактически нужно делать какую-то дедубликацию, это все автоматически делается за вас, вам париться об этом не нужно. Третий сервис, который здесь в комплекте, это Object Storage, который сохраняет исторические данные где-то в клауде. Они говорят, что чаще всего все эти Прометеусы всегда работают в клауде, там Amazon, Google, кто угодно, и всегда есть какие-то большие блок-сториджи, которые позволяют очень дешево хранить данные, нет смысла хранить это на SSD или дисках на каждом инстансе, гораздо лучше запихнуть их туда. Этот Object Storage это умный сервис, который умеет правильно работать с данными, он понимает формат данных, он хранит только immutable данные, то есть вы можете постоянно дописывать из сайдкара, который работает в Прометеусе, все данные сразу в Object Storage напрямую, поэтому двойная запись там не помешает, это все будет нормально. И вообще все сервисы, которые здесь работают в Таносе, они работают через госпротокол, поэтому они видят друг дружку, они понимают, кто это там находится по этому адресу, и соответственно Object Storage, он тоже воспринимается как объект системы, и с ним все работают, как будто бы это сайдкар Прометеуса. То есть когда сервис, который делает запросы, он посылает всем сайдкарам запрос на данные, он посылает их не только сайдкарам, он посылает их Object Storage. Вы это все не видите, как бы это все делается автоматически, потом дедубликатор вытаскивает данные, проверяет, что где есть, чего где нет, что здесь надо, что не надо, и получается такая прозрачная система, довольно удобная. То есть в данном случае мы обходим проблему с большим объемом данных, потому что Прометеус хранит только последние куски, которые нам нужны, все старые объемы данных хранятся в Object Storage, ну скажем какой-нибудь S3, который здесь рядышком висит, вот, и все в шоколаде. Вот, дальше есть еще парочка сервисов, которые тоже важны. Все файлы очень большие, то есть особенно когда на больших исторических данных вы должны хранить эти громадные файлы, и если вы будете напрямую спрашивать при запросах, вы умрете все это скачивать, и поэтому есть дополнительные сервис, который называется Store Gateway, и он кэширует индексы у этих файлов, понимает, что находится внутри, он отлично работает с примитивным типом данных, и он умеет разбирать его, и, соответственно, когда вы записываете что-то в Object Storage, он понимает, что вы записали, для какого типа данных вы записали, для какого time interval это вы записали, и знает, где это искать. Соответственно, при запросе данных к Object Storage он уже понимает, где нужно искать эти данные, и какую часть этих файлов нужно вытаскивать. Он минимизирует таким образом запросы к Storage, и по заверениям разработчиков, разница по сравнению с наивной имплементацией, когда вы просто вытаскиваете все, доходит до 4-6 порядков. Ну, 6 порядков это дофига, то есть в миллион раз фактически количество запросов и количество объем пересылаемых данных, они уменьшают, это прям неплохо. По заверениям, опять же, разработчиков, использование подобного гейтвея позволяет минимизировать время ответов от Object Storage до того уровня, что вы не различаете, это ответ пришел от большого Object Storage с подобным гейтвеем внутри, или от локального SSD диска. То есть, если это на самом деле так, то это просто офигенно. Конечно же есть компактор дополнительный, это как в Prometheus, он производит даунсэмплинг постоянно внутри Object Storage. Есть дополнительный рулер, который производит вычисление правил и алертов, это как в Prometheus, для того, чтобы ускорять запросы и алерты. И, пожалуй, все. То есть сервисов довольно много, 2, 3, 4, 5, 6, да, 6 штук получается. Они все работают рядышком с Prometheus, но зато они позволяют вам сделать большую структуру, все данные будут храниться где-то у вас во внешнем хранилище, не внутри, не на SSD. То есть, они ускоряют доступ, они удешевляют использование, потому что вы должны не на дисках хранить, а где-то в блоковом хранилище. И теоретически, все очень легко и просто устанавливается, у них есть замечательная инструкция по установке, вплоть до того, что там надо реально 5 команд вбить, и оно все заработает. Мы пока в продакшен это не использовали, но все идет к тому, что, наверное, будем. Как начнем, я расскажу интересные какие-нибудь моменты. Пока что все. Друзья, у кого-то еще есть опыт работы с Prometheus, или чем-то таким? У меня есть. Продолжай. У меня есть опыт работы с Prometheus, он не закончил. Ну подожди, ну каково твое впечатление, какие задачи ты пытался решить, с какими сложностями ты столкнулся, нам все это ужасно интересно. В смысле, я работаю в компании, которая делает свой продукт на основе Prometheus, поэтому задача стояла зарабатывать деньги при помощи Prometheus. Вы его делаете как коробочное решение для клиентов. Ну да, то есть наше решение PMM, Percona Monitoring Management, это Prometheus, Grafana, свой софт, все это в коробке, набор экспортеров, и клиенты его ставят себе. Ну а на open source ты можешь тоже скачать себе поставить. Я вообще про Prometheus делал доклад на Лите, как раз в мае был, и в том числе касался и темы Remote Storage, и Thanos, и там видео где-то наверно есть, можно посмотреть. А что еще имеет смысл посмотреть, это на Moscow Maker Faire, и я не уверен, как правильно назвать это, конференция или митап, или хакатон, что это.",
    "result": {
      "query": "Prometheus Thanos architecture overview"
    }
  }
]