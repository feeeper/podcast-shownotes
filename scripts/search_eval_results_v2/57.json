[
  {
    "segment_id": "d67aeec8-6241-4ef7-b6fc-ae62e4a110dd",
    "episode_id": "b92128cb-66c5-4406-aa6c-5f442fc0c5da",
    "episode_number": 57,
    "segment_number": 2,
    "text": "Вот, я, наконец, сделал и прочитал пейпер про Corfu. Что такое Corfu? В общем, есть такой остров, в греческом, называется Паксос. Ну, и такое слово в контексте распредельных систем, возможно, вы тоже слышали. Вот, и есть с недавних пор Corfu. Это продукт старания Microsoft Research. Это не совсем про такой консенсус, хотя на нем и консенсус тоже можно сделать. А что это такое? Это распределенный шаредлок. В общем-то, это как кавка, но если бы ее не нужно было портиционировать. Вот, то есть, собственно, что привлекло моего внимания к пейперу, это то, что по утверждениям, ну, мы имеем несколько машин, или даже, поскольку вообще Corfu расшифровывается как Cluster of Raw Flash Units, то бишь, на самом деле там даже нельзя для машины, потому что одна из реализаций в пейпере утверждается, что они сделали одну из реализаций на FPGA с SSD. Вот, собственно, в чем идея? Вот у нас есть несколько вот таких вот умных носителей серверов, или чего там еще, и у нас по ним размазывается шаредлок, при том лок не портиционированный, но при том он масштабируется. Чем больше вот таких вот единиц масштабирования мы добавляем, тем у нас, типа, высшая пропускная способность. Вот. Выглядит довольно так, ну, необычно, потому что автоматически срабатывает рефлекс. Вот как так, если не портиционируется лок по каким-то ID-шникам, то как он при этом может быть таким более-менее горизонтальным масштабируемым? Да-да-да. Ну вот, оказывается, может. Собственно, об этом пейпер. В этом плане они, как так скажем, доставляют. Ну, там еще одна вещь привлекла мое внимание в этом пейпере. Они в абстракте говорят, что вот они поверх этого лога запилили геораспределенную базу данных. И вот в это я уже не очень поверил. И правильно не поверил, потому что сразу проспалилю. У них в конце есть секция applications, и они говорят, что, ну, значит, короче, мы можем сделать NoSQL базульку, вот у нас даже есть такая прототипная реализация ее, она может в мультике транзакции, вот это все. А для геораспределения давайте просто возьмем и сделаем slave кластер геораспределенный, который будет просто копию лога. Ну, то есть, поскольку у нас это лог, то сделать копию лога не проблема. Это элементарная задача. Просто как бы реплицируем на другой такой же кластер. Ну, то есть, она как бы геораспределенная, но толку вот этой геораспределенности ноль, потому что вот в другом георегионе просто slave, грубо говоря. Писать мы туда не можем. Зачастую цель геораспределения в том, чтобы каждому пользователю было поближе вот что-нибудь. А так, получается, мы с того георегиона только читать можем, а писать всегда нужно в один георегион. Это довольно не то, что они обещают в абстакте. В этом плане… Тебя не обязательно писать в один регион, потому что ничто не мешает поднять мастеру в одном регионе, мастера в другом, и реплики этих мастеров в двух регионах. Другой вопрос, что тебе нужно как бы шардить все это вручную, но мне кажется, в масштабах регион… Так нет, понимаешь, понимаешь, Саш, смотри, идея Carrefour в том, что у тебя нет шардирования, ну в смысле между айдишниками, и поэтому у тебя возможны молотяки транзакции. Как только ты делаешь то, что ты говоришь, у тебя молотяки транзакции с лавками красивые такие, вот сразу ломаются. Но ты хочешь сказать, что она просто не лучше каких-то других решений? Нет, она в смысле лучше в одном регионе будет, потому что в одном регионе она будет, ну, как минимум, довольно просто, то есть если ты умудрился реализовать Carrefour, то потом довольно просто поверх него положить уже почти любую базу данных в одном регионе, она будет обладать свойствами, которые получше многих существующих базулин. Вот, ну, по крайней мере так утверждается. И ход-отип вроде как более-менее неплохо выглядит. Вопрос. Я либо залип, когда ты объяснял, либо ты не объяснял. Расскажи, как же они так реплицируют лог, что он еще и масштабируется. Я вот сейчас до этого как раз дойду. Собственно, вот да. Во-первых, Carrefour, он полностью рулится с клиента, то есть, ну, вот там сами сервера это в экстремальном случае просто флеш-диски с некоторой дополнительной логикой. То есть там определяется протокол для вот этого носителя, в котором 4 что ли или 5 команд, там по-моему 3 основных, а остальное там garbage collection и прочая такая фигня. Вот. И идея в том, что мы берем, разбиваем заранее, то есть, ну, допустим, у нас пока меняется конфигурация, они смену конфигурации тоже потом описывают, правда, кстати, не очень хорошо, я про это еще поддельно скажу. Но представим для начала, что у нас просто какая-то статичная конфигурация машины. И в этой конфигурации машин у нас имеется какое-то масштабирование, например, там, не знаю, диски, например, парами обслуживают, да, то есть мы, например, берем, бьем лог на, ну, весь лог целиком берем, бьем на кусочки, ну то есть присваиваем каждую позицию какой-то номер, бьем на кусочки, и дальше говорим, что у нас такой-то кусочек этой парой обслуживается, нет, точнее вот этой группой обслуживается такой-то кусочек, этой группой обслуживается. А внутри группы у нас внутри кусочка, у нас как бы round-robin между носителями. Плюс у каждого носителя за ним еще может быть цепочка реплик. Вот. И так получается, что у тебя соседние позиции, они оказываются на, там, round-robin, раскиданы по двум, трём, четырем разным группам реплик. И поскольку ты в одну позицию можешь писать только для одной позиции лога, то получается, что у тебя шардирование на самом деле происходит просто оно по времени. Я правильно понял, что по сути у тебя диски образуют кольцо? Нет, не обязательно кольцо, там хитрее топология, но в рамках одного рейнджа адресов, да, кольцо. Так, и ты каждую новую запись в лог пишешь в следующий элемент этого кольца, круто говоря. Ну, там, с рису-попаровкой на репликацию, как я понял. Ну да, да. То есть реплики, они как бы не в кольцах, они за каждым узлом как бы еще тянется цепочка реплик, сколько тебе их нужно. А правильно ли я понимаю, что пропускная способность этого кластера, она равна пропускной способности одного диска? Вот нет, ну, то есть дело, что нет. Так начинает казаться, но на самом деле, если ты так вот подумаешь, то на самом деле у тебя каждая новая запись в следующий диск. И при этом в параллельную большую количество можешь. Да, да, да. Нет, это я все понимаю, но у тебя тогда получается на чтение будут большие проблемы. Нет. То есть тебе нужно свести со всех дисков, прочитать на всех дисках. У тебя карта заранее готова. Ты знаешь заранее по оффсету, который ты хочешь прочитать, на какой диск тебе за ним идти. И ты можешь раздать асинхронно им задачи, типа сведите мне в эту точку, а я дальше там просто прочитаю, что у вас получилось. Ну, в смысле, зачем сведить? У тебя у каждого клиента есть карта полностью для текущего момента времени. Оставим пока вопрос реконфигурации восстранения, а для текущего момента времени у нас есть карта, ты знаешь, на каком оффсете ты остановился Ты знаешь, на какой диск пройти, чтоб прочитать этот оффсет. Ты прочитал оффсет, сделаешь с ним что-то, пошел на следующий оффсет. Опять-таки, ты знаешь точно, где искать следующий оффсет. Тебе даже никем координироваться не надо, чтобы его узнать. Ты его полностью локально знаешь, ты просто можешь если сказать по карте. Вопрос. Вот насчет как раз координировать не надо. Если у меня, допустим, у нас есть запечатанная конфигурация, есть бэкэнд, у него несколько экземпляров, работают одновременно, да? То есть как бы две копии одного приложения. Им нужно в этот твой лог писать логи. Как они поймут, по какой позиции писать? Вот да, это как раз то, с чем я собирался дальше поговорить. Ну то есть там есть на самом деле 2 момента. Сам алгоритм, то есть в чистом виде протокол, он предполагает, что ты в каждую ячейку лога можешь записать не больше одного раза. Если что-то уже есть, у тебя вторая запись просто обломается, тебе нужно пытаться писать на последующей позиции. Понятно, что если это делать наивно, то у тебя просто вот как раз вот случится вот то самое, когда ты говоришь, что у тебя пропускная способность равна, ну, типа одному диску, потому что все будут ломиться вначале в один диск хором, узнавать, что там кто-то один добежал, ломиться в следующий хором и так далее. Поэтому на самом деле там вводится такая сущность, как сиквенсер. Это просто нода, возможно, там несколько нод, которые там лидеры между собой выбирают. И она не делает ничего, кроме того, что генерирует просто целые числа автоматно. Ну, соль. Даже не соль, они даже не идти подряд. Она генерирует просто ковсеты. Она раздает по одному элементу лога или пачками? По одному, по одному. Но все упираются в эту ноду? Все упираются в эту ноду, но в измерениях они утверждают, что у них, типа, им пришлось 32 ноды поставить и забить полностью пропускную способность на этих нодах, прежде чем у них оно уперлось в сиквенсер, который был реализован в юзерспейсе довольно примитивно. Вот, типа, что, во-первых, можно сиквенсер разогнать еще 10 раз. Во-вторых, его можно тоже как-то умно пошардировать, например, не знаю, четный у тебя раздает один сиквенсер, нечетный другой и так далее. То есть, короче, это решаемый вопрос, и в крайнем случае ты можешь иметь вообще хардварный сиквенсер, который будет сколько угодно быстро работать по сравнению с твоей сетью. Хорошо, когда этот сиквенсер или один из нескольких пошардиных сиквенсеров падает. Ну, во-первых, как мы сказали выше, у тебя сам протокол не полагается на сиквенсер, сиквенсер — это оптимизация. Мы только что выяснили, что… А, ты хочешь сказать, что можно писать очень медленно, но… Да, да, это раз. Во-вторых, у тебя может быть просто перевыбор сиквенсера. Ну, как бы я идею в целом понял, и я так чувствую, тут очень много нюансов с самореализацией. Да, да, да, там очень много нюансов с самореализацией, и на самом деле там довольно большой пейпер. То есть, ну, мы, понятное дело, сейчас так по верхам пробежались. Что мне в пейпере не понравилось? Это то, как описана переконфигурация. То есть, переконфигурация самого карфу описана, но дальше сказано, что, ну, короче, нам для того, чтобы эту переконфигурацию согласовать между клиентами и сиквенсерами, короче, нам нужен какой-то консенсус где-то. Ну, мы его как-то вот сделали, короче, но вот даже при том, мы его не внешне сделали, мы его как-то на наших дисках сделали. Ну, как нам описывать лень? Ну, короче, какой-то консенсус у нас там. Вот это второй пейпер, в котором я такое читаю, при том, в первом это Paxos Made Live, Google сделанный, и они там специально секцию про реконфигурацию вот так написали, и там есть такая тролличная фраза, типа что, к сожалению, это не очень хорошо описано в литературе, это большая проблема, индустрию надо это решать, мы вот это решили, но это слишком, короче, неинтересно и тривиально, поэтому мы описывать не будем. Вот там это было как откровенный троллинг выглядело, здесь троллингом не выглядит, потому что действительно примерно понятно, что они имеют в виду, но как бы если у них есть живая готовая реализация рабочая, почему бы не описать точно, что они сделали? Вот меня это просто выморозило. Мне кажется, нужно было написать немножко по-другому, из-за разряда, что нахождение консенсуса вот в этом месте мы оставляем читателям в качестве задания. Ну типа там. Да-да-да. Ну вот у Google примерно так и написано, там это выглядит откровенным троллингом. Слушайте, а для чего эту штуку можно применять? Для чего они предлагают ее применять? Ну в смысле, ты на этом можешь, во-первых, есть прямо от них же, от самих авторов Carrefour есть такая система, называется Tango, это типа распределенные, реплицированные, произвольные структуры данных. То есть у тебя как бы есть приложение, реплицированное, у тебя у каждого экземпляра приложения есть какая-то структура данных, например, какое-нибудь большое дерево, и вот он через этот лог, все операции через него всегда проходят. Такая система называется Tango, тоже интересный пепель, рекомендую почитать. Вот я как раз про это хотел примерно спросить. То есть мне понятно, как происходит дозапись туда, но как происходит, скажем, редактирование, то есть у тебя write-only, когда структуры, у тебя фактически всегда должно быть как это, суммарное прочтение всего, а потом объединение для того, чтобы понять, что... Ну, в смысле у тебя строго порядочный лог. У тебя каждый писатель пишет в конец всегда, ну и каждый читатель, то есть он же может быть тем же самым, кто и писатель, всегда вычитывает до самого конца. Да-да-да, то есть если у тебя произошло 100 удалений одного элемента и одновременно 100 добавлений этого элемента друг за другом, то тебе надо прочитать. Нет, ну смотри, ты можешь так симантику организовать, например, что у тебя ты на элемент всегда вешаешь какой-то порядковый номер, дальше с тебя в лог пришли две операции, удалить ветку с номером, удалить ветку с версии 1, то вторая команда, она потом просто не выполнится. Я имею в виду, удалить единичку, добавить, добавить единичку, удалить единичку, добавить двойку, удалить двойку, добавить тройку, удалить тройку, и в итоге у тебя 200 операций, хотя в результате у тебя не изменилась сущность. А, ну как бы нет, ну смотри, во-первых, ты, там на самом деле предусмотрен лог компакшн, конечно же, то есть когда у тебя все клиенты дочитали, ты, конечно же, можешь освобождать старые данные. Когда все клиенты дочитали. Грубо говоря, у тебя клиент, он предполагает, что он читает и делает какой-то локальный снапшот на основе того, что он вычитал. Ага. Вот, и у тебя клиент может сообщать системе, ну как бы он не обязан, но может, говорит, что я вот после этого момента мне ничего же больше не надо. Если у тебя все сказали, что у меня вот в этот момент ничего больше не надо, то ты можешь взять просто и выкинуть старые данные, тем самым освободить место. То есть ты, короче, не обязан накопливать вообще всю историю за все время. Ну то есть я правильно понимаю, что сделать что-то не eventual consistent поверх этого... В смысле, eventual consistent? Не, не eventual consistent. Я имею в виду, ты можешь сделать поверх этого compare-and-swap, например, какого-то элемента в какой-то базе? Конечно можешь. Как? Ну в смысле, как раз эта штука для строго консистентных систем. Я понимаю, что она для строго консистентных. Как сделать compare-and-swap, я же тебе только что вот говорю, что вот есть, например, у тебя, смотри, у тебя каждый клиент читает лог просто как вот операции. У тебя лог операции, представь, что ты же те же самые операции, просто себя не знаешь, они тебе приходят, например, по хттп. Представил, ага. Если тебе пришли две операции, там, не знаю, compare-and-swap, там, не знаю, ключ A со значением 1 и версии 1 заменит, там, типа назначение 2, и вторая такая же операция, там, назначение 3, но с той же версией. У тебя только какая-то одна из них исполнится. Тебе не важно, они у тебя в логе или не в логе. Это понятно. А вот если я, представь себе, я выполняю вот сейчас эту операцию, да? Угу. Как мне понять, она успешна или нет? Ну, блин, как бы спросить у того, кому ты ее применяешь. Ну, то есть, смотри, предполагается, что, ну, работает это так. Ты, по крайней мере, как я понял, ты идешь как совсем клиент системы, не клиент кайфу, а клиент системы в целом. Ты идешь в какой-то клиент кайфу. Ты ему говоришь, например, я хочу записать там то-то, то-то. Он берет и идет в кайфу, просто плетет команду в конец лога. Дальше он сидит и ждет, пока он сам эту же команду прочитает, потому что когда он ее записал, он знает, с кем обстоят там его записал. И дальше он сидит, читает из лога и ждет, пока он на свою же команду наткнется. Дальше он по дороге применяет все, что было между ними. Когда он потом натыкается на эту же команду, он ее применяет и смотрит, она успешно применилась или не очень. Если успешно, отвечаешь, что успешно. Если не успешно, отвечаешь, что не успешно. Я понимаю, но ты согласишься с тем, что это довольно дофига неоптимальный алгоритм? Потому что пока я делал свой комперенс веб, там влетело, не знаю, миллион других записей в этот лог. Ну, во-первых, это снижает их. И я их должен еще накатить. Потому что клиентов много, вот почему. Во-первых, ты не все их можешь накатить, ты не все их обязан накатить. Ты можешь скипать те, для которых ты не являешься шардом. Во-вторых, что еще? Во-вторых, у тебя их там будет не миллион, а может быть сотенка. То есть у тебя там, не знаю, все происходит в рамках секунды, например. Но если у тебя тысячи записей в секунду разом, ты тысячу может прочитаешь, а может быть одну десятую из них применишь к своей машине. Ну то есть это не так плохо на самом деле. Ну и учитывая, что ты читаешь тоже в параллель, то есть ты же не с одного диска сосешь все время, а ты читаешь в параллеле со всего массива дисков, и они тебе отвечать тоже могут в параллель. Ну понятно, упираемся просто не в диски, а в сеть. Но я понял, что надо мерить на реальных примерах. Ну то есть, скажем так, оно себя, по тем результатам, которые они в пейпере привели, оно в принципе выглядит достаточно вкусно в плане производительности. А какие результаты? Слушай, я не буду сейчас врать, открой, там обе ссылки есть, там вот ссылка, которая на PDF, ты можешь открыть и посмотреть там просто в конце все прямо напечатано графичкой. Сам лог вообще супер быстрый. Реализация kvalue, она не то, что прямо мега быстрая, но там десятки тысяч молтики операций в секунду. Ну то есть у Гугла на его кластере, на неопубликованном алгоритме есть какие-то цифры, я понял. Ну почему, во-первых, не Гугл? Это не Гугл, а Майкрософт. Моя ошибка, извини. И даже не Microsoft Production, а Microsoft Research. Ну от этого прогнозов не меняется, да? Ну да, да. Ну то есть они, я не говорю, что это прям супер, всем надо бежать и использовать, я просто прочитал пейпер и делюсь своими впечатлениями, если что. Я просто отмечаю, что бенчмарк невоспроизводимый, больше ничего. Ну пока да. Ну на самом деле, да, по поводу воспроизводимости, на самом деле, тут еще последняя ссылка, которая по этой же теме у меня. Я пока все это читал, мне стало интересно, есть ли какие-то живые реализации. На самом деле, живых реализаций прям совсем к корфу одна, называется Zilog, это такая реализация к корфу, по-первых, угадайте, чего, по-первых, с Ceph. Есть такая система распиленная файловая, называется Ceph. Точнее, даже не файловая система, это Object Storage распиленный. Вот, и поверх него запилили корф. Это довольно интересно, на мой взгляд. И довольно круто. Я не смотрел, как оно работает, я это просто нашел, и хочу как раз как-нибудь взяться тут, померить, что у него там с производительностью. Это первый момент. И второй, в такой, там не совсем уже корфу реализован, это скорее музычкой навеяло. Есть такой сейчас экспериментальный проект у Башо, называется Machi. Вот, оно тоже в каком-то смысле реализует корфу, но там оно скорее не корфу, а так вот, как я уже сказал, музычкой навеяно. Такие вот дела. Я закончил. Я смотрю на производительность, они тут на каких-то Flash Units, это что у них? Это FPG-шка с SSD-шкой. Вот, на 28, на 32 у них запись порядка 150 тысяч 4-килобайтных энтрис в секунду. Это порядка 0.75 гигабайт в секунду. Ну, интересно. И при этом растет линейно, но вот судя по их графику. По крайней мере вы даете их до 28. Ну да, ну то есть на самом деле это 28 Flash Units. Мне кажется, это такой приличный размер кластер. Ну ты понимаешь, что это специализированный кластер. Я понимаю, да, но с другой стороны, в принципе, если ты поставишь это обычные сервера, я не думаю, что они прям будут супер медленнее работать. Ну да, с Flash, с Flash этими SSD, в принципе, пусть медленнее работает, но если оно линейно при этом нормально масштабируется. Вопрос. Внимаю. Почему это не масштабируется? Ну, то есть я понимаю, что в папере это не написано, почему это как бы очевидным образом не гео-реплицируется, масштабируется вот это все? Не гео, потому что, смотри, у тебя тогда будет супербольшая latency. Если ты хочешь иметь реплики в разных регионах, у тебя там репликация, это почти chain replication, но это chain replication, которые не цепочки, не связывают друг друга под цепочки, а командой передают. А у тебя клиент каждый сервер как бы пинает следующий, и если клиент в какой-то момент обнаруживает незаконченную цепочку, он ее типа тоже репейрит. Вот. И у тебя в chain replication-то в ванильном, где у тебя от сервера к серверу, где клиент плюет в первый, и из последнего оно вылетает, оно для гео-репликации плохо, потому что у тебя, если три региона, у тебя будет, не знаю, тройная latency. То есть ты в первый плюнул, там первые 50 миллисекунд прошли от клиента до сервера, потом сервер из первого региона плюнул во второй регион, еще 100 миллисекунд, потом из второго региона в третий регион, еще 100 миллисекунд, потом обратно к клиенту, допустим, 150. Это сумасшедшая latency для одной записи. И это просто ванильный chain replication. Если у тебя еще такой chain replication, как вот тут, у тебя клиент пишет в первый регион, клиент ждет ответа из первого региона, клиент пишет во второй регион, клиент ждет ответа из второй региона и так далее, то есть latency будет еще более сумасшедшая. То есть это просто писать в несколько регионов, или такой тип репликации ставить в несколько регионов, это очень плохо. Я не совсем понял, что мешает делать репликацию асинхронно. Это chain replication, ты не имеешь права писать в следующую единицу, пока тебе первый не ответил успехом. Я не говорю сейчас про chain replication, я говорю вообще. Ну в смысле у тебя алгоритм предполагает, что ты, скажем так, у тебя свойства тех операций, которые тебе нужны от диска, то есть, грубо говоря, ты в голову, если ты в юнит, в группу юнитов записал в какую-то позицию, она сразу же становится недоступной. И единственный способ это сделать, это у тебя есть один, какой-то, не знаю, среди них самый главный, и ты вот в него записал, как только у него записалось, никуда больше записать нельзя. Вот все, я понял. Да, и остальные, ты не можешь остальных обновить, пока он не обновился. А здесь еще у них chain replication, поэтому на самом деле ты как бы каждого следующего обновляешь только после предыдущего. Ну ок. Ну то есть теоретически, наверное, можешь сделать master-slave, но это уже точно будут не тупые юниты, потому что они должны будут друг с другом говорить, и у тебя все равно будет как минимум ожидание мастера. То есть это все еще не очень круто работает. Подожди, подожди. А у них сделано так, что они могут читать по географическим, а писать они все равно должны в мастер. Ну то есть, может быть, для каких-то workload-ов это хорошо, но если ты хочешь записать в каждый регион, это не будет работать. Идея, я считаю, все равно полезная, и надо услышать такое в уме. Безусловно, потому и рассказываю. А удивительно, что только сейчас додумались. Ну не совсем только сейчас. Первый paper датируется, дай-ка гляну. Я уже закрыл, кстати, свою вкладку, но там, что-то, по-моему, восьмого, что ли, года или десятого. Ну то есть поменьше, на мере пятилетней давности штука. Удивительно, что только сейчас стали появляться какие-то более-менее реализации, похожие на paper. Уже чешется лапки написать свой как рочь, да? Как рочь? Ну нет, если свой как рочь возьмусь писать, я, скорее всего, буду брать все-таки эгалитарианский paxos, потому что вот он как раз, его распределенно должно идеально работать. Первая статья 2011 года датирована. У меня вопросы иссякли. Ну и прекрасно, переходим к следующей теме. А следующая тема, Саша, опять твоя, и вот как раз она тоже про распределенные базюльки. Да, я хотел побурчать немного про каучбейсы и Кассандру, потому что там такое типичное... Почему сразу побурчать, почему бы не помурчать? Так о чем я? Да, побурчать. То, что есть такое видение, что NoSQL, это всегда прям здорово, замечательно, потому что там реализационные базы тормозят, а тут мы возьмем классную Кассандру, она сама масштабируется, и все с ней здорово. Я в последнее время подозрительно часто сталкиваюсь с тем, что не все здорово, и вот хотел поделиться такими моментами. Например, каучбейс работает, у тебя кластер в Амазоне, все здорово, замечательно, потом у тебя в Амазоне случается что-то с сетью. В Амазоне довольно часто, по моим наблюдениям, случается что-то с сетью, ну там, видимо, уходят админы, выдергивают прям флешки. Уборщицы моют пол. Ну, стоподобно, да. И каждый раз так ворчит, опять этих проводов тут насовали. Что это еще? UPS, нафиг надо, вот тебе шваброй. Короче, что-то у них там постоянно не лается, и в результате очередной такого момента с сетью каучбейс сказал, что я ту ноду долго не вижу, и только ее остальные вижу, все хорошо. Поэтому я считаю, что эта нода упала, и я делаю автофейловер. Ну, все хорошо отработало, все как запланировано. Проблемы начались потом, когда ты приходишь в понедельник утром на работу, смотришь, ага, у меня сработала автофейловер, одна нода, она как бы вне кластера. Ну, окей, сеть вроде установилась, я нажму, типа, сделать rejoin. И проблема заключается в том, что он не происходит. Ну, в смысле, не происходит до конца. У тебя там в каучбейсе такие красивые графики, что типа идет-идет-идет ребалансинг, все прям сейчас будет пучком, и там где-то там на 99,5% все это останавливается и перестает двигаться. И проблема даже не в том, что... Ну, очевидно, это какой-то баг в каучбейсе, и я написал об этом сначала им в рассылку, потом зашел в IRC. В рассылке, кстати, никто не отвечал, видимо, никто ее особо не читает. То есть, если что, по каучбейсу надо сразу в IRC бежать. Проблема не в том, что есть такой баг, что я с ним столкнулся, а в том, что ты когда с этим столкнешься, ты даже не знаешь, что сделать вообще. То есть, я после такого момента начал намного лучше понимать людей, которые предпочитают поднять какой-нибудь Zabbix или Nagios, мониторить все-все-все-все. Если что-то сломалось, то сразу админу приходит смс, звонок ночью, там баба железным голосом говорит, что чувак, поднимайся, чини. Потому что помимо простых случаев, когда у тебя просто падение ноды, я еще могу понять, у тебя еще есть... Это как бы самое простое, самое элементарное. Бывает еще 100-500 непонятных моментов, когда, например, у тебя внезапно возросла нагрузка, и айза-бас стала очень сильно дубить, потому что она слабее остальных машин. И ты не можешь в этом случае сделать фейловер. Тебе нужно, чтобы человек посмотрел и понял, что, ага, короче, вот мы тут во что-то упираемся, надо, не знаю, выделить сервер посильнее. И очень-очень много случаев, когда реально автоматически ничего починить нельзя. И когда у тебя сделано именно поверх какой-нибудь, ну, для примера, реализационной базы данных, там руками пошаржено, руками среплицировано и вот это все, и с ручным фейловером, ты хотя бы в любой непонятной ситуации знаешь, что можно сделать. Ты можешь в любые данные залезть, как угодно, и в любые таблицы перенести. То есть для тебя это не скрыто под какими-то слоями абстракции, что типа, чувак, ты просто там кнопочки нажимай в веб-интерфейсе, оно само будет работать. А ты смотрел в CouchBase, может, там тоже в лоббер что-нибудь сыпалось? Ничего там не сыпалось. То есть ты залазил, глядел, да? Ну, как ты думаешь, конечно. Я и пробовал его выключить-включить, я и пробовал повторно сделать этот rejoin. Ты колесо пинал, пинал, пинал. Да, вот так все. Я об этом буду. А, да, как вы сегодня говорили. Я просто хочу закончить историю, что был, вот я начал про CouchBase, и был аналогичный неприятный опыт с Cassandra, что, казалось бы, там, настроил бластер, все здорово, все замечательно, само реаплицируется, а потом ты замечаешь, что с дефолтными настройками, например, Garbage Collector может по 150 миллисекунд останавливать базу данных. Я не скажу, что это прям супербольшая проблема в случае конкретно с Cassandra, потому что ты в драйвере, в котором ты ходишь, в Cassandra можешь указывать, там, в серии, там, считай мне данные из трех нот, и какая там первая ответила, или какие первые две ответили, вот эти данные считай, а на третьей можешь забить, не наждать. Поэтому это не факт, что обязательно прям 100% аффектит клиента, но все равно такой неприятный осадочек, и неприятно еще в том смысле, что, например, в Postgres такой проблемы вообще не бывает. То есть его тоже, конечно, надо потюнить перед тем, как использовать, потому что дефолтные настройки тоже ни под что не оптимизированы, кроме того, что главное, чтобы он запустился везде. Но там хотя бы эти настройки более-менее понятны. Здесь все как-то сложнее намного. Но вкратце у меня все. У меня по этому поводу такие две мысли. Во-первых, действительно нужно всегда иметь, не только полагаться на автофейловер базы, а всегда иметь какие-то мониторинги, потому что действительно бывают ситуации, которые автофейловер, хотя, кстати, опять-таки, у коканувшего лета FoundationDB автофейловер умел даже тупящие машины по их маркетинговым материалам, можно было из них понять, что они умеют даже просто тупящие машины выводить не то чтобы из кластера, а из-под нагрузки. И по этому поводу неплохо работает еще такая вещь, как HA-прокси перед твоей косандричкой или ряком. По-моему, его можно строить, чтобы он... Я, кстати, не очень в этом уверен. HA или прокси. В общем, что-то такое слышу, что если база сама не умеет балансировать нагрузку с нод, которые заняты делать это внешне толзой, в-третьих, даже вот ты говоришь, что по взгляду с ручками удобнее вот это все. У нас тут сравнительно недавно был на продакшене случай как раз вот из серии Noda 2Pit, поэтому лентансия средняя по кластеру, по ответу пользователя увеличилась. Мы это, конечно же, в мониторинге увидели. Нам позвонила баба с приятным сексуальным металлическим голосом. Вот. И благодаря тому, что это был ряк, и там, не знаю, диск умирающий... А, нет, не диск, там сетевая карточка на ноде умирала. Вот. Нода была выведена из обслуживания просто буквально одной командой. И, то есть, это не нужно было какие-то сложные вещи делать. И точно так же потом была введена в строй одной командой. Поэтому все-таки все еще есть плюсы расширенных баз данных. И такая вторая вот мысль, хотя это может быть даже и третья получается, что есть такая приятная привычка, точнее, неприятная полезная привычка читать исходники баз данных, которые используешь. Потому что в подгрессе, мне кажется, труднее разобраться, чем в Cassandra или Ryaki. Тем более, что, например, в Ryaki я более-менее за рулец, который с ним работал, я вроде бы, мне кажется, разобрался. В смысле прямо в исходниках. И в случае чего могу просто посмотреть в исходники и что-то даже понять, что сделать. Мне кажется, это больше зависит от умения и желания читать код на SIP. То есть он там в принципе довольно простой, понятно, я смотрел. Ну, в смысле, для реализационной базы данных. То есть все понятно, да, то, что там одна только реализация B3, она там совсем нетривиальная на самом деле. Я хотел дополнить про Cassandra, что был еще один неприятный случай. Вот, совсем неприятный. То есть про Stub the World еще там можно пережить. Случай был такой, что работал Cassandra Cluster, работал-работал, никого не трогал. Все было прям отлично, очень долго. И внезапно одна из нот упала. Или, да, по-моему, в этот раз только одна. Неприятность в том, что упала она с out of memory после того, как очень долго нормально работала. После какого-то там очередного комплекшена Cassandra уперлась в heap, который там по дефолту у нее в cassandra-env.sh файле указан. И просто умерла. Вот это я считаю исключительно неприятным, потому что опять же в том же Postgres-е такого никогда не будет. В том смысле, что там ты, как минимум, если какие-нибудь квоты не указал, ты просто используешь всю память операционной системы, все хорошо. В Cassandra так нельзя, ну, во-первых, потому что есть все-таки какие-то лимиты на heap Java, которые указаны в конфиге. А если там укажешь в качестве размера heapа всю свободную оперативную память, у тебя начнет прямо адово тормозить GC. Ты не можешь так сделать. То есть то, что у тебя база может работать, работать, работать, а потом упасть по совершенно тривиальной причине и даже автоматически не перезапуститься, это, я считаю, очень неприятно. Блин, ты говоришь про одно, а ведь есть еще и другое. Ну, там, я не знаю, какой-нибудь автовакуум запустился, и она у тебя начала внезапно тупить. То есть и тупит она, скажем, пару дней, потому что давно не запускался, или какие-то сложные вещи должна сделать. Ну, то есть ты избавляешься от одних проблем, а получаешь головную боль от других проблем. А вообще, ты сегодня как-то много брешь, всех наших слушателей распугаешь, они вообще перестанут пользоваться всей системой, которую мы обсуждаем. И правильно, правильно сделают. Так Postgres'ом же тоже перестанут. Нет, неправильно, не надо так делать. Ну, основное просто у всего есть... Вот в React бросайте, потому что в React нет range scans и нормальных транзакций. Надо в React бросать, есть там range scans при желании. А, кстати, расскажи, как сделать правильно range scans. Зависит от того, что тебе нужно. Ну, серьезно, зависит от того... Началось! Ну, хорошо, допустим, у меня есть бложик, в нем лента постов, и я хочу делать спагинацию, дай мне посты из 1 по 10, из 11 по 20. Тебе React не нужен для этого. Да, да, серьезно, тебе React для этого не нужен. Вот вообще не нужен. Вот и я говорю, не нужен React. Нет, смотри, то есть у тебя не бложик, а сервис бложиков. Ага, да, чувствуешь разницу так резко. И у тебя сервис бложиков уже в одну машину не влезает. Тут уже можно интереснее делать. Я уже, кажется, рассказывал, по-моему, даже в подкасте, как мы это делали в предыдущей команде, как мы делали эффективные range запросы на React. Идея в том, что мы просто делаем кастовную функцию хеширования и кусочек ключа мы не хешируем. В этом кусочке ключа как раз какой-то порядковый номер чего-то. И вот у нас получается, что у нас все посты, грубо говоря, одного пользователя, они оказываются на одной группе реплик. И по группе реплик сделать range запрос можно, при помощи React pipe очень эффективно. Вместо того, чтобы ходить вообще по всему, черт возьми, кластеру. Есть большая разница. Понятно, то есть всего лишь нужно взять React Core и написать немного кастомного. Нет, вот, Саша, не надо говорить про React Core. Тут React Core не пахнет. Тут нужно написать на Erlang две строчки, чтобы была кастомная функция хеширования. React Core вообще ни при чем. React Core у нас был для другого. Подожди, подожди, я не очень понял. То есть кастомная функция хеширования, она хеширует, скажем, только имя пользователя, да? Ну, грубо говоря, ты просто говоришь, что у тебя у меня в ключе, вот после, не знаю, двоиточия, например, я эту часть не хеширую. Вот, просто не хеширую. И в таком случае у тебя просто все, что там до двоиточия будет хешироваться, оно все будет в одну реплику, а после двоиточия у тебя там будут разные вещи, принадлежащие этому пользователю. И они все будут на одной группе реплик храниться. Я понял. Ну, то есть, если у тебя... Что ты будешь делать в случае, если у тебя вот есть некие сущности, ну, у тебя же посты, да, например. Представь себе, что ты писал-писал, потом и время от времени еще удаляешь посты, да? И представь, что это может быть не посты, а вообще сообщения в чате, и какие-нибудь там самоуничтожающиеся, я не знаю. Короче, у тебя могут быть очень большие дырки в твоих рейнджах. Вообще не проблема. Это у тебя левл дб уже нижележащий обслуживает. Ты не обязан, то есть ты когда говоришь React-пайпу, или там даже можно встроенный MapReduce использовать, можно как раз сделать этот трюк с ограничением, нет, MapReduce не пойдет уже здесь. Короче, идея в том, что можно при помощи React-пайпа точно попросить React сходить на одну ноду и выгрузить рейндж ключей с одной ноды. Точно можно сделать. Ну хорошо. Ну то есть в этом плане Cassandra, безусловно, удобнее, если ты об этом. В Cassandra рейнджи, они просто прямо в публичном OBS нормально работающие. Да нет, в этом плане вообще Postgres очень удобен. А еще RefinkDB очень удобен, и как раз RefinkDB очень удобен. Даже Production Ready. Только без драйверов, да, мы помним. Ну да, да. Я про то, что вот у тебя любой выпускник любого вуза, у которого по специальности учатся, он отлично знает, как работать с Postgres. Он отлично знает, как сделать рейндж сканы, он отлично знает более того, как работать с транзакциями. Погоди, погоди. Да, но он не умеет, скорее всего, его отменить, во-вторых, правильно его расшардировать выпускник вуза не сможет никогда в жизни. Я вообще не уверен даже в целом, что он хорошо с Postgres умеет работать. Ну то есть да, он умеет работать с QL, скорее всего, наоборот, что его научили. Если у него был хороший преподаватель по базам данных, то он, возможно, знает про реализационный алгебру и сможет тебе с QL странслировать про реализационный алгебру. И, возможно, даже про какие-то хитрые индексы знает. И, возможно, даже знает, какие хитрые индексы внутри реализованы. Но конкретно подпись он, скорее всего, не умеет. Вы от этого темы-то не уходите. Я чувствую, что вы меня забалтываете. По Postgres или MariaDB, или что там, на самом деле, неважно. Я про другое хотел сказать, я к чему все вел тут-то. Представь, что ты делаешь проект с нуля на Cassandra. И я довольно сильно уверен, что у тебя с самого начала никто не будет заморачиваться насчет консистентности, потому что у нас там NoSQL, выбери 2 из 3, вот это все. С Postgres у тебя есть нормальные транзакции в рамках одного подданного сервера. И, скорее всего, ты будешь использовать изначально транзакции, но если ты неполный нехороший человек, ты по умолчанию будешь до всего использовать транзакции, а уже потом, когда ты во что-то уперся, ты начнешь думать, ага, вот тут неплохо бы данные динаморализовать, вот тут можно немножко забить транзакцию, потому что если немножко разъедется, не страшно, в коде подопрем, и так далее. А при использовании этих ваших модных NoSQL у тебя даже нет такого инструмента. И вряд ли ты будешь писать свою обертку, которая будет поверх Comparence Web делать твои собственные транзакции. Ну, во-первых, зачастую достаточно такие транзакции, для чего-то типа пользователей, прочей такой фигни, как правило, достаточно такие, ну, Comparence Web на один ключ, а он сейчас есть, вроде, так прямо вообще везде. И вообще я соглашусь с тем, что для проекта с нуля действительно, скорее всего, не надо брать Cassandra или React, если вы не знаете, зачем она вам нужна. Я с этим соглашусь совершенно. Другое дело, что смасштабировать, то есть, что я хочу сказать, что если в команде нет человека, который умеет готовить Postgres, то и проект нужно будет масштабировать, то это будет даже тяжелее, чем если в проекте нет специалиста по React, и проект нужно масштабировать. Но в обоих случаях будут большие-большие проблемы. То есть, если вы пишете проект, в который приходит много пользователей, и у вас есть какой-то storage, вам нужен специалист, который умеет готовить этот storage. Вот всегда, абсолютно. Я думаю, что если в команде нет специалиста по Postgres, то специалиста по React там уж точно не найдется. Ну, спорный, мне кажется, вопрос, потому что специалистов по Postgres у них в мире примерно столько же, сколько специалистов по React. Прям специалистов, которые хорошо умеют его приготовить. А если такового нет, то, ну, блин. Ума одинакова проблема. Такового можно потом нанять будет. Но всегда начинается так, что давайте мы сперва логику напишем, прикрутим базовую, не знаю, MySQL, который из коробки что-то умеет делать, работает через какую-нибудь ORM, и дальше начнем работать. А как начнутся проблемы, ну, наймем тебе специального, который нам расскажет, что мы все мудаки, и сделает все правильно. Ну, хорошо бы, если бы так. Проблема как раз в том, что берут сразу, не знаю, ребята, у нас там новый проект, короче, пока никакой нагрузки, ничего, там, что лучше, HBase или, не знаю, Memcache. Понимаешь? Да, понимаю. Это большая проблема. С другой стороны, мы сейчас в продакшене, в большинстве, с того, что я знаю, нового, вообще ушли от использования, так скажем, ну, или почти ушли, не скажу, изо всех, но многие сейчас используют Dynamo или вообще, прости господи, S3, просто для хранения профилей пользователей. Хотя есть, конечно, и местами React все еще, но идея в данном случае в том, что бывает ситуация, когда точно знаешь, сколько к тебе придет пользователей, если ты таки взлетишь. И ты понимаешь, что если они к тебе придут, они к тебе придут все сразу, как только ты взлетишь. Больше того, когда ты взлетишь, у тебя их будет даже больше, чем у тебя их будет потом. И тебе надо заранее так подстроиться, чтобы эту нагрузку выдержать. Да, логика хорошая, но почему не использовать уровень абстракции, не знаю, там, писать файл до тех пор, пока не начнет увеличиваться, а там переключиться на то, что выдержит. Ну, потому что смотри, вот, например, у нас есть такая штука, я думаю, я могу про нее говорить, потому что я помню, даже про нее уже упоминал, называется SBS, Simple Backend Services, для игр, которым не нужна специальная логика на бекенде, это просто такой Storage. Вот оно прямо наружу в клиентский SDK вытаскивает то, что данные надо уметь мерджить. Вот просто потому, чтобы это хорошо и быстро работало, и хорошо масштабировалось, и выдерживало пики, когда у тебя маркетинг выливает деньги, и у тебя игра в топе, и у тебя сумасшедшие толпы пользователей приходят, чтобы оно при этом не падало, чтобы оно, в общем, хорошо себя вело, с тем как бы контустроено. Вот просто поэтому, потому что если ты напишешь на большом, ну, на таком серьезном уровне абстракции, потом ты поймешь, что у тебя этот уровень абстракции, чтобы смасштабировать, нужно гораздо больше усилий вложить, чем если сразу взять правильную абстракцию, типа вот, например, понимать, что нужно данные мерджить, то ты потом... у тебя потом просто не будет времени сделать это правильно, и ты будешь в очень сложном положении. Да, согласен. Но это... я предполагаю, что когда кто-то делает большую систему, то уровень абстракции придумывается правильно. Ну, вот как бы да. Но вот тут уже, опять-таки, тут нужен специалист по тому, чтобы выбрать правильный уровень абстракции. Короче, в любом случае, если проект нужно масштабировать, нужен какой-то специалист, который будет знать, как оно все будет устроено. Да. Понятное дело, что в любом случае, я вот сейчас говорил про нас, то есть про игры, понятное дело, что в прототипе вообще никакой сети с бэкэндом никогда нет. В прототипе игры. Но вот там еще есть такая стадия production и soft launch, вот в эти моменты уже критично выбрать правильную абстракцию. Слушай, вот про это интересно было услышать. У вас в прототипе нет сети и бэкэнда, а как вы многопользовательские игры без сети и бэкэнда делаете? Во-первых, скажу сразу, что у нас не то, чтобы так много прям совсем многопользовательских, у нас есть, и конечно же говорю, у нас даже есть игры, о которых я только что про simple backend services говорил, которым вообще логика на бэкэнде не особо нужна. То есть оно там просто как бы, как это, просто использует бэкэнд как сейф-гейм и конфиг и вот это все. А сама в себе игра однопользовательская совершенно. Во-вторых, чтобы спрототипировать многопользовательскую игру, тебе достаточно к одному устройству подключить скрипт-контроллер. Тебе не нужно заморачиваться с полной сетью. Думаю, ты с этим согласишься. Да, понятно. Вот. А настало время сказать добрые, теплые, отличные слова в пользу, ой, не в пользу, в адрес нашего следующего спонсора. А именно с 18 по 19 сентября в городе Екатеринбурге пройдет третья международная конференция для Python-разработчиков Python PyCon Russia. В общем, конференция предполагает выезд группы питанистов на два дня за город и днем слушать доклады, мастер-классы, вот это все, а вечерком веселиться на природе. Ну, в общем, я так понимаю, это стоило бы назвать, наверное, PyCamp, а не PyCon. Среди участников разработчик CPython, активный участник многочисленных Python-проектов, включая PyPy, Benjamin Petterson, Python Core Developer Андрей Светлов, разработчик PyCharm Андрея Власовских, разработчик Яндекс.Такси Валентин Синицын и многие другие. Помимо докладов от спикеров будут мастер-классы и lightning talks, на которых любой желающий сможет показать и рассказать свой проект.",
    "result": {
      "query": "Corfu distributed log system"
    }
  }
]