[
  {
    "segment_id": "045df9e6-f7b7-4e51-bea6-9c8916a5c8e2",
    "episode_id": "79703b9a-ad66-486a-b947-1bba87d95fcf",
    "episode_number": 398,
    "segment_number": 3,
    "text": "И они, короче, легли на 2 или 3 дня, я так понимаю, что они начали делать какой-то мейнтен на сети и, короче, намейнтенились. И они все выходные лежали и такой понедельник починились. Да, как-то так. Окей. Если кто-то из слушателей знает, как так нужно катить апдейты, чтобы полстраны остались без мобильного, притом не самой маленькой страны, вы, пожалуйста, придите в комментарии и расскажите. Теперь, я думаю, мы, наверное, можем поговорить про такой… Мы поговорили, чем мы учились за неделю, но в прошлом выпуске, кроме того, что прошла неделя с прошлого выпуска, в прошлом выпуске еще была одна вещь, которой я хотел… Сам ли две вещи, к которым я хотел добавить в свой комментарий. Один я просто оставил под прошлым выпуском, про то, что вот к этому pg-евент, короче, который по сокету мышлет, есть нормальные продукты. В смысле, не в смысле, что ивент какой-то ненормальный, а в смысле, что он просто кусок софта, а есть прям полноценные продуктовые компании, построенные вокруг идеи того, что вы хотите реалтайм-событие получать от чего-то пастгрэсообразного, например, с Ubase. Я никогда не пользовался, но этот стартап, вроде бы, успешный стартап. В общем, если вам реально такой эскиз интересен, рекомендую посмотреть. Вторая вещь, про которую я хотел более подробно при этом поговорить остановиться, это задавались вопросом, когда обсуждали умбру, как же так вот нам брать и обновлять наши memory базы, чтобы, ну и в принципе рестартовать, не только обновлять, чтобы не потерять весь их memory state. Ну, что значит, чтобы наши киши были теплыми и наши волосы были шелковистыми. В фейсбуке в 15-м году задавались, точнее, даже пейпер вышел в 14-м году, то есть вопросы, наверное, задавались в 12-м, 13-м. У них, я не знаю, есть ли оно у них до сих пор, но в тот момент была система, которая называлась Scuba. И она, во-первых, она in memory, но она критичная в плане производительности, но она, например, не реплицированная, насколько я понимаю. И они не очень хотели дорожать систему, добавляя репликацию или еще какие-то вещи делать. У них было состояние на диске, но оно чисто для crash recovery. И crash recovery с диска, в том случае это все еще были, как я понимаю, обычные крутящиеся диски, занимало что-то вроде, то есть 120 гигабайт данных перечитать с диска и там провалидировать, у них занимало 2,5-3 часа. Я подозреваю, что это не просто чтение, потому что я не думаю, что 120 гигабайт читаются, даже со старых дисков читались бы 2,5 часа. Я думаю, что они к нам именно связаны с тем, что нужно провалидировать все и перестроить какие-то внутренние структуры данных. То, к чему они пришли, это... Ну, то есть я не буду пересказывать всю статью, вот прям совсем вкратце, они поднимали отдельный процесс, который выделял кусок shared memory, перекопировали туда, почти полностью сохраняя формат данных, ну просто, то есть не один в один, но структуры копировали, ну, короче, там типа очень похожие структуры, я подозреваю, там, видимо, были какие-то изменения связанные с тем, чтобы оно как-то было более универсально между версиями версий. Перекопировали туда все данные, при том они перекопировали так, чтобы занимать не в два раза больше памяти, а вот типа вот сюда скопировали, а вот здесь убрали. Вот, перекопировали, и это как бы был такой частью механизма выключения сервера. То есть он как бы уже, я так понимаю, не должен... Я могу путать, я давно на это смотрел, я отрыл старую статью у Эдриана Койлера в блоге, вот, и саму статью не перечитывал. Но, в общем, если я правильно помню, то они как бы, когда этот процесс происходит, сервер уже никого не обслуживает. Они просто берут, перекладывают данные в этот shared memory, выключают основной процесс, запускают новую версию основного процесса, потом перекопируют обратно, и при том, когда они копируют, вот, опять же, они не 2x место используют, а именно перекладывают данные, и потом выключают этот вот временный процесс, который держал данные у себя в shared memory. После чего восстанавливают операционную деятельность. И вот этот процесс у них занимает 2-3 минуты на сервер. Я не уверен, насколько такой подход оправдан при наличии NVMe дисков, потому что поднять стейк с NVMe диска, а потом просто, ну, как это... Да даже просто поднять стейк с NVMe диска, то есть просто заснапшотить память на NVMe диск современный, а потом его оттуда поднять, мне кажется, 120 гигабайт займет... Вот я не готов прям вот-вот-вот зуб дать, потому что я как-то не производил эти вычисления и замеры, но мне кажется, что до 120, а то даже и больше гигабайт, за 2-3 минуты с современного PCI-Express 4 NVMe диска в память поднимутся. Хотя, может быть, я путаю, но мне кажется, поднимутся. У меня возник вопрос, насколько в современных реальных для этого реально заиспользовать кэш-файловые системы, ну, то есть, типа, перед рестартом записать все данные в диск, и я не уверен, нужно ли их прочитать, чтобы они в кэш попали, не настолько интересовался. В контексте Umbra у них же был свой buffer manager. Да, но я имею в виду, что... И там же идея была в том, что тебе нужно унижать часть структуры данных в памяти, а часть на диске. И типа у них какие-то указатели были специальные, которые в случае чего просто проваливаются на диск. В случае Umbra, согласен. Но ты говорил про систему Facebook, которая целиком in memory, ну, с точки зрения того, что она еще и в диск персистит, но вообще она in memory. И мне интересно, в случае этой системы, проканает ли перед рестартом сделать вид, что ты сохранил все файл, потом ты делаешь рестарт, а потом тебе файловая система... Ну, то есть, ты делаешь рестарт, но читаешь, как бы, на самом деле не с диска, а из кэша. Я думаю, что им было бы очень тяжело регулировать этот механизм, ну, то есть у них же база данных in memory, и она берет и отжирает всю память, сколько может. Я думаю, им было бы очень тяжело регулировать, что вот, ага, вот сейчас мы это отдаем файловой системе, и освободить достаточно памяти, чтобы файловый кэш не вымыл тут же. Понимаешь, в чем я, да? Да, нет, я почти уверен, что в кэш попадет не 100%, но даже если туда попадет 90%, это уже неплохо. Но все равно, короче, ты же, поскольку не контролируешь, было бы очень сложно гарантировать, ну, даже не то, что гарантировать, хотя бы просто хоть какую-то невымываемость обеспечить, потому что, как бы, здесь они переложили байтики, все, они точно знают, что их можно из своей памяти убирать. А тут они что-то записали на диск, ну, и вроде можно убирать, конечно, но не факт, что там это осталось в кэше, и вообще сколько там вообще памяти нужно заранее подосвободить, чтобы кэш наполнялся. Да-да, согласен. Но как-то, с другой стороны, если ты делаешь вот этот хак с кэшем файловой системой, тебе не надо никакой код, по сути, мейнтенить, если это работает, конечно. Да, интересный вопрос. В общем, конкретно с умбро, если возвращаться к умбре, я не думаю, что им бы такой механизм помог, потому что перекладывать байтики в рам, в отдельный лежащий рам против перекладывания байтиков просто, даже не то, что им, они же все равно персистентная база, я думаю, им бы пришлось просто запомнить, какая часть их структур была в этот момент в памяти, и просто при старте прогреваться. Я думаю, что вот этот механизм прогрева у них бы за 2-3 минуты отрабатывал. Ну, я точно не знаю, наверное, может быть, на современных размерах данных, то есть там было бы уже не 120 гигабайт, а, не знаю, 100 терабайт, может быть, в памяти бы лежал, и там вот уже, я не знаю, я думаю, что терабайт с NVMe уже за минуту, наверное, не вычитаешь. Так что, может быть, все еще имеет место быть, такой подход просто на других масштабах. Нужно садиться с калькулятором и посчитать. Согласен. И совершенно непланный переход к следующей теме. На этой неделе зарелизился, ну как, не зарелизился, а стал доступен в публичный превью, наверное, это бетой можно назвать, редактор FLIT от JetBrains.",
    "result": {
      "query": "JetBrains Fleet public preview"
    }
  }
]