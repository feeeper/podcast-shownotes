[
  {
    "segment_id": "5f66a552-ddb1-4b1c-a5c5-9dad92359386",
    "episode_id": "825064f9-fe09-450c-b3f4-491a381bd679",
    "episode_number": 142,
    "segment_number": 9,
    "text": "То есть в чём проблема, как обычно текстурируют. Берём полигональную сетку, поговорим о текстурных координатах и говорим, что вот лепи вон ту текстуру с такими текстурными координатами. Для чего-то типа человека оно обычно отрисовывается всё и просто развертка мапится на полигональную сетку. А вот для всяких вещей типа terrain, типа foliage и в общем весь environment, он зачастую текстурирован так, что какая-то текстура где-то повторяется. И это хорошо в том смысле, что у тебя небольшая текстура, которая покрывает сразу большую площадь, и это как бы дёшево довольно-таки. Плохо то, что это обычно визуально видно, что оно повторяется. Собственно идея Кармака была, а давайте сделаем как-то так, чтобы у нас текстура была везде уникально нарисована, нигде не повторялась, и оно бы как-то при этом работало. Как это физически достигается? Ну то есть понятно, никто не рисует абсолютно уникально, всё-таки есть тулинг для этого, но как это работает технически? Технически у нас есть специально описанный способ собирать кусочек из обычных логических условий, что-то делают, чтобы не совсем всё время растры хранить, но это не важно. Главное, что когда нам нужно показать какой-то кусок поверхности, нам туда нужно ресурсов подгрузить в динамике каждый раз. То есть игры стримят ресурсы из диска в память GPU уже давно, и в последних поколениях они это делают довольно много. То есть проблемы наладить обмен с GPU, насколько я знаю, нет. Это вопрос, наверное, от того, чтобы это появилось в CUDA API, насколько я понимаю, оно появилось. То есть вообще в принципе технически у тебя GPU может делать RDMA как минимум с оперативной памяти основной, а с основной оперативной памяти ты можешь делать так или иначе обмен с диском, то есть если ты хочешь, наверное, маппить, я думаю, можно найти, не знаю, если не RDMA, то какой-то другой способ в кусок памяти в main memory отдать с диска как-то, чтобы при этом цепу не сильно участвовал. А другой регион, где уже данные подготовлены, отдать его, чтобы GPU его загоняло. Ну то есть если возможно выстроить обработку, что пока GPU молотит один кусок, ты стримишь в оставшуюся память GPU другой кусок из памяти без участия процессора, а в это время процессор или кто-то ещё подготовит следующий участок памяти в основной памяти, чтобы потом этот GPU стримил, то есть когда ты вот этот pipeline выстроил, в принципе оно должно быстро работать. Окей. То есть это не тривиальная задача, согласен, но технически оно вроде должно осуществляться. Ну то есть RDMA между GPU и main memory есть. Какую тему ты имеешь в виду, можно подцепить, я что-то не вижу похожий. Сжатие слонов. Сжатие слонов, давай я тогда сейчас сразу добавлю, может тогда лучше заодно и подцепим. Вот, собственно, ну вот то, что упомянули про сестор, и я так вообще думаю, почему вообще Postgres, в этом сравнение есть, Postgres вообще не очень аналитическая база. На самом деле Postgres, хоть и не очень аналитическая база, он очень хорошо расширяемый во все места базы, и есть такая компания небезызвестная, по-моему уже упоминали её как-то раз в контексте стримминг репликации, или в контексте логического репликации мы её упоминали. Такая компания, которая растачивает Postgres в сторону аналитики. У них есть, собственно, Citus Data продукт, и у них есть такое расширение опенсорсное, сестор FDW, шифровать как сестор foreign data wrapper, то есть Postgres есть механизм foreign data wrappers, который позволяет какую-то другую базу притащить в свою, и к нему будут работать все обычные select и вот это всё. И поскольку в Postgres нельзя делать кастомные бэкэнды, ребята извернулись и кастомные бэкэнды для хранения втащили как foreign data wrapper. Что довольно хитро. У этого есть свои ограничения, потому что у foreign data wrapper интерфейса внутри есть ограничения.",
    "result": {
      "query": "Postgres foreign data wrapper limitations"
    }
  }
]