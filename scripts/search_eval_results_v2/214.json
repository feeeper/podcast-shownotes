[
  {
    "segment_id": "65c26279-4c98-4648-b38d-400102ed1e2c",
    "episode_id": "1845796a-b6cb-4603-8d11-332be014f066",
    "episode_number": 214,
    "segment_number": 9,
    "text": "провалидировать. В связи с этим ребята, которые делают ЭКТО, собственно, те же самые ребята, которые делают Elixir во многом, почесали репу, решили разделить проект, правда я не очень понял, почему они его разделяют так, как они его разделяют, то есть мотивация, как я её понял, именно чтобы можно было использовать change set и схемы, но однако отрывают именно кусок, который связан с SQL, оставляя такие вещи, как репу и квери, внутри ЭКТО, вот ядра ЭКТО, что для меня немножко странно, потому что я делаю отсюда вывод, что они имеют какую-то статистику, что ЭКТО используется больше как генеральная библиотека для валидации. Да, допустим, даже так, но почему они тогда оставляют репу, то есть интерфейс к базе и квери, как куски библиотеки, коренной, если оно, ну, как бы без базы не очень нужно, то есть мне просто линия отреза кажется странной, или я, может быть, линию отреза неправильно понял, или мотивация для этого отреза, но вот мне, во всяком случае, странные впечатления остались, может быть, кто-нибудь мне объяснит, зачем именно так разделять. Я видел несколько статей, как можно ЭКТО использовать с другими типами, базами, хранилищей данных. Соответственно, кверия, она точно так же там использовалась, там просто не было SQL. Не, ну смотри, подожди, кверия, она в принципе очень SQL, то есть мне очень тяжело представить, интерфейс кверии можно использовать для чего-то, кроме SQL, я знаю, что там, его там типа на скотч примотали к Монге, к Динамо. Ну, слушай, там просто этот кверия, там натуральный селект что-нибудь, про что-нибудь, джойн, что-нибудь, куда-нибудь, ну, то есть понятно, что можно какие-то операторы просто не поддерживать, но все равно, как бы, для баз типа Динамо и Монга, наверное, просто интерфейс Ripple должен быть вот единственным остающимся кусочком, а кверия, мне кажется, нужно было целиком тоже оторвать тогда, потому что, ну, совсем странно уж он выглядит без SQL. Хотя, кто я, чтобы их судить, им виднее, на самом деле, они проекты разрабатывают. Вот, ну, в общем, в связи с этим будет ломающее изменение, которое, то есть, по сути, ЭКТО-3, если вы используете его с SQL-базой данных, вам нужно будет завести двух проектов, ЭКТО и ЭКТО-SQL. Дальше они, наконец, выкинули кастомные кустарные календар-тайпы и теперь полагаются на типы, ну, там, для дат и времени, которые были в языке уже, типа, с версии 1.3, что ли, то есть в языке оно уже давно, больше года, сильно больше года, и в новой версии библиотеки, наверное, бессмысленно поддерживать странные там комбинации этих типов. То же самое поддержку, то есть есть теперь официально благословленная библиотека для работы с JSON, есть все еще способы использовать какие-то другие библиотеки, но вот теперь есть официально благословленная и использовать стоит ее. Это называется библиотека JSON. Я еще не пользовался у нее в продакшене, до сих пор поизон, но JSON считается, типа, что он настолько быстрый, что местами может сишечкаться. Ну, или даже так, типа, написано, что он всего в два раза медленнее сишной имплементации местами, что, в принципе, на самом деле хороший показатель для языка, который вообще, ну, для некомпилированного языка. Вот. Еще там некоторые изменения по мелочам из таких вот, из первого поста, уже, на самом деле, аж три полпоста вышло. Еще из интересных изменений сильно улучшили, собственно, эргономику работы с Query, то есть там теперь есть именованные join, то есть скажем так, одна из вещей этого механизма запросов, который там, механизм конструирования запросов, в том, что их можно друг с другом склеивать. То есть есть функция, которая принимает запрос, и, например, можно к нему что-нибудь ее дописать в этот запрос. И раньше вот дописывать, например, join было не очень удобно, потому что нужно было как-то, ну, как ты будешь референсить что-то, что в каком-то другом месте проименовано, а тебе в другой функции нужно до него добраться, потому что у тебя уже нет никакого контекста между этими функциями, то есть не было никакого референса, по которому можно было бы заименовать какую-то сущность в другом месте созданную. Вот, теперь для join'ов еще некоторых мест можно именовать как бы вот их... А ты реально пользуешься, что у тебя в другой функции используются. .. Постоянно, то есть, например, как минимум в rare condition'ы у меня постоянно так докручиваются, то есть ты можешь делать функции, типа там какая-то базовая выгребалка, типа выгреби мне все такое-то и такое-то, дальше другие функции, которые ограничивают критерии поиска. И ты берешь так, типа говоришь, например, дай мне все, скажем так, у меня в проекте называется сегменты, дай мне все сегменты, а теперь только такие, которые может видеть пользователь, а теперь только такие, которые, там, не знаю, для этого какой-то, там, не знаю, одного приложения, и еще что-нибудь, и еще что-нибудь. Так ты критериев просто функциями добавляешь, и каждая следующая функция вызывает, если пользователь очередную галку тыкнул. По-моему, офигенно удобно. Согласен. Слушай, из чата вопрос. Валера, есть ли у тебя опыт работы со Scale-like GDBC, и если да, можешь ли сравнить его с ECTO? Что у них общего, в чем ключевые различия? У меня опыт работы со Scale-like GDBC очень ограниченный, он есть, но мне кажется, что именно сами проекты целиком, как вот в Scale-like GDBC к нему есть еще какие-то кусочки, которыми я не пользовался, то есть я не пользовался макросами, которые в Scale-like GDBC вместе идут, но они на самом деле довольно малую часть вещей на себя берут, и к нему еще есть, если не ошибаюсь, какая-то ормоподобная штука, которая тоже не пользовался, то есть так сходу могу сказать, что Scale-like GDBC ближе к написанию голова SQL, чем к написанию чего-то на каком-то DSL, который потом трансформируется в язык запросов к вашей базе. Scale-like GDBC гораздо меньше, если я правильно помню, берет на себя по части валидации всякого такого, а Scale-like GDBC в силу того, что это GDBC, он работает с конечным количеством трэдов в то время, как ECTO, поскольку он Erlang-овый, если там драйвер хорошо написан к базе данных, то есть, например, по сгрессовой, там все клево асинхронно, хотя у Scale-like GDBC тоже есть асинхронно прикольное. Да, у ECTO есть асинхронный на нете и всяком таком. Это не совсем Scale-like GDBC, там у него есть какой-то отдельный бранч. Да, это совершенно, да-да, отдельный проект, форк, как угодно. Вот, то есть, как бы Scale-like GDBC штука клевая, но я им пользуюсь довольно ограниченно, и я им пользуюсь всегда в таких местах, где, скажем, то есть тот код, который я с его использованием писал, он мне тяжело сравнивать с кодом на ECTO, потому что ECTO я использовал в честном таком круде, и там в том же приложении, где у меня крут на ECTO в одном из, у меня там есть довольно сложный запросик базы, который написан пишут, ну, они к другой базе, и там они просто генерируются из моего собственного DSL. А там, где у меня запросы, там, где у меня Scale-like GDBC в скальном коде используется, там заведомо настолько хитрые запросы, которые, мне кажется, ECTO до сих пор бы не смог. И вот, как раз таки Scale-like GDBC в этом плане был приколен, потому что ты пишешь просто SQL с твоей базы, с какими-то возможными ее фичами, и просто он туда аккуратненько засплайсит значения, проверив типы, и в принципе, мне кажется, Scale-like GDBC для многих задач, это, наверное, вот такой, как бы правильный уровень абстракции с базой данных, для работы с базой данных, но конкретно ECTO, он еще добавляет хороший такой слой для валидации, для проверки, то есть, как бы, валидация, которая двоевно хорошо работает и с данными, и с собственно, с базой. То есть там вот это вот в ChangeSet валидация, ChangeSet, который определяет валидацию, его можно писать как для каких-то, туда можно писать как какие-то вещи, которые будут на стороне приложения выполняться, так и какие-то вещи, которые проверят, например, какой-нибудь constraint в базе, а при попытке производить действовать с базой. Плюс в ECTO миграции есть, вообще, как класс. Если я правильно помню, в Scale-like GDBC миграции отданы на откуп почему-то еще, я могу ошибаться, но мне кажется, это было так. Мне кажется, в мире Java там вообще нет какого-то такого нормального, единого, красивого способа делать миграции, их там больше одного, и я вот как небольшой специалист из там Scala Java и так далее, я каждый раз теряюсь, что вообще использовать для того, чтобы базу там мигрировать. Спрашивают, а вот этот твой крут на ECTO в open source или нет? Нет. Это прям сильно closed source. Ну, я скажу свой небольшой опыт. Я немного проектов написал на Elixir, но везде, где было, мы ECTO использовали в простейшей функциональности. Вот так вот сложно динамически не создавали запросы, как Валера создавал, а на особенно сложных местах мы просто SQL-куски вкручивали прямо в query, на сенсе в ECTO, для того, чтобы не мучиться с этим сложным... Ну, надо сказать, у меня я SQL-кусков выпускал только в тех местах, где у меня прям конкретно такой дрэш-шак творится, типа я использую там подгресовый какой-нибудь супер фичу, которая мне там в одном месте нужна, чтобы я, так скажем... У меня есть место, где у меня синхронизируются данные из нескольких баз, и там удобно сделать просто такой, типа прыжок под выпад, а чёрт, не удалось. Ровно то же самое. То, когда у тебя ECTO плохо ложится на твои данные или какие-то... Нет, вот как раз потом ты видишь, у меня так вот плохо ложащегося ECTO на данные у меня не было. У меня были варианты, когда просто мне нужна фича базы, и через ECTO к ней продираться это сложно просто. То есть там у подгреса есть вполне такие... Если нужно позвать просто экстенш или даже ещё какие-то такие вещи сделать, которые прям укладываются в рамки стандартного запроса, всегда есть fragment. И он просто ты можешь туда засплайсить в запрос почти любую фигню. Вот. А вещи типа скопировать из foreign data в wrapper во временную базу, а потом там короче через трёхэтажные сети ещё короче как-нибудь сейчас что-то сделать и потом вставить только нужное, вот это в ECTO, к сожалению, пока не написать, просто потому что там сети в данном случае уже не совсем трёхэтажные, но всё равно то есть как бы... Да, сети, если кто не знает, есть такая фича, которая позволяет SQL писать не изнутри наружу, а как нормальный код последовательно. Вот. И такого рода вещи, они просто на ECTO всё ещё не пишутся, это придётся как раз писать, не знаю, странно. Ну или там опять же какой-нибудь хватать какие-нибудь сложные локи. Или... Я, к сожалению, не помню точно, что оно написано. Copy comment, например, как таковой в ECTO поддерживается, но если этот copy нужно сделать какой-нибудь сложный, то есть типа copy из сложного на основе какой-нибудь запроса, который делает join, я не уверен, что это в ECTO легко написать. Или там copy, который происходит с какой-нибудь временной таблицей. Я могу сейчас подсмотреть пример, но мне для этого нужно замьютиться. Вот. Но вообще, продолжая сравнение со Scala GDBC, у ECTO просто больше scope, я бы вот так сказал. Но Scala GDBC клёвый. Меня серьезно интересует вопрос про якобы существование правильного способа иммиграции базы данных, потому что, насколько я осведомлен, это всегда... Ну, если ты не пишешь что-то простенькое, там а-ля джанга один веб-сервер, то это всегда боль, это всегда вручную, это всегда скрипты по той причине, что у тебя часть таблиц нужно сразу создать, сразу заполнить, часть данных ты иммигрируешь в фоне, потом ты выкатываешься не сразу на все, а у тебя часть багендов работает с одной версией, часть с другой, потом ты решаешь, что на самом деле надо все откатить как было назад, и да, то есть тебе нужны не только апгрейды, но и даунгрейды, и они тоже должны не класть сразу все твои инстансы базы данных. Ну, скажем так, смотри, у ECTO иммиграция достаточно гибкая, как минимум достаточно гибкая, чтобы иметь даун, и даже если ты пишешь на как бы довольно сейфовом подможестве этого языка иммиграции, он тебе даже ап и даун сгенерирует автоматически, исходя из изменения, которые ты сделал. Он, в принципе, если у тебя реально там как бы на разных багендах разные базы, то в принципе ты можешь эту иммиграцию применить не хором ко всему, то есть ты можешь так конфигурацию сделать. Другое дело, что я сомневаюсь, что это совсем такая уж хорошая практика. Чего я как-то, я не уверен, что это, то есть я этим ни разу не пользовался. Я обычно вот, ну, у той части приложения, которая в моих случаях крут, она обычно маленькая, и там нет прям какой-то мегабигдат, и хватает просто вот эти автоматические накатывалки, которые просто делают, ну, более-менее синхронно. Ну, даже если бы она делала там, типа, то есть там дописать куда-то create index concurrent, или что-нибудь такое, это обычно не очень большая проблема. Больше что, в принципе, если быть аккуратным, и ты пока в позвольте строить так, что ты добавляешь колонки с дефолтными значениями, то подобные вещи, это обычно довольно дешевая операция, и как бы нет проблем ее накатить даже синхронно. И то есть как бы тут уже просто вопрос того, что ты делаешь некоторые вещи не в одну миграцию, а в несколько шагов миграции, и тем не менее у тебя все еще есть какой-то инструмент, и он дефолтный или там правильный, он не в том плане правильный, что он решит все ваши проблемы с миграциями навсегда. Да, возможно, какие-то вещи на продакшн-базу придется так, ну, чуть более аккуратно накатить, чем на тестовую базу. А он дефолтный в том смысле, что он работает для, может быть, 90% проектов, и он просто идет, поставляется вместе с твоим же фреймворком для работы с базой данных. А не так, что тебе нужно как-то там что-то еще куда-то притащить и подумать. Ну да, если речь идет про фреймворк, а ля там, ну, у нас как джанга только не джанга, то да, охотно верю, что вполне возможно. То есть если речь идет про high load такой серьезный, то там у тебя возникает банальная проблема, что ты прочитал строчечку из базы старой версии, она еще медленно не смигрировалась на новую там в фоне с криптом, и тебе нужно в коде приложения предусмотреть как бы вот во всех местах, где ты к ней обращаешься, миграцию. Слушай, ну вот это совершенно ортогонально фреймворку для миграции, или там план для миграции, ты это просто должен делать. То есть как бы даже без всякого high load у меня были ситуации даже вот на этом простеньком код приложения, когда я, типа, выкатывал какие-то изменения в 3 подхода. Первое, я накатывал миграцию, которая обратно совместима со старой версией кода. Потом я, собственно, выкатывал новую версию кода, которая могла как бы работать с этим новым полем и по дефолту писал в него. Потом я уже, последнее, что я делал, я даже после 4 подхода я накатывал миграцию, которая перемещала данные, например, старого места в новое и удаляла старую колонку. И последнее, я выкатывал версию кода, которая вообще ничего не знает про старую версию схемы. И то есть, как бы, да, это нужно делать аккуратно в любом случае, но это совершенно не зависит от того, какой инструментарий, дефолтный или не дефолтный, используется для управления миграциями. Видимо, Саша не хочет спорить. Саша абсолютно согласен со всем, что ты говоришь заранее. Я тут подсмотрел, какой у меня код написан прям среди акта делаю SQL. Это действительно копия command, подгресовый, внутривременной таблицы. В общем, ну такое, далеко не всякому такое нужно и оно очень изолировано, и оно не, там нет никакой бизнес-логики. Это реально просто такие вызовы, которые, типа, такой, собрать, короче, такую трубу данных, а потом уже внутри нее, как бы с результатом этой штуки уже выполняется нормальный акта. А так, чтобы прям велосипедить акта и SQL сложный мне обычно не приходилось. Единственное, что я хочу сказать, что вот переходя как раз к теме следующих изменений в акта, там стало еще, то есть сейчас по мелочам, добавили индекс хинты для MySQL, добавили еще более клевую работу с префиксами, которые в случае Postgres'а транслируются в схемы Postgres'а. Но что самое клевое, добавили гораздо более хорошую пышку работы с агрегацией. То есть раньше, вот почему в проекте, про который я сейчас упоминал, там отдельная совершенно, мой собственный DSL для работы с, из которого генерируется SQL, потому что там вот другая база, она аналитическая. И вот аналитический запрос писать на акта раньше была большая-большая боль. Сейчас как бы начали, вот начиная с третьей версии, подтаскивать довольно клевым образом всякие аналитические штуки типа оконных функций, типа фильтрации в игре и еще много всякого разного. Добавили union, accept и intersect. То есть, ну потихонечку выстраивается как бы достраивают вот этот вот DSL до строения запросов, до состояния, когда им можно будет еще аналитические запросы покрыть какие-то простые. Ну пока им какие-то отчеты для там, если это нужно в каком-нибудь дашборде показать, можно будет прям не выходя из SQL делать. Сори, не выходя из экта. Это, мне кажется, самое большое изменение, которое вот экта 3.0 приносит. Самое клевое. Все? Вроде да. Закончилось? В чате не клинкер спрашивает, нет ли случайно круда на экта в опенсорсе? Я уже ответил, что нет. А, сори, я прослушал. Там еще спрашивают, есть же фрагменты для сложных случаев. Это пример компромисса. Я уже упоминал. Фрагменты, да, фрагменты клево работают, если нужно позвать какую-то такую штуку, типа вот внутри запроса позвать какую-то штуку специфичную для базы, и да, это обычно хватает. И этим пользовался, это удобно. Но есть вот, как фрагментам нельзя сделать штуку, типа подрисовывая копию через временную таблицу, а потом еще что-нибудь куда-нибудь. Это фрагментам не напишешь, к сожалению, и главное, это совершенно не нужно. Потому что ну, типа, данный перенес, а потом уже можно вообще без всяких фрагментов нормальным актом уже в нужное место сходить, после того, как данный перенес. Предлагаю переходить к следующей теме, поддерживаю. Ой, следующая тема будет моя тема. Тема про то, как компания Etsy работает с документацией. Мне показалось, она интересной. Смотрите, в чем основная идея. У нас получается... Прости, пожалуйста, пока совсем далеко не ушли. Чем занимается компания Etsy? На самом деле, мы несколько раз ее затрагивали, ее блог. Слушай, этих модных блокчейн-стартапов сейчас знаешь сколько? Их все не упомнишь. Нет, это не блокчейн-стартапы, это peer-to-peer сайт для продажи ручной работы и каких-то старинных вещей. Какие-то уникальные вещи, которые ты хочешь продать, вот ты там размещаешь. Peer-to-peer как eBay и Aveta peer-to-peer или как Open... Это имеется в виду именно eBay. Я понятия не имею, что они там внутри делают, я ни разу не пользовался. По внешним словам внешних наблюдателей сужу. Итак, в чем у нас идея? Основная идея документации, чтобы ты хранил информацию про твою систему, но получается, что чем твоя система чаще меняется, тем быстрее твои знания об этой системе устаревают, которые сохранены в каком-то статическом файле. А чем быстрее устаревают знания, тем больше нужна документация, потому что приходят люди, там из соседнего проекта зашел человек, он понятия не имеет, что происходит, документация уже устарела. То есть, такая странная вещь, что чем ты быстрее развиваешь продукт, а все хотят это делать, тем хуже у тебя документация будет всегда. И в качестве примера они рассматривают, ну, этот пример идет сквозь всю статью, это про то, что вот у тебя есть секция про базу данных, которая объясняет, как у тебя база данных устроена, почему она устроена именно так, и есть отдельная секция, в которой поясняется, как надо с ней правильно работать. Ну, к примеру, там есть какой-то скрипт для импорта или экспорта, или чего-то еще. Вот. И получается, что есть, по крайней мере, две секции в этой документации. Это секция Y, то есть, почему описание концепции, описание, почему мы так сделали, и так далее. И концепция How, это как нам с ней работать, какие просто шаги надо предпринимать, или какие шаги надо запускать. И получается такая вещь, что чаще всего, когда меняется документация, Y-секция остается неизменной, а изменяется только How. И если правильно разделить эту документацию, и правильно ее, правильно управлять этой документацией, то получается, что устареванием документации можно управлять с помощью того, что ты How-секцию поддержишь в хорошем состоянии, с нужной тебе скоростью, а Y-секцию не трогаешь вообще. Вот. И для того, чтобы это удобно сделать, они придумали способ, как ввести систему How-документирования через Slack. То есть, Y у них описан где-то в Wiki, которая меняется редко, или вообще не меняется. А с How-секция они меняют ее из Slack'а и делают ее с помощью каких-то дополнительных команд. То есть, знак вопроса fye, то есть, for your information, это как раз запомнить этот кусок текста где-то в каком-то хранилище их внутреннем. И, соответственно, когда ты в следующий раз спросишь, а я не помню, как вот эту штуку сделать, у тебя эта система хранилища, точнее, бот, поищет в этой системе хранилище и скажет, слушай, последний раз эти слова упоминали вот в этом чате с таким-то timestamp'ом. Соответственно, если кто-то что-то написал, типа, вот правильно этот скрипт использовать таким образом, ему можно ответить реакции в Slack'е точно так же fye. Вот. И бот точно так же будет отслеживать. И когда ты ищешь информацию, у тебя будет она сортироваться по timestamp'у, соответственно, timestamp показывает актуальность, то есть, если это было 5 лет назад, скорее всего, все уже давно устарело. Вот. И оно сортируется. Все довольно удобно. Вот. Кажется, или это не документация, а какая-то херата? Почему? Потому что я открываю, когда документацию скажем, по API, ну, для простоты, да, или там по формату файла, я ожидаю, что там будет прям целиком записан, ну, как бы описано, что вот у меня есть структура такая-то, да, в ней какие-то поля с какими-то типами, с такими-то ограничениями, и т.д. А это какая-то, простите, херата. Конечно, ты говоришь про правильные системы, которые имеют интерфейсы и которые имеют проверенные интерфейсы, все задокументировано и протестировано, но это не про внутреннюю структуру компаний. Это больше про взаимодействие либо одной компании с другой компанией, либо, я не знаю, там банк клиентах работает. Когда у тебя есть сильно динамичный, меняющийся стартап, это все было сильно в кавычках, то у тебя подобного интерфейса не будет никогда. И документации не будет никогда. Соответственно, у тебя будет максимум... Давай, Настя. Мне вот проблема очень хорошо знакома, которую все отписывают. И у нас очень много документации, и действительно можно разделить ее на why и how. Собственно говоря, мы такие разделяем, типа отдельно описание высокоуровневое, что это за штуки, как они работают, схемы, описание там security considerations, и отдельно как запускать те или иные модули, как запускать те или иные команды, какие есть параметры. И параметры постоянно меняются. Новый релиз, новые параметры нужно постоянно дописывать. То есть мне это все очень близко резонирует. С другой стороны, я из этой статьи не очень поняла, при чем здесь Slack, и как ребята ботам в Slack дополняют свою документацию. Потому что получилось, у них source of truth разделился на два разных. Отдельно живет как бы вики, более статичная, а отдельно апдейты в Slack. У меня есть ответ на этот вопрос. Потому что их поделка, она никак не решает проблему документации. Она делает две вещи. Во-первых, она копит технический долг, и кто-то через лет пять им все это припомнит. А во-вторых, она является информационным поводом для написания клевой статьи в глобусе. Слушай, ну ты прям набрасываешь. Я абсолютно серьезно говорю, я сталкиваюсь с этой проблемой, которая описана у Этси регулярно. Не каждый день, конечно, но раз в неделю как минимум. Я захожу в вики, или у нас есть в проекте рейдмишки какие-то рядом со скриптами лежат, и они всегда устаревшие. Всегда от слова «ты можешь даже не читать» сразу выкидывай. Да, и вы сделали более нормальный код ревью. Я ищу в Slack, когда последний раз упоминали название этой тулзы. У вас проблема, ну, поправь меня, конечно, если я фигню несу, но в моем понимании у вас в компании проблема с бизнес-процессами. А если точнее, у вас не налажен код ревью, у вас пролезает код, который не документирован. Я совершенно согласен. То есть, если я вижу, в текущих проектах я сам лично стараюсь держать хотя бы рейдмисы своего проекта и какие-то вещи, которые закомичены в репозитории, менять одновременно кодом, который оно документирует. Если в репозитории кто-то вшлет pull request, который меняет документацию в нем, должен менять документацию в нем, она не изменена или не поправлена, я стараюсь это замечать. Ну, как бы, нужно просто строить процесс так, чтобы любое ревью таким было. Тогда люди рано или поздно такие ошибки будут находить. Это как с тестами. Если мы говорим, что мы можем замерить pull request без тестов, значит у нас никогда не будет нормальных тестов. Тут нужно делать ровно то же самое. Документация к проекту лежит в репозитории с проектом. Если мы мерим код, мы должны мерить изменение в документации и никаких исключений. Мы сделали еще проще. Мы просто убрали документацию. У нас сейчас, если есть документация, то это makefile, в котором есть make с какой-нибудь целью, и вот эта вот штука, она работает. А документации рядышком просто нет, а если есть, то она устаревшая. Подождите, а какой API описываете тогда, например? Если есть API, то это API, документированное и тестированное. Я говорю про how, а не вот то, что ты говоришь API, это уже больше к статической документации относится, а не к динамической. Ну, скажем так, новые API-иконцы добавляются все равно, по идее, они могут меняться. Это и тестами покрывается, и документацией. Я говорю про другую часть документации. Это как работать системой, как работать с скриптами временными. Я не знаю, как разворачивать новый параметр с помощью терраформа, а вчера туда дописали новый какой-то параметр и так далее. Вот этого ничего нигде нет. То есть есть в makefile какой-то цель, и все это максимум? Ну, обернется все тем, что чувак заходит, запускает скрипт, не понимает, как им пользоваться. Он набирает минус-минус help, все еще не понятно, как им пользоваться. Он заходит в Slack, и там такая долгая история, что, куда менялось и ничего не понятно, вместо того, чтобы когда ты меняешь свой скрипт, добавить нормальный help. Или, если ты... Если мы говорим про зависимости, например, как собрать тот или иной проект. Опять же, я запускаю makefile, а он не собирает проект. Я смотрю, на что он ругается. О, thatlib, короче, нет. Ну, например. Я ставлю thatlib, я снова запускаю makefile, он опять не собирается, в этот раз в другом месте. Вот, и я все эти 50 зависимостей должен выяснить по падениям makefile, вместо того, чтобы просто в ритме указать пакеты. Это бред. Слушай, пакеты и makefile, ну, ты прям, я не знаю, с 20 века пришел. Слушай, ну подожди. Тебе все еще валить, например. То есть, у меня, вот я сейчас открыл ритм своего проекта, ну, одного из своих проектов. У меня так все, более-менее, всех проектов, к которым я имею отношение, обычно что-то такое есть. Первое, как запустить тест и построить проект. Второе, куда и как он деплоится, какими командами. И дальше, какая-то минимальная документация для API, если она там есть, или как это запускается, если запускается. Я не хочу сказать, что у нас нет проблем в проекте, у нас есть там, типа, крупные места, недокументированные, есть какие-то крупные куски, которые, у которых этого нет, но просто потому, что я туда еще руки свои не прикладывал. Но вот все, к чему я прикасался, более-менее, оно все имеет такую структуру, потому что, ну, блин, иначе как вообще можно, не знаю, кого-то вообще в проект добавить, если все эти знания это просто word of mouth. А я определяю документацию на разных фрагментах, да. То есть, есть код, есть комментарии к коду. Это одна часть документации. Есть то, что попадает в ритм, и то, что хранится в одном и том же репозитории. И здесь я полностью согласна, что pull requests, которые меняют код, но не меняют эти куски документации, должны не приниматься. Но есть еще общая документация, которая может лежать не в репозитории, которая может лежать в вики, которая может лежать на documentation сервере, которая лежит отдельно. И как построить процесс так, чтобы код, ну, чтобы продукты релизились, и релизились версии документации? У меня есть еще одно... Хороший вопрос, давайте мы его запомним. Есть еще пример, почему описанный подход, это... Я, мягче слова, чем бред, не могу описать. Проект OpenWrt имеет прекрасную вики, которая поддерживается сообществом, там описано много интересных рецептов. Угадайте, что с ней происходит? Она устаревает. И проблема в том, что ты заходишь на эту вики и, не знаю, начинаешь искать, типа, а как мне сделать, чтобы мой роутер мог работать с файловой системой x4 на внешнем носителе, подключенном по USB? Ты начинаешь читать эту статью, ты делаешь, мне делаешь, делаешь, делаешь, и у тебя не получается, у тебя какие-то там странные ошибки лезут, неважно. В чем суть примера? В том, что ты уже нашел доку, она устарела, ты потратил время на то, чтобы по ней идти, и только в середине пути ты понимаешь, что дока-то невалидная, и тебе нужно там в слэке набрать какую-то команду, чтобы понять, в каких местах она невалидная, и в уме применить патч.",
    "result": {
      "query": "Ecto vs Scala GDBC comparison"
    }
  }
]