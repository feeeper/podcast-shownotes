[
  {
    "segment_id": "84bd2d00-b12d-4cc1-a157-edae80508ba8",
    "episode_id": "28a6fadf-6945-4d1b-b27e-73cb4b0d9fd3",
    "episode_number": 369,
    "segment_number": 9,
    "text": "Вот это все мы уже обсуждали тысячу раз, я поэтому не буду надеюсь останавливаться, и на этом статья заканчивается. Вот. А следующая статья, его следующая статья мне понравилась прям классно. Это Bill Jean Ibrahim, продукт менеджер Red Hat. Вот. Конечно же, я когда увидел, что он из Red Hat, я сразу подумал, что сейчас будет рассказ про то, какой Kubernetes классный, и какие классные решения делает Red Hat. Вот. Оказалось наполовину, правда, то есть Kubernetes правда классный, вот, но про Red Hat там мало. И рассказывается, ну есть немножечко, но не так сильно, как я ожидал. И в основном рассказывается про то, как Kubernetes появился, в чем его сильная сторона изначально, и меня прям удивило вот восприятие сильных сторон Kubernetes, а сейчас я дойду. Дойду, дойду, дойду, где-то у меня вот здесь было. Да. Написано, что самое главное, что сделала Kubernetes очень популярным, это наличие Health Probes, как это правильно перевести, тестирование здоровья и готовности подов для приложений. Автор пытается показать, что до Kubernetes это вообще было очень мало распространено, то есть как бы ну запустилось, оно как бы как-то должно запуститься и проявиться. В Kubernetes это прям обязательная штука, ну вы можете конечно не делать, тогда это станет опционально, но по факту там вас очень сильно об этом просят везде на всех уровнях, и вы добавляя там два типа проверки. Первое, что у вас ваш процесс стартанул и он уже готов, и он уже работает, а второе, это то, что ваше приложение уже там внутри себя все инициализировало, что надо, и оно готово к приему трафика. Вот когда обе проверки проходят, к нему, к этому приложению начинают поставлять трафик, запросы и так далее. Вот и автор говорит, что до Kubernetes это было очень редко и мало используемо, а после Kubernetes это добавили даже во все языки, библиотеки и приложения, и теперь это стандарт de facto как бы используется везде. И типа это основной плюс к Kubernetes был на, ну в смысле люди начинали переходить, потому что это было очень полезно. Ну я тут вообще не согласен, конечно. Давай, Саша. Мой суть интересный, но мне кажется, это немножко притянуто за уши. Мне тоже, да. Ну то есть у Kubernetes и Docker это очень яркий пример, как мы это называем, набора популярности через разработчиков. Когда ты создаешь вот эту агрессивную компанию, что вот у нас новая технология, она такая классная и очень активно пиаришь ее на Hacker News и вообще в интернете. И я с автором не согласен. Я думаю, что Kubernetes выехал благодаря этой стратегии, и основная мысль, которую несли те материалы и до сих пор Kubernetes несет и все книжки в Kubernetes несут, это то, что вы декларативно описываете свою платформу, свое облако, а дальше он за вас сам. Он сам починит, если что-то упало и так далее. И вам не нужно там руками чинить конкретности, и вам не нужно знать, где запущен конкретный контейнер. Вот как раз про это второй пункт. Он говорит, вот за это, именно это вторая вещь в Kubernetes, которую все приняли с большой благодарностью, потому что, если вы следуете правильным методикам, то есть вы подготовите заранее свое приложение, вот это 12-факторное приложение. Через конфиги вы получаете все настройки, в смысле, через переменные окружения получаете все конфиги и готовы всегда, что ваше приложение будет убито и начнет работать на другой машине. Если вы ко всему этому готовы, приготовили свое приложение, то вы автоматически получаете массу плюшек в виде того, что за вас подумают, где это запустить. Если вы указали еще какое нужно количество ресурсов, еще и подумают, сколько влезет на один инстанс. Если что, убьют, перетащат в другое место, стартуют в другое месте. И вот эта вот автоматизация жизненного цикла приложения, она на самом деле очень-очень хороша для всего. Пока у нас Kubernetes не было, мы это делали самостоятельно своими руками. И 12-факторное приложение. Опять же, для всего со звездочкой. То есть для каких-то задач Kubernetes отличный инструмент. Я могу представить, как под тонну микросервисов стейтлес надо или может быть, как называется, софт стейтам, типа в памяти, может быть немножко и на файловой системе, но не очень важным стейтам, как очень клево рулить толпой микросервисов. Насчет Subd, сейчас люди пытаются запускать Postgres на Kubernetes. Насколько это удачная идея во всех-всех-всех случаях, я не очень убежден. В каких-то случаях это может быть окейная идея, в каких-то это может быть кошмарная идея. Надо опять же смотреть по ситуации. Да, согласен. Postgres такой, что Kubernetes — это инструмент. Не надо все задачи решать инструментом, который для всех задач. Таких не бывает. Следующий пункт. Он нашел 4 пункта мощных в Kubernetes, которые он считал, что толкнули к Kubernetes в период к использованию. Следующий — это то, что вы определяете, сколько ресурсов надо, и дальше уже Kubernetes решает, сколько на каждом институте запустить ваших приложений и так далее копий. Мы это уже обсудили. И последнее, что конфигурации. Очень легко конфигурировать ваше приложение, и конфигурация оказывается внутри вашего пода. Автоматически вам не надо об этом думать, вам не надо думать, где установить консул или еще что-нибудь такое, для того чтобы получить то, что вам надо получить. И я никогда не рассматривал Kubernetes в таком ключе, но похоже, да, есть такая плюшка. И дальше он переходит к тому, как правильно расширять. Он немножко пишет про операторы, как бы вы можете с помощью операторов сделать все, что угодно. Дальше начинаются уже более высоковысоковоуровневые решения, которые позволяют что-то делать внутри Kubernetes. То есть первым он рассматривает это Service Mesh. Это концепция того, что вы можете с помощью декларативных конфигов определить, кто с кем может связываться, каким образом идет трафик, то есть data flow может быть четко определен с помощью конфигов. Не так, что Kubernetes предоставляет, у него есть много ограничений, а именно так, как хотите вы. И каким образом ваше приложение попадает снаружи трафик? Это PRV, второе это KNATE. Мы несколько раз упоминали в подкасте это решение о… На самом деле там как бы два разных решения внутри. Первое это Knative Serving. Это когда вы делаете как сервер-less приложение. Это вы пишете приложение, которое по умолчанию имеет ноль подов. То есть оно не запущено в вашем кластере. Но если приходят запросы какого-то типа, который вы заранее описали, то может быстро подняться какой-то контейнер, ответить на этот запрос и выключиться. Или выключиться не сразу, подождать еще каких-то запросов и так далее. В итоге получается, что у вас бесконечно скалируемо, потому что в нем состояния внутри самом нет, и вообще никак не будет. Оно будет где-то снаружи. Ну или его не будет, если у вас совсем простой Hello World. Но зато вы можете масштабироваться до очень больших величин от нуля. Не от единички или двоечки, как мы все привыкли обычно. А вот реально от нуля. А второе это Knative Eventing. Это как следствие первое. То есть если вы можете сделать обработчик каких-то событий, вы можете очень здорово реагировать на любые сообщения. И соответственно какие-нибудь попсабы или обработчик каких-то потоков данных или, я не знаю, читатель и писатель в кавку. Все что угодно вы можете сделать с помощью того, что вы просто описываете приложение. Говорите, у меня вот в этом контейнере есть приложение, которое с помощью HTTP умеет общаться. Вот. А дальше Knative уже сделает все и масштабирование. Он будет следить сколько надо поднять инстанцев и так далее. Все это через конфиги декларативные вы можете описать. Вот. Это полезная штука. Мы ее уже обсуждали. Следующий это Dapr. Дапр. А да, Microsoft Dapr. Классное название, мне нравится. И он это сложная концепция. Я ее никогда не использовал и я в районе мало читал. Вот. Но здесь вкратце описано, что это вещь, которая позволяет, она вообще не для Kubernetes, но она в том числе работает в Kubernetes и она позволяет связать много разных приложений в единую систему, включая Kubernetes тоже. Вот. Вы описываете разный способ Control Flow и Data Flow. Вы описываете, кто с кем соединен, какие-то типы коннекторов. То есть тоже такая высокоуровневая концептуальная модель, когда вы декларативным образом описываете состояние своей системы идеального кластера, включая Kubernetes. То есть вы, например, с помощью Dapr можете описать, что вот у меня снаружи есть какой-то источник данных. Внутри Kubernetes у меня запускается потребитель этих данных. Потом я хочу в какую-то базу данных, которая вот здесь стоит, так что-то складывать. Вот. И это все делается на уровне декларативных конфигов и оно из этих же декларативных конфигов каким-то образом поднимается. Вот. Соответственно, в Kubernetes Dapr он устанавливается внутри каждого пода как Sidecar, то есть это как дополнительный маленький контейнер, который отвечает за соединение сетевые, рассылку конфигурации. Вот. Ну то есть что-то вроде Service Mesh. То есть в Service Mesh тоже у вас есть Sidecar, внутри каждого пода запущен. Вот. И все это работает в Kubernetes через операторы, опять же. Вот. И с помощью Dapr он Dapr немножко дальше обсуждает. Пак-пак-пак-пак. Сейчас, подождите. Да, он рассказывает каким образом писать операторы. Это мы опустим. Это мы опустим. Это мы опустим. Да. Вот. И он говорит, к чему мы приходим. То есть какие я вижу тренды. Прямо сейчас говорит, я вижу, что Service Mesh, соединение сервисов. Как это перевести? Я не помню, как мы в прошлый раз придумали классный перевод. Вот. Мишанина сервисов. Солянка. Солянка, да. Она изначально была очень слабо распространена. Сейчас получает все больше распространение, потому что многие пробуют, многие понимают, как это работает, обкатывают технологию и избавляют ее от багов. И сейчас она все больше получает применение и использование. Вот. Затем даже на уровне Service Mesh уже появляется какая-то стандартизация. То есть можно несколько разных сервисов Mesh объединять вместе в разных системах. На уровне Service Mesh появляется все больше L7 протоколов для, скажем, последнее время появились для MongoDB, для Dynamo, для Zookieper, для MySQL, ReadyScavki. Вот. При этом прямо на уровне этого Service Mesh можно делать всякие разные хитрые штуки. Например, автоматический encryption, какое-нибудь фильтрование трафика, трансформация трафика на лету. Например, у вас есть запросы из Scavki, которые потом могут преобразовываться в HTTP-запросы куда-то еще. Ну, то есть как бы все, все что угодно, что вы хотите придумать. И, например, он там рассматривал, что вы можете на уровне Service Mesh снимать трафик, записывать его куда-то для того, чтобы его либо повторять потом и так далее. То есть как бы сейчас есть много решений для такого. Это как бы на уровне Service Mesh делается очень легко и просто, потому что это как бы часть, часть протокола. Вот. Так, так, так, да. На уровне HTTP некоторые они начали вводить HTTP Cache. Это маленький sidecar, который работает внутри Kubernetes, и он позволяет делать кэширование, как вы хотите, так как вы можете его настроить, зная что у вас за приложение, какое у него использование. Вот. И это позволяет вам делать это кэширование не на уровне приложения библиотек, а они могут быть очень глупыми и плоскими, а кэширование будет в маленьком контейнере, которое рядышком крутится с вашим приложением. Вот. И мы в итоге приходим к тому, что... так, так, так, да. Вот это. А еще штука это Cloud State. Это решение, которое позволяет сделать... то есть я сам его еще не смотрел, но хочу поглядеть. Решение примерно следующее. Например, у вас есть там 20 подов, запущенных в кластере, и они работают с каким-то общим стейтом. Чисто теоретически вы должны придумать, каким образом они будут между собой обмениваться сообщениями для того, чтобы синхронизировать этот стейт. И получается, что на самом деле концепция не слишком простая. То есть, ну, конечно, от стейта зависит, что вы хотите синхронизировать. Вот. Но сама по себе вот эта распределенная система получается достаточно сложная. Однако есть несколько общих шаблонов, и если вы понимаете, какому шаблону принадлежит ваше использование, вы можете взять и поставить рядом с каждым вашим приложением какой-то маленький сайт кар, который будет бежать, и они, вот эти сайт кары, вот между... между собой взаимодействовать и обмениваться данными, чтобы поддерживать это общее состояние. Вот. А ваше приложение, оно будет очень простое. То есть, оно будет обращаться к локальному контейнеру, по локал хасту, там, и как будто бы это у него там, не знаю, кусок памяти там есть или там, я не знаю, какая-то локальная база данных. Хотя на самом деле она будет, хотя и локальная, она будет распределенная. И получается такая хитрая система, что ваше... оно не будет работать с этой сложностью, оно будет работать со сложностью, как будто бы это как локальный файл. В то же время вот эта система, она внутри себя содержит эту сложность, потому что за вас ее уже написали. То есть, как дополнительная инфраструктура, которая уже написана, и в теории, если ее обкатают и там найдут все баги, это такое неплохое решение. Большую часть вопросов с распределенным стейтом можно закрыть с помощью подобного решения. Вот. И в итоге получается очень хитрая система, что чем дальше мы идем по вот этому стеку технологий Kubernetes, тем больше мы приходим к тому, что на Kubernetes накручивают, навинчивают очень много мелких решений, которые сами по себе, ну такие приятные неожиданности, но мелкие. Но когда они все вместе будут запущены, получается, что у нас Kubernetes отвечает как платформа. Она отвечает за кучу всего, начиная с перидеплоя, проверки работоспособности, работу с ресурсами, работу с тестированием вашего приложения постоянного, там скедалинг и так далее и так далее. Вот. Поверх этого вы работаете уже, у вас есть какой-то уровень сетевой, когда эти запущенные контейнеры между собой взаимодействуют так, как вы написали в каком-то конфиге, где бы они ни были, в какой бы части света ни были, вы можете иметь какую-то такую сложную систему, но они будут видеть, как будто бы они запущены на локальных машинах. Вот. Поверх этого вы имеете систему, например, типа Dapro или Knative, которая позволяет соединять их в более сложные вещи, например, работать с какими-то протоколами, там скакавка по HTTP или работать с источниками данных, которые наружу куда-то ходят, но опять же незаметно для вас, то есть ваше маленькое приложение, работающее внутри контейнера, montakes, оно будет работать как будто бы с локальными файлами, или вот это распределенное состояние, о котором мы говорили. Они работают на очень простой модели, что это насколько-то локальный файл или это как будто было запрос в HTTP, хотя это на самом деле позади это не локальный файл, а распределенное состояние и не запрос по HTTP, а это идет в кавку и там как-то складывается, но это все не видно вашему приложению и поэтому вашじ приложение получается достаточно простым и очень маленьким и легко обновляемым. И его очень легко раскатывать и рестартовать, а все эти остальные кубики, которые стоят вокруг вас, они все тоже маленькие, они все независимые, их очень легко обновлять, их очень легко заменять одну на другое, потому что есть какой-то стандарт по протоколам и конфигурациям и получается такая очень классная штука, что наши приложения, наши как разработчиков, они будут очень сильно упрощаться из-за этого. Уже не надо раз включать внутрь своего приложения, уже не надо думать о ой, блин, я тут версию кавки библиотеки, она там плохо и так далее, вы просто по HTTP обращаетесь. И становится жизнь приятнее, а волшебство шелковисти. И он говорит, к чему это может привести дальше. Он говорит, что мы изначально уходили от каких-то монолитных архитектур, потому что раньше не было таких сетей, сложностей и так далее, и WS не было. Мы постепенно пришли к микросервисам и это позволило нам на уровне микросервиса ограничить область доменных проблем и таким образом облегчило нам жизнь. Дальнейшее движение, оно пришло к какому-то фасу function as a service и так далее, но очевидно, что это не очень удобно, потому что, ну хотя оно скалируется бесконечно, вот эти функции, вот, но доменную область в одну функцию не заключить, нужно какие-то все равно более сложные концепции. А вот подобные, которые я только что писал, концепции, которые он назвал Mecha architecture, то есть multi-run time это одно название и mechanism это вот получается Mecha, Mecha, не знаю как это правильно это износит. Вот, это когда у вас на самом деле очень сложное решение, но оно состоит из маленьких кубиков, которые друг с другом работают по как-то стандартизованным протоколом и ваш кубик, который вы программируете, он может быть на любом языке, он будет работать либо с локальными файлами, либо с локальной сетью и вся сложность будет скрываться уже в других кубиках, которые вы напрямую не должны программировать, это уже запрограммировано для вас. Вот, и он говорит, что это скорее всего вот в этом направлении пойдет и дальше и это такой очень интересный вывод. Вот, третья статья вообще неинтересна, мне не понравилась, я ее не буду описывать. Мне статьи очень понравились, прям классные, я думаю, может быть стоит почитывать этот журнал дальше, там неплохо. Вот. Какие для себя выводы мы можем из всего этого сделать? Я имею ввиду, ну было сказано очень много слов, самое главное в сухом остатке, потому что мы, если мы на кубере, то нам не нужно думать про рафт или как бы ты это резюмировал. Я, если честно, у меня смешные чувства, с одной стороны, меня очень зацепила идея про когнитивную наружку и я понимаю, что если есть маленькая команда, в которой количество человек не очень большое, то введение кубернетеса и не дай бог еще кубернетеса с кучей вот этих кубиков, это настолько увеличение когнитивной нагрузки, что я даже боюсь это описать. С другой стороны, использовать кубернетес, который кто-то сделал и вот подобные кубики уже собрал в том же WES или в гугле, возможно, на самом деле очень сильно снизит когнитивную нагрузку, если все люди будут примерно представлять, что это такое. То есть начальное образование надо для того, чтобы понимать, как оно работает, как его деплоить и какие плюсы и минусы подобной архитектуры. Но с другой стороны, когда мы с AWS начинаем работать, нам же тоже надо понимать, как это работает и что мы делаем. То есть здесь получается то на то, но зато плюшечки очень-очень приятные. Мы пока не в этом далеком будущем, у нас пока нету подобных систем и пока вот таким образом кубики складывать не получится. Но вполне возможно, что через, я не знаю, лет пять у нас подобные системы будут появляться в Amazon, в Google, в DigitalOcean и так далее. И надо будет это использовать тогда, конечно. Я вот когда такие статьи и прочее про куберный тесс читаю, обычно всегда восхваление, что там кубики, что оно упрощает нагрузку, снижает. Но сейчас Ваня тоже хорошо отметил то, что команда небольшая. Вот мы как раз с этим столкнулись, когда прошлый стартап был с онлайн игрой. Мы думали что-нибудь заюзать такое, чтобы не сами менеджить вот эти поды и прочее. И как раз тогда куберный-то смотрели. Тогда в ОВС его еще нельзя было поднять, насколько я помню, это 14-15 штулю год. Он тогда вообще по-моему в бетке был. И вот мы у себя разворачивали и все вот эти плюсы, то что он сам поднимает, переподнимает, конфиги декларативные, все классно-классно, все упирается и разваливается в тот момент, когда что-то в нем не работает. Вот там банально у нас и TCD падал, непонятно почему, коннект к нему не проходил. И вот это по сути все плюсы может перечеркнуть. Сейчас конечно попроще, если не самому этого хостить, а там вот ОВС как сервис взять, они типа там все это управляют и скорее всего оно не упадет. Но это вот тогда тут минус получается, потому что ты надеешься на чужую инфраструктуру и что они свою работу хорошо сделают. Тоже такой сомнительный. Как с Amazon тоже, когда ты с CEE начинаешь взаимодействовать, ты же тоже на них надеешься. Но я полностью согласен, что я отлично помню, когда вот Kubernetes только появлялся и мы на него начинали глядеть, я понимал, что это добавляет сложности. Но мне очень нравилось, потому что базовые концепты лежат в основании Kubernetes очень классно. И это на самом деле классная система, придуманная, продуманная классная система. Но когда я вот сейчас гляжу на эти кубики, которые мы сейчас только что обсуждали, я понимаю, что мне тяжело их осознать до конца и увидеть всю картину целиком. А вот скажем новичок, который приходит после института и начинает работать на этой системе. Но это же полная магия. Но это магия настолько высокого уровня, что он через 10 лет использования не будет понимать до конца, что происходит у него в системе. И хорошо ли это или плохо, я не знаю. То есть его код будет простым, чистым, маленьким и спокойным. Но он не будет понимать до конца всю свою систему и не будет понимать, как она работает. Я не знаю, хотим ли мы в это будущее или нет, но похоже мы туда идем. Может быть это и не плохо. В каждой компании есть специалисты разного уровня и возможно это как раз плюс то, что разработчик пишет свой сервис, свой микросервис, свое приложение. Он знает, как это нормально написать, чтобы оно потом нормально развернулось под Kubernetes. Нормально описывает все правила и конфиги для сборки образа, а дальше там DevOptions и прочее. То есть это уже на них. Возможно ему и не надо вообще знать, как там Kubernetes работает, что там у него внутри и прочее. Вань, ты сейчас начально, я просто отвлекся, долго вас так и держал, сказал такую крутую фразу, что Kubernetes требует начального образования. Я так себе представил, что такие едут начальники в начальной школе, изучают Kubernetes. Ну, а мы же к этому идем. Да, так и будет. Сразу после уроков природоведения. Так, сегодня у нас Kubernetes. Я отлично помню, как у нас в институте лекции были про виртуальную машину Java. Ну, это была интересная идея. Я никогда с Java не работал, но сама концепция преподавания Java и виртуальной машины, как оно опосредованно работает. Классно, классно. Классно то, что есть люди, которые могли это преподавать. Я с глубинки. У нас специальность хоть иротичная, но специалисты, они всем довольно молодые. И вот эти внутри, там языка виртуальных машин никто не преподавал. То есть там у нас C++ преподавали, ИС, и там максимум, что было, написать свой класс про имплементацию строки. Вот максимум такое. Ну что ж, я думаю, мы продуктивно поговорили про Kubernetes. А следующая тема-то Павел Андреи. Она про Wordle. Что бы это могло быть? Ой, я не туда посмотрел. Пропустил что-то. Я что-то пропустил. Я Леша пропустил. Леша, как я мог? Да, ты вообще пропустил, но мы тут уже параллельно и JEPs насудили новые. И еще тут я другой уже час ввязался. В общем, новость такая более короткая, чем вся история про Kubernetes. Про то, что сейчас, когда мы все работаем вдаленно, если мы все не проводим вдаленно, если мы общаемся вдаленно, есть такие ситуации, когда вы собеседуете с одной человеком, а работа выходит другой. И вот тут история, которая развивалась буквально в прямом эфире с апдейтами каждые полчаса, час, два интервала, когда собеседовали с одной человеком, а работа вышла другой, и выглядит он не так, и общается он не так, и история, которую он рассказывает, она не такая. Ну и самое главное, работать не может. И в течение, я так понимаю, что на второй день он уводился, когда его сделали звонок с HR, он еще до того, как ему что-то сказали, понял, что сейчас зашел звонок, его раскрыли, он сказал, что я увольняюсь и положил трубку, и с тех пор трубку не брал. Вот. Морали никакой нет просто, что вы думаете по этому поводу? Я поспрашивал знакомых HR, и они подтверждают, что это очень так.",
    "result": {
      "query": "Kubernetes architecture evolution"
    }
  }
]