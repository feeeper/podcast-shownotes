[
  {
    "segment_id": "a8d4939e-6a7a-4380-956a-9554e11d5db9",
    "episode_id": "05332033-da09-4b8b-a467-6c17c0a293e9",
    "episode_number": 149,
    "segment_number": 11,
    "text": "Собственно, очередной релиз капки, казалось бы, ну, релиз, релиз, капка периодически релизится, что-то такого интересного. Однако, конкретно в капку 0.11 завезли некоторые интересные вещи. В частности, туда завезли exactly once delivery, то, что они называют exactly once delivery. И туда же завезли транзакционные апдейты, транзакционные посылки, сообщения. Собственно, и то, и другое подняло некоторое количество непонимания в интернетах, на что были посты на хакер ньюз, на которые потом был ответ. Точнее, так, на начале был пост в блоге Confluent, потом там был... От которого бомбануло хакер ньюз, после чего jkapps, один из основателей капки, сделал ответный пост, в котором он попытался все разложить по полочкам. Что такое exactly once вообще в случае капки? Я даже вначале отойду от того, что такое вообще в релизе, как вообще капку пользуют. Капка прекрасна тем, что она сама по себе, просто с самых первых версий, она умеет в at least once. То есть, если вы в неё что-то положили и вам пришел окей от брокера, то это можно будет выгрести. И больше того, она обладает уникальным айдишником, поэтому на стороне выгребателя было довольно легко обеспечить тот факт, что сообщение потребляется строго один раз, а просто если там вот этот оффсет, вместо сообщения, которые выгребли, класть, например, в ту же самую базу данных, куда результат выгребания или каким-то образом патентные операции делать. В общем, на стороне потребителя это уже было довольно легко сделать надежным. Однако на стороне продюсера там было сложнее, то есть если что-то где-то отвалилось, одно и то же сообщение могло быть списано дважды. На самом деле люди уже это обходили путем того, что не знаю, например, на стороне консюмера вводили дополнительный айдишник, который на стороне консюмера капки дедуплицировали. Или, например, продюсер перед тем, как стартанут начать снова продюсировать, смотрел в конец опика и проверял, что там с последними сообщениями, которые он посылал. То есть, там были варианты, как люди делали. И вот сейчас, наконец, к версии 0.11 ребята дозрели и сделали то, что они назвали idempotent producer. На самом деле, если читать их внутренние вики по разработке, там гораздо меньше вопросов возникает, что они сделали. То есть там нет никаких exactly once claims, которые... Ну, то есть exactly once, в чем проблема с этим клеймом? Потому что это такая гарантия, которая не может быть у какого-то компонента системы. Вся система должна быть написана правильно. И вот то, что в картке сейчас сделали, оно не исключает того, что компоненты системы должны себя адекватно вести, чтобы получить exactly once. Просто система дает возможность его построить с рамительно малой кровью. По факту, что было сделано, была сделана детупликация сообщений на стороне брокера на основе id-шников. По большому счету, это очень похоже на TCP. То есть, когда TCP детуплицирует, у каждого сообщения есть просто инкрементирующийся id-шник, который, если брокер переведет второй раз сообщение с таким же id-шником, он просто его дискардит. Но есть особенности. В TCP сессия, если она отжила, то она... если у нас случился дисконнект, то у нас никто снова в эту сессию не придет, она всё, её просто больше нет, не будет и забыли все эти инкрементирующиеся счётчики. И до свидания. В случае с Kafka у нас детупликация нужна в первую очередь для того, чтобы рестартанувший продюсер, если ему нужно что-то перепослать, или продюсеру, у которого отвалился коннект с Kafka, если ему нужно перепослать послание сообщения, он не понял, дошло он или нет, чтобы он мог его перепослать и не беспокоиться. В случае дисконнекта с Kafka, там в принципе даже просто инкрементирующиеся id-шники, которые в интервью сообщения есть, оно нормально будет работать, особенно если их прямо в интервью сообщения держать, оно будет реплицироваться в механизм Kafka и просто тот другой брокер, который перехватит потребление, он просто сможет из сообщения достать последний известный sequence number и просто по нему отсекать. Ну там sequence number на каждого продюсера на самом деле нужен отдельный. То есть там, ну да, есть проблема, что если у нас продюсер рестартует, собственно, решают её довольно простым способом, просто вводят ещё пару id-шников. В первую очередь у каждого продюсера есть уникальный id-шник, который, однако, не должен меняться между рестартами. Во-вторых, есть id-шник каждого сообщения, а во-третьих, есть id-шник поколения. Если продюсер рестартует, просто инкрементируем id-шник поколения. И соответственно, если кто-то где-то, какой-то из брокеров уже увидел id-шник поколения, то никаких пересылок из прошлого поколения уже быть не может, поэтому если где-нибудь какой-нибудь задремавший, у нас там, не знаю, предыдущий продюсер не упал, а задремал, то то, что он потом будет пытаться ссылать, просто пойдёт в никуда. Можно вопрос? Да. Я не очень понимаю. Ну, допустим, вот у нас продюсер послал последнее сообщение, и потом перезагрузился. Он рестартует, и у него нет информации, дошло оно или не дошло. Ну, например… Смотри, оно у него может быть, может не быть. Да-да-да. Как у продюсера написано. Допустим, его нет. Но я имею в виду, например, он не получил ок от сервера, или он не записал этот ок, или ещё что-нибудь. В этот момент следующее поколение продюсера хочет удостовериться, что он послал, и он посылает заново то же самое сообщение. У этого сообщения будет уже следующее поколение, следующая генерация. Не обязано. Не обязано. Это по-хорошему наводку по имплементации продюсера, по-хорошему, если ты прям хочешь совсем сильный гарантий доставки, ты на продюсере где-то пишешь какой-то локальный стораж, пишешь сообщение, что вот я хочу его отправить, сразу присваиваешь ему там же айдишник, и уже при рестарте ты его перепошлёшь, а потом уже заинкрементируешь в новое. То есть ты уже новые свои сообщения будешь ставить новым поколением. Понятно. То есть ты добьёшься, чтобы тебе брокер ответил ок, и потом ты уже будешь менять поколение. Ну то есть, грубо говоря, брокер тебе предоставляет инструментарий для того, чтобы детуплицировать твои айдишники без особой боли. То есть раньше боли было больше. Детупликацию нужно было делать или уже в некавке, или как-то хитро писать продюсеру, чтобы он там проверял, что в топике уже есть. Сейчас просто сделали так, что просто дали ещё один инструмент. Это не отменяет того, что нужно включать голову, и продюсер не должен продюсировать одно и то же сообщение под разными айдишниками. Ну то есть думки не поменяли. Да, понятно. Собственно, Джейк Репс это в своём посте именно это, в общем-то, и описывает, и что там было на хакер-ньюс очень сильно бомбило, что как же так, экзакриванс не бывает, смотрите, там типа теряем о двух генералах, но на самом деле, теряем о двух генералах он не совсем об этом, а о том, что как раз экзакриванс — это такая штука, которая должна быть во всей системе предусмотрена. Ну и вообще он там довольно забавная статья. Там даже не теряем о двух генералах, там примеры о двух генералах. То есть, на самом деле, экзакриванс — это штука, которая сводится к консенсусу. И консенсус он в определённых ситуациях возможен, на практике пока не менее возможен, в нашей практической сработает в интернете, консенсус возможен. И соответственно, экземпляр 1 тоже возможен, просто нужно, ну, его нельзя получить из ниоткуда. Вот. Вторая вещь, которую сделали, это транзакционная отправка. Тут я буду меньше вдаваться в детали, потому что, ну, детали там сложнее сильно, а в двух словах это двухфазный коммитт на топиках Кафки. Плюс специальные кастыри, чтобы продюсерам, ну и наоборот, консюмерам не было видно то, что пока не должно быть видно, то что не до конца закоммичено. Или чтобы было видно, если они этого хотят. Собственно, это очень клевый инструмент, потому что теперь вот в Kafka Streams, которая вот такая базовая библиотека, она вот, эта фича превращает ее практически в аналог того, что есть в каком-нибудь Flink'е. Ну, то есть Flink умеет с батчами работать, но идея в том, что Flink, один из плюсов Flink'а, в том, что у него были, ну и до сих пор есть достаточно сильные гарантии в плане того, что процессится и что там, в случае, если что-то посередине отворилось, оно будет дальше тоже надежно процесситься, и что там можно много-много Stage Pipeline'а построить, и оно все будет атомарно, exactly once, процеживаться через Flink и там все будет прекрасно. Вот, а в Kafka самому по себе так раньше было использовать тяжело, по описанным выше причинам. А вот теперь еще, поскольку вот можно транзакционно сразу в несколько топиков писать, в частности, можно свои offset, то есть можно, ну для чего это нужно? Для того, чтобы всякие метаданные можно было транзакционно писать в соседний топик. То есть, там, не знаю, подребили какое-то сообщение, что-то с ним сделали, результат записали в еще один топик, а метаданная о том, что мы результат записали в тот топик, записали в топик с метаданными. То есть такого рода вещи можно делать, и за счет этого можно очень сложный процессинг строить просто на Kafka и Kafka Streams клиенте. Я не знаю, насколько это востребовано, но в целом фича с транзакционным коммитом в много топиков, она наверняка приятная, и тот же самый Flink может упростить, наверное. Ну и в целом это приятно тем, что можно пользоваться только одну систему вместо, не знаю, Kafka плюс Zookeeper, плюс там все равно будет руками Zookeeper, плюс там еще какой-нибудь Postgres сбоку, вот это все зачастую можно будет редуцировать просто до Kafka. Вот, как-то так. И это по задверениям все еще очень быстро работает, что в принципе удивительно, потому что там двухфазный коммит не на каждое сообщение, а Kafka же агрессивно бачит, поэтому ей двухфазный коммит переживать не так страшно. Собственно, это основные хайлайты того, что вышло. Можно еще раз, почему переживать ей не так страшно двухфазный коммит? Потому что у тебя сразу на много сообщений он идет. У тебя Kafka очень-очень агрессивно бачинком занимается, поэтому у тебя перформанс хит, который у тебя overhead на двухфазный коммит, он размазывается, то есть ты не каждое сообщение коммитишь. Да, понятно, то есть они есть, просто за счет того, что он его размазывает на несколько сообщений, они не так критично сказываются на скорости. Да, ну то есть я честно скажу, я в имплементацию двухфазного коммита глубоко не смотрел, я так открыл, пропозал, увидел, что там куча деталей, для себя это по большому счету двухфазный коммит, потом поверх того, что уже реплицировано и типа очень надежно, то есть там проблемы двухфазного коммита, типично они не должны вылазить, и вроде все должно быть хорошо. Ну то есть по сути они добавили транзакцию на добавление в несколько топиков и дополнительно несколько счетчиков. Да, да, да, то есть это просто добавили примитивы, которые позволяют делать хорошо, плюс в Kafka Streams клиенте научились их использовать, научили его их использовать. Но при этом надо всё равно правильно ставить продюсеров. Ну если у тебя Kafka Streams, то я подозреваю, что он скорее всего всё правильно делает. Там же и коннекторы ко всему есть и вот это всё, то есть тут интересный момент во многом в том, что уже очень много инфраструктуры написано, то есть зачастую, это же очень редко нужно писать продюсеров руками. Зачастую, ну ладно, привираю, если ты принимаешь трафик от какой-нибудь HTTP, то да, там нужно писать. В принципе, если ты трафик принимаешь с Java, то ты Kafka Streams клиентом можешь использовать всё равно. Я, честно говоря, с ним не работал, но в общем, что я пытаюсь сказать, что вся почти инфраструктура вокруг Kafka, если она ещё даже не подцепила эти изменения, очень быстро их подцепят, и мир станет немножечко лучше. Это сильно может упростить жизнь, я согласен. Что ещё могло бы сильно упростить жизнь, это процессоры, которые не содержали бы багов. Об этом нам чуть подробнее расскажет Ваня, а я подержу. Хитрый. Я больной котик, пожалейте меня. Новость состоит в том, что в процессорах семейства Skylake и Cadillac была найдена ошибка в режиме, в многопоточном режиме в хитрый трейдинге, и у нас здесь несколько статей будет прикреплено в шоу-ноты. В них с разных сторон рассказывается эта история. То есть, во-первых, это интересная статья про то, как, от первого лица, как я нашёл баг в процессорах, и мне она очень понравилась. Во-первых, она написана как детективная история, и начинается она с очень весёлых повествований, что когда студенты пишут свою работу, свои курсовые и какие-нибудь контрольные, они всегда обвиняют систему в том, что у них что-то не работает. Вот у меня операционная система сбоит, вот здесь у меня процессор как-то не так работает, или компилятор. И автор говорит, что я нашёл единственный случай, когда это было на самом деле так, и дальше он рассказывает про то, как он разбирал ошибку, у них была программа написана на Acamly, и он разбирал ошибку, которая возникала у клиента, у их клиента при работе на Skylake семействе. Он сам разработчик Acamly. Хорошо. Не, ну, да, это так. Очень весёлая, смешная история про то, как он это делал, причём довольно поучительная, но мне очень близкая, потому что я подобные вещи делал для Льюберуса, ну в смысле разбор ошибок до железки. И две остальных статьи, это как раз более подробное пояснение о чём, и рассылка в Debian, как это называется, там почтовый лист, как это называется по-русски? Мейлинг-лист по-русски. Список рассылки. Да, спасибо большое, список рассылки. Вот, поясняющая ошибку. Я не знаю, стоит ли опускаться в детали, то есть проблема проявляется только-то когда вы загружаете все ядра вашего процессора, и причём загружаете, то есть процессов, активно кушающих CPU больше, чем количество ядер, когда он включает гипертрейдинг, и это может вызвать какие-то проблемы. Там интереснее. То есть, во-первых, это должен быть либо Skylake, либо Kaby Lake, про последнее, кстати, очень мало слышал. Во-вторых, действительно, процессор должен быть загружен, так, чтобы гипертрейдинг был активен, и в программах, которые в этот момент выполняются, там должен быть короткий цикл, который одновременно работает с 8-битным регистром, там, AL, AH, вот этими вот всеми, и в этом же коротком цикле с большим регистром, который включает в себя тот малый как часть, ну, то есть, ты одновременно работаешь с EX и AH, например, по-моему, так. Тогда у тебя процессор, он вычисляет ерунду, и всё... Ну, короче, попортится содержимое регистров. И наступает Puppies Day. То есть, вы заранее точно не можете знать, что у вас пойдёт не так, потому что, ну, понятное дело, никто не лезет в кишочки всех своих программ, чтобы посмотреть, не наступает ли вот такое событие, когда эти два регистра в цикле используются в одном месте. Ещё интересно, что такой код генерируется, вот как я описал, он генерируется ГЦЦОЙ, но не генерируется шлангом. Поэтому, как бы, в зависимости от того, чем вы собирали вашу программу, это может проявляться или нет. И разбор затруднён в связи с тем, что большинство людей пытаются в отладочном режиме собирать, то есть, не на О2, а с минус Г, и это чаще всего приводит к тому, что перестаёт проявляться проблема. Как средство исправления, все советуют пока что выключать гипертрейдинг, но Intel уже объявила, что патч готов, и они разослали изменения для всех производителей железяк, соответственно, скоро выйдет обновление прошивки, чтобы это не воспроизводилось. А на Хабре пользователь KivApple привёл хороший пример, почему это на самом деле уязвимость, потому что вы заходите на VDS-ку любого VDS-провайдера, триггерите там эту ошибку, и наступает DDoS всех виртуалок на этой машине физической, например. Вопросы, дополнения, безумная идея. Как дальше жить? А, очень просто, смотри, давай подведём итоги того, что мы узнали за почти 150 выпусков DevZen. Сеть ненадёжна, жёсткие диски ненадёжны, память ненадёжна, питание, всякие UPS, вот это всё ненадёжно, и процессоры ненадёжны.",
    "result": {
      "query": "Kafka 0.11 new features"
    }
  }
]