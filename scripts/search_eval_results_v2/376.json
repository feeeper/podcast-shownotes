[
  {
    "segment_id": "789ade20-9519-4102-9609-9b277f9b6f6a",
    "episode_id": "977b79b9-0991-485c-bd8a-03e974a64c25",
    "episode_number": 376,
    "segment_number": 6,
    "text": "опять же, какие узлы у нас доступны, какие узлы у нас себя плохо чувствуют, где какая реплика находится, вот это все будет Control plane, Data plane будет тут, запись запроса на чтение. Вот видишь, что снова был про железки, видишь, это про софту. Не обязательно про железки, это в принципе, с моей точки зрения, это про то, какой у тебя путь данных проделать, если это какие-то данные, с которыми обменивается польза или там, типа, очень большой поток, это Data plane, если это какие-то внутренние данные системы, и оно нужно для того, чтобы управлять системой, это Control plane, это не обязательно админка в буквальном смысле слова. Ну да, да, согласен. А еще в сетях, кстати, я действительно это слышал достаточно давно, Control plane, Data plane и еще Back plane часто, часто такая была терминология, когда ты начинаешь данные, данные начинают все-всем слайд, если вдруг у тебя получается такая нагрузка, то получается, что заявленная пропускная способность каждого порта, естественно, не достигается, потому что упираешься, вот как раз пропускная способность Back plane. Ну да ладно. В общем, я говорил, да, про то, что мы делаем, вот у нас есть Slash на Rust, есть вот этот наш Control plane на Go и Postgres на C. В общем, мы начали работать в Марве, сначала было три человека, там тогда еще ребятки из Postgres Pro пришли, собственно, за год мы от трех до тридцати выросли, сейчас чуть меньше, там тридцати, двадцать, двадцать, двадцать семь, нас человек всего, из них инженерных двадцать, один, двадцать два. Соответственно, в чем, где-то примерно к лету ближе мы начали проходить, даже раньше, чуть чем лет, начали проходить регрессионные тесты. В этом сетапе с тем, что Postgres все данные запрашивает из внешней, из внешнего хранилища, которое забирает из Postgres поток репликации. И там было много достаточно всяких забавных race conditions и особенностей Postgres, то есть в базах данных есть вариант, что перед тем, как поменять страничку, страничку, собственно, в самих данных, обязательно это должен залогировать. Ну и вот на этом предположении основная репликация, ты можешь как бы игнорировать сами данные, тебе достаточно всегда, достаточно всегда лога репликации, чтобы все сгенерировать, вот. Но в некоторых местах есть всякие оптимизации, типа, например, когда ты делаешь какой-нибудь большой индекс, ты зая, что его создаешь, то вот Postgres нарушает это правило, потому что ты знаешь, что внутри транзакции ты можешь сначала что-то начать менять в файлах, вот, а потом, когда, ну, там, для того, чтобы сделать индекс, тебе нужно много чего сортировать, вот. Поэтому, чтобы логировать каждое еще несортированное изменение, или эта сортировка на диске может происходить, слишком много ты бы салвал, поэтому можно сначала там все менять, а потом уже, когда уже все готово, один раз это залогировать, вот. Для нас такой вариант не подходит, потому что это обычная работа с буфер-кэшем Postgres, и ты можешь из него, ну, типа, это обычная стратегия вытеснения, у тебя во время того, как транзакция работает, может вытесняться страница, и потом мы-то ее выкидываем в надежде, что мы ее сможем прочитать со storage, вот, а она не была залогирована. Ну, вот и всякие такие штуки нам приходилось фиксить, ну, и вот где-то, где-то ближе к лету мы это пофиксили, начали все проходить, ну, и там, то есть я могу там в деталях рассказать, какие штуки в основном. Подожди, прежде чем ударишь на детали, мне такой момент вспомнился, что у Aurora одна из интересных черт систем была в том, что у них может быть этих реплик, не реплик, этих вот узлов, которые отвечают на запросы, ну, то есть мы, не знаю, давайте введем какую-нибудь терминологию, чтобы было понятно, в парке узлы мы говорим, у нас есть узлы, которые... Компьют storage. Да, в общем, да, давайте разделим на компьютер storage. Компьют это Postgres, storage это вот что-то космическое. Да, вот то, что ты сейчас говорил, это как бы прозвучало в контексте одного компьют-узла, а у Aurora, потом, по-моему, в первом же пейпере у них было много компьют-узловов, и потом были... Ну, вначале у них там было такое, что один главный компьют-узел, остальные подчиненные, чисто Redonly, а потом у них даже появились, как бы, я так понимаю, у них там были отдельные пейперы про consensus, из которых я уже их плохо помню, но, по-моему, я из них сделал вывод, что у них теперь все узлы, все компьют-узлы могут писать. Какая у вас сейчас ситуация? У вас один компьют-узл... Не-не-не, в Aurora есть master, replica, это более-менее обычная база. А о чем тогда был этот вот consensus, который они обучали? Про consensus этот берет на BI-high availability, то есть ты можешь без replica, то есть и master и replica не хранят данные, то есть это задача storage несколько раз хранить данные, и задача storage разруливать ситуацию, когда у тебя появились два master, и они оба думают, что они master. А, и то есть задача storage вот это разруливать, вот это, где у них был consensus? Да-да-да, у них там достаточно хитрый, то есть статья называется on-avoiding, consensus-avoiding, там бла-бла-бла, и они описывают, как они делают всякие штуки свои, вот, и они просто вот, и я, я убежден, что это, это просто один из, один из вариантов Paxos'a, они как бы не, не идут в те, в самой статье, они не идут в те детали, которые, ну, позволяли сделать этот вывод, но я уже разговаривал с несколькими людьми из AWS и спрашивал вот конкретно, вот такой постановкой, а что происходит, когда у AWS два master-нода? Ну, типа, а у нас там есть захват generation, ну, типа, который инкрементиться, вот, и тут я уже начинал, ну, слушай, ты знаешь, что захват, если у вас есть фаза с захватом generation, и после этого у вас есть фаза с собиранием majority ответов, вот, то звучит, да, звучит, да, звучит, очень звучит, как Paxos, ну, типа, там вам несколько требований нужно выполнить, это, ну, типа, это уже модификация Paxos'a, ну, и вот в обоих случаях я получал ответ, что, слушай, ну, я не знаю, вот, кто-то другой этим занимался, вот, поэтому, поэтому, поэтому это не ко мне вопрос, а найти того, кто конкретно этим занимался, то есть, судя по всему, первые люди, которые написали, они думали, действительно, что они избегают так консенсуса, и, возможно, они даже не делали там generation, но потом пришли уже другие люди, которые пофиксили это, и мне кажется, оно стало эквивалентно Paxos'у, но, в целом, не принципиально, у них там действительно есть очень, очень необычная, очень необычный подход к изменению membership, то есть, вот, обычно, обычно, когда делаешь RAF от Paxos, то ты просто коммитишь новый, новый сет участников Quorum'а, после того, как он закоммитчен, все начинают работать в новом Quorum'е, они сделали по-другому, и",
    "result": {
      "query": "Aurora consensus protocol details"
    }
  }
]