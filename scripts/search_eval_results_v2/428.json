[
  {
    "segment_id": "8c7922af-a0c7-48bf-9d04-845092edda9b",
    "episode_id": "5d8f6dae-4979-466f-b00f-7dd6fedac1fc",
    "episode_number": 428,
    "segment_number": 3,
    "text": "И переходим к темам Валеры. Валера, расскажи нам свои темы. Что ж, в общем, пока я летал в самолетах, я от большого интереса к базам данных и от того, что в самолете нечего делать, посмотрел несколько видосов из серии CMU Advanced Databases весна 2023, то есть то, что они выходили вот этой весной. Вот. Я, наверное, на два обращу внимание. Первый, про который я совершенно точно хочу поговорить, это Amazon Redshift Internals. 24-я лекция. Чем она интересна? В первую очередь тем, что это, в принципе, один из немногих материалов по тому, как сейчас современный Redshift внутри устроен, она охватывает много всего. Он не очень вглубь идет, потому что я думаю, они и по времени бы не уложились. И, наверное, не все уж прям так хочется рассказывать, но в принципе сказано довольно того, что было сказано, достаточно для того, чтобы понять, насколько Redshift сейчас отличается от того, что про него принято думать, что, мол, где купили какой-то стартап, который поздно сдопировали для того, чтобы он был пригоден для лап, в общем, там ничего уже толком этого не осталось. Современный Redshift, он... Постой, то есть Amazon даже не сам это делали? Я не знал. Redshift, он был на основе какого-то купленного продукта. Это очень давнейшая история. Ты можешь посмотреть на... Наверное, на Википедии даже это было. Я сейчас вот тебе не скажу. Вплоть до того, что у них квартира в другом месте, потому что у Amazon она в Сиэтле, кажется, а Redshift в Рендмэнде сидят. По-моему, так. Потому что гениальные инженеры AWS сели, напрягли извилины, а ну вон он... Там много гениальных инженеров AWS, много чего сейчас сделали, но просто исторический Redshift — это компания, которая выросла из чего-то еще. Так вот, да. Современный Redshift — это полностью дезагрегированный сторидж. У них... Мы, в принципе, обсуждали, мне кажется, много раз Snowflake. У них не совсем Snowflake, скажешь, текстура, точнее, но у Snowflake у них там есть такое типа общее число метаданных, которые вообще общие-общие для всего, которые поверх FoundationDB крутятся. Здесь все-таки отдельные провижены, ну, как бы отдельные инсталляции под каждого клиента, которые вообще никак не зависят друг от друга почти. Вот. И каждая инсталляция выглядит так, что есть какой-то лидер-нод, я так понимаю, он может быть больше одного. И есть... Он занимается тем, что он принимает запросы, разбирает, ну, и диспетчеризует куда-то дальше. Есть Compute-узлы, которые поверх чего-то под названием Amazon Nitro, AWS Nitro. Я не уверен, что это. Видимо, какие-то узлы с PGA или еще что-то. Я хотел посмотреть и забыл, что это. По-моему, это плюсы, да. Мы как-то обсуждали давно, правда. Да, Nitro-карты — это, короче, какие-то специальные IEO-карты. В общем, да, это какие-то специальные узлы, у которых есть какое-то особо модное аппаратное настроение. Но на них нет... Они на них ничего не держат. То есть там может быть какой-то кэшик, но токен там... Ну, то есть нет там базы данных. И есть, собственно, Redshift Managed Storage. Это какие-то такие storage-узлы, которые, как я понимаю, подгружают данные с S3, занимаются тем, что обновляют страницу, если есть какие-то обновления, и отправляют обратно на S3. То есть понятно, что никакие протоколы они здесь не разбирали, в смысле в докладе. Но архитектурно похоже, получается, на то, что, как уже упомянул Snowflake. Вот. Это, во-первых, позволяет использовать слой абстракции в виде S3, который уже есть у Амазона. И, в принципе, это... Для них это была одна из проблем. То, что у них есть... Они могут произвольный большой объем данных обслуживать с очень высокой доступностью. Потому что, даже если что-то упало, всегда можно просто переподнять заново. Данные никуда не делись. Они на S3 лежат. А S3 как бы решает доступность за вас, по сути. Вам нужно просто переподнять ваш Compute Layer. Из еще интересного у них запросы исполняются... Во-первых, у них не Volcano модель, а у них Push-based модель. Ну, то есть, в Volcano модели, когда у вас строится план, и план потом выполняется так, что у вас каждый оператор в плане, он у оператора ниже говорит «Дай мне следующую строчку в классическом Volcano или в более современных снимах обычно дай мне следующий бетч». Этот вызов блокирует, пока там бетч не накопится. У этого есть большой плюс, что это довольно легко имплементировать. Большой минус в том, что весь Control Flow, он вот, по сути, вот, вот, вот, вот, позвал верхний уровень узел, типа, «Дай мне следующую строку». Если, чтобы вычислить следующую строку, нужен час, вам очень сложно с этим что-то сделать. Если прямо так буквально написать. Ну, еще чтобы... Если у вас план распределен по нескольким узлам, вам нужно придумывать специальные узлы в плане, которые бы занимались обменом данным, всяким таким. Push-based работает так, что у вас каждый оператор знает, что он делает. Каждый оператор знает, куда дальше пересылать данные. Вам, соответственно, нужно планировать гораздо более детально. В частности, когда вам нужно данные куда-то пересылать, ну, скорее всего, нужно заранее каждому узлу плана сказать куда, по какому ключу что пересылать. Плюс в том, что гораздо гибче можно управлять всем этим таким вот вот планом, после того, как вы научились такой план исполнять, после того, как вы закодировали, как вообще такой исполнять. Да, вы можете там бы... в любом месте делать всякие backpressure, всякое такое. Плюс это гораздо лучше подходит для того, чтобы делать всякие хитрые компиляции запросов. И, собственно, что подводит меня к тому, что Redshift компилирует запросы, потому они очень старая система, поэтому они компилируют очень таким дедовским способом. Они берут, значит, на некоторые узлы плана, просто буквально генерируют cpp-файл, в котором нужны, ну там даже не один оператор, там как бы под, как бы получается группа операторов в плане, которые будут запланированы как единый кусочек в этом push-based плане. Я не помню, как они называют это. Кажется, plan node или что-то такое. Нет, называют, не помню. Или plan segment. И этот cpp-файл уже так написан, что он в себе содержит кучу оптимизаций. То есть там он написан, он написан с использованием 7D, он написан с использованием всяких там хитрых cache prefetch, то есть, например, они там очень хитро, когда они строят таблицу для того, чтобы делать hash join, они эту hash таблицу, они ее типа делают такое фейковое обращение, потом занимаются чем-то еще, потом возвращаются к тому, что они там обратились, чтобы типа там заранее из кашлини в кашлини положить то, что нужно из оперативной памяти. То есть там прям куча-куча-куча всяких оптимизаций, но это не вручную пишется, это когда генерируется. Дальше они это скармливают GCC и GCC это компилирует и отправляет как раз на эти compute-узлы, которые потом нужный кусочек плана выполняют. Насколько я помню, у них еще есть кэшек объектных файлов. Да, да. Очень важная часть этой системы. Да, я как раз хотел сказать, что типа GCC можно cpp-файлы очень долго компилировать. Как они это решают, у них, да, у них как бы есть кэшек и на их масштабах это офигенно работает, потому что эти кусочки плана, они достаточно абстрактные, чтобы одним кусочком плана можно было, чтобы он можно было применить не только к конкретному приложению, но вообще и ко всем клиентам. Вот, и у них есть глобальный кэш планов, которые они, ну я, кстати, не знаю, как они после каких-нибудь апдейтов это переизвели, или пересчитывают. Ну, видимо, как-то пересчитывают. Вот, и да, оно им очень сильно помогает, потому что им практически никогда ничего типа цифра не приводилось, что это 99.9% запросов, и не нужно делать свежую компиляцию, они уже находят нужный сегмент в плане и просто типа его подставляют, и оно исполняется. Да, для компиляции у них есть какой-то дельный сервис. Сейчас. Да, в общем, компиляция запросов, это был очень интересный момент, который меня прямо вот так удивил. Там дальше много всякого такого, что не то, что прямо очень интересно. Да, интересная такая мысль была еще, что вот, значит, мы бенчмаркаемся, вот в наших бенчмарках мы самые первые, мы лучше всех, но типа вообще как определить самую хорошо перформующую систему среди всех? Это значит взять бенчмарки разных, дейтабейс вендоров, взять систему, которая чаще всего оказывается на втором месте, вот это будет на самом деле самая универсальная производительная система. Забавная мысль, да. А ты тоже смотрел или? Нет, я просто согласен, что это забавная мысль. Вот, дальше у них там много всякого про то, как они это делали серверлесс, но, короче, я думаю, что это, больше всего, это достаточно обычная история про, ну, значит, короче, вы просто шлете запрос, а оно дальше само. Интересно, как у них устроено масштабирование этого. Оно у них, про виженца, грубо говоря, типа, есть какая-то базовая, что ли, распределение того, как у них будет это сделано, ну, то есть у вас есть, скажем, вы покупаете два узла, оно может заавтоскейлиться до четырех, и там, возможно, там до восьми, или что-то такое. Ну, то есть у них есть какое-то заранее просчитанное статическое распределение того, какие компьютерные узлы будут какие партиции данных трогать при параллельном исполнении. Вот, но если вам нужно прям резко заберститься, и не хочется решафлить все данные, и как бы у вас уже достаточно много, то есть вы уже как бы заавтоскейлились до состояния, когда вам сложно без перешафла данных добавлять компьютерных узлов, они делают такую хитрую штуку, они просто берут и поднимают еще один полный такой же кластер, поскольку все данные на S3, ну, как бы, ну и чё. Понятно, что такой же кластер не может, он не может на запись работать, но на чтение у вас просто типа, ну, получилось в два раза больше, или там, не знаю, в четыре раза больше ресурсов. Вот, и они это делают довольно прозрачно, но понятное дело, что вам нужно включить его, вы за это денег заплатите, но с точки зрения, как бы, покупки, вам же без разницы покупаете четыре компьютерных узла в одном раньше в кластере, или вы покупаете, там, не знаю, два узла по два, два кластера по два, ну, там, и так далее. То есть, я так понимаю, что здесь основная экономия в том, что вот такой второй кластер включается гораздо быстрее, потому что данные не нужно перенарезать, у вас уже есть какое-то распределение на какие-то партиции, вы просто те же партиции обслуживаете, типа, параллельным набором компьютерных узлов. Вот. Еще из интересного... Вообще, криминальное решение, как-то они могут и 5х сделать. Да-да-да, ну, и можно любой х, короче, то есть, как бы, вот, покуда тебе хватает компьютер... Не хватает, как его, производительности по чтению, можешь добавлять новые кластеры. Да-да-да, то есть, как это... Это элегантный выход, то есть, как бы, с одной стороны, у тебя есть статическое распределение того, какой компьютерный узел занимается каким куском данных, потому что это, эффективно, особенно, учитывая, что у них эти push-based планы, я думаю, им реально сложно как-то по-другому в динамике это менять. Вот. А с другой стороны, хочется в динамике добавлять мощностей. Вот. Такое, да, прикольное решение. Но это не каждой системе нужно, потому что некоторые системы просто легко меняют количество узлов в динамике. Самое интересное, что я в этом докладе для себя отметил, это то, что они очень активно занимаются автоматизацией того, что база данных делает для своей, для улучшения своей работы в последнее время вообще не спрашивая пользователей. То есть, оно будет подстраивать схему типа ключи партиционирования, оно... Потом у них давно уже была фича типа порекомендуй мне ключ партиционирования, но раньше ей нужно было воспользоваться, сейчас оно просто само это сделает. Точно то же самое у них есть всякие настройки, типа вакуума автоматически подстраиваются. Я уже даже не помню, у них там целый набор настройок, которые автоматически подстраиваются. Автоматически гоняют аналайз на таблицах и даже сами предложат вам и построят материализованные вьюхи. Вот. Короче, прям очень много всякой автоматизации, и это все вещи, которые вот просто вот если вы хоть раз оперировали базу данных, вам что-то было наверняка нужно. Там вакуум, скажем, довольно-таки специфичная штука, кстати, мне это интересно, зачем у них вообще там вакуум сейчас. Но в целом это как класс задач, это то, что приходится решать просто каждому оператору баз данных, и очень здорово, что они смогли могут для своих клиентов это очень сильно облегчить. Также они могут разделять, они могут посмотреть на запрос и предсказать, это будет долго, гоняющийся запрос или быстро гоняющийся запрос. На основе этого они могут как бы очень хитро планировать, и они стараются так приоритизировать запросы, чтобы маленькие, как можно больше маленьких запросиков просунуть между большущими задачами, чтобы система казалась более отзывчивой. Вот. На этом, наверное, все про Redshift. Мне кажется, там больше не было каких-то очень сильно умных идей, которые меня бы прямо голову разорвали. Вот. Но я впечатлен тем, насколько они консервативно подошли к тому, как копилировать запросы, и меня очень порадовало то, насколько у них развита автоматизация управления базы данных, что типа почти ничего оператору не остается делать. Вот. Это был Amazon Redshift. Другая вещь, про которую я хочу говорить. Ну, интересно. Тот факт, что они используют GCC, помогает ли им при отладке? Ну, он там проскакивал, что ну, типа, да, это просто C++, его можно взять и прочитать. В отличие от того, что вы там у себя собрали дерево абстрактное LLVM. Ну, прочитать, да, но, честно говоря, отлаживать прям хардкорный C++ не очень приятно. Мне интересно, насколько он у них хардкорный. Ну, про это речь, если не заходило, то я не помню. Вот. Redshift, он весь такой вручную написанный, вручную оптимизированный, система с давней историей. Но сейчас, на сегодняшний день, мы уже неоднократно про это говорили, что система баз данных в последние годы собирается просто натурально вот, а-ла карты. Вы просто, не знаю, выбираете себе Storage, выбираете себе транзакционный лог, там, не знаю, Kafka, RedFund, выбираете себе хранилище по вкусу, там, не знаю, S3 или какой-нибудь Ceph, что-нибудь еще, выбираете себе движок заплатного, запросов. И вот движок запросов это обычно то, что вещь, которая вот как-то универсально ее не появилась, их есть много, в разной степени популярности, то есть, если вы прес-то, например, или как там сейчас PrestoDB есть, был раньше. А как кольсайт не считается движком запросов? Кольсайт во многом да, но он сам по себе, кольсайт сам по себе без каких-то упражнений не будет супер здорово работать. Тебе нужно вокруг него что-то навертеть. Ну, то есть, типа, чтоб ты понимал, Presto ты поставил, сказал вот там данные, он уже будет молотить. Кольсайт это, в первую очередь, фреймворк. Ну, вот сейчас мы к этому подойдем, да. Есть, короче, какие-то готовые движки запросов. Наверное, Smart можно таковым считать. Наверное, можно... Как же назывался-то? Ладно, неважно. Hive уже помер, нет, я что-то другое хотел вспомнить, но забыл, как он назывался. И это абсолютно неважно, потому что о чем я хочу поговорить, что движки запросов, поскольку у всех приложения разные, это то, что, на удивление, до сих пор пишется новое, и далеко не все это то опенсорсное.",
    "result": {
      "query": "Amazon Redshift architecture internals"
    }
  }
]