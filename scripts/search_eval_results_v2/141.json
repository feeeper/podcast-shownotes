[
  {
    "segment_id": "dc7c8755-70cc-4282-98c8-2c5407be87a2",
    "episode_id": "4bcd0443-eb86-4629-8598-793e3caa6446",
    "episode_number": 141,
    "segment_number": 3,
    "text": "Ну, что мне кажется странным, потому что кто угодно может тобой представиться. Давайте поговорим про Векарфу, а точнее про него поговорит Валера, который за весь выпуск не промолвил ни слова, и ему поможет Ваня. Это было очень внезапно. Я как-то не успел с мыслями собраться. Так берешь, так и как-то как сзади подкрался, напугал. Ну ладно, хватит тривий. О чём я хотел поговорить. Тут в нашем любимом блоге у Адриана Койлера вышла обзорная статья на довольно интересную штуку. Мы некоторое время назад уже обсуждали Корфу, или Корфу, неважно. Это, напомню, такая система, дизайн системы, где используются несколько достаточно тупых устройств, которые поддерживают довольно лимитированный протокол, там append, trim и что-то ещё. Ну и, да, чтение. То есть append с такого адреса по такой-то, trim всего, что до такого-то адреса, и get с такого-то адреса по такой-то байке. Чё-то вроде такого. И достаточно умный клиент, который при этом выполняет что-то в духе chain replication. У этого всего безобразия, за какие-то звуки должен быть координатор, в котором что-то аксессообразное или рафтообразное крутится, чтобы стейт, какие девайсы писали, то есть, грубо говоря, к карту репликации, шародирования, чтобы где-то держать, есть это аксессообразное, ну или какой-то такой координатор. Ну и, собственно, основная работа происходит на клиентах. Дизайн интересен, во-первых, тем, что почти всё происходит на клиентах, во-вторых, в том, что абстракция в итоге даёт такой лог, который, с одной стороны, поддерживает очень большую интенсивность записи, а с другой стороны, он при этом всё ещё полностью... Как это? Ну да, полностью... Он даёт гарантию линейизуемости и сериализуемости. Ну, то есть, как пример, если брать кавку, это тоже лог, тоже поддерживает очень большой throughput, но кавка, например, чтобы выдерживать много записей, она просто нарезает клиентов по какому-то ключу, ну, если нарезать все записи, и просто определённые записи идут в определённые партиции. Соответственно, никакого порядка между разными партициями нет. Порядок существует только внутри одной партиции, а в случае Corfu порядок есть глобальный на все записи. Потом, соответственно, из этого Corfu лога могут вычитывать, например, реплики какие-то или какие-то специализированные индексы, ну или вообще что угодно, что может сесть на вот такой лог репликации, и, собственно, там уже происходит какая-то database логик или application логик, что нужно. Можно вопрос сразу? Да. Я вот исходя из статьи и paper не понял, как у тебя происходит вот это разделение, то есть когда у тебя есть, скажем, 100 серверов, и ты хочешь каким-то образом шардировать. Вот это шардирование как делать? Смотри, короче, ты читал только bCorfu, ты Corfu оригинальный не читал. Нет, не читал. Дело в том, что это объясняется в оригинальном Corfu, ну и мы тоже, по-моему, это обсуждали, там, собственно, смотри, идея в том, что у тебя партиционирование, по большому счёту, получается не по ключу, а по времени. То есть у тебя как бы так получается, что, во-первых, просто разбиение, у тебя всё пространство offsets, по которым ты можешь записать, по-моему, я вот сейчас боюсь соврать, но по-моему да, разбивается на некоторые сегменты, потом сегменты разбрасываются по storage devices, и они так и так разбрасываются, чтобы у тебя было на каждый сегмент несколько реплик, и так, чтобы сегменты более-менее чередовались, так, чтобы у тебя все подряд записи не летели в один диск. Ну и понятно, и всем этим управляет какой-то sequencer, как они его называют. На самом деле прикол в том, что sequencer на самом деле оптимизация, а corv работает без sequencer, просто медленнее. То есть у тебя просто протокол репликации на уровне того, как ты протокол, который storage devices должны поддерживать, если у тебя кто-то начал писать по какому-то offsets, то у тебя там может произойти, я опять боюсь соврать, но может произойти одна из двух ситуаций. Либо он допишет до конца, ну и тогда по этому offsets такая-то запись, и её все увидят. Либо это превратится в ничего. То есть можно просрать offsets, но я боюсь соврать, но я corv учитал года полтора назад, оригинальный. Но суть в том, что у тебя не может произойти, то есть у тебя не может так, что у тебя две разные реплики начнут писать одно и то же. Окей. Ну просто потому, что считай, что у тебя каждый сегмент, вот у тебя там, не знаю, каждый диапазон offsets, если ты сгенерировал себе offsets при помощи sequencer, если sequencer нет, то ты можешь сразу сгенерировать offsets, просто попытавшись записать, не смог по offsets записать, потом пошёл по следующему писать, ну и так далее. То есть в любом случае, ты как-то нашёл offsets, пошёл попытаться по нему записать. И у тебя сегмент, как бы реплики, которые отвечают за этот сегмент, они, ну то есть там не совсем оно, но можно представить, что там chain replication. У chain replication всегда есть голова, поэтому ты должен начать писать всегда с головы. А голова всегда знает, трогали этот offsets уже или нет ещё. Давай уже к VCorfu. Да, к VCorfu давай. В чём там новизна, почему, чем вообще корфу людей не устроил? Я на самом деле не до конца понял, я рассказываю про PayPay, я не до конца понял, зачем они это делали. Идея в том, что, ребят, значит, у нас есть лог глобальный, но где мы, ну то есть сам PCP-лог, это такая штука, с которой мало что можно сделать. То есть корфу, чтобы на его основе построить базу данных, нужно представить какие-то индексирующие штуки, которые будут его вычитывать там и складывать по ключу, например. Соответственно, если мы хотим найти все записи для ключа A, нам нужно прочитать всё доступное, что у нас про это A есть. На самом деле там на практике, скорее всего, нужно snapshotить и лог подрезать, собственно, для того там три мы есть в протоколе, по оригинальному корфу. Но в любом случае вычитывать лог сначала это не очень классно. При этом всё равно для любого практического применения есть необходимость делать какие-то индексы, как-то вот шардировать всё-таки не по времени, а по ключу, потому что если мы хотим читать, нам нужно как-то понимать, где найти последний версий этого ключа. Соответственно, что ребята предлагают? Они предлагают сделать интересную вещь. Значит, да, у нас основной лог должен быть глобально... Таким же. Глобально упорядоченным. Ну да, и в общем-то таким же. Но поскольку у нас есть такие вот всякие странные реплики, нам же никто не мешает как бы на репликах, как бы репликацию как-нибудь по-другому организовать. Ну и вот, собственно, у ребят основная идея в том, что давайте мы, значит, реплики вместо того, чтобы делать как бы точными копиями оригинальных драйвов, просто будем убеждаться, что у нас всё записано в два места, и вот первая группа реплик у нас образует вот этот вот глобально упорядоченный лог, а вторая группа реплик образует распиленный по primary key лог, ну точнее как бы под множество лога распиленный по primary key. Притом независимо от того, если у нас реплика в одну из сторон падает, очевидно, что мы можем эти данные восстановить по другому набору, просто это не очень быстро будет работать, нужно будет как бы больше данных прочитать, чем мы могли бы. Но при этом в любом случае... Да, то есть представим, что у нас есть две группы реплик, в одну идёт, как в Curve раньше работал, а в другую группу реплик идут подмножество лога, которые относятся к ключу, который должен на этой реплике лежать. Как-то так. Как они называют это, материализованный стрим. Да, да. И понятно, чем это хорошо, если нужно восстановить какой-то подсегмент, то есть по большому счету, если у нас есть материализованный стрим, нам вообще не нужен быть специальный какой-то ещё потребитель, который бы как бы индексировал это всё, потому что стрим уже скорее всего так устроен, что мы можем по конкретному ключу выцепить данные довольно легко. Ну это, на мой взгляд, если я не знаю, упоминается ли это в статье, но на мой взгляд там очевидный даунсайт, что жертвуем временем восстановления. И записи. Временем записи не жертвуем, там записи столько же получается. Почему это? В смысле у нас стрим-реплика, то есть там идея в том, что мы... Если бы это... С чем это отличается от обычного Curve? К обычному Curve в принципе ничто не мешает сзади Curve поставить ещё какую-то шнягу, которая бы индексировала как нужно. Собственно, его так и предполагается готовить. Но это ещё больше машин, ещё больше latency. Поэтому что ребята делают? Они вместо того, чтобы иметь, например, две реплики, которые тебе образуют... Ну то есть по две реплики на каждый сегмент лога, они говорят, окей, значит у нас первая реплика на сегмент лога, а вторая реплика, в которую пойдут эти же данные, она на самом деле... Там стримы для ключей с APM. Ну то есть они уменьшают количество реплик обычного лога за счёт вот этого материализованного стрима. Да, да. Притом понятное дело, что ты оригинальный лог можешь восстановить, потому что данные все те же самые есть, просто они по-другому нарезаны. Но при этом ты не можешь добавить ещё один материализованный лог. Почему? Можешь? Обычный лог. Ты можешь увеличить до трёх, скажем, количество? Наверное, да. То есть насколько я понимаю оригинальный Curve, там тебе без разницы, куда сколько раз писать. Ну то есть там chain replication-образная штука, она драйвится клиентам полностью. Ну то есть ты должен дожидаться окончания, то есть ты увеличиваешь количество, ты больше... Дольше писать просто. Да, да, да. То есть в этом смысле, с точки зрения записи, ничего радикально не меняется, они, кажется, наоборот, экономят, получается. Но то есть у Curve и так уже время восстановления не самое клёвое, потому что у него там внешний координатор, драйвится запись клиентам, поэтому там нужно всё везде остановить. Как бы понять, что где оборвалось, и восстановить. Он быстро восстанавливается, но, наверное, можно быстрее. А тут, ребята, вообще так получается, что если у нас какая-то реплика падает, нам нужно понять, по каким местам её могло разбросать, и из тех мест её обратно притащить. И это прям просто... При этом получается, что у тебя несимметричная логика, то есть падают логи, тебе надо из стрима вытащить всю информацию, причём из разных реплик. Падают стримы, тебе надо из разных логов вытащить всю информацию, чтобы заполнить. Именно, именно. То есть получается, что мы оптимизировали на чтение, а сломали на восстановление. Я, кажется, говорил, у нас очень похожая ситуация, я недавно упоминал, когда мы, по-моему, обсуждали Calvin, ты спросил, какие ещё системы с глобальными транзакциями, я упомянул Tapir. У Tapir похожая проблема, они, значит, оптимизировали, чтобы во время записи было меньше... То есть, когда мы делаем транзакцию, чтобы меньше фигни делать, чтобы не воротить двухфазный коммит поверх групп Paxos, потому что у нас как бы и Paxos линеризуемый, и двухфазный коммит нам для линеризуемости даст, если всё хорошо, уровнем ниже. И получается, что у нас как бы два тяжёлых алгоритма, хотим мы на самом деле линеризуемость один раз, зачем нам её в двух местах поддерживать. И ребята даже построили такую систему, она действительно быстрее работает, ну, по крайней мере, по их бенчмаркам в пейпере. Одна проблема, там совершенно долбанутый алгоритм оставляет, который ещё медленный. Для любой практической системы, особенно если это OLTP, ну, то есть, на самом деле, конкретно это VCorf, он в принципе жизнеспособен, ну, то есть, его можно посунуть куда-то, где нужно что-то кавкообразное. Непонятно, зачем, причём это будет принципиально лучше кавки, но можно. Ну, понятно, чем, если вам нужен какой-то кавкообразный воркловод, или какое-то кавкообразное применение. Но вам всё-таки очень важно иметь глобальную сериализуемость для чего-то. В принципе, ну, можно себе такое представить, наверное. Но для OLTP, где мы хотим, если что-то сломалось, быстро подняться, оно, к сожалению, не выглядит хорошим вариантом. Погоди, можно ещё вопрос? В каком оно сейчас состоянии, в плане, можно ли это потрогать, или как обычно, там, типа, код закрыт, мы только пейпер написали? Слушай, ну вот у корфу точно код есть на гитхабе, у оригинального, у vCorp, без понятия. А, ну, как бы, код есть для галочки, или, типа, production-ready, что-то такое? Скорее всего, для галочки. Ну, то есть, слушай, я знаю мало систем, которые бы писались для пейпера и были бы реально production-ready. Ну, то есть, я не знаю никого, кто оригинальный корфу бы пытался отгонять в production, скажем так. Правильно я понимаю, что теоретически, если у тебя ALAP, и тебе не важно, как долго лежит эта штука, тебе очень важна скорость чтения, то может быть, имеет смысл. Да, то есть, при этом непонятно, зачем в ALAP нужны такие сильные гарантии по линеризуемости и сериализуемости. Да, вот это довольно только. Ну, то есть, сейчас есть такое, ну, собственно, в ALAP всех крафт устраивает именно потому, что нарезать по primary key и набихать данных всем обычно ок. Что приходит в голову, это сейчас пытаются делать гибридные системы, которые и для того, и для другого могли бы подойти. Наверное, где-то там такое могло бы прокатить, но, опять-таки, если у вас LTP страдает в момент, когда эта штука начнет подниматься, то есть, тут оригинальный корф получается, ну, да, труднее выцепить данные по отдельному ключу, да, нужно больше машин, но зато с точки зрения их характеристик вроде поднятия обратно, оно должно лучше работать. Я не знаю, может быть, конечно, ребята в V-CORF это как-то решили, я просто попустил, потому что я пейпер не читал. Там еще были эти какие-то локальные или удаленные объекты, ты про это не хочешь рассказать? Я, честно говоря, не помню уже, о чем там. То есть, там ты можешь строить объекты, которые ты хочешь менять, которые хранятся в V-CORF через дополнительный материализованный view, я не знаю, как это правильно сказать в их терминологии, то есть, у тебя в твоем рантайме, ну, во-первых, у тебя всегда должен быть рантайм какой-то в твоем приложении, который следит, который знает, что где работает, который знает, куда обратиться, вот это все. Этот рантайм в принципе не маленький. И этот же рантайм отвечает за то, что у тебя какие-то объекты, которые ты хочешь очень быстро читать их состояние, они у тебя в локале будут строиться, и там действует оптимистичная конкуренция, когда ты пишешь одновременно и туда, и сюда, и надеешься, что все будет хорошо, с каким-то специальной логикой по обработке. То есть, когда ты пишешь в V-CORF, ты одновременно обновляешь свой локальный объект, и это делается для того, чтобы у тебя было очень быстрое чтение из этих объектов, если ты одновременное чтение хочешь ускорить. Мне так кажется, они просто попытались танго перенести, который поверх оригинального корфа строился, попытались перенести на свою штуку. На свою штуку? Я могу ошибаться. Ну, на V-CORF. То есть, поверх корфа было, потом следующее за ним такое исследование было танго. Это как раз очень похоже на то, что ты сейчас произнес, только вот оно поверх обычного поработало. Ну да, получается, на клиенте много чего начинают делать. В целом, вся идея корфа в том, что мы максимально все перенесем на клиента, потому что у нас клиенты – это на самом деле аппликейшн сервера, которые жирные, у них много CPU, а стораж у нас предельно тупой. То есть, если я правильно помню, в оригинальном корфу у них были SSD-шки с FPGA-шками в качестве стоража девайсов. И FPGA-шки там были только потому, что им было так быстрее запрототипировать логику этого вокруг стоража. То есть, по-хорошему, ты можешь просто в SSD-шку вставить какой-то прошитый чип, который будет отвечать на 3-4 простых команды из сети, и тебе этого будет достаточно, чтобы построить корфу в оригинальном кеймере. У меня такое чувство, что тема себя исчерпала. Да, наверное. Тогда давайте перейдем к следующей.",
    "result": {
      "query": "VCorfu architecture and comparison"
    }
  }
]