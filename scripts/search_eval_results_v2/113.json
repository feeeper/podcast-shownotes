[
  {
    "segment_id": "ab488ebc-cc36-41d4-9eed-272fc0b0d6de",
    "episode_id": "952c5dc7-f9e6-4b2a-88a5-185510b31dce",
    "episode_number": 113,
    "segment_number": 2,
    "text": "Подожди, можно я тебя перебью? Можно. Я сходу так понял, что его не нужно обучать, что он просто, вот у тебя есть какая-то таблица, ты в нее пишешь, пишешь, пишешь, словарь растет. А тут ты говоришь обучаешь, то есть этот словарь можно экспортировать и потом импортировать или как это вообще работает? Он per table или per database или как это вообще? И Олег Акбаров говорит, что у него много JSON в Postgres, просит рассказать подробно, но Олег, спрашивай в чате, я что-нибудь понятное расскажу. Плагин, отвечая на твой вопрос, нужно именно обучать на типичных документах и время от времени переучивать, если у тебя документы сильно меняются. То есть он на лету процессором не шуршит, все документы не парсят. Это нужно сделать один раз, у тебя построился словарь. А по-прежнему, пер что он строится? То есть у тебя же могут быть разные документы в зависимости от разных, там в разных таблицах разного рода документы могут лежать. А как обучишь? Ты же знаешь, как обучишь? Рассказываю. Плагин делает следующее. Берет документ, его бинарное представление, и заменяя строчки, которые попали в словарь в результате обучения, заменяет на 16-битовые коды. 16-бит – это 65 тысяч строк. Это предполагается довольно много для любой реальной базы, поэтому словарь общий на базу, на все документы. Соответственно, ты скормил плагину какую-то выборку, говоришь, вот это мои типичные документы, ты можешь по всем документам прогнать, можешь сделать временную табличку, как тебе удобнее. В результате получается словарь. И ты вместо того, чтобы использовать тип JSON-B, начинаешь использовать тип TheSone. Оказывается, в Postgres в нем есть имплиситы. Они там были задолго до того, как стали модными в скале. Это позволяет TheSone неявно кастовать в JSON-B и обратно. Поэтому у тебя с ним работают все те же операторы, все те же процедуры, строятся индексы и так далее. И TheSone работает как тот же JSON-B, только он становится сильно компактней. Словарь, разумеется, учитывает статистику встречающихся строк, плюс можно дать ограничения на длинный строк, сколько минимум раз они должны встретиться и так далее. Это все параметризуется. Что экономится? Во-первых, у тебя... Зачем это вообще кому-то нужно? Во-первых, база начинает занимать меньше места на диске. Можно больше записать. На моих синтетических тестах я сразу скажу, тестам не верьте, проверьте все на своих данных, на своем железе и так далее. Но у меня база одна сжалась в два раза, например. Плюс у тебя, поскольку документы в памяти хранятся тоже в сжатом виде... Извини, можно я тебе еще раз перебью? На секундочку, я вот не помню, в Postgres heap как-то сжимают автоматически или в Postgres heap не сжатый? Сейчас расскажу. Задержи эту мысль. В памяти у тебя тоже помещается больше документов, потому что они там хранятся в таком же виде, т.е. все эти строчки заменены на коды, и у тебя не только память экономится, туда еще и помещается больше данных, поэтому ты реже ходишь в диск, и у тебя становится меньше IEU. За счет этого тоже возрастает скорость. Так вот, в Postgres у тебя есть действительно сжатие. Это алгоритм PGLZ. Он в душе примерно как GZIP, но чуть-чуть другой. Но по характеристикам примерно такой же. Есть там ряд эвристик, при котором он применяется или не применяется. Например, если данные слишком маленькие, то Postgres даже не пытается их сжать. Если они достаточно большие, то он пытается их сжать и смотрит, сжалось ли или нет. Притом он, по-моему, первый несколько килобайт пытается сжать, и по ним судит, вообще это сжимаемые данные или не сжимаемые. Ну вот. The Sun, он его не пытается как бы заменить и убить, он его дополняет. Почему? Подожди, можно еще вопрос, пока далеко не убежали? Он у тебя применяется к любой странице, он у тебя применяется к группе страниц или к PGLZ? К чему он применяется, к какой единице? PGLZ применяется к, как называется, к элементам тупла. К каждому по отдельности? Угу. А словарь пер что делается? Словарь делается пер базы данных, но обучается на конкретных документах и жмутся конкретные документы. Пер базы данных как instance или пер базы данных как database? Словарь у тебя на, не instance, нет, словарь у тебя на базу данных, которая внутри инстанса. В инстансе может быть много баз данных. Ну вот. И почему PGLZ не очень хорошо жмет JSON, ну в смысле он его хорошо жмет, но если у тебя есть один документ, он его пытается сжать, ну в нем, например, 20 ключей, он их сжать не в состоянии, потому что он жмет документ отдельно от остальных документов. Вот, что делает JSON? Он смотрит на все твои документы, видит, что в них используются, например, одинаковые ключи или одинаковые значения где-то и выносит их в словарь. За счет этого он как бы дополняет PGLZ и жмет то, что PGLZ сжать не в состоянии. Вот подожди, я тут мысль не понял как раз. Вот ты выше по тексту сказал, что у тебя словарь на базу. Точно так же в JSON? А почему в JSON он это и видит, а в PGLZ нет? Нет, подожди, что значит то же самое в JSON? Я про него и говорю. Нет, в смысле в PGLZ ты сам сказал, что в PGLZ у тебя тоже словарь хранится, у тебя сжимаются отдельные элементы тупого, а словарь хранится на базу. Мы тут друг друга не поняли, нет. Значит, у JSON словарь на базу, вынесенный и в памяти кэшируется. В PGLZ он просто как GZIP жмет кусок данных и никакого пошаренного словаря у него нет в принципе. Понимаю. А, то есть он жмет только те элементы тупого, которые большие. Да, но он как встроенный GZIP. Нет, ну просто именно что встроенный GZIP, он бывает очень разный, то есть это у того же Лайва Владимировича, у него Snappy. Snappy работает, если не ошибаюсь, на нескольких страницах, то есть он берет группы страниц и жмет группы страниц, если не ошибаюсь. Не могу сказать ничего про Snappy, я не специалист, но я знаю, что... Это не важно, какой алгоритм, алгоритм я к тому, что некоторые базы применяют компрессию не к внутренним страницам, а к уже записанным страницам, то есть сами страницы уже как-то зажимаются. Вот к чему. Да, действительно, можно сделать постраничное сжатие, и есть такой проект, я даже спалю немножко тему. Опять же, у него есть свои нюансы. Например, у тебя не факт, что в одну страницу попадают документы, в которых повторяются строки, более того, ты, если их сжал, то у тебя страница довольно небольшая, она 8 килобайт всего лишь, а документы у тебя могут при помощи тоста быть, не знаю, какие угодно, типа мегабайтные. И постраничное сжатие – это крутая тема, просто она может работать лучше, может хуже. Надо все тестить на своей базе. То есть постраничное сжатие, оно жмет твой блок из 8 килобайт, а засунь, он жмет весь твой документ, хоть он там 300 мегабайт. В общем, засунь, товарищ, засунь свой JSON в засунь, а еду в диплёринг. Правда, Света? Да, я, как всегда, добавляю тему на этот счет. И моя тема касается того, что... Вот вы когда-то... Давай начну с другой вообще, к концу. Вы когда-то с Сашей поднимали тему, как вы худели. Помните такое было? Было, было, было. А вы пользовались какими-нибудь приложениями для этого? Я пользовался, но похудел, когда выбросил. А ты, Саша? Нет, у меня просто был тренер, который говорил, что надо делать. А вы когда-то трекали то, что вы ели, вы мужскому дневник вели, я не знаю, может там записывали калории считали? Я как раз трекал, читал калории, это бесполезное занятие. Бесполезное абсолютно. То есть, ну, вот серьезно, для меня неважно, сколько калорий я съел, там важнее, что я ел и когда, и так далее, чем конкретно калории продукта. Вот там на самом деле, знаешь, есть такая тема, я, конечно, извиняюсь, что сейчас диет-зен-подкаст, но опять-таки мы обсуждали тему, как чувак натренировал, как чувак делал, как это, ну, гонял всякую статистику на основе просто лога того, что он ел. И выяснилось, что он худеет хорошо, не потому, что там калории, это вообще в последнюю очередь, и там другие параметры, типа, какой там индекс, не помню, чего у еды, то есть насколько там у тебя хорошие или плохие углеводы. Вот это влияет сильно больше, что, мол, углеводы с жирами плохо смешивать, потому что они разными этими химическими соединениями расщепляются. Ну, на самом деле, автор той статьи установил, что всё зависит от сна. Нет, он немножко не так сказал. Там сон был самый важный характеристикой про всё. Там же, как он это объясняет, дело в том, что когда мы спим, мы не едим. Ну, собственно, если бы вы сходили к специалистам в этой области, они бы вам объяснили, что на самом деле калории считать важно. И есть много людей, которые используют такие предложения. Ну, так что-то дело, что специалисты в этой области говорят, что калории считать важно. Они назвали все дураки. То есть сейчас есть ряд статей, которые говорят, что все, кто считает калории, они все дураки. Что, мол, там были работы ещё чуть ли не в 50-х, которые были похоронены пищевой индустрией, потому что где только все переориентировались на калории, тут кто-то пришёл и сказал, что калории считать это плохо. Его все засмеяли, и сейчас только диетологи постепенно переключаются с калорий на альтернативные способы подсчёта всякого. Не верьте всегда специалистам, если он у вас только один. Между прочим, есть очень простое объяснение, почему бесполезно считать калории, потому что сколько бы ты их ни считал, организм с тебя стрясёт ровно столько, сколько ему нужно. Просто стрясёт он этот, например, ровно перед сном. Вы закончили. Спасибо, что передали мне слово. Но специалисты по-прежнему уверены в том, что калории считать важно. Есть много людей, которые используют для этих целей очень много приложений. Которые стоят всего 5 баксов, простите. Но обычно в этих приложениях, как это работает, у вас есть какой-то большой список, дерево всех возможных продуктов, вы выбираете, сколько вы чего съели. А тут решили использовать новейшие разработки в сфере машинного обучения. А что, если избавить человека от этого печатания и этого скучного выбора нужного тебе товара в большом списке? А что, если мы просто сфотографируем еду? Ведь все же любят фотографировать еду, разве не так? И выкладывать это в Инстаграм. А у нас приложение автоматически распознает всё, что вы съедаете. Вы указываете, сколько вы этого съедаете, и оно автоматически рассчитает, сколько в этом калорий. И, собственно, есть приложение, которое это и делает. Авторы статьи рассказывают, как они используют Deep Learning для этих целей. И они... Есть датасет. Вот я рассказывала про датасет для видео. А есть датасет, который с кучей-кучей картинок еды. И тоже про лейблинный датасет. И на этом датасете тоже можно глянуть свои алгоритмы. Так вот, они, используя подход глубинного обучения, достигли порядка 87% непонятно, конечно, чего, но горят точности. И они таким образом обещали побить рекордсменов этой области, те, кто использовали решающие деревья для того, чтобы распознавать, какая еда на картинке. Я добавила ссылки на вейпер, который рассказывает, как используются решающие деревья для распознавания объектов на картинках. И ссылка на то, как использовали глубинные машины обучения для этих целей. Но те, кто использовали алгоритм, который считается лучше, они не рассказали деталей. И вейпера у них пока нет. Хотя может быть у них что-нибудь есть более подробнее на их сайте. Вот как-то так. Я радуюсь тому, что машинное обучение, все вот это, идет в массы. И все больше и больше всяких консюмерских приложений это используют. Меня это радует. Я сейчас побрыжу, лучше бы casual yorder и broadcast запилили. Мне кажется, очень много что можно так оценивать. Можно фотографировать книжку и оценивать количество страниц, фотографировать девушку и оценивать количество макияжа и так далее. Я очень доволен такой штукой, которая даже без всякого диплернинга работает. Называется VIVINO. Вот, ты фотографируешь этикетку из бутылки вина, оно тебе говорит, хорошее или плохое. Потом даже зачастую конкретно эта бутылка этого производителя, то есть этого импортера и экспортера. Вот, это довольно забавно. Слушай, а оно может даже и белорусское, знаешь, такое, которое чернило какое-нибудь. Насколько я понимаю, оно crowdsourced, поэтому если туда уже на crowdsourced-или, то наверное может. Ну, на самом деле, с другой стороны, оно не все итальянские знает. То есть, там какой-нибудь итальянский, какого-нибудь мелкого вендора, который, ну, не важно, хорошее или плохое, просто какой-то не очень такой... Импортер не очень известный или вендор не очень известный, но может про него знать примерно ничего. Мне кажется, это неправильная формулировка, хорошее вино или плохое. То есть, я, например, не очень люблю сухое, мне полусладкое нравится. А там можно поискать по всяким... То есть, оно тебе, во-первых, если оно знает, то первое, оно тебе все это скажет, но, с другой стороны, такие вещи не на бутылке будут написаны. А во-вторых, там же можно и в обратную сторону, ты можешь сказать, какое вино я хочу и спросить, что такого продается в магазинах недалеко от меня. Вот, ну, мне кажется, наверное, хватит про бухло. Давайте про тему слушательных. Кстати, я... Про тему забух на тиск. Я добавлю одну тему, то есть, расширю одну тему, просто я хотела про это рассказать, но как-то я забыла. Ты, Валера, рассказывал про блог-пост Адриана Коулера, правильно? Да. И на этой неделе у меня была возможность увидеть этого человека вживую, а я помню, давно-давно мы обсуждали его блог The Mourning Paper, если я правильно помню. Да, да, мы эту статью сегодня тоже красили. Это же молодой фреш, абсолютно. Мы поднимали тему, что это за работа у человека, что он всё время читает пейперы и пишет только про эти пейперы. Я помню, Ваня даже говорил, что вот бы ему такую работу. Вот, и я узнала. Да. Я узнала, кем работает этот человек и что у него за деятельность. На самом деле, он является техническим товарищем в одном из венчурных фондов, и он занимается оценкой технической части стартапов. И вот он недавно говорил с моим знакомым, который, помните, ссылку тоже нам когда-то давали слушатели на Just-In-Time компилятор для C++? Да. Вот, и они с ним недавно встречались по поводу возможного инвестирования, и он рассказал, и они тоже пообщались немножко больше по поводу того, что чем он занимается всё-таки, Адриан. И по большей части, ему вот этот венчурный фонд, в котором он состоит, он ему платит за то, чтобы он постоянно печатал свой блог, для того, чтобы он постоянно писал и изучал пейперы от разных стартапов, потому что на самом деле это у него работа такая – оценивать стартапы, потом они решают, на базе этого стоит ли в них инвестировать, либо нет. Они онемают, и ему и дальше платят. Вот, но работа на самом деле классная. Ну, есть один маленький кевиат, если фултайм только делать, то читать пейперы рано или поздно. Но не только, надо сказать, он состоит, он на борде некоторых стартапов. Нет, я про то, что если куда-нибудь писать, то… Ну, то есть, я думаю, у него есть какие-нибудь свои поделки, но он участвует, по крайней мере, в технических вопросах в нескольких стартапах, он на борде сидит, как технический человек. А мне кажется, должность называется, получается, технический читатель. Нет! Прекрасно! Чётко! Технический читатель, действительно. А должность у него, наверное, просто партнёр в фонде. Но прикольно, я не знала, просто мне тоже было интересно, что это за профессия такая, я бы тоже себе такую хотела, должность. Вот, как-то так. Вот, ну что, к темам слушателей. Угу. Времени осталось немного, но, с другой стороны, тем популярных тоже раз-два. Слушай, насколько я осведомлён, у нас ещё 2 часа впереди. Вот. Подожди, 2 часа через полчаса начнётся. Да подождите вы, мы уже записались на 2 часа, кто это всё будет слушать. Собственно, главное, 2 темы, которые на самом деле одна, которые у всех пригорели. За то, что, к сожалению, огромному компанию RelethinkDB закрывают двери. Это прям просто грустно, стеска, печаль, уныние и хочется сделать утренний харакери. Вот, потому что это, ну, с точки зрения использования, это хороший, годный продукт. Это одна из немногих NoSQL баз данных, которая не грешит проблемами NoSQL баз данных. То есть, даже если оставить вопросы типа консистентности в распределённой системе, то, чем грешит Mongo, у Монги, у той же, ну и почти у всего, что, почти любая open-source на баз данных, она построена, я как-то в комментах недавно это написал, когда мы из выпусков, они устроены почти все на consistent hashing и сделать просто нормальную выборку по рейнжу почти нигде нормально нельзя. Без каких-то, без создания существенной нагрузки на базу, без того, чтобы иметь индекс на одной ноде, которая только эта нода, индекс там потом по определённому полю. Вот, Relethink, он, во-первых, у него нормально всё с консистентностью, во-вторых, у них всё нормально с выборками.",
    "result": {
      "query": "RethinkDB closure reasons"
    }
  }
]