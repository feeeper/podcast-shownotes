[
  {
    "segment_id": "da357325-0573-48e0-89c5-b8a5132b1500",
    "episode_id": "878ec328-a603-4378-b20c-f4de9520166d",
    "episode_number": 115,
    "segment_number": 2,
    "text": "Но это, ну как бы я говорю, что он вроде как не теряет данные, судя по его архитектуре, но я не готов положить голову на отсечение, что он никогда ничего не потеряет. Ну, то есть, как бы пойнт в этом, нужно быть всегда готовым к тому, что что-то пойдёт не так. Двигаясь постепенно дальше, почему-то эти две темы у меня объединены, но хотя мне кажется, они слабо связаны. Во-первых, стал доступны видео со Scilla Summit, я напомню Scilla, это Scilla DB, это база данных, похожая на Cassandra, но совместимая с ней, только на C, и по синтетическим ничего на практике не означающим бетчмаркам, она прям сильно-сильно дерёт Cassandra, и можно там типа кластер в три раза меньше сделать. Не знаю, насколько она стала стабильнее или нет, по-моему, мы обсуждали какие-то такие эпичные баги, ну потому что это C++, всё очень увлекательно и здорово. Но в любом случае, видео доступно с Scilla Summit, посмотрите, если вы смотрите видосы и интересуетесь базами данных или распределёнными системами, или распределёнными базами данных. Никто случайно не посмотрел хотя бы один видосик? Нет. Вот. Да, а как вы смотрите на то, чтобы поговорить про решардинг на основе релиционных субботов? Давай сначала определим, что значит решардинг. Подождите, это как-то, да, у меня срочная шутка в номер. Значит так, давайте каждый из нас сейчас пойдёт запишет свой собственный подкаст, а потом мы объединим кусочки. Как вам такая офигенная идея? Буду-бум-тссс! Я её не понял. Я понял, это про решардинг, да? Да, ха-ха-ха. Я же сегодня как-то просто тумбочка Петросяна. Кстати, я погуглил картинки там в Красавице и Чудовище, не было тумбочки, там были часы и была свечка. И буфет. Буфет? А, буфет был, при том он, по-моему, женского пола. Да. В любом случае, я к чему всю эту тему начал, у меня как в прошлом, да и в настоящем немного, у бэк-энд-разработчиков всегда был большой вопрос, какую базу данных выбрать как наиболее универсальное решение. И если, например, плохо брать какую-нибудь Орг. Кассандру или Сциллу, потому что неудобно, нужно, чтобы данные оставались консистентны, всё делать через КАС, а потом тебе захотелось какой-нибудь такой хитрый индекс, чтобы он не разъезжался и становится вообще капец, можно расходиться. Можно взять Пасгрэс, у него даже вот теперь появился фейловер на тот момент, он был в лице Патроне, но мне его не рекомендовали. Появился автофейловер, но возникает вопрос, хорошо, а если мы не влезаем в одну машину, что делать? И вот в последнее время у меня как-то начало складываться понимание, при том это на удивление просто, но раньше у меня такого понимания почему-то не было, вот я надеюсь услышать конструктивную критику по этому поводу. То есть мы научились строить реплико-сеты на столоне, мы примем это за рабочую теорию, пока норм, да? Да. Окей. Мы сделали сервис Discovery и шардирование по V-нодам, это пока тоже не лишено смысла, то есть у нас есть одно отображение, что мы по какому-то ключу почитали хэша, взяли остаток отделения на 1024, пошли в словарик и поняли, на каком реплико-сете это надо искать. Ок. Вот, теперь возникает самое интересное, мы время от времени у нас какая-то V-нода распухает, и нам нужно перенести её с одного реплико-сета на другой. Вот смотри, вот тут начинается интересно, у тебя V-нода распухает, или ты сразу нарезал так, чтобы V-нода было достаточно много? Нет, я нарезал сразу, чтобы их было достаточно много. А тогда зачем, тогда тебя распухающая V-нода волновать не должна? Почему? Ну потому что если у тебя консистент хэшинг, хороший консистент хэшинг, то у тебя распухающая V-нода, это какой-то совсем патологический случай, который не случается. Нет, ну распухающая или высоко нагруженная, вот у меня Джассин Бибер попал в V-ноду один, и туда все ходят. Ну вот это плохое развеяние. Ну не знаю, да, Леди Гага попала на другую V-ноду, и туда другие все ходят. Не, это реально хороший пример, потому что у нас ровно такой пример был. У нас была Леди Гага, и она попадала все время в одну V-ноду. Ужасная штука, это просто плохой выбор алгоритма разбиения на шарды. А в чем проблема взять вот эту V-ноду и подвинуть ее на отдельный сервер? Вообще не проблема, на самом деле нет. Ну вот и я не вижу проблемы. Ну просто у тебя может со временем возникнуть ситуация, когда отдельный сервер не выдержит Леди Гагу, что вот у нас было. А если она плавно распределена по всем V-нодам, так что у тебя нет единой точки, куда приходит одна Леди Гага, у тебя не будет такой проблемы. Но с другой стороны потом, когда тебе нужно что-то по Леди Гаге отдать в интернет обратно, ты потом это хрен соберешь. Ну а у тебя хоть как консистент хешинг теряет локалити. Ну нет, если ты консистент хешинг делаешь по юзер ID, то не теряет. А, ну ты имеешь в виду, что у тебя бизнес-логика должна соответствовать консистент хешингу, но это не всегда получается. И вопрос у нас как раз именно в этом получается. Ну тут все очень сильно зависит от приложения, давайте пока в такие дебили не уходить. Не, ну почему, сейчас у тебя уже пришлось уйти в эти дебили, потому что у тебя так получилось. Нет, я имею в виду, что я могу сделать там 1024 V-нода, а могу сделать, вот как ты предложил, что по одному пользователю кусочек данных на одной V-ноде, кусочек на другой, то есть это все, ты понимаешь, что в реальной системе это все в процессе выясняется. Никто заранее такое не спланирует. Ну конечно, да. Нет, почему, ну спланировать можно, все равно какие-то такие кривые случаи есть. А давайте логи записывать по годам, а года будем на V-ноды складывать. Ой, 2016-ый что-то у нас заполнился быстро. Ну то есть все равно это как-то, может быть, что-то понятно заранее. Точно так же, как Lady Gaga. А давайте отдельных пользователей, давайте. Ой, что-то Lady Gaga заглючила. Давай так сформулируем, это не совсем вот та проблема распределенная система, которую я хочу поднять. Это проблема, я согласен, но она решается на уровне приложения, а не на, ну то есть выше. Ну вот, собственно, вопрос, мы хотим переместить одну V-ноду с одного реплика на другой. Да, кстати, Валера мне тут недавно подсказывал, что это правильно называется все-таки как-то типа ребалансировка, а не решардинг. Решардинг — это изменение числа шардов. Поправь меня, Валера, если я не прав. Все так, да. Еще решардинг часто называют репортишеринг. Ну я как бы не то, чтобы на 100% с этим был согласен, но во избежание путаницы и того, что у некоторых базах в документации действительно написана ребалансировка, чтобы никого не путать, лучше говорить ребалансировка, а не решардинг, ну или как-то так. Самый простой вариант. Мы переносим какой-то V-шард, ну вот в PathGresing конкретно это довольно просто. Мы можем, ну вот, подход первый. Мы говорим пользователю «чувак», у нас, короче, как это называется, maintenance. Возможно, мы его проводим в удобное для чувака время, потому что мы посмотрели по графикам, то, что нагрузка вот там в 4 часа ночи небольшая, и вот сейчас можно замутить. Вот, в этот момент у него V-нода доступна только на чтение, и мы классно так в PathGresing можем сделать, ну запустить копию с GZIP-ом там во внешний файлик по определенному условию. То есть вот то, что мы хотим перенести, прям скопировать в файлик, потом залить это на новую, ну то есть понятно, где-то в Redis это ставится галочка предварительно, что у нас в процессе решардинга. Простите, ребалансировка. Архивчик аккуратненько копируется на новый replica set, заливается туда, словарик в Redis или где он хранится правится, галочка снимается. Вот, если чувак, он в это время приходит, мы ему можем отработать чтение, а на запись говорим, что сорян, нельзя. Вот, то есть это один из подходов, он вроде норм, пока имеет смысл. Ну разумеется, если он применим в данном приложении. То есть отказ во обслуживании пользователя на время, когда ты проводишь какую-то внутреннюю операцию. Только на запись, обратив внимание, и только, ну если, конечно, приложение это позволяет. Ну дальше есть вариант, как это сделать, чтобы не отказывать ему. Вот. Насчет, как не отказывать. В некоторых приложениях оказывается большинство данных, ну как это ни странно, большинство данных неизменяемы. То есть мы писали, например, Instant Messenger, и там вот реально все ложится на аккуратненький, красивенький лог. То есть условно говоря, у пользователя есть inbox. Если мы, ну один пользователь хочет написать что-то другому, то отдельный вопрос, как делаются распределенные транзакции. Это можно будет дальше обсудить. Ну смысл в том, что кладется сообщение в свой, ну как бы свой outbox к тому, кому ты хочешь отправить в inbox. И inbox это такой длинный-длинный лог всех событий. При этом удаление сообщений, это на самом деле insert тем стона, и обновление сообщений это как бы, ну, тоже insert с определенной меткой, чтобы у нас была одна версия того сообщения стала другая. Или patch. Это на самом деле, ну на первый взгляд это звучит как-то стрёмно, да, как же так, у нас там будут такие огромные длинные логи. Но на самом деле запись, она вообще, ну как бы в типичном вебе записи это типа 10% операции, из них запись на обновление это там еще меньше процентов, то есть это довольно редкие кейсы, и ничто не мешает там в фоне гонять GCC, которые время от времени компактят то, что стало лишним. Ну вот, и схема с неизменяемыми данными, она позволяет очень круто делать решардинг, потому что во время ребалансировки, ну, не решардинг, почему, потому что когда у нас стоит галочка, что мы делаем ребалансировку, мы пишем туда, куда переносим, отчитаем из двух источников, откуда переносим и куда переносим, и поскольку данные неизменяемые, мы гарантированно это на клиенте во что-то мержим, ну разумное. Это имеет смысл, сказанное? Да. Ты на самом деле еще записал схему, примерно то, как работает ребалансировка внутри React? А она, у меня есть подозрение, она примерно везде так устроена. Ну, если они поддерживают... Ну, да, да, да. Ну, на самом деле, знаешь, как еще бывает устроено в консистентных системах? Там бывает просто в целевом месте делают, как это, реплику, дожидаются, пока она нальется полностью, а потом делают такой форсированный фейловер. Еще, кстати, важный момент. Во-первых, конкретно Postgres, он все равно не любит, когда вы делаете апдейт данным, потому что это приводит к вакууму. Во-вторых, многим проектам все равно нужна какая-то версионность хотя бы на случай типа вы раскатили кривое приложение, которое там попортило данные, и вы хотите откатиться и заигнорить все данные, которые были там изменены на новую версию, ну что-то в этом роде. То есть это может иметь смысл по целому ряду причин. Ну и самое интересное, что если мы хотим и менять данные, и одновременно делать ребалансировку с разрешением записи? Мне кажется, это сложный случай, но решаемый. То есть можно переносить, как было описано в первом случае, то есть типа запрещена запись, но мы запись разрешаем путем записи, вот чисто когда у нас происходит ребалансировка, мы пишем лог того, что происходило туда, куда мы переносим данные, и по какой-то логике их мержим. Это сложная логика, которую никто никогда не будет тестировать, но это в теории должно работать. Я понятно изложил идею? Да, все понятно, но мне кажется, это и протестировать не так сложно. Если время от времени искусственно делать ребалансировку на продакшене и проверять, что от этого ничего не ломается, ну в принципе это должно работать. Если ребалансировка делается раз в два года, то по-любому все сломается. Это сделал прямо очень четкий подход к следующей теме, но давайте пока попозже ее ответим. Знаешь, в чем сложность вопросов? Сложность вопросов получается в нагрузке на диск определенных нод, и вообще на определенные ноды, не только на диск, но и на сетевую часть, скажем. А нагрузка, она полностью в твоем контроле? Ты же сам пишешь питание через скрипт, которое тебе данные переносит и импортирует? Да, но обычно ты делаешь ребалансировку ведь в случае, когда у тебя что-то случается, ну я имею в виду, диск забился на 90%, слушайте, давайте, может, начнем переносить, давайте. Ты начинаешь переносить и понимаешь, что у тебя сейчас на сеть нагрузка 90% от capacity, ты не сможешь больше, чем 10% бить. Знаешь, сейчас так это, мы сейчас подходим к такой интересной теме, это вне зависимости от того, кто у нас сделал ребалансировку питания через скрипт, админ Вася питания через скриптом или умнейший высоколобый мужик из гугла, у тебя такая проблема может быть все равно, это кто-то планировал capacity для кластера, сделал свою работу плохо, потому что практически любая распределенная система, она страдает от того, что ребалансировка штука дорогая, больше того, тут есть как-то два стула, можно сделать очень много винод и переналив каждый, будет дешевый. Почему опять мебель? Ну, потому что такое, знаешь, есть такая флиска с капитального стула, ну вот, как по ее вообще выпуску у нас сегодня про мебель очевидно, поэтому почему бы и нет. Вот, собственно, что я брал-то. Если у нас какая-нибудь кассандра или там даже тот же самый постглес мы нарезали очень-очень гранулярно, и у нас переезд данных становится дешевым, но при этом у нас накладные расходы на всю эту распределенщину становятся достаточно высокими. Большинство Data Locality страдает достаточно сильно. С другой стороны, мы можем идти по тому пути, по которому пошла, например, CYL, когда у нас нода настолько жирная, насколько она может быть, у нас каждый, как это, в нашем случае шар, он занимает практически всю машину или там существенную ее часть, но при этом переезд такого шарда, это... Ну, он очень эффективно использует ресурс системы, когда все хорошо, но за переезд такого шарда, ну, в общем, это нужно планировать. То есть, как бы вот, либо одно, либо другое. Я хочу сказать страшную вещь вот по поводу замечания Вани, что диск забился на 90%. Я как-то узнавал у наших админов, типа, а при каком размере, ну, вот то, что мы называем шардом, при каком размере шарда, PASSGRES гарантированно себя хорошо ведет. То есть, я напомню, что у него там есть долгий мучительный автовакуум, есть wrap-around ксидов и многие другие страшные вещи, которые проявляются, когда база большая. Вот, ответ мне дали такой, что где-то до терабайта оно работает более-менее нормально. Это я всё к тому, что вот рассмотрим автовакуум, в худшем случае он отжирает столько же места, в таком искусственном случае вакууме, вакуум в вакууме, удачная игра слов. В худшем случае у тебя вакуум может отжирать столько же места на диске, сколько занимает база, потому что он, например, пойдет перестроить все индексы. И это означает, что на практике у тебя диск должен наполовину пустовать, ну, на всякий случай. Сразу по двум причинам. Во-первых, вот как я уже описал, он может быть нужен внезапно под нужды внутренней самой СУБД. Во-вторых, если ты хочешь что-то куда-то перенести, тебе нужно сначала записать архивчик локально на диск, потом начать его по сети гнать. Так что диск забился на 90%, это вы очень неудачно спланировали, означает? У меня диск никогда не забивался на 90%, это я тебе уже говорю, что ты приближаешься к точке, в которой может быть опасно. То есть 90% от 100%, когда это еще безопасно. Ну, что-нибудь типа такого. Ну, это означает, что у тебя диск на 70% пустой еще. Угу. То есть потом еще 10% пройдет, и ты будешь в зоне уже, когда опасно. Не надо попадать в такие зоны. Насчет сети, она тоже не должна быть забита, потому что надо быть готовым к пикам. Не знаю, через 15 минут первый канал рассказал про твой супер стартап, и у тебя там супер подскочила нагрузка. Это по-любому, но вопрос всегда, что все это, во-первых, неожиданно, во-вторых, все стартапы обычно рассчитываются экспоненциально, и ты просто не успеваешь решать все вопросы. Ну, то есть ты рассказываешь правильный, хороший, идеальный случай, но он никогда не происходит в мире. Нет, это я понимаю, но лучше иметь такой случай, чем другой. Однозначно. Ты понимаешь. Вот, и буквально два слова про распределенные транзакции между Шардайом, потому что всех же этот вопрос беспокоит, по крайней мере, меня точно. Во-первых, в блоге Рысцова расписано, как это можно сделать. Вот я ссылочку сейчас в чатик кину для тех, кто пропустил. Важный момент, нужно в таких транзакциях, когда ты читаешь ключи, работать с ними так же, как если ты их пишешь, иначе у тебя не приходит snapshot isolation. Кстати, эти транзакции, они даются snapshot isolation, что они не сериалайзабл, но во многих приложениях канавиатов, Oracle, например, ничего круче snapshot isolation не умеет. Да, и у меня, кстати, вопрос. Вот эти транзакции, которые поверх Comprehensive Web, которые, я забыл, как они там по названию папера должны называться, которые в некоторых документациях называются 2PC транзакции, а это не тот 2PC. Как они взаимодействуют с нормальными транзакциями внутри базы? Кто-нибудь пытался разобраться? Смотри, если ты прям прикручиваешь их сверху, я так сходу не скажу. Если брать, например, Cockroach, который на самом деле на них, даже он их апгрейдит до нормального serializable snapshot isolation, там, в общем и целом, похожий подход, и там, в общем-то, у тебя просто так получается, что ты в одной такой транзакции, в один шарт шлешь сразу все, что ему нужно знать для этой транзакции, то есть все ключи, которые должны были поменяться в этой транзакции. И дальше уже внутри баз данных, у тебя баз данных должна это уметь как бы разруливать, то есть, например, если у тебя тот же самый ROKS, у него есть по ней транзакция, я правильно помню, которая умеет несколько ключей тронуть. Вот, ты их трогаешь тем способом, который там описан, и, собственно, оно и происходит. Как это работает в случае с базами, которые совсем как это, совсем другие? Позволь, я переформулирую немножко вопрос. Есть две транзакции. Первая транзакция, я вот просто пришел на шарт и там сделал begin, там short shelf, там commit. Вторая транзакция, это мега распределенная транзакция на, в принципе, compare and swap, которая аффектит два шарда, и там где-то есть объект транзакции, я там пишу локально какие-то объектики, вот это все. Вопрос заключается в том, вместе те транзакции и другие, они дают хотя бы snapshot isolation или там вообще все ломается? Ну, в общем, у тебя обычная транзакция, тебе она должна быть по меньшей мере доказываться о том, что у тебя там такое понаписано в метаданных о объекте. Вот, это означает, что на самом деле у меня абсолютно все транзакции должны парсить вот те метаданные, которые относятся к распределенным транзакциям. Парсить, да, то есть им не обязательно ходить в другие машины, но парсить, понимать, что, например, это... Они не могут, извини, но они не могут не ходить, потому что им надо знать состояние транзакции распределенной. Не обязательно, то есть там может быть, ну там в метаданах там может быть описано, что сейчас уже фаза, когда все заканчивается. Или, например, ты читаешь, что тебе ok, stale read и том подобное. А, ну то есть ты можешь сделать себе чуть менее сильную гарантию. Да, да. Ну окей. Вот, вроде... А, интересный случай про, как бы, граничный случай с неизменяемыми данными. Всё, это последнее, что я хочу сказать, честно. Если данные неизменяемые, то ты можешь делать очень классные распределенные транзакции, просто записывая куда-то, что нужно записать на шард такое-то, такое-то и такое-то, такие-то значения. Это удобно тем, что её гарантированно можно докатить всегда. И, ну это не настоящая транзакция, но если типичный пользователь обычно читает с одного шарда, то с точки зрения пользователя, ну вот я, например, возвращаюсь к примеру с чатом, да, я всегда читаю свой инбокс, да, ну я 90% времени читаю только свой инбокс. И если у меня есть распределенная транзакция, которая выглядит так, что запиши в инбокс такого-то пользователя, что он получил сообщение, и в аутбокс другого пользователя, что он отправил, если она там выполнилась, например, наполовину, то с точки зрения одного пользователя всё выглядит довольно консистентно. Этим я хочу сказать, что неизменяемые данные, они позволяют делать очень лайтовый тип транзакции, хотя на самом деле нет, но удобно. На самом деле ты сейчас описал, вот мы много выпусков назад обсуждали такую штуку, она называется core-fu. Это такой хитрым образом шардированный, ну можно сказать, по времени записи распределённый консистентный лог, и вот как бы если то, что ты сейчас описал, как это, довести до экстремального состояния, вот у нас здесь есть лог, который является логом нашей супер-мега-базы, и лог он как бы по определению иммьюзабл. Вот, мы в него всё пишем, а потом у нас этот лог читают столько разных систем, сколько нам нужно, и дальше ну как бы полная окавка начинается, когда у нас каждая система, это по сути такая материализация этого лога, и там, не знаю, у тебя может быть чат, так может быть сделан вообще любое приложение, любая база данных. Это довольно забавно. Ну то есть там тот же самый пейпер описывает, как базу построить со стройкой консистентности, или там потом был более поздний пейпер, назывался Танку, о том, как поверх абстракции вроде CarFull построить типа такого объекта всерелизуемого с транзакционными обновлениями. Вот, поэтому на самом деле это очень мощная идея, то, что ты сейчас описал. Вообще неизменяемые данные, это очень мощная штука, почти в любых задачах, всегда пытаетесь сделать всё неизменяемым, потом пригодится. Да-да-да, сперва сделайте, а потом подумайте. Ну, кстати, ещё такой момент, что я это вижу больше одного раза, ещё во всяких системах, где облачная местами бигдата, может даже или не облачная, но всё равно бигдата, вот там, где бигдата, там очень часто тоже применяют подходы с тюмстонами и корректирующими ставками, потому что если у нас какой-нибудь колоночный сторож, он наколенный, он или не наколенный, идти искать оригинальную запись, что-то с ней делать, это дорого. И, кстати, об этом даже говорился, опять-таки, курс, который Саша в прошлом выпуске упоминал, там был момент про то, как люди живут с memory-алабом, и как вообще делать корректировки старых значений, да никак. Я подозреваю, что в дисковом алабе то же самое, никак это делается, это просто дописывается специальное значение, которое потом как-то сортировывается в обычный массив, когда этих дописанных специальных значений стало слишком много. Потому что, а как иначе, если у нас специально сжатое колоночное хранилище, в котором сжатие сломается, если мы начнём туда просто так вставлять рандомно. Окончательно, зарубая эту тему, хочу сказать, что если уважаемый соведущий или кто-то из слушателей знает ещё какие-то умопомрачительные идеи на тему, как делать ребалансировку на системах, которые не имеют её из коробки, или делать распределённые транзакции, то пожалуйста поделитесь, ну вот в случае с ведущими, вот прямо сейчас, в случае с слушателями в комментариях, потому что меня эта тема очень интересует. То есть в первом приближении это всё, что мы знаем про, ну мы ведущие знаем про ребалансировку и распределённые транзакции. На пасгрэсе, например. Молчание знак согласия. Именно так. Мы какое-то время назад сейчас говорили о том, что если что-то где-то у вас плохо работает и сломалось, как от этого избежать, как попытаться от этого защититься, и что надо заранее всё правильно проектировать. И вот как раз в эту тему в блоге Netflix опубликовали новость, что вышла новая версия «Хаос мамки 2.0». Вообще, на самом деле, это довольно интересная концепция. Я имею в виду, она интересна тем, что она имеет очень много точек зрения на саму себя. У меня многие знакомые на предыдущих работах считали, что это просто бредятельно, как так взять и убивать машину. Они не видели, что за этим видит Netflix. У Netflix очень много точек зрения, вполне возможно, я их не все понимаю, но те, которые я понимаю, мне очень нравятся. И самое важное среди них это то, что когда ты работаешь с «Хаос мамки», у тебя не важно, как ты строишь архитектуру. То есть тебе в принципе можно делать намного меньше дизайна, дизайн-ревью и каких-то рефакторингов архитектуры, потому что, говоришь, ребята, делайте как хотите, делайте, что знаете, только два критерия у нас, чтобы было отказоустойчиво, чтобы данные не терялись, чтобы оно постоянно работало и так далее. А во-вторых, мы вам включим в «Хаос мамки», хотите вы того или не хотите. И у чуваков начинают раскрываться глаза пошире, волосы шевелятся везде, и они начинают сильно-сильно думать в этом направлении, потому что если ты точно знаешь, что твой сервис будет падать в течение недели несколько раз, 100% он упадет в течение недели, ты сделаешь все, лишь бы у тебя все нормально было с архитектурой. И вот этот подход заставляет совершенно по-другому строить дизайн-систему, архитектуру-систему, взаимодействие систем. И получается, что казалось бы, небольшая концепция удаления нот, убивания нот, она сильно улучшает жизнь как самих разработчиков, так и группы поддержки и СРИ. Теперь к новости самой. Вышла версия 2.0, Netflix очень радуется, радуется, радуется. Основные нововведения… Важный вопрос. Извини, это все open source? Да. На чем написано, какая лицензия? Вот версия 2.0 я сейчас могу прям посмотреть, они должны ли ее написать. Они работают на… ну давай, ладно, сейчас поближе. Вот прям заставляет меня… Ты рассказывай, а я посмотрю. Нет, я уже зашел. Go 100%. Apache. И Go 100%. Apache и Go 100%, но не супер ли? Оно работает сейчас на Spinnacle. Spinnacle – это их система для Continuous Delivery. Он прямо интегрирован внутрь системы так, что когда ты свою систему построишь на Spinnacle, у тебя автоматически появляются там галочки «да, я хочу рестартовать эту штуку, я хочу вот ее так рестартовать» с такими настройками и так далее, и она автоматически сама всегда знает на чем, как и где у тебя запущена эта часть системы и убивает ее. То есть Spinnacle позволяет работать с несколькими облачными провайдерами, начиная с… ну в общем со всеми, с основными. И поэтому когда ты ставишь галочку «я хочу это убивать», тебе не важно, запущена она в Гугле, на Амазоне или где-то еще. Что интересно, что мне понравилось, HouseMonkey сейчас тоже сама работает на Spinnacle, то есть она тоже запущена как стандартная система, и у нее стоит дефолтный настройка «саму себя тоже систематически убивать». То есть HouseMonkey убивающая саму себя мне прям понравилось. По сравнению с предыдущей версией, что поменялось, во-первых, вот эта интеграция Spinnacle не позволяет, понятное дело, работать, насколько я понял, возможно я ошибаюсь, но скорее всего она не позволяет работать без Spinnacle, то есть вы хотите работать со своим каким-то непонятным произвольным кодом, работайте со старой версией. Во-первых, во-вторых, они раньше в версии 1.x имели несколько настроек вида не только убивать ноду, но и делать какие-то с ней пакости, вида «а давайте мы CPU на 100% чем-нибудь забьем, или мы диск отключим ей, или еще что-нибудь такое». Их практика показала, что эти все остальные тесты, они, во-первых, очень редко срабатывали и давали какой-то положительный результат, но положительный результат в смысле находили ошибку, которую надо исправлять. Получалось, что они дофига сделали функционалы, дофига проверок проводили, но это не дало никакого плюса. Поэтому они все эти вещи убрали и оставили только тупо убивать систему. И второе изменение, раньше убивание системы можно было настроить, либо ты просто терминейтишь систему, либо ты запускаешь какой-то скрип для терминейта, и всякие умные люди вставляли в терминейты, например, мягкий graceful shutdown, то есть «давайте ее осторожненько остановим». А ребята из Netflix сказали, что вот эта осторожная остановка, она не имеет ничего общего с теми проблемами, которые случайно случаются на предакшене. То есть, когда вас внезапно вырубает половину хостов из-за нехватки электричества, у вас никто в этот момент плавненько не останавливает ваши диски, базу и так далее. Поэтому вам надо быть готовым к самому худшему варианту, и вот вам самый худший вариант. Мы встроили его по умолчанию, по-другому делать ничего нельзя. Как-то так. Ну что, будете использовать? Я отключился, что ли? Меня что, не слышно? Алло? Ой, Саша, Афка, я, короче, на мире сидел. Понял. Вот. Ну что, я могу сказать за себя, что я большой фанат, и я считаю, что нештатных ситуаций быть не должно. Хаоспанки делают любую нештатную ситуацию штатной. Это знаешь, есть такая полемика вокруг ассертов, что мол, дебаг-версии собираются ассертами, а в релизной версии их выпиливают. Это довольно странно, потому что, знаешь, не помню, где прочитал, в какой-то книге была такая забавная метафора, что это как ходить на суши с плавательным кругом, а потом в море выходить и его выбрасывая. Да-да-да. Вот. И это такая же история, получается, что у нас есть какие-то меры предосторожности, мы что-то пытались натестировать вне боя, а потом в бою мы такие дрожим над тем, чтобы всё было хорошо. Если у нас в бою всё может быть нехорошо, то это отличная ситуация, отличная история для того, чтобы люди писали свой код сразу нормально. А я знаю, откуда это идёт, Валер. Это идёт же... То есть мы с тобой просто привыкли let it fail, умирать как можно раньше, и для того, чтобы не доставлять своим непонятным дальнейшим поведением системе проблем. А большинство тоже не привыкло, я имею в виду стандартного старого мира, не привыкло к этому. Это сейчас всё постепенно переходит. Ну, не знаю, я не сказать, что Эрланг, мой взгляд, на это поменял, я скорее просто наблюдаю за тем, как люди работают вместе и как люди не хотят думать о плохом. Да-да-да, чрезмерный оптимист. Да, и в общем лучше сразу на них набросить страшный мир. Но опять-таки после некоторого наблюдения за... То есть ещё есть такая проблема, когда у тебя отдельная команда AppSoft, которая не пишет код с нового приложения, то есть SRE не пишет код в сервис, то получается, как обычно... Ну да, у тебя есть какой-то сервис, который написан людьми, которые его не оперейтят, а у тех, которым занимаются оперейшнс, у них есть некоторый опыт того, от чего встают волосы в разных местах на теле. И разработчики сервиса не знают, они считают, что они живут в прекрасном мире с бабочками-радугами и единорогами. А потом, когда оно случается с их сервисом, оперейшнс не знает, что с ним делать, потому что ребята, которые писали сервис, вообще об этом не знали, потому что они его никогда не трогали. Вот я к чему веду, потому что если у вас прям хардкорнейший девописк, например, было в Вуге, когда оперейшнс и девелоперс это вообще абсолютно единая сущность, то есть что написал, то потом оперейтишь. Там хаос манки может быть не так важен, потому что понимание, что случается, оно и так есть, и очень хорошее. С другой стороны, если у вас такие раздельные команды, то мне кажется, это скорее необходимость, чем даже... То есть не стоит себя спрашивать, нужно ли мне это, это нужно делать, если есть такая возможность. Если вы в позиции, когда это решение принимается, его нужно принимать в положительную сторону, если у вас раздельные команды. Слушай, ты знаешь, я в последних местах работы был все время сам оперейшнс, то есть девопс чистый, но я все время видел точки, в которых введение хаос манки сильно ему помогло. Я постоянно к этому стремлюсь, но я ни в одной конторе пока не смог его включить. Ну просто смотри, ты был сам себе оперейшнс, ты при этом всем сервисом владел, или ты только кусочком? Я только кусочком, но я даже на этом кусочке не смог. Ну просто смотри, внутри своего кусочка ты знаешь о всей фигне, которая может произойти, и ты уже, скорее всего, напишешь код так, чтобы не просыпаться ночью от пейджер-дьюти. Нет, в том-то и дело, что мой сервис, скажем, он даже может быть и будет продолжать работать хорошо с хаос манки, но все остальные, скажем, могут не рассчитывать на то, что внезапно айпишник поменялся, понимаешь? Ну то есть вплоть до такого могло доходить. Я к тому, что там, где есть девопс, введение хаос манки все равно поможет все эти узкие места найти. То есть мое стремление включать хаос манки, которое ни разу не было успешным, оно все равно приводило к тому, что мы кучу фигней выносили из кода и заменяли нормальной версией. Понятно, да? А, еще знаете, как у меня вопрос про хаос манки? Вот представьте себе ситуацию, я вообще в целом хочу теоретически понять. Допустим, у меня есть сервис какой-то, который должен пользователей обслуживать. Допустим, даже в нем нет данных, то есть стейтлос. И допустим, у меня... Нет, без данных неинтересно. Допустим, он стейтфул. И у меня есть три ноды. То есть падение любой ноды приводит к тому, что у меня остается два ноды, остается кворум, я могу еще обслуживать. Теперь, предположим, я хочу подключить хаос манки. Как мне это делать? Если я буду просто на эти три ноды натравливать хаос манки и надеяться, что за те минуты или две, пока будет стартовать и собираться новый большой кворум, ничего не произойдет, то в принципе можно так сделать. Но получается, что ты уменьшаешь эти кейсы своего сервиса. Если же ты хочешь полностью сделать надежным, то на время включения хаос манки ты должен увеличить надежность своей системы. Верно? То есть, грубо говоря, увеличить количество нод до пяти, для того чтобы падение одной ноды под хаос манки, а второй ноду случайное совпадение, оно не привело к падению твоего сервиса. Ну смотри, на самом деле это читерство и так делать стоит. Нужно именно, чтобы хаос манки эмулируют реальную ситуацию. На данный момент описан такой случай, оно эмулирует ситуацию, что твоя нода легла, вы ее не стали поднимать, потому что у вас же еще есть резерв, и резерв тоже свалился. И мы ее вообще никогда не поднимаем, оно само должно подниматься. Я к тому, что добавление хаос манки, когда у тебя есть система и случайный фактор, когда у тебя есть система, случайный фактор и хаос манки, у тебя система должна быть более надежной. Скажем так, можно у хаос манки поставить какой-то разумный рейд, что хаос манки не делают фигни чаще, чем раз в час. Даже с этим часом у тебя все равно... Представь, есть вероятность, что за этот час 0,05% что за этот час упадет нода. Смотри, на самом деле это можно немножко считерить, если у тебя уже и так реальная проблема на протекшене, можно хаос манки выключить. Однозначно, это тоже однозначно. Но с этим все равно у тебя уменьшается теоретическая SLA. Ну, почитай проценты. Уменьшается, но, короче, смотри, за счет эффекта того, что хаос манки тестирует твой систем постоянно, на самом деле увеличивается. То есть оно в моменте уменьшается, глобально увеличивается. Увеличивается за счет более правильной архитектуры, за счет более правильных процедур и всего такого. Я понимаю, с тобой согласен. Но вообще получается, что если я правильно все настрою, то есть у меня идеальная система, то не включенные хаос манки это теоретическое уменьшение SLA. Ну, как бы, наверное, если ты свою систему никогда не модифицируешь, и у тебя больше нет багов, то, наверное, хаос манки можно выключить.",
    "result": {
      "query": "хаос манки netflix"
    }
  }
]