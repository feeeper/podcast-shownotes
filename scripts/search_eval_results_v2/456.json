[
  {
    "segment_id": "cdb3bfb0-84c4-47d7-8cbb-cd8e4733086e",
    "episode_id": "67c25fa8-aac1-48f0-9b73-891d8bc417c0",
    "episode_number": 456,
    "segment_number": 3,
    "text": "эту систему, то мэп no-no-no, то есть как бы ни в коем случае. Мэп не нужно использовать ни в коем случае, но не по тем причинам, о которых ты говоришь. То есть я, может быть, не до конца понимаю, конкретный сценарий, который в данном случае пытаюсь решить, но в чем в целом претензия к мэпу? В том, что у тебя выделяется кусок виртуальной памяти, и ты говоришь, что я буду класть туда данные, а на диск, то есть у меня есть физическая память, ее меньше объем, да, чем виртуальный, и лишняя операционная система за меня будет вытеснять на диск. И я за счет этого получаю то, что в подкасте, то есть я получаю то, что в подкасте, то есть я получаю то, что в подкасте, называется Shared Buffers, то есть пул для данных, которые вообще-то у меня лежат на диске, но я часто используем и хочу пошарить в памяти, говоря просто. То есть получается, что область памяти, в смысле памяти меньше, чем размер файла, который ты хочешь, с которым ты хочешь работать? У меня база данных на терабайт, у меня памяти, скажем, 100 гигабайт, я хочу самые часто используемые страницы, куски индексов и так далее положить в память. И обычно, когда речь заходит про mMap, говорится, что у меня операционная система за меня разберется, с чем я чаще всего работаю. Претензия заключается в том, что операционная система не знает характер нагрузок твоего приложения, твоей базы данных или чего-то, и поэтому может загружать и выгружать эти данные саб-оптимально. Обычно это хорошо работает, то есть мы обсуждали несколько пейперов по этой теме, mMap обычно хорошо работает, когда у тебя последовательное чтение больших объемов данных. Когда операционная система действительно правильно предсказывает, что вот ты идешь по диску, читаешь данные последовательно, наверное, тебе понадобятся те данные, которые идут следом. Ну и здесь река Кольцевой буфер, поэтому, полагаю, под это подходит. И вероятно, вполне может быть, что да, под эту задачу подходит. Но в общем случае, в общем случае, про то, что мы говорили, что это не так, проблема в том, что ты можешь хотеть прифетчить какие-то куски индексов, ты можешь хотеть прифетчить, не знаю, таблицу в обратном порядке, ты можешь знать, что у меня сейчас 5 транзакций работают, которые активно сканируют вот эту таблицу, и вот эти данные, которые уже закэшированы, их ни в коем случае нельзя извлекать из буфера. И в общем случае mMap на сложных нагрузках, он проигрывает. На простых случаях, таких как последовательное чтение, обычно он не сильно хуже собственной реализации, то есть он все равно немножко проигрывает, но там типа на 2% в пределах погрешности измерений в эксперименте. То есть если бы мы взяли обычную базу данных с очень сложной нагрузкой и вот с теми параметрами по памяти и объему, которые ты сказал, и положили в mMap только, скажем, индекс, то у нас не было бы проблем. Какие у тебя запросы? Ты только индекс, ходишь только по индексу и не обращаешься к данным? Ты по индексу ходишь только вперед или можешь ходить назад? Ну и кто кэширует, собственно, данные? Нет-нет, с данными отдельный вопрос. А, ну да, я понимаю, это же одна и же база данных, у нас не может быть отдельного запроса. Я понял, да, идея. Но я согласен, да, здесь половинчатый случай, то есть как бы здесь не идеальный случай, что мы всегда записываем вперед, мы записываем не sequentially, но мы записываем в предопределенную область, и они, соответственно, защищаются с помощью внешней системы от того, что ты не записываешь туда, куда не надо записывать, и у них всегда область памяти совпадает, ну как бы они не сделают ситуацию как с базами данных, что у тебя там несколько десятков терабайт, а памяти там гигабайты.",
    "result": {
      "query": "mmap vs database shared buffers"
    }
  }
]