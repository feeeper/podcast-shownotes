[
  {
    "segment_id": "e1e62702-6861-419c-9745-a85590eac10c",
    "episode_id": "5670cae8-970b-483c-963d-f17cf343b8d3",
    "episode_number": 199,
    "segment_number": 4,
    "text": "kubectl await, вот это, конечно, маленькая-маленькая толика того, что было изменено, и пожалуйста, смотрите официальный пресс релиз, там все намного интереснее. У тебя этих изменений больше всего, что радует? Сложно сказать, здесь они все достаточно минорные, прям таких больших изменений нет, я думаю, что то, что заменяют iptables на ipvs, это хорошо, это значит, что большее количество, то есть это решение более производительное и будет лучше работать с большими кластерами, быстрее будет перенаправляться трафик, вот это все станет удобнее лучше, все остальное вроде как по мелочи, ну как-то так. Ну core dns тоже степень как бы. Да, да. Вот, а тем временем я на неделе наткнулся на статью, притом не новую статью, от девушки-инженера, которая, я понимаю, в тот момент, когда я писал, работала в какой-то компании по, не знаю, хостингу или кэшированию картинок, сейчас она, если я правильно понимаю, работает в apple, вот, а статья о кластерных шедулерах, и там сравнивается кубернитес, который мы только что выше подтягиваться обсуждали и такая штука nomad, которая у нас как-то раз, ну пару раз проскакивала, но мы только не обсуждали, я решил как-то, не знаю, как-то сказать это, в общем, мне кубернитес, я с ним игрался в какой-то момент, я честно пытался застапить кластер этого безобразия, там, ну то есть прям устрапить на железе по хардкору, чтобы не пользоваться всякими, как это, детскими настраиваниями кластера, это же не true, нужно же понимать всю боль, вот, ну то есть, как бы тест очень простой, собрать кластер этого говна на железе в каком-то или digital ocean и запустить на нем, там, не знаю, банальный какой-нибудь выпростовалку, даже без стейта, без всего, просто вот самый базовый use case, понятно дело, что там в кубернитесе со стейтом, там отдельные приключения, вот, и кубернитес, в общем, это просто, извините, какой-то хатанический пиздец, на мой взгляд, именно с точки зрения настройки, я не знаю, вот как вот вы, Ваня, вы его настраиваете сами руками или вы проходите процедуру хатанизации? Что такое процедура хатанизации? Когда вы берете, короче, вот, значит, сами руками накатываете везде кублеты, в каждом кублете пишет конфиг, чтобы там запустился минимальный кластер, что потом оттуда забудут страйпить, вот как у вас. Нет-нет, у нас нет такой хатанизации. Вы копсом пользуетесь, да? Извини, у меня тут прорвалось, что еще раз? Вы копсом пользуетесь. Ну да, то есть мы пользуемся стандартными tools, плюс что-то меняем, если что-то не понравилось. Я правильно понимаю, что копс без облаков не работает? По-моему, да. Вот, короче, у меня есть такой какой-то шкурный интерес, чтобы это работало на своем железе, потому что я считаю, что кубернитес, конечно, возможно, неплохой слой абстракции от облака, но какой-то шедрлинг хочется просто вообще, ну, в общем случае, иметь похожий tool, но для... который мог бы работать вне облака. А кубернитес, он, скажем так, вот, я думаю, что судя по тому, там, Ваня, может быть, другое мнение, как человек, который с ним работает, я считаю, что чисто вот пользоваться кубернитесом, пока не для простых случаев. Опять же, Stateful там очень плохо все, ну, что если нужно кастылист отставить. В принципе, для простых случаев или soft-state приложений кубернитес достаточно удобная штука. Для Stateful, ну, есть кастыль как бы как-то более-менее худобедно, но проблема в том, что кубернитес просто идеологически, он рассчитан на клауд, то есть вам то, как у вас IP на каждый контейнер, вот он должен быть вот, и у вас сеть должна быть такая, и она должна поддерживаться, и, соответственно, вам кладут в сети IP, какая-то штука должна в сети IP конфигурировать. То есть в облаках, где сеть и так уже software-defined, это небольшая проблема. А на hardware это, ну, сложнее настраивается. Ну, в смысле, в том плане, что больше требуется от софтины, которая будет всем этим заниматься. То есть, я понимаю, что калика, которая используется у Вани в компании, она, наверное, БГП может настроить и на физической сети, но для этого у вас сама физическая сеть должна быть так настроена, чтобы в ней работало БГП. То есть вам как бы админы должны построить довольно такую неплохую сеть для этого. То есть в маленьком кластере это уже то еще приключение. Опять же, хранилище. Кубернитес, конечно, если я правильно понимаю, там сейчас какие-то зайчатки волиумов, которые на локальных машинах, я не знаю, Вань, поправь меня или нет, но там вроде были какие-то зайчатки локальных волиумов. Типа для стейтл приложений, чтобы можно было какое-то локальное, там, где оно сидит, вытащить место, его использовать, чтобы оно не улетучилось. Да, локальный сторич есть. А ты смотрел для чего? Ну, получается, он у тебя не распределен, он будет привязан к концу. Да, для условных кассандра, скажем так. Ну да. Потому что раньше даже это была проблема. То есть раньше кассандру ты в персистент волиум ее пихал, а персистент волиум как бы... Где-то по сети, и она с дополнительным лейтенсом его еще надо настраивать. А тут фактически тебе хочется локально что-то иметь. Да, да, да, именно. То есть сейчас какие-то худо-бедные касталями с лидфулсетами и локал волиумами это решается. Но общий случай – это монтируйте, пожалуйста, какой-нибудь ваш сетевой сохранилище. То есть вам в дополнение к вашему хардверному кластеру, в котором дорогое железо, скорее всего, потому что зачем даже вы собрали хардверный кластер, еще нужно поднять кластер ковни СЕФа или чего-нибудь такого, и чтобы на нем персистент волиумы хранить. Ну то есть, вот поэтому, как я выразился выше, это просто какая-то хтонь. То есть вне зависимости от того, насколько тебе хорошо пользоваться, и насколько это дает бенефиты для команды, а я считаю, что дает бенефиты для команды, то есть такая модель разработки, когда есть какая-то абстракция типа кластер на сервис, пусть он там как-то крутится, а дальше админы разберутся, как эта штука должна работать. Для админов это вне облака неприятно. А вторая такая принципиальная вещь с Kubernetes, то что он работает только с контейнерами. Это прям может быть претензией в том смысле, что если у вас в компании еще не контейнеры, то внедрять Kubernetes, чтобы везде всем срочно контейнеризовать, возникает как бы разумный вопрос. Мальчик, ты не хочешь себе там что-нибудь контейнеризовать? Уйди отсюда. Вот. Как бы у людей есть другие занятия, им нужно как бы код для продакшна писать, фичи разрабатывать, там не знаю, с техническим долгом бороться, а не в контейнеры все запихивать. Собственно, есть две альтернативы. Это Nomad и Mesos. В общем, прочитав эту статью, которая, ну, спойлер, там выиграл Nomad, я подумал, почесал репу и решил приключиться с Mesos и Nomad. Притом Mesos я раньше опасался, потому что он кажется большой, сложно, тяжело конфигурировать. Ну, скажем так, доки там очень плохие. Mesos такой конструктор лего, ну, по моим ощущениям, и такое ощущение, что такой конструктор для людей, которым хочется собрать какое-то очень специальное управление кластером. То есть нужно вначале заставить Zookeeper. Zookeeper заставится непросто. Там нужно на каждую машину в конфиге прописать всех, всех, всех соседей по Zookeeper, потом в правильном порядке их запустить, чтобы они, и при том быстро друг от друга, чтобы они вовремя в порт собрались. После этого можно настроить Mesos мастера. После этого нужно настроить Mesos слоевые. После этого нужно, как бы, Mesos, при том сам по себе пустой Mesos, он вам ничего не дает, чтобы запускать на нем сервисы. Например, вам нужен Marathon или Aurora. Ну, я Marathon сверху поставил. То есть вот этих шажков многовато. Дальше, если у вас какой-нибудь Flink или что-то такое есть, то извольте еще отдельный фреймворк поставить для этого Flink, например. С другой стороны, в принципе, это не так страшно, как мне казалось. Я осилил это за пару вечеров на DigitalOcean, просто ручками, ну, там, Shell Script поднять. В принципе, из плюсов больших, как бы, да, он очень конфигурируемый. То есть вплоть до того, что можно писать свой фреймворк, и он будет скедировать, куда хочешь, что хочешь. Оно позволяет изолировать не только докером, чем нравится. То есть я понимаю, что контейнер – это клево и удобно. Там, собственно, та же самая статья, с которой у меня все поехало, упоминает, что главная инновация всей этой докеризации и всего остального – это не то, что у вас есть изоляция или что-то такое. Изолировать раньше умели. Главный, как бы, сильная сторона докера – это, на самом деле, универсальный формат пакетов, который работает везде. То есть это как такой DEP или RPM на стероидах. Если у вас уже собираются DEP или RPM, вам, возможно, не нужна контейнеризация. Или если у вас просто один бинарник, вам тоже контейнеризация не нужна. Если у вас Java с JAR'ами, вам тоже контейнеризация не нужна и так далее. Конкретно Java, там, еще ее в контейнер запихивать нужно уметь, чтобы там хип у JVM. Может, начнешь у тебя поправить. Если ты собираешь VD-пакет, это еще ничего не означает, потому что всякие там питоны и прочее добро, их очень клево в контейнер запускать, чтобы разрешать зависимость. Да, я согласен, что языки со сложным рантаймом, они хорошо, вот, как бы, с исключением JVM, который в контейнере плохо называется. Ruby, питоны и прочий веб, да, для них это удобно. Для Erlang тоже, в принципе… JVM обычно не нужно. JVM обычно не нужно, ты JAR'ку собрала, все. Ну, я говорю, да. При том, сам себе JVM, его нужно уметь запустить в контейнере, чтобы там все было правильно с размером хипа. Там просто бывает… Я не знаю, учинили это в новых JVM-ах или нет, но раньше JVM по дефолту выставлял максимальный хип или что-то такое, типа, в то, что он видит с хоста, а контейнер, на самом деле, раньше его по OM-у прибивал. Там были особенности. Но смысл в том, что… Ладно, это неважно, это все детали, и я пытаюсь сказать, что умение запускать не только докер, при том с настраиванием, какие вещи изолировать и нет, это добро, на мой взгляд. Но с другой стороны, там выбора два. Или докер, или что-то такое, там все группы черут опциональные изолировалки, ничего больше нет. Ну, или по камере по дефолту нет, ничего больше. Пока ты не пошел дальше, ты не рассмотрел установить кластер с помощью Qbadm? Нет, потому что, еще раз повторюсь, я не рассмотрел дистрибутивы. Я хотел понять, как это уставить на полу железо. Ну, Qbadm работает на полу железе, это одна из целей. Вот, а во-вторых, заодно понять, сколько там кусочков. То есть, смотри, если какая-то клевая штука абстрагирует от тебя сложить установки, она не абстрагирует от тебя все равно сложить все эти кусочки, которые друг с другом взаимодействуют, рано или поздно придется потрогать. Согласен. Ну, то есть, это было такое специальное ментальное упражнение, вот забустрапить руками. Потому что, даже если за тебя забустрапила какая-то штука, так, и уже сказал. Ну, то есть, ты просто немножко подменяешь понятие. Ты решил посмотреть на разные шедулеры, а фактически ты говоришь про другое. Я хочу ручками это все собрать и разбираться во всех деталях. Ну, в смысле, да, я хотел посмотреть на разные шедулеры с точки зрения того, насколько это применимо для команд разного размера. Давай так. И которые работают не в облаке. То есть, у меня вот такая принципиальная позиция, это должно работать не только в облаке. Ну, у меня вот, например, я сейчас работаю в компании, где облако не пользуется, а джаст облаком. Это известная публичная позиция компании, что вот мы не пользуемся облаком. Вот, это дает клевые плюшки. У нас есть очень быстрые диски, например. Доступ к NVMe на Амазоне появился очень недавно, и я так понимаю, что там это дорого и не так много, как у нас, например. Продолжай, пожалуйста. Вот, я говорил про мезос, да? Я хотел у Светы спросить, какие у Светы ощущения про мезос? Света! Я знаю, что работала с мезосом. Да, действительно, просто этот шум, видимо, какой-то снова гол произошел, и на улице... Все закончилось. Хорватия победила. Следующая тема, можно не обсуждать. Окей, насчет мезоса, да, действительно, был опыт его внедрения около четырех лет назад, и мы его внедряли для шторма. Основной причиной этого было, потому что в шторме есть проблемы с масштабированием, в частности, если одна топология как-то будет... Ну, то есть, если одна топология возьмет все ресурсы, то теоретически, практически, она будет мешать другим топологиям работать. И чтобы от этого избавиться, был внедрён мезос, и, как ты говорил, действительно использовался фреймворк, то есть мезос, поверх еще марафон, и потом еще фреймворк назывался Storm on Mezos, вроде как-то так, и вот это все добро работало. Попробую вспомнить, что там было из проблем, которые мы видели. Это то, что у тебя получается очень много таких вот движущихся частей. У тебя есть инфраструктура мезоса, там еще зоокипер, там еще марафон, там еще дополнительно шторм, и у шторма там свои тоже части. Если что-то ломается, то дебажить это добро довольно сложно, потому что не очень понятно, где сломалось и с чего начинать. Вот, это то, что я помню из такого. Но, в общем, в целом, оно действительно работало. Опять же, это было 4 года назад, и не знаю, как сейчас выглядит мезос. Ну, скажем так, если на годы 4 работало для вас нормально, то я думаю, что сейчас точно не хуже работает. Нет, я имею в виду, может быть, сейчас есть лучшее альтернативы. Возможно. То есть на тот момент это была хорошая идея, и учитывая, что шторм скорее сейчас развивается в сторону хирона, который написан на других технологиях, я не знаю, насколько оно будет хорошо работать. Да, API переносимо для топологий, но насколько шторм, точнее хирон можно ли поместить на мезос, и насколько это будет удобно, не знаю, трудно сказать. Но, в общем, в целом, у нас было позитивное отношение к этому добру. Ну, так я немножко отвлекусь от обсуждения мезоса, но Flink, например, умеет деплоиться на все. Он и кубернитес умеет, и мезос умеет, и ярл умеет, и стендл он умеет. Вот. Про мезос, да, собственно, минус я такой же вижу, это плюс большая конфигурируемость, минус из-за этой большой конфигурируемости и какая-то невероятная сложность из-за того просто прет. То есть если кубернитесом нужен специальный кубернитес-админ, который будет пасти кубернитес, то в случае с мезосом вам, наверное, нужен... Вам, наверное, кроме этих админов еще нужны будут специальные разработчики, которые будут вам кусочки собирать, отдавать админам, админы деплоить, а потом еще пользователю пользовать. То есть оно чем-то таким пахнет. Не сказать, что оно совсем сложно, возможно, немножко утрирую, но конкретно в Авроре есть марафон для запуска сервисов, а есть Аврора. Аврора, она твиттерская. И вот, например, твиттеровский open-source. И там прям прямым текстом на главной странице написано, что извините, ребят, Аврора – это наш кусок open-source, который мы писали для себя, у нас очень специфические нужды, поэтому если вы это потащите в продакшен, на ваш страх и риск. То есть, как бы, типа, вам будет непросто. Мы ее за open-source-или, но вам будет непросто. То есть я думаю, что для тех, кто строит кластеры в твиттере, возможно, мезос – это хорошая идея, ну или как бы в организациях похожего размера. Там, скорее всего, по-другому нельзя, ну или сложно. Ну и опять же, там, скорее всего, настолько много разных ворклодов, что имеет смысл вся эта заморочка с фреймворками отдельными. Вот. Ну и, как бы, самый минималистичный, как бы, вариант – это Nomod от HashiCorp. Вот если куберники сходят за ручку с etcd, а мезос сходит за ручку с zookeeper, то Nomod на самом деле может работать как бы чисто как Nomod. У него консенсус прям встроен. Однако, пользоваться им гораздо удобнее, становится приятнее ваше волшебство и шутковистие, когда у вас есть консул. Поэтому, ну, я на самом деле тестировал связку консул с Nomod. Это не совсем чистый Nomod. Вот. Бустрайбиться крайне приятно, потому что там консул раскладывается на все машины, им указывается, в какой машине коннектиться и сколько узлов ожидать в коруме начальном. Или если вы в каком-нибудь Digital Ocean, там даже можно просто специальную строчку прописать, и оно само у Digital Ocean спросит, к какому узлу цепляться первому. Вот. Оно само собирается в кластер, и просто вот тот консул у меня собрался вообще без проблем. Да. Конфигурация мезоса, я возился до вечера. Nomod у меня сам поверх этого завелся не совсем сразу, но там была моя ошибка конфигурации, поэтому это не претензия к Nomod, я очень глупо сделал. My bad. Вот. Nomod после этого поверх консула заводится, и вот если не сделать дурацкую ошибку в конфиге, он заводится просто стартованием. В Nomod это один бинарь, но на самом деле там структура такая же, как у мезоса, как у планировщика. Там есть мастер-узлы, есть клиент-узлы. В принципе, ничто не мешает их запускать как один процесс, кроме вашей паранойи. Потому что ваша паранойя не должна этого запускать, потому что вот сервер, который торчит наружу, и с которым общаются клиенты, он по-хорошему должен быть под каким-то очень обрезанным пользователем, и он занимается... эти же узлы занимаются, собственно, самим скедюлингом. А программы, которые занимаются, собственно, запуском процессов, им как бы... их можно, в принципе, без рута запускать, но тогда у них очень много функционала отваливаются. А можно с рутом запускать, но тогда нужно внимательно смотреть, как бы... Я не нашел способа совсем выключить прослушивание в сети. Я помню, что все время к докеру у меня была претензия, что это рутовый демон, слушающий сеть. Вот здесь, к сожалению, такая же история. Это рутовый демон, слушающий сеть. Правда, сеть, он слушающий... клиент можно обрезать просто localhost. Мне это все очень не нравится, и, возможно, какой-то ручку пропустил. Я завел им тикет на GitHub и спросил, как бы, где ручка, чтобы такое выключить. То есть чисто архитектурно эта сеть к номот-клиенте не нужна. Вот. Еще из интересных плюшек – это не только докер, не только запуск изолированных процессов, это еще запуск процессов вообще без изоляции, это еще запуск процессов к уему, Рокета и еще там разные технологии, Elixir в экспериментальной стадии. То есть в этом плане все очень клево. Также, как у Мезоса есть всякие настройки, то есть и у Мезоса, и у Кубернитеса, и у Номода есть все в порядке с AC сериями и прочим таким. То есть если у вас большая организация, крупнее, там, чем три человека, вы, в принципе, можете разрегулировать, кто какие задачи по таким юзерам может стартовать. Это, типа, в этом плане у всех примерно паритет. Номод еще приколен тем, что в отличие от Кубернитеса, у него и от Кубернитеса, и от Мезоса, Кубернитес вообще, в принципе, все считает таким долгоживучим сервисом по проекту Неваня. Там есть, по-моему, или долгоживучий сервис, или сервис, который, типа... Там джебы есть, как и в Номоде тоже. Да. Ну, в общем, у Номода есть три класса вещей. Какая-то такая штука, которая крутится вообще везде, какая-то штука, ну и при этом должна, типа, всегда быть. А какой-то сервис, который есть N-инстансов, есть, типа, батч джебы. Вот. У Мезоса ничего из этих концептов по себе нет, у него фреймворк сделан, а то и будет. В принципе, я не нашел кейсов, которые можно было бы сделать... Нельзя было бы сделать на Номоде, но можно было бы сделать, например, на Марафоне и Кроносе в Мезосе. То есть в Мезосе можно захотеть сделать что-то сумасшедшее и написать свой фреймворк, но то, что предоставляют все дефолтные фреймворки для Мезоса, оно, в принципе, есть все и в Номоде. Больше того, у Номода есть API для скедулинга, то есть если вы захотите для своего какого-нибудь Spark'а, у Spark'а даже есть форк для Номода, чтобы он прям нативно скедулился, вы, в принципе, можете, используя API-шку Номода, сделать так, чтобы нативно скедулить ваш фреймворк. Другое дело, что вам нужно будет похакать ваш фреймворк для распредленных вычислений. Но, в принципе, я, например, не вижу никакой проблемы в том, чтобы тот же самый флинг просто запустить как сервис, типа, который... ну, не сервис, как системный... как-то, да, систем-таск, который везде крутится и просто в него кидает задачи. Вот. В принципе, Номод мне понравился больше всего, потому что он очень легок в настройке. Если у вас, как у меня, проблемы с тем, чтобы запускать клиент под рутом, есть вариант запускать нерутовые клиенты с отключенной изоляцией, чтобы они запускали просто процессы под тем юзером, под которым вы запускаете. И может даже, если у вас несколько юзеров, можно их под разными юзерами запускать по несколькому машину. В принципе, в такой ситуации это, наверное, терпимо. Вот. У меня два вопроса. Во-первых, они хотели сделать какую-то плагинную систему для таск-драйверов, ну, то есть, чтобы можно было своего собственного доделать. Не доделали, нет? Я давно не смотрел. Все еще в состоянии хотели. Ну, то есть, оно, по крайней мере, судя по GitHub, куда-то движется, но... Не в релизе, да? Нет, в смысле, к тому, что в релизе этим пока пользоваться. Ну, по крайней мере, официально этого никто не релизил, скажем так. И, во-вторых, я помню, когда я смотрел последний раз, там были проблемы в случае, если у тебя... Ну, я смотрел как аналог кубернета, я смотрел на докер, докер-драйвер, и какая-то была сложная система с сетями. Она до сих пор не совсем простая. То есть, ну, по сути, кубернитес – это такая супер-опиэнтная штука, которая типа вообще все за тебя менеджит. Минус – нужна эта специальная инфраструктура с тем, что вот IP-шник – это контейнер, туда-сюда. Мезос отдает фреймворку. Честно говоря, я не смотрел, как я как раз-таки тестировал случай без контейнеров, потому что то, что мне было интересно. Как я понимаю, и Мезос, и Marathon, и Nomad отдают сети на откуп механизма контейнеризации. Ну, с драйвером. Да. И то есть, конкретно про Nomad, там у него есть конкретная опция, типа какой сетевой драйвер отдать докеру. То есть, что попросить докеру, чтобы он использовал в качестве сети. И какие параметры ему передать. То есть, в принципе, если хочется использовать какую-нибудь Calico, VWorks или прочую фигню, я так понимаю, можно. Просто это не домен Nomad, а Nomad просто передаст параметры, которые ты бы хотел передать. То есть, у кубернитеса плюс в том, что ты настроил эту штуку на всем кубернитесе, и больше в каждой таске это писать не нужно. Здесь в каждом таске нужно написать, что в стартовании с такой сетью. В принципе, ну, такой компромисс. Если хочется подключить решение для контейнеров в облаке, кубернитес, возможно, будет удобнее. Если хочется именно, как мне, какой-то гибкости и именно минимального набора функционала, типа у тебя реально в контейнерах либо ничего не крутится, или вот как в статье, которая легла в основу этого обсуждения, там у них одна или две задачи крутиться в контейнерах, а все остальное вне. Как-то Ваня упомянул, или Саша, что питоны всякие удобно паковать в контейнеры. У них как раз питонячая задача запакована в контейнер, а все остальное крутится вне. Я согласен, что Nomad для маленьких команд, у которых нет желания разбираться в кишочках кубернитеса и плакать, и рисковать в случае, если таких людей нет, которые будут разбираться. Но Nomad гораздо более правильный выбор. То есть, если вам хотелось, хочется, то есть, я сейчас проведу такую в качестве саммари, вот как я для себя это определил, что хочется. Если вы знакомы с такой штукой SuperVisor D, вот если вам хочется кластер на SuperVisor D, вот это оно. Ну и да, там есть интеграция с консулом, поэтому там всякий лоадбелансик, это все одно прикручивается. Сори, Вань, ты, кажется, перебил. Да нет, в принципе, я тоже все сказал. То есть, это хороший выбор. Единственное, что меня сейчас смущает немножко в Nomad, это в том, что Hashicorp стал очень мало в него вкладываться в последнее время. И мне непонятно почему. То ли у них новые курсы, а не только стабилизация и баги, то ли они просто допилили уже до состояния, когда они больше от него ничего не хотят, и только драйверами или кастомными какими-то операторами уже будут развиваться дальше. Я буквально недавно видел, что они искали Project Manager на проект. Возможно, у них просто были какие-то проблемы с типа чисто человеческого толка, что… Чистой техничкой. Да. Вот я зашел сейчас в Hashicorp-овский GitHub-репозиторий Nomad, там 360 открытых тикетов на Nomad. Ну, справедливости ради, это… короче, не стоит на количество талкот тикетов смотреть, потому что Hashicorp позволяет использовать репозиторию для вопросов. То есть, я вот в Nomad, в тикеты, просто спросил вопрос. Мне лень идти в рассылку. И они это позволяют, и в принципе, я считаю, это нормальная история. Там огромное количество тикетов от эквашены, или какие-то обсуждения. Там прям критичных багов, ну, если они есть, то я точно… ну, то есть, на своем мини-эксперименте я их не собрал, скажем так. Нужно описать два открытых тикета с лейвлом бага. Ну, не знаю, я как бы не готов, я не читал их. Я согласен, что у Kubernetes поддержка и хайпа существенно больше. И Nomad, и… ну, Mesos, он сам по себе, как Mesos, вообще не хайпится. Там есть другая штука, которая хайпится по названиям dc-os, но это, как я понимаю, это такой дистрибутив в духе COPS, где там, кроме собственно Kubernetes и Samsung, или там, в случае с Mesos, кроме Mesos, упаковано еще всякого другого, что хорошо работает в этой экосистеме. На мой взгляд, короче, dc-os – это еще больший сброд разных технологий, некоторые из них клевые. Там есть, например, совершенно клевый load balancer, типа вот… который работает, ну, как раз таки, в… ap tables правят. То есть там Erlang, который крутит какой-то, вообще очень модный распределенный алгоритм кластерного мембраншипа, потом меняет ap tables, чтобы раутить, типа, айпишник на сервис. Все очень модно, клево, молодежно. Одна проблема – у вас один кусок на C++, другой на Java, третий на Erlang, четвертый на Python. То есть просто, чтобы запустить dc-os, вам нужно поставить все существующие языки рантаймы на ваши машины. Как бы странно. Я, когда выбирал и сравнивал Kubernetes и nomad с Docker, я в итоге перешел к Kubernetes по причине очень сильной продуманности к управлению релизами. То есть у тебя декларативный стиль работы с системой, это огромный плюс. То есть ты не выполняешь какую-то команду «я хочу запустить дото», а ты говоришь «я хочу видеть вот такое конечное состояние системы». Это раз. И второе – это то, что релиз – это тоже объект в системе Kubernetes. В nomad тоже называется deployment. У него этот deployment гораздо меньшими гибкостями обладает, чем в Kubernetes. Ну, по крайней мере, на тот момент, когда я смотрел, был. С точки зрения там rolling-upgrades, каким образом изменяешь систему, ну и вот такие вот штуки. Я сейчас не помню. Я не знаю. Или ты очень давно смотрел? Я давно смотрел, как только появился первый версии. А, ну очень возможно, потому что сейчас ты не упомянул ничего, чего бы nomad не умел. Ну, то есть сейчас у nomad вот эти релизы – это прям полноценная богатая функциональность. Опять же, я запускал какие-то совсем игрушечные тесты, но я прочитал документацию, в документации есть rolling-upreleases, bluegreen deployments и так далее. То есть, возможно, он не настолько гибкий, как в Kubernetes, но до хорошей степени есть. Надо поглядеть. Ну, то есть, я, скажем так, для себя сделаю вывод, что я буду продолжать играться с nomad, то есть с Mesos, скорее всего, не буду, потому что он слишком переусложненный, а nomad – это что-то такое, что похоже на то, что можно использовать в коллективе, не знаю, на 30-40 инженеров. Начал смотреть в интернете сравнение nomad и kubernetes deployments и нашел в hightower-репозиторе, называется nomad-on-deployments. Тире он, тире кубернетес. О боги. Жжу проблема, простите. Ну, это так же, как запускают кубернетес на Mesos, но если кубернетес на Mesos, я хоть как-то могу понять, что Mesos – это прорисовался к растеру, а кубернетес – это про управление процессами. Но вот на кубернетесе это уже старанненько. А где скачать Docker container с кубернетесом, я хочу попробовать. Там интернет от тебя внутри нужен, там всего интернета, тогда на диске это не поместится. Интернет на диске это помещается, но без Docker. Это была какая-то сложная шутка, очень. Поехали дальше, я предлагаю закрыть тему. Окей, да. В общем-то, эта тема заняла многовато времени. Прежде чем мы закроем, я хочу единственный момент сказать, что если у кого-то, у слушателей, есть опыт с чем-то из перечисленного, кубернетес, Docker, Mesos, оставьте ваше мнение в комментариях, мне очень интересно почитать, возможно, послушать. Да, Саша правильно сказал, предыдущая тема заняла многовато времени. Следующая тема про время, и если вы любите шутки про время, я очень рекомендую почитать эту статью, потому что потратить свое время, чтобы почитать эту статью, потому что в ней много шуток про время. Вообще, я на статью обратил, в первую очередь, свой взгляд, потому что она очень красиво свёрстана, то есть если вы её посмотрите на компьютере в браузере, не знаю, как она на мобильном выглядит, на компьютере она выглядит очень красиво, тут картиночки, видео, разные шрифты, разные фоны, вставки и всё такое, и пришёл к поддержке, но вообще всё тоже очень по делу и очень здорово. Автор Зак Холман, вы можете помнить его как человека, которого выперли из гитхаба, и сейчас он делает свой календарь, и поэтому у него есть большой опыт с тем, как работать с временем. Секунду, секунду, скандалы, интриги, а за что его выперли? Это несколько лет назад было, и все желающие могут подмоглись, там, на историю, была косвенно связана с тем, когда выперли техническую директору и сооснователя гитхаба. А, там кто-то кого-то домогался, как обычно? Ну да. Сам господин Холман никого не домогался, он просто оказался в ненужное место в ненужное время. Всё, я понял. Он был не совсем так, на самом деле. Ну окей, ладно, я там не был, но судя по тому, что я знаю, он там косвенно связан.",
    "result": {
      "error": "API request failed: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-xSG9D345gtYlIKC330wCyrEG on tokens per min (TPM): Limit 30000, Used 23836, Requested 13336. Please try again in 14.344s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
      "stack_trace": "Traceback (most recent call last):\n  File \"/home/andrei/Projects/podcast-shownotes/scripts/build_search_eval_dataset.py\", line 157, in generate_search_query\n    response = await self.client.chat.completions.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 1927, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1767, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1461, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1547, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1547, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1562, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-xSG9D345gtYlIKC330wCyrEG on tokens per min (TPM): Limit 30000, Used 23836, Requested 13336. Please try again in 14.344s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
    }
  }
]