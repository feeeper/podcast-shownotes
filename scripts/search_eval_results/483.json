[
  {
    "segment_id": "1c65333c-615c-4d7c-902b-f1075b8de39f",
    "episode_id": "4c6e6e8e-c65f-4c1e-acae-b5e684319408",
    "episode_number": 483,
    "segment_number": 7,
    "text": "У нас есть еще чему мы научились? Если вы если что есть, то давайте, а то уже дальше пойдем. Кстати, Валер, а вот и все остальные, Вы когда покупаете, я задумался, что я хочу купить тоже себе NVMe для Каж для моего NASA. Вот. Я внезапно задумался, а где его купить, и я понимаю, что кроме амазона я сейчас даже всякое такое в основном амазоне то есть я более крупные компоненты часто то есть я не знаю насколько это в США ой в юке знаю есть и Verkl Красный укей там по-моему у них есть магазин я не уверен в Германии есть кейс кинг в Германии есть Mindfactory в Германии есть Alternate в Швеции есть Proshop который в Германию доставляет вот это то где я затариваюсь в зависимости от того чем собственно например моя 4090 foundage edition она как раз с прошопа потому что это то где сейчас и через кого сейчас Nvidia продавал в тот момент в Европе свои foundage edition карты Амазон я все еще использую для мелких каких-то вещей которые мне не очень жалко для больших вещей я зарекся после того как я очень долго до себя в какой-то момент мне кажется рассказал эту историю что я купил 4090 она приехала оказалась буквально чуть-чуть не в том месте где нужно слишком большой я смог ее засунуть в корпус который планировал засунуть опять же про это по-моему рассказал в подкасте я решил её вернуть ну в смысле я её даже не вставлял в компьютер просто типа вот я её рядышком поставил посмотрел что она слишком большая и даже вот не вынимая из антистатического пластика отправил обратно Вот и как бы мне конечно деньги в итоге вернули, но судя по трекингу она никогда им не вернулась. Вот и после этого мне с ним еще было пару таких моментов, когда они мне в какой-то момент писали письмо, что чувак, если ты сделаешь еще возвратов, мы тебя просто забаним. Вот я подумал, что если у них такая чудесная система возвратов, что они сами теряют свои посылки, там не знаю, или присылают тебе какую-то поломанную херню, потом говорят, что ты сломал, то я пожалуй просто не буду у них ничего дороже чем типа на 150 евро заказывать и все другие остальные вещи. Заказывай не с амазона. А как ты узнал, что оно не доехало? Ну в смысле может доехало, но просто типа трекинг застрял. Собственно почему обычно деньги тебе возвращаются в автомативке, когда их трекинг понял, что им доехало. У них трекинг просто застрял и висел больше месяца. Понятно. Если речь про электронику типа NVMe, то Озон, Яндекс. Маркет это основные площадки. Удобно, что там все есть. Пункт выдачи близко к дому. В принципе там сейчас продается все, кроме алкоголя и каких-то взрывчатых штук. Ты не можешь купить зажигалку, потому что это потенциально взрывчатые штуки. Если ты хочешь немножко сэкономить, то можно поискать бу на Авито. Это такое, что ты ищешь какую-то не очень нужную вещь. Если можно купить в 3 раза дешевле, почему бы и нет. Можно сказать на алиэкспрессе, но если ты готов подождать. Обычно там дешевле. Еще надо остерегаться подделок или китайских SSD-шек. Надо отзывы смотреть. Ну и тут единственное что я бы рекомендовал брать не то есть для с точки с точки зрения кэша на асе тебе важно не то чтобы он был самым быстрым из современных SSD, тебе скорее важно чтобы это был какой-то такой тип вот у меня оба это Samsung Pro серии потому что у них больше пролеченных циклов перезаписи у них еще драм встроенный у Pro серии ну да еще и открой но они все MLC если есть шанс купить где-нибудь по дешевке SLC диски они больше серверные, это будет более надежно. Небольшой ресурс. Как-то так. Спасибо. Поищу. Слушай, а вот встроенная дыра, когда я услышал это, крутить на этом базы данных как-то опасненько звучит. А откуда ты поймёшь, что там и в синг работает? Это всё равно же речь про NVMeшный кэш на NVMesh. Он влияет были на запись кэш на самой dram на ssd потому что с драмом самые дорогие ссдшки потом есть ссдшки без драмма есть ссдшки без драмма но у них ссд кэш это какое-то количество гигабайт SLC флеш-памяти но она тоже изнашивается и не такая быстрая как DRAM а и есть сейчас самые новые это без DRAM но они используют системную память для кэшинга. Ну тоже самое. Пойнт Саша в том, что откуда он знает, будучи базой данных, что Noize DRAM перешло на стабильный кусок диска. В смысле были прецеденты, когда у тебя рейт имел кэш и делал вид, что понимает твой F-Sync, а на самом деле он его не понимал. Были прецеденты, я только поэтому. На Домашней Мнайсе мне это не очень страшно. В сербе я бы такой номер не ставил. Насколько я знаю, все хорошие SSD как раз имеют RAM кэш, но он на чтение, а не на запись. То есть он хорошо подходит для какого-то там произвольного доступа там к вашим системным файлам и всем таком, Но на запись он write through должен быть сразу. Я тут могу ошибаться тоже. Еще я понял, что те, которые с DRAM, они еще менее энергоэффективны. Не забываем, что это не бесплатно. Что-то мне еще и на playstation нужно покупать NVMe. А вот там тебе как раз нужен скорее что-то с максимальным фрупатом. Там есть Western Digital Black что ли какой-то там 850 или какой-то такой он там он не то чтобы очень дорогой но возможности дешевле просто игры удалять. Просто я в плойке теперь можно обновлять хранилище? Да, в пятой плойке всегда было можно. Не то что обновлять расширять там основное распаяно, но у тебя есть слот под даем мешку. Прикольно. Поудалять это вообще не вариант, потому что толпа народа все играют в разные игры. И у меня прямо дома уже голосование, какую мы игру удаляем следующую, потому что хочется поставить еще одну. Вас понял. Ну да, тогда 100 евро за терабайтный SSD. Да да да. А кто это Арт сказал? Какой ты советуешь сервер NVMe для NASA? Не могу сказать производителя, не знаю. Нет, ты тип какой-то сказал, что есть два типа. Есть SSD с драмом, есть SSD с SLC кэшем. Прости, я не админ, можно расшифровать, а SLC это технология флэш-памяти. Там одна SLC это единичка записывается как единичка в одной ячейке mlc это значит множество единичек нулей записываются в одну ячейку она там как бы как стек выглядит вот и она типа менее надежна быстрее изнашивается зато объем большой Интересно, спасибо. Спасибо, да. Запишу себе. Буду искать. Пошли дальше, а то мы чего-то долго на разминки хватит. Арт принёс тему про кубернетес. Мы всегда за темы про кубернетес. Kubernetes Вот я смог прежде всего я смог перенести наш стек в компанию на Kubernetes И это достижение И всем рекомендую Если у вас сервисов много, переезжайте на Kubernetes. Вначале трудно, а потом все рады. Потом снова все трудно. А, ну да. Я как раз хотел думать, что пошутишь сначала трудно, а потом трудно. У нас прежде всего менедж кубернетис от Амазона. И туда только закидывай поды и плати деньги. Меня подмывает, подкальнусь на тему, что сначала трудно, а потом ты самый незаменимый человек во всей компании. Ну, с одной стороны, тоже такая штука есть. Я пытаюсь обучить других работать с кубернетисом. И что-то получается. Не все разработчики помнят команды для кубера, вот поэтому я сделал одну утилиту называется rov по-русски это будет york Сделай хорошо хорошо. Она так и делает. Там всего пару команд, чтобы зайти на под, чтобы клонировать под, чтобы задеплоить код, например, с локальной машины на стейджинг побыстрее. И не запоминать вот эти простыни и длинные команды еще есть такая вещь что так как кубернетис не один их у нас раз два три три стили кластера потому что разные environment и переключение контекста это тоже такая боль в консоли переключился потом надо посмотреть что там происходит выключаешь контекст или какие-то баши себе пишешь и вот этот ёж ёж, ёж позволяет, как прослойка, запускать команды на разных кластерах. А можно быстрый вопрос. Быстрый вопрос. Не в тему. Postgress на Кубере или сбоку? Мы на Амазоне и значит Postgress он на Ордесе. Значит сбоку? Сбоку, да. Это все правильно сделано. Я одобряю. Вот был Redis одном сервисе Кубере иногда коды что-то слетали но перенесли на менеджере редис тоже бывает вот а какие вообще рекомендации когда начинаете работать с кубером прочитать какую-то статью мало вот для того чтобы войти в кубер и чего-то сделать я просмотрел часов 60 курсов на лдэме вот чтобы разобраться как эта штука работает потому что у амазона у него своя специфика там мог по себе развернуть кубер натиском легко но связать например тот же Load Balancer с кодами это надо прописывать правила доступа, там все эти json, все эти аутентификации. И это вот это была боль, которую ради которой мне пришлось пойти на курсы чему-то научиться писать в тароформе и все это вот сделать короче надо научиться и тогда будет проще вот еще есть легенда ж легенда что может быть можно оптимизировать и будете платить меньше за вашу инфраструктуру но вот мы переехали с EC2 на кубернетис И платить меньше не стали потому что подов стало больше разработчикам понравилось и они начали клепать сервисы Ну вот и в результате наших сдерживающим фактором было то, что сложно новый сервис ввести в строй, а теперь сервис ввести в слой Извините русский язык мой. Сервис ввести в строй легко и это стало проблемой. Стало проблемой, да. Вот. Подождите, то есть как бы у вас просто грязными руками любой разработчик может как это наводить в строй или? Нет, нужно все-таки где-то что-то задолго. Где-то нужно принять какой-то пулу реквестик? Ну, можно создать репозиторий, планировать в каком-то проекте так называемый файл конфигурации, как это деплоиться, там поменять параметры и задеплоить его. Он автоматически регистрируется, автоматически прописывает url и готов к использованию. Я упустил момент. У вас Kuber предоставляется Амазоном как сервис. Я, кстати, когда пользовался Amazon, он так ещё не умел. Или вы поверх EC2 сами его задеплоили? Нет, Amazon даёт ECS сервис. Это Control Plan, как я понимаю, Kubernetes овский. А сами машины ноут грубо это физические и c2 вот на которых запускаются поды поды можно еще запускать на argate это значит что он запускается где-то где ты не знаешь но он как-то связан с той инфраструктурой и все работает чуть больше платишь но я как бы такой олдфаг и пока запускаю ноды на EC2 Подожди, форгейт это же сервер или у вас компьютер? Нет, сервер как сервер? Какие-то сервера есть, но они Ну я имею ввиду, что это типа запускается где-то, ты не знаешь где. Ну, то есть это где-то в Амазоне ты не знаешь где это, в смысле, где-то вообще в любом другом облаке. Нет, в Амазоне. Все в Амазоне запускается, но где А, в смысле, что ты просто не менеджишь эти сервера. Да, ты говоришь, Отпусти мне этот пот, он тебе его запускает. Всё. Ты не знаешь где. Но он связан с твоей VPC. И доступен. Чуть-чуть больше платишь, зато не имеешь головной боли с node группами, что типа влезут туда поды или не влезут. Amazon за тебя это менеджер. Можно гибридизировать? Можно. Отлично. Я пробовал и работает, но мне все еще как сделал кубернетис мне все еще лень переносить теперь на фаргейт. Там свои особенности тоже есть с фаргейтом, поэтому пока нет. Но зато я часть инфраструктуры перенес в Amazon лямбды. Но это как-нибудь расскажи в другой. Слушай, а зачем тебе лямбды? Просто мы тут как раз не так давно, как раз, по-моему, даже с Алексом в подкасте обсуждали, что лямбда это прикольно, но количество кейсов, когда это реально экономически осмысленное действие оно очень мало. О, это очень прекрасная технология я тоже задавался вопросом кому это надо? Потому что лямбда максимально живет 15 минут стоит конских денег А вот представь тебе, скажем, за минуту надо получить пару тысяч процессоров. Если ты будешь запускать EC2, это займет некоторое время. А лямбда скейлится моментально и несколько тысяч ядер ты можешь получить за секунды. Типа, если есть какие-то очень редкие бёрсты, которые требуют очень много компьютера и после этого уходят нафиг. Да. Я транскодирую видео на лямбдах и это дорого, но прекрасно. Очень быстро. Вот я могу как похвалиться если, например, недавно обрабатывал терабайтный файл, видеофайл у вас в прорезе, и 22 минуты заняло конвертировать полуторачасовое видео в FullHD, HD и SD форматы. И еще делать ремикс аудио каналов. Вот и это все лямдэ заморочено, конечно, но оно того стоит. А если ты можешь об этом говорить, сколько стоил вот этот процесс конвертирования? Ну, сколько стоил больше 10 долларов с процесса. Нет, имеется ввиду конвертирование со старого в новый. Вот усилий много потратили? Насколько ты доволен скоростью перехода, насколько ты доволен количеством ошибок в процессе и так далее? То есть совпало с твоими ожиданиями? Заняло несколько больше времени, чем планировалась разработка. Сколько? 3-4 месяца. Потому что изначально я не имел знаний в лямбде, как это деплоить, как написать все конфиги. Это пришлось все делать в процессе. Плюс я все делал на гошечках. В гошечках я тоже был новый, но оно того стоило. Потому что это очень сильно ускорило процесс конвертации видео у нас и клиенты счастливы вот у нас интерпроцентные клиенты им надо побыстрее и прямо сейчас. А в ретроспективе вот этот переход на кубер? Я так понял, что дешевле не стало. А оно того стоило? В чём выигрыш по сравнению с тем, что было? Например, проблема это очень медленный деплой. Потому что у нас сделано было по старинке в 2015 году. И это было Прости, на АММИ или на пакетах? На АММИ. Я так взял. Я подтверждаю. Это очень медленно. Но удобно, но очень медленно. Очень очень медленно. И встал вопрос на что мы переезжаем. Для новеньких. Что такое Ami? Вот есть Amazon виртуальная машина AC2 скажем так. Ты на ней с помощью антипла, шефа или другой технологии ставишь все что тебе нужно в приложение, и потом как бы и выпекаешь образ этой машины. Знаешь команду Амазона, Амазон останавливает машину, делает копию диска и сохраняет где-то в репозитории, В регистре. А, понял. То есть образ виртуальной машины. Да. И потом этот образ ты можешь запускать в любом количестве, если тебе надо горизонтальный автоскейлинг. И да, удобно, но медленно. И плюс айсибл это отдельная проблема, потому что за 10 лет, 9 лет он превратился в то, что поднимает один человек. Мы делали без ансамбля. То есть у нас была, по-моему, Джабба в Team City, если я не гоню. И там прямо клиентскими командочками к Амазону создавался иситу образ. На него ставилось то, что тебе нужно, запекалось всё это в amy, и потом оно чуть ли сразу не деплоилось. Прикол в том, что в теории у тебя AC2 работает поверх я не знаю чего в данное время суток КВМ, Xeno, чего-то, И оно как бы не должно тормозить, остановить КВМ, снять с неё образ, задеплоить. Но именно в Амазоне оно почему-то очень тормозное. Да, 10 минут на создание Ammy. Вроде не зависит от ну как-то зависит от размера диска который ты делаешь ну там например 12 16 гигабайт в среднем это вот 10 минут у нас было много ошибаться и это было медленно потому что если ты хочешь разрабатывать и проверять, то все работает и потом шипит на продакшн. Медленно, медленно. И поэтому я предложил переехать на Кубер. А ускорение составило? 40 секунд. Так, ладно, давай. Если деплоить без кэшей на GitHub Actions, это занимает 2 минуты 45 секунд, насколько я помню, средний сервис. Если деплоить локально машину например на staging это занимает 30 секунд хорошо вот и и как еще интересный эффект получился для этого штуку сделали я же говорил что сделал так называемую обертку над хеломом команда для хелома для кубернетиса так подожди пока мы хелом убежали вы используете кубернетис для какой-нибудь координации или чисто как вот способы деплоя? Координации в смысле? Смотри, один из моментов, когда я в далеком девятнадцатом году кажется ранил про выбор кого-нибудь Шедлера для Астро, собственно подкасте это как кажется номат купернетис и как называлась боже умершая система на джаве марафон сравнивал и у меня тогда задача была не просто что-то декловить в кластер в принципе у меня был, там как раз ансибл вполне справлялся. Проблема была в том, чтобы деплоить с ролями, деплоить так, чтобы у тебя какая-то конкретная роль была не более чем один раз в кластере, то есть нельзя было чтобы это было больше чем один раз в кластере и тому подобные вещи. Типа не знаю периодический запуск по такому кубернетисному кроулу и всякие такие вещи. Так первое у нас сервис можно клонировать и задеплеить по-другому это может принести проблемы тут я не ограничивал на роли не вводил дополнительные крон я имею ввиду что у вас в принципе такие кейсы вообще в принципе бывают или или это типа ну ты не знаешь чем твои разработчики занимаются нет у нас таких кейсов не бывает у вас команда в принципе маленькая backend и мы все знаем что кто делает. Какой-то junior туда может задушить, кто-то не то задеплоить, но хотя зачастую наверное у самых молодых нету просто доступа в кубер. В статичный кубер. У нас Presentation Production. В стейджинге можно натворить дичь. Все равно можно все снести и пересоздать, если что. Слушай, скажи, а у вас джиджинг, у каждого разработчика свой или общий? Нет, общий. Как вы не наступаете друг другу на ноги? А вот так в Slack пишешь: Пожалуйста, сейчас я буду крутить-вертеть, не трогайте. Вот так вот усавис. Да звучит странно, но есть еще над чем работать. Команда небольшая, это вполне разумный подход да но тут кроны упоминали кроны кроны тоже в старом линт и на ACID 2 это были просто кроны на на на на ACID 2 да То, что надо, это будет считать. Ты меняешь инфраструктуру, чтобы сделать крону. Альтернатива даже А, собственно, это было полезна, это как бы какой у меня был область вообще действий, в которой мне это было нужно. Мы делали Data pipeline довольно замороченные и у тебя могут быть разного рода пайплайны и есть какие-то пайплайны выполняют например такие, ну то есть по сути у нас была база данных поверх базы данных, и у тебя есть, если ты делаешь что-то самоподобное, у тебя может быть такая задача как компекшен, и у тебя может быть отдельная другая подзадача это выбор работы, которую должны совершать последующие компэкшены задачи. Приоритизация компэкшен работы. Это должно гоняться по крону. И это должно некоторые из этих задач, например, в частности формирование списка работы для последующей компактизации должно не больше, чем один раз происходить. Это происходит больше, чем один раз. То есть это лучше, чтобы это не произошло, чем это произошло для конкурентной джабы. И таймауты есть. Что у таймауты есть? Ну у skill jobs это значит что сервис можно запустить и он отработает не более чем 20 минут. Иногда поле забывает Такое ощущение что кусок отвалился потому что я только про тайм-ауты услышал. Буду говорить громче в кубере в кронах еще можно указать тайм-ауты Это бывает полезно для каких-то вещей, которые работают и потом должны или сдохнуть, или закончить работу. Как то так. Ещё могу порекомендовать по поводу альтернативы кронам есть такой есть такой сервис который можно поставить у себя называется temporal вот это как бы work low engine. Там тоже можно определять jobs, которые будут выполняться периодически. Плюс ко всему у этого всего есть UI, в котором можно что-то определить, посмотреть как это раньше исполнялось, когда исполнится в следующий раз, какие-то логи иметь. Вот крайне рекомендую. Извините, мои дети тут наигрались и вышли из комнаты. Может быть немножко шумно. Вот. Ладно. Вопросы по куберу еще есть. У меня такой связанный вопрос. У вас как-то поменялся подход к мониторингу после того как вы кубер выкатили? Ой да это другая боль которая отчасти решена потому что количество метрик с наших сервисов с кубера увеличилось. Мы поставили графану и мы графану используем от амазона. И там просто тонны всего и какие-то дашборды сделаны, но разобраться в принципе можно только вот что я использую это потребление процессора потребление памяти вот это тогда когда какие-то поды падают и надо разбираться почему они упали и кто съел всю память вот уму никто не отменял вы сами свои программы не инструментируете метриками Есть, но опять у нас рельсы приложения и есть джемы от вроде даже называется Ябеда от злых марсиан, а вроде они сделали И там вот вагинов к ней собирать разные метрики там куча стоят мониторят сливают метрики да припоминаю, было время ответа. Короче, много всего. Я сам даже не разбирался. Я делегировал эту проблему коллеге и вот он сделал хорошо но это все равно поле мне не нравится позволю уточним Когда ты сказал, что вы используете графану от Амазона, ты имел ввиду, что она менеджер Амазоном? То есть вы её не в Кубере гоняете? Нет, мы её не в Кубере гоняем. Потому что странно гонять в Кубере то, что ты и мониторить, то что ты как бы chicken egg problem. Да-да-да. Я как раз хотел спросить, кто мониторит мониторит. Поэтому это Причём вроде получается, что сама от графанной базы данных Вот я точно не помню все названия компонентов стека графаны это вроде была веб юай или сервис ау базы данных. Вот Amazon представляет базу данных к этому сервису. Потому что это самая большая проблема хранить кучу данных и потом их надежно хранить прежде всего. Да, мое понимание, что графа она предоставляет дашборды. И если вам сильно хочется туалета, но данные хранить надо где-то еще. Она умеет много откуда ищет. У Lamodona и сервис тоже стоит нормально. Но дешевле вроде чем от самой графаны как компании. Понятно, спасибо. Логи. Другая проблема это была логи. По логи мы используем sass для логов, так называемый betterstack и крайне рекомендую, потому что предыдущее название его это был log и принцип был такой что у тебя как бы есть веб интерфейс потока логов который ты можешь гребать Сейчас они там развели, добавили фильтров кнопочек, но сам принцип, что ты можешь включить теперь это будут бегут логи в реалтайме. Ты там тут кликнешь, там конечно как-то отфильтруешь и видишь то, что тебе нужно. И не такой дорогой как airgreat сервисы. Что мы использовали? Я уже даже не помню. Изначально было что-то другое, но было дорого. А логов генерируется гигабайты просто и это гигабайты в день это чайка. А как долго вы их храните? Ну смотри если staging presentation там долго не надо 15 дней хватит за все. А на продакшне есть у нас есть регулирование там хранить сколько-то лет и вот тут тут сложнее так пробовал логи от Амазона. Амазон Cloud. Я забыл. Мне интересно так. Да, ну вот короче, да, много можно засунуть. Стоит нормально, но трудно найти по тому, что тебе нужно. Давайте о чем-то веселом расскажу. Вот Kubernetes сделал, а в нем возникла другая задача. Есть сервис, который в Kubernetes не засунут и не будет засунут, потому что это стриминг видео. Terraform плюс Cloud init все хорошо, но как-то заморочено было вот плюс пришел еще security аудитор и сказал атата вы храните какие-то секреты в CloudInit. CloudInit это такая штука, которая запускает скрипты при старте инстанса. А скрипты поставляются в инстанс в виде Base 64 данных, которые прикреплены к инсту с у Амазона. Вот.",
    "result": {
      "error": "API request failed: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-xSG9D345gtYlIKC330wCyrEG on tokens per min (TPM): Limit 30000, Used 30000, Requested 10278. Please try again in 20.556s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
      "stack_trace": "Traceback (most recent call last):\n  File \"/home/andrei/Projects/podcast-shownotes/scripts/build_search_eval_dataset.py\", line 157, in generate_search_query\n    response = await self.client.chat.completions.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 1927, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1767, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1461, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1547, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1547, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1562, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-xSG9D345gtYlIKC330wCyrEG on tokens per min (TPM): Limit 30000, Used 30000, Requested 10278. Please try again in 20.556s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
    }
  }
]