[
  {
    "segment_id": "7397519b-b917-48c9-a8f1-39bfb99c091e",
    "episode_id": "ce9d706f-9dd8-42b5-97f6-18b1d3070c10",
    "episode_number": 390,
    "segment_number": 6,
    "text": "Вопросы, возражения, комментарии. А еще что-то будет про конкаренности в этом курсе? Да, в лекции номер, сейчас скажу какой, в лекции 16 нам обещают рассказать про локи, которые более высоко уровнем в локе, и как они, я так понимаю, как они позволяют реализовывать алгоритмы на структурах на диске. А всего сколько лекций? В этом курсе, которое intro, 25 лекций. После этих 25 лекций еще идет курс advanced database management systems, то есть суммарных около 50. Каждая лекция где-то по часу, часа 20. Слушай, звучит очень сильно. Я сначала подумал, что это небольшой курс, но 50 лекций это прям мощно. Ну нет, этот курс 25 лекций, а тот это совсем другой курс. Вот вообще, как ранее отмечалось, это можно рассматривать как большой пересказ, ну или как вольный пересказ большой книжки. Вот там, в одной из первых лекций они ссылаются, называется книжка database system concepts. Это толстая книжка там на тысячу-полторы тысяч страниц. Мне кажется, пересказать ее в 50 лекциях, это сжатый пересказ. Я вижу, что эта тема себя исчерпала, и я предлагаю передать слово Валере, чтобы он рассказал нам про paper, о DynamoDB. Что ж, поехали. Только что не отваливаться. Я постараюсь изо всех сил не отваливаться. Сейчас, секундочку, я буквально на полминутки прервусь. Я тем временем могу быстренько рассказать про другую маленькую тему. Тоже про Postgres. Я на этой неделе узнал интересные моменты про PgStatActivity. В Postgres есть определенные средства интроспекции, предусмотрены системные вьюги, и через системные вьюги ты можешь узнать некоторые детали о том, что внутри Subd происходит. Их около десяти. Есть PgStatActivity, PgStatReplication, Wallsender, Subscription, Process Analyze, Progress Vacuum. И, соответственно, эти вьюги тебе позволяют узнать, о каком состоянии сейчас вакуум. Он там запущен, запущен и так далее. Рассказать я хотел не об этом, а оказывается это очень... Казалось бы, что такое интересно, ты смотришь вьюгу и получаешь статус. Оказывается, все несколько интереснее. Во-первых, вот эта статистика или состояние, они репортятся не постоянно. У тебя есть вот прям процесс, StatsCollector, который агрегирует данные со всех работающих бэкэндов, то бишь процессов обслуживающих сессии, например. И состояние репорт, вот бэкэнд меняется в состоянии не постоянно, он это делает перед тем, как уйти в состояние бездействия, вот это idle. Если он, например, ушел что-то делать, что-то полезное и не переключался в idle, то он StatsCollector о своем изменении состояния ничего не говорит. И, соответственно, у вас не отразится то, что процесс последние полчаса чего-то фигачил, потому что он об этом еще не зарепортил. Это не очевидно для меня момент. Второе, что в принципе у тебя статистика репортится не регулярно, а не чаще раза в указанный интервал времени, он называется PgStatInterval и по умолчанию равен 500 мс. То есть даже если ты постоянно переключаешься в idle и обратно из него, то у тебя состояние меняется около 2 раза в секунду по умолчанию. Но это еще не все. Оказывается, у тебя, когда, допустим, ты создал новую транзакцию и обратился к тому же PgStatActivity, в этот момент у тебя будет взят снапшот этих системных вью и ты получишь некие данные, актуальные на момент, когда ты свой Select исполнил. Если ты после этого выполняешь новый Select, ты получишь те же самые данные, они обновляться не будут. Оказывается, это даже не баг, это фича. Это фича по той причине, что ты можешь хотеть в одной транзакции походить по разным вьюкам, получить с них разные разные данные, а потом с них сагрегировать какое-то, ну, какое-то отчет, еще более сложное представление. И вот для того, чтобы это твоё представление было консистентным, используй этот самый снапшот. Его можно сбросить. Для этого есть хранимая процедура, называется PgStatActivity Snapshot. Кстати, всё, что я говорю, это в документации подробно описано и ссылки, надеюсь, в шелонутах я просто пересказываю. Еще интересный момент, что в Postgres до версии 14 включительно для сбора статистики использовались временные файлы. В версии 15, которая сейчас в бете и должна выйти где-то осенью, все это переписали на разделяемую память, должна стать эффективнее намного. Еще интересный момент, что если у тебя суббодия завершится аварийная, то статистика вот эта сагрегированная будет заброшена и собираться заново. А если он завершается штатно, ну, там, типа через PgKtl ты сделал 100, то у тебя статистика будет аккумулироваться. Поэтому в статистике, опять же, ты, ну так, при прочих равных, ты не знаешь, ты видишь какие-то сагрегированные данные с последнего, ну, за все время работы инстанция или они сбрасывались. Ну и наконец, если ты работаешь под обычным непривлекированным пользователем, то к части информации у тебя нет доступа по соображению безопасности. Вот, казалось бы, ну, какие-то системные вьюги, которые тебе репорсят, вот у тебя, вот тот бэкенд, он сейчас там в транзакции или он не в транзакции, а столько нюансов. Вот, все это очень интересно. Если заинтересовало, рекомендую почитать документацию. Вопрос возражения в комментариях. Вот про то, что он собирает статистику дать, прежде чем стать idle. Idle типа, прежде чем его система переключит в не... То есть, это какой-то хук в планировщик, в операционную систему, или что происходит? Нет, это не хук в... У тебя вот есть бэкенд, да, как процесс, и он знает, что, ага, мне прилетел запрос от пользователя. Значит, сейчас я его пропаршу и начну че-то фигачить. Вот я был раньше в idle, я из idle перехожу в intransaction, у меня другой стейк, и он прям говорит, ну, допустим, ты хочешь репортить время, как много времени ты провел в... Ну, делаешь это полезно. Собственно, я совсем не столкнулся, когда начал ревьюить патч про добавление active time и idle time по gestalt activity, и вот это все стало очень интересно. Допустим, ты хочешь репортить active time, как это работает? Ты, когда тебе прилетет новый запрос, ты замеряешь время начала исполнения транзакции, ты всегда знаешь то, что ты видишь в вызове now, фигачишь-фигачишь, потом ты завершил, и ты знаешь, что вот сейчас ты переходишь в состояние из active в idle, и в этот момент ты смотришь, ага, когда я начинал исполнять запрос, вычитаешь из текущего времени время начала и отправляешь в statscollector пакет, который говорит, я провел в состоянии active столько-то микросекунд, по-моему, микросекунд. Все, я понял, типа, по-другому запросы более-менее последовательно обрабатываются, то во всех случаях, как одного бэкэнда, то даже если у нас сразу есть другой запрос, ну просто типа, ага. Вот, а потом ты, как нормальный процесс, встаешь на блокирующем чтении socket или что-то в этом роде, вот, ну просто для меня вот было совсем не очевидно, что ты в одном бэкэнде, например, можешь начать транзакцию сделать, вот прям begin, да, begin, и ничего не делать. Во втором бэкэнде ты хочешь понять, а в каком состоянии у меня та другая транзакция, и ты будешь смотреть на его состояние, сколько он провел в состоянии idle in transaction, казалось бы, вот сейчас он находится idle in transaction, и твоё интуитивное ожидание, что это время оно должно постоянно монотонно расти, оно нифига не будет постоянно монотонно расти, и в документации это прямо сказано. Я не уверен, что понял, но предлагаю двигаться дальше. Оно не будет постоянно монотонно расти, потому что ты не переходишь из какого-то состояния в состояние idle. Если ты в той первой транзакции сделаешь select 1, то этот переход произойдет, и ты статс-коллектору пошлёшь обновленные данные, но если ты просто сделал begin и VSC, то обновления не происходят, и это уже дофига не очевидно. Что ж, давайте к DynamaDB. Очень такой занятный пейпер, и в первую очередь занят тем, что это один из немногих пейперов, которые рассматривают базу данных. Мы очень много в рамках DIVZEN, в рамках до этого EOXCast обсуждали разные пейперы про базу данных, прям практически начинали с этого. Почти всегда это какой-то пейпер про архитектуру, про какой-то консенсус, вот там не знаю, еще что-то такое. Здесь почти ничего этого в пейпере вообще не обсуждается, но дают ссылки на другие места, где это обсуждается, но типа, говорится, сделано как-то примерно вот так. Зато это очень большой такой обзорный доклад про эволюцию системы и всяческие проблемы, с которыми приходится сталкиваться, когда вы делаете database as a service, особенно на масштабе Амазона, особенно для многих тенантов. И это очень интересно, с одной стороны, потому что таких пейперов в принципе очень мало. Мне кажется, другой похожий, который я видел, был от EPLUG про то, как они фондовиджи диппи используют. Но он, по-моему, был менее интересный с точки зрения выводов. И другой момент, который мне показался довольно необычным, и мне из-за того, что я довольно долго прочитал, в первую очередь, потому что из-за такой структуры здесь порядка шести или семи секций, которые прям с контентом, и они все очень слабо связаны. И это такое... Обычно в пейпере одна-две больших идеи. Здесь прям просто на каждую секцию попали идеи, может быть. Такой прочитал секцию, пошел переваривать. Как-то так. Поэтому я не буду пересказывать пейпер, во всяком случае постараюсь мне заниматься. У меня есть такой список всем уж верхнеуровневых пойнтов, которые я в конце в любом случае как-то просуммирую. Но хотелось бы это, не знаю, так с ведущими в рамках такого наброса пообщаться. То есть я делаю наброс из очередного пункта, который мне там подчеркнут. А ведущие у меня спрашивают вопрос, остальные соведущие, и я тогда попытаюсь раскрыть мысль. Или он не вызывает некоего резонанса, и я такой типа, ну, едем дальше. Первый наброс я уже делал. Я пытался вспомнить в уме, как перевести KTKW. Ключевой момент, да, вроде? Да, ключевой момент, да. Первый наброс я немножко уже набрасывал в прошлом выпуске, поэтому я не жду этого обсуждения, потому что мы его уже не... Хотя, кстати, Ваня в прошлый выпуск же прогуливал, я правильно помню? В прошлом я был. Или нет? Обидно было. Окей, соря, запамятовал. В общем, я один наброс уже делал, поэтому, да, тогда не жду ни от кого комментариев, что прям практически в интердакшене у них есть такая мысль, которая потом позже раскрывается, что для многих систем предсказуемость и однообразность поведения и перформанса гораздо сильнее и существеннее влияет на качество сервиса, чем абсолютная такая сырая... перформанс в большую часть времени. И мы об этом сильно поговорили в прошлый раз, поэтому если кому интересно эту мысль, чтобы эту мысль раскрыли, послушайте предыдущий выпуск. Смысл такой, что если у вас где-то есть кешек и... или там, не знаю, какая-то другая штука, которая у вас как бы вот в нее, через эту штуку что-то работает 99% времени, у вас может так оказаться, что в 1% времени у вас система совершенно не готова к этому 1% времени, и вы это узнаете очень редко и с очень большими сюрпризами. Это, наверное, один из таких пунктов, который со мной сильнее всего в этой статье срезонировал, потому что я тоже в такие ситуации попадал. Ладно, пока поедем дальше. Следующие такие интересные моменты, которые у меня подчеркну, то есть тоже из интердакшена, что тут есть такие четыре главных таких пункта, на которых paper будет фокусироваться. И вот adapt to customer traffic patterns и maintaining availability через всякие game days, formal proof, все такое мы миллион раз обсуждали. А вот чего мы почти не обсуждали, это то, что они регулярно проверяют data address и делают кучу других усилий в фоне, чтобы убедиться, что все хорошо, и это дальше в paper обсуждается. И второй, это снова, очередной раз paper напоминает о том, что мы делаем систему предсказуемой, а не максимально производительной. Вот. И, наверное, первый такой момент. Можешь пояснить, что ты видишь по data address. At rest. Типа данные, которые сейчас не трогают. То есть у них там, мы дальше будем обсуждать, у них есть два механизма, которые они используют, чтобы убедиться, что все данные всегда не прогнили. Во-первых, любая коммуникация, которая совершается внутри систем Amazon, она всегда с чексумами. Записи в Волл с чексумами, в ВП3 чексумы, любое сообщение, идущее по СТ, имеет в себе чексуму. В опишке, прям С3, есть возможность сказать, вот эта чексума того, что я пишу и так далее. То есть вот везде есть чексумы. Но чексумы такого рода помогают тогда, когда у тебя эти данные кто-то или что-то трогает. Когда ты записал бэкап, или когда ты просто, у тебя есть какой-то ключ в пространстве твоего ключей, который, ну там, не знаю, пользователя год не было, ничто не избавляет тебя от битрота там. Больше того, у тебя когда случается битрот, если ты активно с ним никак, если ты его не ожидаешь, тебе может попасть в интересную ситуацию, когда у тебя на двух репликах все нормально, на одной битрот случилось. И тебе нужно, по-хорошему, пофиксить, а не распространить. Когда у тебя отдельный взятый бит, почему-то поменял состояние. Да, да. Гниение байтиков по русски. Еще, когда ты говорил по поводу предсказуемости, с точки зрения бизнеса, опять же, есть мнение, что, ну опять же, то есть я смотрю с точки зрения контрактов. Вот мы продаем, например, DynamoDB как сервис, и когда вы продаете, вы должны какую-то указать SLA. И когда у тебя предсказуемая система, SLA понятно, ну, то есть, что там должно быть. И с вот этой позиции при продаже сервиса много проще продать, когда ты знаешь предсказуемо, что ты продаешь, чем производительно. Понимаешь? Прекрасно понимаю, да. Я абсолютно с этим согласен. Просто это, знаешь, такая... Это и бизнесу проще, и инженеру на самом деле проще, когда инженер не просто это ради бенчмарков и статей разрабатывает, а когда ему это потом он колит. Вот. Поэтому интересный очень момент. То есть мы к нему еще вернемся, я думаю. Наверное, такой первый поинт, который хочу вбросить, это то, что... Ну, и ожидая обсуждения, то, что исторически появилась первая система, которая появилась, была Dynamo. И при нем был Paper. И он был давно, году восьмом, мне кажется, если я ничего не путаю. Потом... И он используется чисто для внутренних каких-то систем Amazon. Он не был публичной системой. Потом в AWS и также для внутреннего использования появились сервисы SI и SimpleDB. И Paper утверждает, что, собственно, причина, по которой они собрались делать вообще DynamoDB был в том, что внутренние команды Amazon, они предпочитали S3 и SimpleDB даже в тех случаях, когда они не очень хорошо подходили для того, что эта команда делала. Там у SimpleDB были довольно в срат ограничения в духе что-то типа 10 ГБ на таблицу или что-то такое. Но у него был более приятный API. И что более важно, он был managed. DynamoDB внутри Amazon, какая команда его решила использовать, она его развернула и сама поддерживает. Поскольку SimpleDB был частью AWS, ну, как бы другие команды могли просто его использовать тоже как сервис. И, то есть, самым большим драйвером делать DynamoDB как сервис было то, что все, включая команду Amazon, предпочитали использовать database as a service, который поддерживает кто-то другой. Это очень интересный такой поинт, потому что сейчас это то, как мы видим, сейчас развивается database компании, включая то, где я работаю. Все, никто больше не пытается продавать on-premises базу данных, все пытаются продать облако с базой данных, и люди покупают, потому что мало кто хочет сам это managing. Ну, да, как это так аккуратно сформулировать. По необщечно доступным данным многим людям нравится использовать определенные базы данных именно on-prem, вместо конкретного облака. И в процентном отношении людей, которые используют subd on-prem, их очень много. Но, скорее всего, это не production какие-то истории, или production большой компании, у кого уже есть db. Давай так, даже если вот взять Postgres, я думаю, можно достаточно смело утверждать, что инсталляции в каком-нибудь, как он там, RDS их относительно инсталляции по миру сильно меньше. Ну, мне кажется, если сапросуммировать все db, с которые предоставляют Postgres, вот эта вот цифра будет больше, чем он прям Postgres, особенно если исключить инстанции, которые люди купят все просто на машинах. Ну, мы все равно здесь не обладаем объективными данными, чтобы прийти к консенсу, но я в самом случае хотел сказать, что точка зрения, что вот все живут исключительно в облаках, она может быть слегка преувеличена. Почему не обладаем? Мы же обладаем. У нас есть опыт работы в каких-то компаниях, и мы можем поделиться этим опытом. Он не... Ну, во-первых, не можем. Нет, во-первых, не можем, потому что это не публичные данные, а, во-вторых, он не экстраполируется на другие базы данных. Ну, то есть, на моей памяти было и такое, и такое. Компании по-разному это делают. И со временем доверие к системам, которые за тебя кто-то менеджит, оно растет. Или, давай так, вымирают те, кто готовы быть db в компании. Они не то что умирают. Это очень интересный момент. Одна из компаний, где я работал, у них была просто была агрессивная философия, что мы по возможности не занимаемся тем, чем мы могли бы не заниматься, если это можно купить, потому что это не наш бизнес. То есть, это прям была философия одного из технических лидеров. И при этом в этой компании долгое время менеджили базу данных. Даже больше, чем одну. Это не потому, что очень хотелось этим заниматься. Потому что были проблемы. То есть, даже какие-то команды уже пробовали разные DBAs, налетали на какие-то проблемы. Или плакали, продолжали пользоваться, или плакали и съезжали обратно. А потом, постепенно ситуация стала меняться. Я хочу сказать, что, наверное, у меня какой-то DBA связан именно с подгрессом, но я просто знаю, что есть компании, такие как EnterpriseDB, DataEgret, PolgrasPro и другие, которые конкретно продают услуги, поддержки баз данных, установленных on-premise. То есть, их услуга, они продают услуги, это прямо услуги DBA по поддержке твоего on-premise. Никакого облака у EnterpriseDB нет. Поэтому я не думаю, что такой бизнес может столько лет успешно существовать, если у тебя нет таких столетий, все в облаках. С другой стороны, Microsoft Azure купила этот сайт. Я думаю, мы очень нерепрезентативны здесь, потому что мы работаем в компаниях, которые довольно современные и такие стильно модные молоденые. А когда речь идет про вообще индустрию и все, мы должны не забывать, что это есть много разных компаний, есть государственные компании, которые тоже нужны технические специалисты. У Амазона сейчас есть государственные облака. И не всегда это работает, не для каждой страны это доступно, к сожалению. Это правда. Соответственно, есть секторы, в которых такого нет. Но, опять же, одна из компаний, в которых я работала, там все было on-prem. И это имело смысл финансово, если понимать, как эти сервера добываются, например. Поэтому я думаю, мы не видим полной картины мира, когда речь идет про более традиционные компании. И там миграция в облака. Они и хотят, но это очень долгий процесс. Я думаю, наша понимание действительно не репрезентативна. С одной стороны, да, с другой стороны, мы же можем видеть тренд. То есть, если сейчас большинство компаний, которые создаются, они создаются уже в облаках, то наступит момент, когда просто не останется тех компаний, которые достаточно маленькие еще, но еще не в облаке, понятно, что большие крупные игроки всегда будут со своими облаками. Мы же не про них говорим. А хотя даже вот внутри этого амазона, вот эта статистика, она же тоже говорящая. То есть, они начали делать базу данных, потому что облачное решение, когда кто-то уже отвечает за другую базу данных, было выбрано даже с учетом неудобства использования этого решения. Есть много куча enterprise, которые такие классические, которые пытаются перейти в цифровую трансформацию, и их много, таких компаний огромное количество. Там очень много on-prem инсталляций. У нас мало людей, которые там работают, но это огромный сектор. Я переформулирую свою точку, за немножко свой тейк, что хотелось бы как разработчику не заниматься этим. Бывает так, что нельзя, но вот есть явный тренд на то, что оно туда движется. Может быть оно не доминирует пока, но оно все туда движется. Новые базы данных, вот ни одна новая база данных не продает он прям. Ну с этим я тоже могу поспорить. Не он, они хоть и делают облака, но... Ну давай так, я бы сказал, что ребята пока ищут product market fit. Ну ладно, в любом случае я думаю, что мы сильно ушли от DynamoDB Paper, но мне сильно резанул слух, мне кажется это очень контролирует. Я согласен, да, я слишком агрессивно набросил, я хотел наверное все-таки набросить в сторону того, что этот тренд в ту сторону, а не то, что мы уже все там. А мне было бы интересно посмотреть на цифры, наверняка где-нибудь в Амазоне эта статистика есть. То есть какой там total addressable market, то есть сколько всего у нас есть там инсталляция, грубо говоря, подкреса и какая у нас ситуация рынка, то есть сколько из них уже в Cloud. И посмотреть сколько еще осталось до максимальной ситуации рынка, где должна быть такая статистика.",
    "result": {
      "error": "API request failed: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-xSG9D345gtYlIKC330wCyrEG on tokens per min (TPM): Limit 30000, Used 30000, Requested 9432. Please try again in 18.864s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
      "stack_trace": "Traceback (most recent call last):\n  File \"/home/andrei/Projects/podcast-shownotes/scripts/build_search_eval_dataset.py\", line 157, in generate_search_query\n    response = await self.client.chat.completions.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 1927, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1767, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1461, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1547, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1547, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1562, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-xSG9D345gtYlIKC330wCyrEG on tokens per min (TPM): Limit 30000, Used 30000, Requested 9432. Please try again in 18.864s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
    }
  }
]