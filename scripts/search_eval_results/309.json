[
  {
    "segment_id": "e3f36f23-2dc4-4624-a0bb-306156ff4c8d",
    "episode_id": "32d39d5e-7f15-4c5b-a75b-fb1d01c31a34",
    "episode_number": 309,
    "segment_number": 7,
    "text": "За счет каких механизмов они не успешны? Другие коллеги их отвергают или как это работает? Я пытаюсь понять, какие процессы лежат за тем, что Brilliant Jerky не успешны? Теперь попробую умирить себя, когда я не говорю, и теперь мне сложно себя размирить. Так почему они не успешны? По двум причинам. За Brilliant Jerky обычно люди не очень хотят идти, во-первых. То есть собрать себе в команду толковых инженеров они обычно не могут, да, поэтому скажем это наичерли ограничивает скоп того, что они могут быть инволвты, что они делают. Это значит, что существуют механизмы, позволяющие инженерам переходить между командами, ну легко. Переходы между командами вообще свободны. В рамках одного Division они обычно не требуют вообще ничего, кроме согласия от той команды, в которую ты переходишь. Между Division обычно это не требует какого-то дополнительного внутреннего интервью. Ну скажем просто, если ко мне приходит человек, с которым мы работали уже много лет в соседних командах и все было хорошо, то я без проблем его к себе возьму. А если ко мне приходит человек, который я первый раз вижу, который до этого работал где-то в офисе, я про него вообще ничего не знаю, то я его проинтервьюрую сначала, но потом наверняка тоже возьму. Процесс внутренних интервью значительно проще, чем процессы внешних интервью, потому что мы доверяем тому, что если уже людей наняли в Microsoft, они наверняка уже умеют программировать, у них неплохо с дизайном и вот это все. Это в основном TeamFit, нежели чем что-то еще. Я тебя перевел, ты, кажется, говорил про два компонента. Первый, то что от Бриллианжерка можно идти в какую команду, а второй, или мне показалось бы только от… Да-да, второй на самом деле точно такой же, как тот, про что я говорил до этого. Им гораздо сложнее достигать какого-то импакта, потому что они, ну, Бриллианжерки, они делают что-то по-своему и очень плохо объясняют почему так. Даже очень часто бывает так, что они правы, но они не могут этого нормально объяснить, как правило, или даже если… или они просто не правы. Если они не правы, то обратно это вылезает импактом, и они просто в рамках каких-то годовых ревью или чего-то еще, они оказываются где-то в какой-то момент без бонуса, без промоушена, без чего-то еще. Поэтому они в компании просто не вырастают. Вы, наверное, знаете этот закон про то, что каждый вырастает до уровня своей некомпетентности. То есть они достигают этого уровня значительно раньше, поэтому импакта от них значительно меньше, чем могло быть. Я думаю, здесь прикол еще в том, что если вот Бриллианжерки попадают нам довольно в начальные уровни, то действительно они им очень трудно растей, и их, как правило, замечают менеджеры, и на ревью это сказывается. Но бывают случаи, когда вот так называемые Бриллианжерки, они наняты на более высокие позиции, и вот тогда намного сложнее от них избавиться и выявить, потому что их импакт уже изначально довольно большой на компании. Я так понимаю, что по ответу Олега, они должны за некоторый интервал времени спуститься вниз, и их импакт должен уменьшиться. Да, примерно так. Ну и плюс то, что я сказал, если так случайно, если так получается, что такая организация создается, то все толковые инженеры из нее разбегаются, и эта организация, как правило, беспомощна в том, чтобы что-то сделать. Получается хорошо, если в организации позволено перебегать, но это же одновременно может быть и плохо. То есть все будут перебегать туда, куда им нравится, но на неинтересных направлениях, которые между тем надо как-то поддерживать, никто не будет работать. Ну это хороший вопрос. А что такое неинтересное направление? Если я владею командой, и я не могу объяснить своим инженерам, почему то, что я делаю, это интересно и здорово, то наверное занимаюсь чем-то не тем. Это очень индивидуальная штука, Ваня. Понимаешь, люди, я работала с очень разными людьми, у которых очень разные интересы, и когда я слышу, что людям интересно, иногда я не могу понять, я не могу с этим как-то relight the dice, но людям это нравится. Ну то есть вы говорите, что после случайного стечения обстоятельств, всегда совпадет то нужное количество людей, которое нужно в компании, с тем, какие есть люди разные в компании. Ну я вообще в это не верю. Ну мы очень много нанимаем. Нет, на самом деле, да, грубо говоря, я, например, очень сильно верю в то, что билдовый инженер это совершенно склад характера. То есть найти человека, которому было бы реально интересно решать проблемы там build-off или engineering pipelines, это не то же самое, что найти человека, который бы хотел писать массируемый скалируемый веб-сервис, к примеру. И составлять одного, делать одного этого человека делать то и другое, как правило, хорошо не будет. То есть человек будет неинтересен. Плюс, на самом деле, это еще другой фактор. Мы же друг друга знаем, мы работаем вместе, у нас складываются команды, которым интересно и хорошо работается вместе, которые высокопроизводительны, потому что людям просто приятно вместе работать. В такой команде я могу сказать, что, ребята, вот знаете, я понимаю, у нас есть какой-то один неинтересный проект, но нам надо его выкинуть за следующие 6 месяцев, поэтому давайте, who wants to take one for the team? И это всегда работает. То есть другое дело, что если я из года в год начинаю одну и ту же политику пытаться применить, чтобы заставить людей делать то, что им неинтересно, ну они все уйдут, и это будет нормально. А у меня еще вопрос по поводу design review и вот этого процесса принятия решений архитектурных. Расскажи, как этот процесс, ну, то есть насколько он открытый. И просто я об этом думала, и очень часто многие компании делают процесс только для таких синер плюс инженеров. И в то же самое время это очень классный способ научиться для более начинающих разработчиков. И я поняла, что вот людей, которые новички в профессии, очень классно приглашают на такие митинги, потому что, ну, как бы, они задают очень классные вопросы, которые заставляют подумать очень высокоуровнево. А нужно ли нам это делать? И они учатся, и вопросы классные задаются. То есть это очень полезно. Вот приглашаете ли вы более начинающих людей? Да, обычно, да. Там есть только одно разделение. Есть проекты, про которые мы просто пытаемся минимизировать количество людей, которые про них знают. Вот пресловутый X-Cloud, например, по которому сегодня уже упоминали, да. Есть еще другие проекты, которые мы занимаемся, даже сейчас, про которые не всем стоит знать. На дизайны таких вещей обычно приглашают очень считанное количество людей. В основном, например, если какие-то просто изменения в дизайн или вот, как мы только что обсуждали, предложения по фишам, то вся команда имеет к этому доступ. Все документы открыты на SharePoint или на Viki, в зависимости от того, что команда или Division использует. И митинги, они обычно приглашают довольно... Т.е., скажем так, там есть несколько разных стадий. То есть есть митинги, на которые, к примеру, мы хотим получить какой-то существенный инжиниринг input, и мы вообще не уверены про тот solution, который мы делаем. На них мы обычно не займем всех сразу. Обычно завем буквально 2-3-4 человека и делаем таких митингов разных нескольких. К примеру, мы тут делали недавно миграцию из одного варианта с форс-контролла в другой вариант с форс-контрола, и обсуждали, как автоматизировать, к примеру, процесс подписей, чтобы он более хорошо работал, с нормальным инжиниринг-спириенсом. Мы отдельно встречались с security-ребятами, отдельно встречались с ребятами, которые build-ами занимаются, отдельно встречались с ребятами, которые занимаются менеджментом артефактов, подписами, вот этим всем. Потому что так просто быстрее и проще, когда обычно, когда собираешь 20 человек в комнате, то какое-то решение принять невозможно всех какой-то своем мнении. Если собрать все мнения отдельно и потом написать про них документ, то получается обычно проще. Однако, когда документ уже написан, то мы приглашаем очень много людей, практически 50-100 человек, нашу большую команду. Не весь Division обычно, но мы начинаем с одной какой-то команды, в которой мы обычно включены. Но это уже не обсуждение, это скорее презентация, что мы сделали вот так, потому что вот так. Но все вопросы welcome. Люди задают вопросы, мы объясняем, почему мы приняли такое решение. Если есть какие-то сомнения, то мы опять же документируем и потом делаем фоллоап уже в офлайне. Ну, я так понимаю, это происходит в культуре доверия команд друг другу. То есть, когда к вам приходят инженеры, задают вопросы, они не так, что все идиоты неправильно сделали, а они интересуются. В том смысле, что нету недоверия между командами. Мы же все время работаем в атмосфере того, что cloud очень большой. Cloud очень большой, а идентики тоже очень большие. А у идентики там десятки продуктов. Я принципиально не могу знать нюансов и деталей про абсолютно все. Я могу разбираться в чем-то, что касается каких-то областей, которые я уже до этого потрогал, например. Или областей, которых я действительно очень хороший специалист или что-то еще. Если я вижу, что ребята делают какую-то ошибку, например, я пробовал это, я так уже пробовал, это не работает, то, конечно, я им скажу, расскажу и объясню. Но если они пытаются что-то предложить такое, что я просто, я могу сказать, что даже не всегда понимаю, о чем они вообще говорят, то я, наверное, лучше просто промолчу и попробую законтраберить там, где я могу законтраберить. Как-то так. Расскажи, как происходит обмен знаниями между командами. Я имею в виду, ну, во-первых, предусмотрен ли он это подход, как бы вся истина в коде, или в коде и в комментариях, и в полреквестах, и в комитах, или это подход, мы все максимально закомментируем в Confluence, или что использует Microsoft? Ну, как это примерно выглядит? В идеальном случае у нас мы обычно пользуемся разными способами написать вики, да, и все документируется вики, но обычно документация обычно в вике, дизайн обычно в SharePoint, да, то есть дизайн это обычно какой-нибудь документ, который описан, а документация это про то, как она все-таки уже действительно в продакшене работает, и как ей пользоваться. Обычно там все есть. Обратно, почему? Потому что команды, которые делают тул, которым люди должны пользоваться, и не создают для него документацию, они обычно неуспешны, поэтому эти плы или эти продукты, они просто не летят, да, ну, через что и документация, они и в полете тоже не нужны. Вот, то есть, безусловно, вещи, которые, ну, unfortunate, да, все еще есть вещи, в которых документация исключительно идите и прочитайте, вот, да, но это обычные вещи, которые делают что-то либо узконаправленное, либо что-то такое, что, ну, работает прозрачно, да, то есть, к примеру, у нас есть документ, опять же, возвращая темик хаос инжиниринга, у нас есть документация про то, что он делает и почему, но не про то как, да, и там есть тоже, там есть, на самом деле, понимание, почему мы не хотим, чтобы наши кастомеры знали, как мы это делаем специфично для хаос инжиниринга. Но вообще, обычно документации проблем нет, то есть, промышленной археологии заниматься, ну, обычно не нужно. И перейдем, как мы перейдем к хаос инжинирингу, давай последний вопрос по поводу инфраструктуры. Ты много говорил, что вы знаете свои касты, и это очень важный фактор в принятии решений, но очень сложно оценить касты для платформенных команд, платформенные команды, которые делают инструменты и платформы для других команд внутри компании, и, опять же, зачастую это из-за постоянных размазанных издержек, из-за R&D, то есть, это не только касты, которые легко посчитать из разряда, машины, сколько вы используете, но там есть очень такой невидимый слой, который размазан, и очень трудно учитывать. Как это вообще концептуально посчитать? Я не говорю, как отвезаться в Microsoft, я говорю, как это в принципе понять настоящий каст платформенных команд. Очень хороший вопрос, правильный. Я постараюсь на него ответить, правда, в этот раз. На самом деле, общий каст посчитать несложно, потому что мы точно знаем, сколько чего мы используем, сколько людей мы нанимаем. То есть, тот доллар вэлью, сколько мы стоим, в смысле того, насколько мы хорошие ребята, насколько мы реально денег за год потратили, это мы знаем точно, даже для платформенных команд. Вопрос в атрибутировании кастов, грубо говоря, что на что потратилось и что на что ушло. Это сложнее, это действительно очень больная тема, про то, например, у нас мы платим много денег за storage. А что мы там храним? А как это посчитать? Вот у меня вчера Diorabro пошел, создал еще одну базу данных в этом же storage и хранит там еще что-то. И он на самом деле никому не сказал пока еще, что это, и зачем оно нужно, и как оно нам поможет в будущем. То есть, тут достаточно сложно, не всегда понятно, это правда. Мы пытаемся делать какие-то вещи, например, как в важере есть понятие subscriptions, и все ресурсы обычно атрибутированы к subscriptions. И мы используем разные subscriptions для разных вещей. Для поддержки production это что-то одно, для R&D это что-то другое, для CI-CD это что-то третье, для телеметрии что-то четвертое, и вот это вот все. Это немного помогает. Ну в целом, прямо до последнего доллара все считать, наверное, не всегда получается, но мы можем сосчитать очень много всего. С одной стороны. С другой стороны, для платформенных команд еще очень часто бывает так, что огромная часть кастов уходит в какую-то одну группу. То есть, например, когда мы занимались хардвером, то огромная часть наших кастов уходила, собственно, стоимость хардвера. И, например, одна из вещей, которые мы делали, мы занимались проектом, который уменьшал время простое этого самого хардвера. То есть, это был один из способов уменьшить кост для компании. Потому что если мы купили машину, и она ничего не делает, то это соответственно зря потраченные деньги. Я надеюсь, это как-то отвечает на вопрос, хотя он, конечно, большой и правильный. Это, то есть, я думаю, в компании, где вы, где каждая команда имеет какой-то отдельный биллинг, наверное, довольно легко посчитать. Когда компании меньше, и, допустим, этой компании используют один большой аккаунт на AWS, это становится много сложнее посчитать, потому что он шарится между многими людьми. Вот. И, наверное, еще такой тонкий момент, когда ваша команда, то есть у вас есть, например, 10 инженеров в команде, и вот эти 10 инженеров, они по-разному распределяют свое время. И вы можете, в принципе, посчитать, сколько времени или процент времени, который тратится на какой-то технический долг, либо на саппорт отдельных команд, либо на CICD pipeline. И вот тогда, в принципе, можно понять, как размазаны эти косты. Ну, таким-то… И это несложно сделать. Не, ну, то есть, как бы это можно сделать, но, получается, нужно тогда трекать время каждого инженера, что люди не очень любят, и такое. То есть, все равно это очень сложная задача, чтобы понять конкретный кост всей команды. Это делается в рамках обычного планирования следующего квартала, и ты смотришь оценку предыдущего квартала. За предыдущий квартал мы сделали вот эти пять задач, шестую начали, и вот люди так тратили свое время на эти пять задач. То есть, там плюс-минус указываешь, и это уже достаточно неплохое определение, что каждая команда и каждый человек внутри этой команды вложил в отдельный продукт. Да, это, на самом деле, правильно, мне кажется, особенно в рамках девопса, поскольку у нас, в общем-то, продукты часто прибиты к людям гвоздями. То есть, девопс, человек тратит время на продукты или на набор продуктов, и он делает для них, собственно, все. Поэтому нам не нужно собирать какие-то bits and pieces, когда у нас где-то есть тестовые инженеры, где-то есть поддержка, где-то есть ICD инженеры и вот это все. Поэтому это немножко проще, мне кажется, с этой точки зрения. Я поняла. Хорошо, спасибо за эти ответы, и давай-ка мы будем переходить к инженерии и хауссу. Расскажи вообще про концепцию. Откуда она взялась? Что это такое? Зачем это нужно? Какую проблему ты решает? Это отличный вопрос. Собственно, откуда она взялась, это, на самом деле, придумали это в Амазоне, мне кажется. На сколько я помню, на сколько я помню историю, да? На самом деле, история может быть чуть менее интересная. Я думала, в Netflix. А, это хороший вопрос, потому что все люди, которые в Netflix этим занимались, они все пришли из Амазона, из команды, которые этим занимались. Так это, как историческая справка. Просто, амазоне не очень нравятся, поэтому они все оттуда ушли. В Netflix, да, там Кейсер, Зенталь, собственно, и его замечательные ребята, они пошли от простого к сложному. То есть, на самом деле, то, что они пытались сделать, они пытались улучшить качество вещания, естественно. На самом деле, что вообще стоит за концепцией хаусс инжиниринга? За концепцией хаусс инжиниринга, как обычно я это объясняю, стоит стейтмент качества платформы. Вот, к примеру, то, что мы обычно делаем, когда мы продаем «Мажор», мы говорим людям, вот у вас платформа есть, которую вы пользуетесь, которая вообще никогда не будет практически отказывать. У нее надежность будет 99,999. И то, что это реально транслируется, что платформа будет, ну да, буквально две-три минуты за год. Если вы пользуетесь такой платформой, ваша мотивация что-то делать для того, чтобы обрабатывать эти ошибки, она, на самом деле, не очень высока. Потому что, скажем, если ретеншн период для разработчика порядка, предположим, от 2 до 4 лет, для джунгер-девелопера или для мид-левел инженера, то есть плохой шанс, что он за все свое время пребывания в компании, он вообще никогда, ни разу не столкнется с какой-то проблемой надежности, отказа платформы или чего-то еще. Чего ради в нее инвестировать? Это один стейтмент. Второй стейтмент — это когда мы говорим, что, ребята, вот у вас есть платформа, она вообще очень-очень надежная, но отказ обязательно когда-нибудь случится. Мы просто гарантируем, что хоть раз, хоть когда-нибудь она откажет. Тогда, наверное, такие вещи будут закладывать в дизайн, о них будут думать, о них будут разговаривать, о них будут придумывать какие-то ранбуки. И когда они будут происходить, кто-то будет выволакивать из пыльного шкафа этот ранбук, нужную страничку. Учительно следовательно будет пойнтом. Каждый третий из них не будет работать, потому что никто никогда не пробовал сделать по-настоящему. Каждый третий. Ну, к примеру, да-да-да. И вот теперь, например, третий стейтмент. Да, третий стейтмент заключается в том, что, ребята, вот вам платформа, и она вот будет отказывать каждый день. Мы гарантируем вам это. Вот. И когда она начинает отказывать каждый день, и это гарантировано, и это случается, мы не можем просто включать это в какие-то дизайны, да, или мы не можем как-то просто говорить о том, что, ну, когда-нибудь случится, поэтому вот мы тут писали, что делать, когда нам нужно сделать фейл-оуер. Ну, нам приходится как-то включать это прямо вот в работающий код продукта, да, то есть когда мы действительно, ну, делаем это автоматически, и оно действительно работает, потому что каждый день сидеть на звонках, потому что оно где-то сломалось, да, и как-то что-то пошло не так, ну, невозможно, да, опять, пресловутая концепция девопса, да, делали, при этом платят этот кост, да, поэтому они стараются сделать так, чтобы, знаете, знаки не попадать. Вот. И, собственно, то, что Chaos Engineering делает, и то, что вот Netflix в свое время сделал, они создали этот Chaos Monkey, да, который ежедневно, ну, ежечасно создавал какие-то отказы в системе, да, то есть, и они делали это очень просто, да, они просто приходили, выключали какую-то машину и смотрели, что получится. Вот. И, собственно, у нас, когда мы начали этим заниматься, мы сделали примерно то же самое, да, то есть мы берем машины в продакшене, мы их прибиваем, да, и мы гарантируем людям, что это будет происходить для каждого сервиса, каждый день, да, поэтому у них просто нету возможности, как сказать, от этого отказаться и сказать, да ладно, да нам это не надо, да оно не будет работать, да. Победить, победить, секунду, а можно вопрос? Да, конечно. Вы с самого начала репортируете систему с этим условием или вы перешли от работающей системы в продакшене к этому? Мы это сделали много лет назад, но мы перешли от работающей системы в продакшене к вот этому, и это был очень долгий, болезненный и сложный переход. Я могу про это тоже немножко поговорить. Очень сложно, да, очень сложно объяснить разработчику, почему система, которая уже работает, надо переписать по-другому, потому что она ненадежна в крайних состояниях, да, но, потому что обычно аргументы сводятся к вопросу, так вы же выключите свой fault injection и оно все нормально работает, правда, да, то есть мы грубо говоря возвращаемся в первый стейт, в котором разработчик говорит, ну на мой век хватит, а дальше это не так важно. Вот, собственно. Я добавлю пока ты вспоминаешь, это реальная проблема, про каждый третий не работает, то есть у нас есть проект, который в продакшене крутится последние несколько лет, и он падает примерно раз в год из-за того, что кто-то из соседей начинает плохо сеть убивает или еще что-нибудь такое, то есть не ломался вообще никогда. Вот, и в какой-то момент времени у нас внезапно нам понадобилось его рестартануть из-за связанных проблем, опять он отлично работал. Вот, и мы полезли с ним открывать ранбуки, оказывается, что ни одна из команд не работает, потому что у нас просто не оказалось прав, потому что совсем недавно были переделаны системы прав компании, и мы просто не проверяли эти ранбуки с тех пор. Сервис же работает, зачем их проверять, не мучатся. А была какая-то команда, которая владела этим сервисом? Это была моя команда. То есть получается, команда есть, но по факту, то есть и владение есть, но люди не знакомы с этим. Более того, мы обновляли версию не так давно, и все работало, и через две недели случилась эта проблема, оказалось, что у нас ничего не работает. Как показывает практика, нужно, я не знаю, раз в месяц, раз в две недели проводить какие-то, не знаю, учения, мы называем это game day, когда вы пробуете, экспериментируете, что-то делаете там. Это же очень типичная история, когда какой-то сервис, если он не работает, то ему уделяется внимание, люди знают, какая там внутренность этого сервиса. То есть к этому сервису добавляются ресурсы. Когда что-то работает хорошо, знаешь, все, мы умыли ручки, работает, не трожь. И вот мне кажется, это такая бомба замедленного действия. Это просто вопрос времени, когда оно сломается, и люди из-за того, что оно никогда не ломалось, просто понять не будут, знать, как с этим жить, работать. Это очень, ну, частая проблема в очень многих компаниях. Я пытаюсь умчительно вспомнить, вот это работать, не трожь, к какому уровню отнесло, ну, Ванину команд. Это третий или второй? Это, Олег, мы вспоминаем один из недавних подкастов, где мы разбирали уровни развития команд. Да, это интересно. На самом деле, это вот то, что ты сказал, то ровно про то, о чем я сказал между первым и вторым стейтментом. То есть когда ты говоришь, не то, что оно всегда работало и всегда будет работать, а когда ты говоришь, да, нам бы надо периодически проверять, что оно работает. Так вот, то, что мы пропагандируем, на самом деле, это третий уровень развития в этой истории. Когда ты говоришь, не то, что нам надо иногда это проверять, а когда мы говорим с этим, надо как-то жить. То есть, до тех пор, пока какое-то изменение, которое что-то ломает в системе, это вообще в принципе является ивентом, у вас все еще есть проблема. У вас нет проблемы, когда это состояние, это нон-ивент совсем. То есть, как бы, ну, сломалось и сломалось. Да, до бокса. Оно каждый день ломается. Вот, как-то так. Как это можно достигать, например, например, фейл-овр, нормальный реализованный фейл-овр или нормальный реализованный рекавери или что-то еще. Не важно, что. Я могу привести какой-нибудь пример из бинга, из ранних дней, когда мы об этом говорили. Что, к примеру, когда мы переходили от состояния, что одна машина сломалась, а все плохо, у нас было четыре реплики, теперь у нас стало три, и нам теперь не хватает их для того, чтобы захендить весь трафик, который у нас есть. И это беда, беда, нам надо срочно пойти вручную туда добавить еще одну машину. Когда мы доходили до состояния, что просто, ну, окей, у нас каждый день какие-то машины включаются, какие-то включаются, какие-то, какие-то вы питание увалится, какие-то по сети, да и черт с ними, да, как у бизнеса с Южом. Мы, там есть другая, на самом деле, проблема, другой подводный камень, да, который мы начали, мы не сразу про него поняли, да, но он заключается в том, что мы в таком случае начинаем пропускать маленькие проблемы, да, то есть, грубо говоря, проблемы, которые, ну, система очень-очень становится очень резилент, и она какие-то маленькие проблемы, ну, просто проглатывает, да, то есть, скажем, если у вас нет никаких проблем с инфраструктурой, а в сервис просто начинает крешиться, а сервис начинает крешиться там каждый, каждый второй день, да, на каждой машине. И поскольку инфраструктура это, ну, нормально съедает, да, и может куда-то вас зафоллоу-урить, остановить все остальное, вы просто даже не замечаете, а, онлайн, вы на это смотрите прям вот специально, да, что, а как все-таки мой сервис себя ведет, да, и вы просто трекаете рейт того, насколько часто вы делаете рекаверы операции, фейлоуверы и все остальное. Но если вы это делаете, то вы уверитесь, что рейт возрастает, да, и неплохо бы что-то, наверное, пойти и разобраться, а почему он растет, и, наверное, это, наверное, это не очень хорошо, но он как-то-то... Ну, есть, еще раз, да, там есть разница между, между, мы все-таки иногда делаем тренировки, там, типа, раз в месяц того, что она все работает, да, и есть разница с тем, что это не тренировка, это не эскалация, это не звонок, это нет никакого ранбука, да, просто, просто иногда у нас случаются проблемы, система их, ну, просто переживает, и все. Вот ты рассказываешь, а я понимаю, там, наших слушателей, которые живут в меньших компаниях и работают в меньших компаниях, и вот они понимают, что... Живот было смешнее. Живут в меньших компаниях. Собственно, у них ситуация очень другая, то есть, у вас инфраструктура уже дошла до того момента, когда, в принципе, уже все работает, и надо придумать что-то экстро, вот, как chaos engineering. В очень многих компаниях они еще не дошли до того момента, когда уже все и так работает. У них это, знаешь, chaos engineering, он и так у них каждый день случается, потому что что-то не работает. Вот, расскажи, как быть людям, у которых, ну, просто еще не дошли до того момента, когда он нужен. Вот, на эту тему можешь поговорить? Да, это на самом деле очень сложно. Ну, не то чтобы сложно, просто хороший вопрос, правильно? Тут вопрос же не совсем, не совсем про то, что, то есть, то, что ты говоришь, что есть компании, в которых отказы и так случаются каждый день, зачем же их создавать еще больше, да? На самом деле, это как раз-таки работает, мне кажется, в обратную сторону. Чем больше компания, чем больше скейл сервиса, тем больше там натуральных отказов, которые просто случаются. Просто потому, что чем больше у тебя трафика проходит через сервис, тем быстрее ты вытащишь из него все баги, по большому счету. На самом деле, там есть нюанс, который заключается в том, что они, конечно, случаются, но они, например, случаются не в каждом регионе, они случаются не с каждым фейлор-модом, они случаются более-менее в одно и то же время, например, когда у тебя пик трафика или что-то еще, а что будет, если, скажем, у нас будет коррелированная проблема в нескольких регионах одновременно, а что будет, если оно случится не на пике трафика, а наоборот, ночью, к примеру, такого рода вещи. И вот эти вещи, их тоже, в общем-то, надо тестировать при помощи того же кавокавокавост инжиниринга и фуллт инжекшена. И это, собственно, то, о чем мы, вот книжка, которую Иван упомянул, мы пытались про это как раз объяснить. И то, что ты говоришь опять про маленькие компании, маленькие сервисы, надо решить, на самом деле, что сервис должен, в принципе, автоматически переживать. То есть, если мы можем задекларировать в дизайне, что сервис поддерживает, а что не поддерживает, то то, что сервис поддерживает, и то, что, в принципе, скорее всего, вы в продакшене уже не видите. Потому что у вас есть нормальный дизайн ковик, который подразумевает, что сервис должен, например, справляться с пропаданием одного инстанса с этой сервисой. Или если у вас какая-нибудь база данных, она, в принципе, должна справляться с тем, что у нее одна реплика исчезает, она должна в моменте ее пересоздавать. То вот проверять вот эту функциональность очень надо, потому что мы привыкаем к тому, что оно просто работает из коробки, и когда оно не работает, мы вообще не знаем, что делать. Более того, мы берем еще и зависимость на это. Извини, пожалуйста, я перебил. Я думаю просто опять же вот про реальных людей, которые слушают наш подкаст, я представляю, вот работающие в компании, у них через день случается какой-то инцидент, и там куча людей пытаются инцидент починить. Инженеры очень уставшие, люди злые. И вот приходит слушатель нашего подкаста в такую компанию, говорит, я вот тут послушала или послушала подкаст, да, тут говорит про chaos engineering, давайте мы внедрим. Я понимаю, на этого человека просто посмотрят такими бешеными глазами и скажут, иди ты лесом, у нас так куча проблем, зачем нам еще дополнительные проблемы? Понимаешь? Конечно, я когда-то, знаешь, когда-то давно, лет, наверное, шесть назад, я сидел в кабинете одного из архитекторов Ажура, да, я ему говорил, ты знаешь, нам надо в продакшене сделать float injection, чтобы прямо вот брать машину в продакшене и прямо вот их вот выключать, да, и смотреть, что получится, чтобы мы были резилен к этим событиям. Он на меня так посмотрел и говорит, иди отсюда, и больше никогда не поднимай эту тему. Вот, это был такой еще дискарджен экспириенс, конечно, но мы как-то с ним потом справились. Так вот, понимаешь, дело в том, что chaos engineering позволили таким компаниям уменьшить количество инцидентов, а не увеличить, да, и все раз, когда мы говорим про chaos, как-то вот объяснил. Я согласна, как продать это людям, которые просыпаются через день и они ненавидят свою жизнь за это? А почему они просыпаются? Почему они просыпаются? Они просыпаются, потому что система не может автоматически справляться с какими-то проблемами, правильно? Chaos engineering как раз о том, чтобы научить систему справляться с проблемами автоматически. Понимаешь? Просто смотри, тут ровно то же самое, что ты говоришь, да? Когда система работает, когда система начинает просто справляться с каким-то классом проблем автоматически, эти проблемы исчезают со звонков, правильно? Теперь вопрос, а вот этот код, который обеспечивает это решение проблем, его кто-нибудь тестирует в продакшене? А как его тестирует? Он вообще тезится в продакшене хоть раз, хоть когда-нибудь? Да, и собственно chaos engineering частично, это как раз таки про то, чтобы создать путь, к которому этот код в продакшене будет эксисизаться постоянно. Если там что-то сломается, то мы об этом узнаем сразу. Причем мы об этом узнаем в контролируемом инварнити, который мы можем в конце концов просто... Chaos engineering в конце концов можно просто выключить и пойти спокойно досыпать дальше, сразу же разбираться, что там сломалось, почему и о чем. Да, я согласен, что это сложно продавать людям, которые и так уже сидят в эскалациях, но это же не способ создать больше эскалаций, это способ уменьшить количество эскалаций. В книге есть отдельно полглавы, посвященную тому, какие аргументы надо говорить, чтобы продавать эту идею. Валера хотел что-то сказать, я помню у Валеры было очень много инцидентов на какой-то из работ. Там была комплексно, да, там отчасти была моя вина, что я типа просто вместо того, чтобы как-то подпирать то, что падало, я ушел в разработку другой версии этого компонента, который бы by design не имел бы некоторых фейлер модов. Там были сложности к... В общем, там было очень плохо ватапатизированный процесс перезапуска, который отчасти был в основном, как в линке snapshot были сделаны в той версии, и отчасти в теку. В общем, не важно. Я тут могу сказать только то, что чтобы это хорошо работало, нужна поддержка всей организации, нельзя начать людям делать fault injection, а если они не имеют организационных ресурсов, вокруг него построить какой-то fault tolerance и не имеют поддержки от, скажем, команд инфраструктурных, которые должны помочь им построить fault tolerance. Это первая такая вещь? Это очень частично правильная концепция, потому что если, опять же, мы говорим о том, что у команды сегодня нет fault tolerance, а как они вообще в продакшене работают? Да, то есть никакая платформа не предоставляет 100% защиты от любых инцидентов. Да, то есть это значит, что успех компании привязан только к тому, что и WES будет всегда нормально работать и S3 никогда не ляжет в регионе? Ну, условно, да. Давай так, если ты считаешь, что таких компаний нет, поверь мне, они есть, и их больше, чем ты думаешь, и некоторые из них довольно большие и успешные, потому что, именно потому что почему-то описал, скажем, если компания стала большой и успешной за 4 года, и допустим, не на Амазоне, где фуктуации облака случаются чуть чаще, чем на физических дозуцентрах, если компания свои физические дозуцентры, там и они хорошо манажатся, то у тебя стабильный полет может быть просто годами, прежде чем у тебя случится первый сбой, но первый сбой, который случается, может быть катастрофическим, прям очень катастрофическим, и, ну, как бы, я, в общем, как любитель всяких property bias tests и прочих других штук, я, в принципе, как бы, глубоко сам понимаю Chaos Engineering, и мне вот, чисто как вот мне сейчас, его не нужно в самом деле продавать, но мне все равно было бы интересно послушать аргументы, но если бы ты пришел в компанию, где я испытывал проблемы с тем, чтобы эскалаться и начал там продавать Chaos Engineering, я думаю, там тебя даже бы неба послало, те, кто отвечали за всю систему, потому что там, в принципе, концепция была такая, что вот у нас такая платформа, которая не падает, и типа, короче, мне кажется, если бы мы начали делать fault injection, там бы развалилось бы просто, наверное, все. Не-не, подожди, подожди, опять же, мы, наверное, не про разные немножко вещи говорим, еще раз, то, что я сказал, что у тебя есть, на самом деле, я там как раз писал вот книжки про классификацию того, как мы делаем fault injection, вот это всего, и там как раз-таки одна часть классификации, это то, что, а что мы вообще поддерживаем, да, то есть, есть какие-то, да, это окей, если компания считает, что если с 3 лёг, то и они тоже легли, ну, так бывает, это на самом деле для какого-то размера бизнеса, это, наверное, ну, норма жизни, другое дело, что какие-то другие отказы, они, наверное, все-таки хотят уметь переживать, да, то есть, есть какие-то, например, просто баги, которые мы с деплоили в продакшен, да, к примеру, если у нас есть stage deployment, да, и какая-то stage deployment, ну, развалился, ну, потому что мы какой-то баг, все-таки, попадали в свой катей, мы же должны уметь такое переживать, правильно, в любом случае, то есть, почему бы нам не симулировать такую штуку, ну, достаточно часто, посмотреть, а что случается, когда у нас один stage полностью выпадает для депломета какого-то компонента, то есть, есть, есть дизайн, да, дизайн объясняет, к чему мы толерант и к чему мы не толерант, да, вот вещи, которые, к которым мы не толерант by дизайн, проверять каос инжинирингом не нужно, потому что мы точно знаем, что мы и так развалимся, да, это, это все хорошо, это понятно, да, у нас тоже есть, скажем, если у нас будет там ураган на восточном побережье, он полностью сунует там дата-центр, то, ну, это it's alright, да, мы понимаем, что дата-центр после такого, наверное, скорее всего, не восстанавливается, так, так бывает, или вот у нас там два года назад в Техасе молния ударила в дата-центр, да, и, ну, там куча оборудования просто испарилась, да, вот, прямо, прямо он the spot, соответственно, тоже там ничего восстановить после этого уже невозможно, но, скажем, есть какие-то другие фейлор моды, к которым мы хотим быть устойчивы, да, и вот, вот проверять, что эти фейлор моды правильно работают, можно быть помочь каос инжинирингом. Слушай, ну, опять же, ты к этому всему заходишь с концепцией человека в голове, который уже принял свою душу к каос инжиниринг, а я тебе пытаюсь аргументировать, что если у тебя есть компания, в которой каос инжиниринг, как бы, он на тебя посмотрит просто как на дебила больного, то есть у тебя концепция построения инфраструктуры такая, что, типа, вот мы делаем надежную инфраструктуру, она не ломается, типа, мы там просыпаемся по ночам, все чиним и так далее, но чтобы что бы не ломалось, чтобы, короче, там, не знаю, паровоз пыхтел, и мы, короче, будем подкидывать инженеров в топку этого паровоза, чтобы они там, типа, ночами заделывали дырки, вот, короче, на паровоз должен пыхтеть, типа, никакого, как это, никакого специального дезерапшена быть не может, ну, то есть, и вот как... Каос инжиниринг это не только про дезерапшен. Ну, в смысле, я вообще имею в виду, ну, во всяком случае, никакого не может быть, ну, не знаю, короче, как можно, как можно, как это, не знаю, делать, каос инжиниринг без пол дежекшена и без, без того, чтобы что-то сломать, если его нигде не делали, если еще никогда ничего не ломали, то как мы можем, типа, убедиться, что когда сломают, то не отвалится? Вот, например, мы виндо обновлять будем, например, если мы, ну, если мы виндо пользуемся или даже там, линус какой-нибудь, другое ядро пересажить, или там, например, сертификат, будем ротировать периодически же все равно, правильно? Ну да, будем. Ну, сертификат, например, мы ротируем обычно, все-таки, не очень часто, скажем, раз в полгода, да? Ну да. И все-таки вещи, которые случаются раз в полгода, они имеют тенденцию периодически разваливаться, ну, потому что просто им не давно никто не пользовался. Вот. То есть каос инжиниринг, в частности, про то, что если нам что-то надо ротировать в полгода, то мы могли бы делать это значительно чаще. Да, то есть брать какой-то лимитированный скоб и ротировать, там, скажем, раз в день, раз в неделю. Но у тебя все инженеры заняты тем, что они просыпаются по ночам, разгребают инциденты, у них нету кэпэсити типа ротировать сертификаты чаще. Ну, тут уже no-free lunch, да? То есть нельзя получить что-то, не сделав ничего, да? То есть... Я как раз пытаюсь тебя спросить сложный вопрос. Ты меня пытаешь, как бы, опять же, с позиции типа человека, который уже принял, что это делать нужно. Окей. Я тебя пытаюсь спросить, опять же, про с позиции, не знаю, как это пройти в компанию, которая просто всеми своими, не знаю, как бы существующими институциализированными, как бы, не знаю, процессами будет отвергать, потому что, не знаю, концепция другая. Валер, мне кажется, что если ты работаешь в компании, в которой менеджмент согласен с тем, что инженеры встают по ночам каждый день и не пытаются исправить эту ситуацию, то хаос инженеринка в этой компании ты не сможешь внедрить. Ясно. И никогда мы очень-очень долго нанимаем. Даже внедрять это очень сложно, потому что, представь, у вас есть команда, которая просыпается по ночам, и вы думаете, окей, давайте внедрим хаос инжиниринг. Получается, вам нужно распределить время так, чтобы вы новую... То есть люди, некоторые, продолжали просыпаться и поддерживали институтуру, но есть команда или часть команды, которая будет работать над хаос инжинирингом. И это тоже не так просто сделать, потому что люди будут говорить, а что-то мы просыпаемся, а те не просыпаются. Валера, не слушай их всех, я думаю, ты задаешь очень хороший вопрос, правильный. Ну потому что да, я такое видел больше, чем в одном месте, в разных ипостайсях. И вот когда мы говорим про Google, Amazon, Microsoft и компании, где, не знаю, мне даже интересно, как это в Microsoft началось, потому что очень сильно наверху поменялся, чтобы поменялась концепция сверху вниз. Прежде чем это стало вообще возможно так поменять. Потому что вот когда есть какое-то закостенелое видение, как что-то должно работать, при том сверху, то просто получить ресурсы на это, это нетривиально. И даже если ты получаешь сверху добро, ты не можешь моментально заскейлить команду, которая сейчас занята подкидыванием инженеров в топку, чтобы освободить capacity на хаос инжиниринг. Смотри, чтобы кого-то в чем-то убедить, надо понять, кому это выгодно. Кто-то от этого будет бенефитить от того, что мы что-то сделаем. Например, в нашем случае, я могу сказать, что тот, кто от этого выигрывал, был тим, который занимался платформой. Прямо платформой для хостинга, hardware, network и всего остального. Как будто в бинге начали делать. Собственно, почему? Потому что когда мы только-только начали это делать, мы просто тонули в эскалациях, у меня машина лежит. Да, то есть, у меня легла одна машина, все беда-беда, сделайте что-нибудь, срочно верните мою машину, у меня там очень ценные кастомерские данные, ничего не сделать не возможно. Работа встала. И собственно, in the long run от хаос инжиниринга выиграли мы, потому что мы стали говорить, одна машина down, это вообще не наша проблема. Наша платформа гарантирует вам, что хотя бы одна машина у вас всегда будет down, поэтому это к нам даже не приходить. Понимаешь? То есть, опять же, что ты хочешь улучшить при помощи хаос инжиниринга и кто от этого будет бенефитить? Я думаю, что скорее всего, в том примере, который ты приводишь, люди, которые скорее всего будут выигрывать эту ситуацию, это те самые инженеры, которые сидят в он-кол, потому что в течение какого-то небольшого достаточного количества времени им придется в он-кол сидеть значительно меньше. Они смогут заниматься чем-то еще, прямо настоящий код писать вместо того, чтобы сидеть кастомеров поддерживать. Другое дело, что если у компании вся политика сводится к тому, что мы в принципе не заинтересованы в этом, потому что мы можем очень дешево нанимать инженеров, которые будут просто сидеть в он-кол по ночам, то это тоже выбор, который компания может для себя сделать. И если ты не можешь переубедить руководство этой компании о том, что in the long run это не очень sustainable, и это замедляет рост компании, это замедляет рост инженеров, ухудшает качество инженеров, которые компания может получить и все остальное, то мы очень много нанимаем, например. Потому что компанию можно поменять. Помните, если нет другой рыбы, то и на другую воду. На самом деле, я готов рассказать кучу примеров, как мы внедрили подобные геймдейвы у себя в компании на 20 человек, при том, что мы по ночам дежурили, потому что ночное время в Америке, самая большая нагрузка у нас часто все падало. Плюс, отвечая на один из старых вопросов, в целом инженерия хаоса это не про то, что давайте включим хаос манки и она будет работать, то есть давайте все уроним. Нет, инженерия хаоса это в том, что ты добавляешь понимание системы и стабильность с помощью каких-то действий. Хаос манки это просто одна из практик. Скажем, можно спокойно улучшать понимание системы, в том числе, снижая количество проблем в продакшене с помощью того, что собираются профессионалы какой-то системы за столом с новичками, какой-то системы и говорят, так, профессионалы говорят, так мы знаем эту систему, давайте будем прыгаем в игру, я говорю что случилось вы мне скажите, что получится. И вот и так, сегодня я говорю, что сломался винчестер на этой машинgie, что случится? И幹чłые такие, наверное, вот здесь метрика не подпрыгнет. golf, нет, не подпрыгнет. А наверное, вот здесь simultaneously случится. То есть, как бы ничего не ломая, ничего не выключая, ничего не трогая, ты можешь улучшить хорошо понимание системы просто за счет диалога. Это тоже часть хаус инжиниринга. Теоретический хаус инжиниринг. Теоретический хаус инжиниринг. В некоторых компаниях книжки там написаны, можете почитать, это просто стандартная процедура. Расскажи, Ваня, ты обещал про то, что можешь рассказать, как это внедрялись. Вот даю тебе слово. Ровно так, как по книжке написано. То есть, так как Олег сейчас рассказывал, в какой-то момент после очередного третьего, четвертого вставания среди ночи, потому что все сломалось, мы поняли, что так продолжаться не может, больше надо что-то менять. И мы начали придумывать способы смотреть по сторонам, в том числе нашли вот этот хаус манки. Но для того, чтобы начать применять хаус манки, ты должен очень четко продумать, что у тебя в системе нестабильно, что должно быть изменено. Мы начали проводить геймдейс. Мы посмотрели геймдейс от Амазона, кстати. Приезжал в Москву на один из хайлоудов старых-старых, я не помню кто, из Амазона. Он рассказывал, что какие геймдейсы они проводят у себя. И вот эти две системы мы решили вместе внедрить хаус манки. Мы так и не смогли включить, потому что мы не до конца доверяли своей системе. А вот геймдейс нам сильно помогли. То есть, какой-то момент ты говоришь, что сегодня собираешься что-то сломать на продакшене. Но геймдейс бывает разных видов. Мы использовали чаще всего именно инвестигирование, потому что у нас чаще всего возникали проблемы, когда тебе надо не так что выполнить известную строчку из ранбука, потому что все сломалось. Это мы давно уже автоматизировали. А так что внезапно что-то ломается, внезапно у нас CPU под 100% перестал обрабатывать трафик. А почему, непонятно. Надо разобраться. И поэтому мы проводили геймдейс, который, а давайте мы сейчас как-то не так хитро загрузим систему, что она все равно сломается и надо будет разобраться всем. Выбирался один человек ведущий, который производил подобную систему. У нас было много кластеров. Мы брали те кластера, которые не использовались для важных клиентов или которые вообще были в стейджинге. И соответственно, пускали такой трафик, все ломалось и команда сидела и копала, почему это начинало падать. И совместными усилиями, общаясь в слэке или в чате или в видеочате, в зависимости того как и где это проводилось, приходили к выводу и соответственно находили проблему, чинили проблему. Но в результате всегда получалось, что у тебя возникал документ, в котором, я не знаю, 20 экшн-айтомов. Там дописать эту документацию, изменить вот эту систему. Кстати, вот здесь метрики неправильно посылаются или еще что-нибудь такое. Получается куча всего, но самый главный результат, ты настолько хорошо начинаешь разбираться в этой системе после парочки подобных gmdf, что тебя от аудио больше не страшно. И это один из плюсов. Это все понятно. Извини, пожалуйста, Ваня. Погромче говорите, я тебя просто слышу. Расскажи, а как ты продавал эту идею? Ну, приходит тебе самый главный CTO of VP и такой, а что это вы тут делаете? А у нас тут игровой день. Я купил эту идею на HighLog, а продал эту идею своему CTO за, не помню за что, за 30 серебря. Я сначала выбил пол пятницы. То есть по пятницам она нельзя релизить, потому что перед выходными. И, соответственно, по пол пятницы мы сначала проводили геймдей. Потом я увеличил до пятницы целиком. То есть по четверть-пятницам было более рискованными днями? Мы не трогали продакшн для нужных пользователей, соответственно, либо вообще на стейджинге это все делали, поэтому это было в целом нормально. Подожди, концепция же хаос инжиниринга в том, что ты делаешься на реальной системе, на реальном продакшене, а не на стейджинге? Нет, концепция хаос инжиниринга — улучшение знаний команды и увеличение резиленса. То, каким образом ты это делаешь, не написано в каких-то правилах. То есть все это очень гибко. Я думаю, это очень важный момент, потому что у меня сложилось впечатление после нашего разговора, что это должно быть в продакшене с реальными людьми, реальными пользователями, а на самом деле — нет. Это часть системы. То есть если ты это делаешь в реальном продакшене, с реальными... Все, ты как в Microsoft, ты плаваешь в жиру и уже думаешь, боже мой, что мне такое еще сломать, чтобы оно по-настоящему сломалось? Но начинать-то надо с чего-то. Я рассказываю, как это мы начинали. Мы не дошли до идеальной ситуации. Да, на самом деле, я тут, наверное, сломал понимание, когда начал говорить про то, что мы это делаем на продакшене с реальным кастом, реальным трафиком и всем остальным. То есть тут, Ваня, прав, конечно, надо начинать, во-первых, с припродакшена, во-вторых, надо начинать с маленького скейла, смотреть, что получается, и потом прогрессировать. Там даже где-то книжки про это написаны, про то, как это осторожно выкатывать на скейл. Там, на самом деле, тоже не все треально, потому что то, что работает на маленьком скейле, может совершенно не работать на большом. Я думаю, это очень важный аспект, когда про то, как это продавать, потому что вот если придете, скажете, давайте делать это на реальных людях, это действительно будет очень сложно. Когда речь идет про стейджинг, то это очень другой разговор. Мне кажется, на самом деле, что продажа чего угодно, она всегда свободится к одному и тому же. Who stands to benefit и сколько мы с этого получим. То есть, если вы разговариваете с вами с ТО и пытаетесь объяснить, зачем делать Chaos Engineering, то притяните это к метрикам своей компании или своей команды. И скажите, что я считаю, что если мы будем делать Chaos Engineering, мы будем поддерживать наш продукт лучше, быстрее и дешевле. И насколько? Понимаешь, это тоже такая красивая концепция про то, что мы, говорим, любому человеку из компании, давайте... То есть, вы можете такую инициативу сделать. Вот почитайте метрики, предложите. Эта работа, это довольно много времени может занять. И когда у нашего команда загружена с аутейджерами, и люди встают по ночам, пытаются этим разрулить как-то, то у них просто нету вот этого mental capacity, чтобы сделать такой документ и прямо его обосновать, и убедить. То есть, это работа, которая довольно большая, и у инженеров не всегда есть время этим заниматься. А еще у них нету желания? Как любая работа, она должна бы реализироваться, на самом деле. То есть, как любая работа, это надо... Вообще любые улучшения инфраструктуры, любые улучшения инфраструктуры, они, в общем-то, теоретически должны проходить через планирование, и оказываться в какой-то момент либо в вашем скраме, либо в канбане, либо чем вы там пользуетесь. И теоретически иметь какие-то dedicated ресурсы, которые этой темой должны заниматься в какое-то рабочее время. Если этого не случается, ну, тут это на самом деле проблема менеджмента. То есть, менеджменту неплохо бы понять, что кроме LifeSite Support надо бы делать что-то еще для того, чтобы двигать систему вперед, хоть как-то. И если улучшение надежности не входит прямо сейчас в список приоритетов, то менеджменту хорошо бы иметь хорошую идею, почему. Например, в нашем случае это привязывается к тому же самому review. То есть, если мы выбрали список метрик, которые мы улучшаем, и надежности, например, среди них нет, то неплохо бы иметь хорошее объяснение, почему так получилось. Хорошо, мы поговорили очень много про house engineering. У нас по таймингу сейчас час 30. Можно последнюю мысль про house engineering? Я вот прямо это очень-очень хотел узнать, Олега. Давай. Олег, расскажи, как ты попал в эту книжку? Кейси предложил пусто участвовать. Нет, на самом деле, просто мы довольно давно познакомились, да, потому что когда вся эта тема с house engineering еще в Netflix, это когда только-только начала подниматься, мы начали это полкать в Microsoft. Кейси это делал в Netflix. Были ребята, которые в Amazon, но они как-то потом все немножко растворились. Были ребята в Uber, которые этим занимались. И community было достаточно тесным. Ну, она еще достаточно тесная. И, в общем-то, все друг друга знают. И, собственно, есть раз в год происходит ивент, сейчас, мне кажется, чаще этот, Chaos Day, на который мы в дековидные времена собирались все вместе, обсуждали какие-то новые темы интересные. Netflix на самом деле сделал не только House Monkey, они сделали достаточно много интересных идей. Например, то, что последнее, насколько я понимаю, то, что они делают. Опять же, Кейси уже покинул Netflix, у него сейчас своя компания, называется Verica, и они занимаются той же самой темой. Они делают LDFI, это Lineage Driven Fold Injection и LDFI+. Как-то так. И, поскольку мы все друг друга знаем, что когда Кейси начал толкать идею с книжкой, то он предложил поучаствовать и поделиться нашим опытом того, что мы сделали в Microsoft. Плюс к тому, насколько я знаю, по крайней мере, из того, что я сейчас вижу в индустрии, в Microsoft, я не знаю насчет впереди планеты всей, но достаточно далеко продвинулись. Не очень много компаний продвинулось настолько же далеко. То есть Netflix, наверное, да. Я не знаю насчет Amazon, мы с Amazon не очень хороших отношений, как вы, наверное, знаете, поэтому мы с ними не особо делимся информацией по этим вопросам. У нас эта штука действительно работает в продакшене, причем действительно во всем Клауде. И это, мне кажется, достаточно серьезная степень. Мы действительно делаем сотни тысяч fault injections в каждую неделю. И это так не все могут. Мы так можем. Завершая тему хаос инжиниринга, хочу... То есть это у нас был очень такой часто абстрактный разговор. Мы говорили про концептуальный уровень, но мы очень мало затронули конкретных практик. Из конкретных практик, вот я слышала про Game Day прекрасно, я слышала про то, что есть Chaos Monkeys. Что еще из конкретного? На самом деле, опять же, немного как Chaos Monkeys, но тут есть такой момент, что вообще, в принципе, телеметрия и варьердация качества сервиса в продакшене — это абсолютная часть хаос инжиниринга. Вы не можете ничего абсолютно автоматизировать, что трогает сервис в продакшене до тех пор, пока вы не понимаете, в каком состоянии оно находится и что оно сейчас испытывает и что с ним в принципе можно сделать. Очень простой пример, который обратно привязан к Chaos Monkeys — это, если у вас есть 10 инстансов в продакшене и ваш дизайн подразумевает, что не больше двух инстансов все время лежит, то неплохо бы знать, сколько уже лежит перед тем, как вы возьмете и что-то еще уроните своим хаос инжинирингом. То есть, на самом деле, это одна из тех вещей, которая позволила нам быть успешны в продакшене — что мы очень хорошо понимаем, в каком состоянии наш сервис сейчас находится, и мы делаем бэков на тех сервисах, которым уже плохо. На самом деле, то, что это делает для нас — это делает нам так, что нашим сервисам всегда плохо одинаково. Только иногда это реальные проблемы, а иногда это то, что мы искусственно туда добавляем в качество проблемы. Именно поэтому очень многие вещи, очень многие хардверные проблемы, очень многие проблемы с сетью, они просто прозрачны, потому что для сервисов это всегда бизнес со сьюжем. Другие практики, то, что я уже затронул от LDFI — это понимание зависимости, трассировка зависимости и попытки выяснить, каковы зависимости в действительности. То есть, к примеру, вы знаете, что у вас есть зависимость на какую-то базу данных, вы знаете, что у вас есть зависимость на какие-то девайсы в сетевом пути, у вас есть зависимость на локальные железы. И вы, в принципе, знаете, что случится, но вот как Ваня сказал, что если вот в этой машине умрет диск, то что случится? LDFI предполагает реальный тест, а что случится, если он действительно умрет? Давайте выключим, посмотрим, а что действительно отвалится. Расшифруй, пожалуйста, эту аббревиатуру. LDFI Lineage-driven fault injection, то есть, грубо говоря, исследование зависимости. У меня есть база данных. Я знаю, что на эту базу данных зависит такой-такой-такой сервис. Что случится, если эта база данных лежит? Теоретически, например, сервисы должны идти в другой регион. Пять сервисов, про которые мы знаем, предположим, что мы на практике эту базу данных берем, в каком-то регионе, выключаем, пять сервисов, про которые мы знаем, ушли в другой регион, и что-то еще сломалось. Вот это что-то еще мы про него даже не знали, что у них тоже есть зависимость на нашу базу данных. И вот этот бенефит реально в LDFI, который заключается в том, что мы исследуем реальные зависимости, которые существуют в системе. И их находим иногда. Например, из классического примера, который я, наверное, могу рассказать, это было уже довольно давно, когда мы выключали питание в дата-центре, к примеру, в качестве теста, посмотреть, что реально получится, кто реально лежит, и все остальное. И выяснили, например, что у нас есть проблемы с охлаждением, потому что в дата-центре становится очень жарко, если питание выключить там совсем, потому что охлаждение тоже не работает. Машины потом обратно не включаются, к примеру. То есть это такое тоже, мы нашли зависимость, скажем, между питанием охлаждения, про которое мы на самом деле всегда знали. Там провод есть, в конце и от концов. Но никто ее не представлял ни в одном дизайне. А теперь мы про нее знаем и представляем. Или, к примеру, тоже это скорее более смешная история, когда у нас в дата-центре есть небольшие выделенные места, в которых стоит сетевое оборудование. И там есть, скажем, датчики давления, которые на входе, на выходе воздуха, в которых вдуваются машины в стойках и вот это вот все. Вот, к примеру, есть интересная зависимость между наличием стенки в этой комнате и правильной работой датчика давления. То есть, если, к примеру, он начинает показывать что-то не то, то мы начинаем в эту комнату надувать очень много воздуха, и гипсокартонную стенку оттуда просто выдувает. А потом у вас в воздухе очень много пыли от гипсокартона, которое выглядит, в принципе, как дым для детекторов дыма. И дальше срабатывает пожарная сигнализация, что, в принципе, тоже не очень здорово. Ну, тоже вот, более-таки, это реальная история из Hardware Management. То есть, исследовать какие-то вопросы путем добавления, то есть, идея LDFI заключается в том, что мы добавляем отказ для того, чтобы понять, кроме того, что мы ожидаем, что упадет, что еще упало. Но обратно, чтобы это сделать, мы должны очень хорошо понимать нашу гипотезу. То есть, мы пытались написать саморяд для этого книжки, потому что перед тем, как мы делаем любой совершенно экшен, не важно какой системы, не важно какой методологии, мы должны понимать гипотезу того, что мы пытаемся получить на выходе. То есть, мы никогда не пытаемся получить полный crash-системы. Это глупо. То есть, если мы знаем, что для какого-то нашего действия система полностью ложится, это никому не нужно. Это и так понятно. Мы делаем только те действия, которые, как мы думаем, приведут к какому-то понятному результату. И если они приводят ровно к тому результату, что мы хотели, это очень хорошо. Это ровно то, что мы хотели получить. Если они приводят к какому-то результату, которого мы не ожидали, например, что-то еще ломается или что-то ведет себя не так, как мы хотели, это как раз-таки то самое исследование системы и попытка понять, как она на самом деле работает. Да, это, наверное, более интересно для больших систем. То есть, как я в начале отебесей это говорил, что никто на самом деле не знает, как работает Cloud. То есть, нет ни одного человека, который понимает от и до, как оно работает. То есть, каждый знает немножечко про свой небольшой кусок, но найти какого-то одного архитектора или кого-то еще, который мог бы рассказать все детали, начиная от дата-центров и заканчивая тем, как работает локация для виртуальных машин в разных кластерах, я не думаю, что это вообще возможно. Поэтому, собственно, именно поэтому мы и пытаемся этот вопрос исследовать теоретически. То есть, мы пытаемся посмотреть, как компоненты друг на друга зависят, что к чему привязано и и так далее. Тут, на самом деле, опять же, очень много зависит на предзнание. То есть, как еще мы можем трассировать зависимость и как еще мы можем узнавать текущий стейт для того, чтобы понять, в каком состоянии система находится, есть ли какие-то ботлнейки, для того чтобы сделать более правильную кипотезу состояния системы. Я бы еще хотел две штуки добавить, что во-первых, когда проводишь эксперимент, нужно всегда точно понимать, как Олег сказал, на что ты будешь влиять, но еще нужно понимать, что ты будешь делать, если что-то другое пойдет не так. Как у них выключение питания, выключение базы данных привело к тому, что другие сервисы начали ломаться. На этот случай, наверняка, у них был скрипт, который говорил, сделайте то-то, чтобы все вернуть, как было, потому что ты никогда не можешь знать, к чему приведет, потому что ты не знаешь до конца свою систему. Первое, а второе, очень классное правило в этой книжке написано, мы очень хорошо разбираемся в том, что мы делаем постоянно. Это основной правил. То есть, если вы постоянно делаете ротацию ключей, как говорил Олег, или если вы постоянно проводите эксперименты своим сервисом, вы начинаете в этом хорошо разбираться. В этом включая то, с чем вы проводите эксперименты в ваш сервис. И как результат вы получаете очень хорошее знание. Если ни разу не проводите эксперименты, вы не будете знать свой сервис. Да, плюс 100-500 на оба пункта. Какие-то еще практики, которые мы не назвали? Десятки. Тут, наверное, я просто не знаю, с чего начать даже. Опять же, я не знаю, что у нас с хронометражом. Но вот если... Ну, опять же, я могу порекомендовать посмотреть нашу книжку. Благо, она сейчас бесплатно доступна. Можно просто скачать и посмотреть. Там довольно много всего хорошего написано. Хорошо, тогда я думаю, мы достаточно хорошо поговорили про тему инженерии хауса. И я хочу еще заметить одну штуку, что это очень интересно, что по факту это ведь то самое тестирование в продакшене. Это то, о чем мы говорили давно и слышали, то есть концептуально. Это было известно раньше. Интересно так, что по факту тестирования в продакшене оно обрело какой-то новый виток, но это тестирование. Это про качество вашего продукта. И это такой забавная вещь, что мы... Давайте мы перенимаем QA инженеров, которые занимались тестированием в продакшене, и в том числе каким-то автоматизированным тестированием. Давайте их назовем хаус инженерами. И таким образом мы заапгрейдим их там левел среди индустрии. Это как знаешь... Это кстати да и нет. Это не совсем правда, да? То есть оно мне напоминает ситуацию с актуарной математикой. То есть раньше актуарии это было очень... Ну, люди туда шли, но знали, что будут заниматься, скорее всего, моделями для страховых компаний, и это не было классно. Теперь называется data science, и это супер крутая сфера. Да, она шире, чем актуарная математика, но мы больше не называем актуариев актуариями. Теперь у нас data scientists. И к этому круче относится тоже самое с хаус инженерингом. Ну кстати, на самом деле, одна из целей, которую Кейс оставил на самом первом хаус дэй, было создать новую профессию хаус инженера для компании. Так что я думаю, что ты на самом деле где-то права с тем, что говоришь. Тут я хочу добавить две вещи. Во-первых, да, действительно, мы совершенно забыли напомнить, что есть тест на продакшн, и на самом деле хаус инженеринг он как бы enables тест на продакшн. Потому что, если подумать об этом, то на самом деле абсолютно неважно, как сервис отказывает, если мы умеем правильно бороться с отказами. И у нас есть автоматизация, которая уже разрешает нам иметь отказы сервисов направо и налево, при помощи этого самого хаус инженеринга. И то, что мы на самом деле можем делать, а то, что мы на самом деле, например, мы реально это делаем, мы можем брать инстанции сервисов или там чего-то еще, каких-то инфраструктурных вещей или чего-то удобного, и просто пихать их в продакшн. Потому что мы знаем, что если что-то пойдет не так, продакшн уже умеет с этим справляться. И этот confidence мы можем построить благодаря, точнее, какая confidence, уверенность. Эту уверенность мы можем приобрести благодаря хаус инженерингу. И когда она достигает какой-то отметки, когда мы действительно очень хорошо себя чувствуем про то, что мы можем побороться с любым отказом вот этого конкретного компонента, мы можем этот компонент тестировать в продакшн на право и налево. Например, мы тестируем билды-винды новые в продакшн, направо и налево. И это очень помогает и в Windows тему, и команде Windows для того, чтобы находить какие-то новые проблемы быстрее. И это очень помогает нам, потому что мы можем брать более новые билды чаще. Соответственно это Security с Rob security и вот это вот все. Ну и всякое A-B тестирование и прочий флайтинг туда же. На самом деле мы даже депламинат также можем делать, если есть то, что про то говорить. Да, то есть если мы можем что-то тестировать в продакшн и оно там работает, а не разваливается, то мы можем точно также накатывать его понемногу везде. Обратно использовать те же самые принципы Chaos Engineering. Мне кажется, билды-винды это на название выпуска. Как правильно сказать, версии Windows? Да нет, ну в смысле, оно наоборот классно звучит. Я напоминаю, что у нас хронометраж сейчас час 47, у нас конечно же вырежутся паузы, будет меньше. И я предлагаю дальше идти по теме, которая у нас была от гостей. Я считаю, она тоже классная и скорее всего мы забьем на другие темы, которые у нас сейчас есть. Давайте говорить про эволюция требования к инженерам. А это кстати отлично, да. На самом деле спасибо, что вспомнил, ребята. Для меня это просто самому довольно интересная тема. На самом деле, поскольку времени у нас не очень много, давайте я сразу про самое длинное, на мой взгляд, интересное расскажу. Ну, ты не сильно переживай. Все нормально, времени у нас в порядке. Этот выпуск ведет света, поэтому у нас есть 3 часа. Это довольно типично для моих выпусков, поэтому у меня есть право вето здесь делать длинные выпуски. Хорошо, замечательно. На самом деле, давайте поговорим об этом. У нас есть проблемы в индустрии. Проблема заключается в том, что инженеров не хватает. Если вы на самом деле посмотрите на статистику, то в глобальном понимании, насколько я знаю, я недавно читал отчет наших HR, который по большому счету, то, что там было написано, что 30% вакансий для инженеров по миру не будет закрыты в этом году, потому что инженеров хватает. То есть их просто нет. Их не откуда брать. И, например, то есть на самом деле ключевой понт, про который я хотел поговорить, это что если 10 лет назад мы были чертовски требовательны к инженерам и образование пыталось подстроиться под то, чего нужна индустрия, то сейчас то, что я вижу, все происходит немного в обратную сторону. Индустрия пытается подстроиться под то, что производит образование. И тут, наверное, есть разные компоненты этого. С одной стороны, у нас очень сильно возросло количество инженеров, которые нам нужны, просто грубо говоря. Если скажем, когда я пришел в Microsoft, мы занимались этой инфраструктурой в бинге. Я занимался сетями. У нас в команде, которая занималась вообще всем без сетей, у нас было 30 человек. Сейчас команда, которая занимается нетруком для Azure, это 3000 человек. И, в принципе, вообще вся инфраструктура бинга, включая сети, железо, телеметрию, все остальное, это было 120 человек. Сейчас, соответственно, в Azure десятки тысяч. С одной стороны, индустрия стала гораздо более голодной на то, сколько инженеров нам нужно. С другой стороны, мы стали менее требовательны к тому, что мы хотим, чтобы люди знали. Некоторым образом, мне кажется, то, что сейчас происходит, это некоторые девальвации когда мы говорим, что, скажем, 5-6 лет назад, 7 лет назад, когда я проводил интервью, я мог поговорить с человеком про какие-нибудь красно-черные деревья, про какие-нибудь обходы графов, спросить, как написать подходящую ивистику для какого-нибудь Asara, на которые мы смотрим. Сейчас мы вообще такого не делаем. Сейчас спрашивают на интервью про какие-нибудь алгоритмы на графах, это практически гарантированный фейл. С другой стороны, в индустрии стало очень много работы, которая не требует этих знаний, которая скорее требует быстрой адаптации к новым фреймворкам и способности делать что-то, что хорошо описано в документации. Мое мнение, хотелось бы послушать мнение других людей, заключается в том, что мы переходим к тому моменту, когда инженеры в индустрии становятся commodity. Если раньше мы старались искать какие-то таланты, то сейчас это просто commodity. Нам нужны инженеры, которые умеют делать работу так, как она описана, используя стандартные решения, не отходя от шаблонов, но делать это достаточно хорошо, быстро, качественно. Я тут недавно обсуждал с коллегами, мы сравнивали это с тем, как развивалась индустрия автомобилестроения. Если в начале 20 века были небольшие группы людей, которые собирали автомобиль полностью из заклепок от и до, то к середине 20 века это были конвертные производства, и люди, которые работали на конвертах, они требовали какого-то образования, но образование было достаточно простым, значительно проще, чем объяснить, как работает вся эта штука в целом. Мне кажется, что с софтур индустрии мы сейчас идем по ровно тому же пути. У нас есть очень большие команды, которые производят очень простые решения, производят их на потоках. Грубо говоря, они клепают одни и те же веб-сервисы или они клепают одни и те же базы данных. Есть шаблоны, которые объясняют нам, как делать resiliency, как делать reliability. У нас есть стандартные решения для каких-то вещей в сетях, нам не надо ничего нового придумывать для этого. И, наверное, как мне кажется, мы движемся где-то в этом направлении. Есть кулстория. Меня хорошо слышно, я могу сделать погромче, если просто... Нормально, говори. Да, ты говорил про красно-черные деревья на интервью, и мне вспомнилась небольшая кулстория. Как раз некоторое время назад одна компания, ну, со мной связались ее рекрутеры на позицию архитектора. Ну и там, слово за слово, я поинтересовался, как у них выглядит процесс найму, какие этапы интервью предстоят. Выяснилось, что будет несколько этапов с вращением деревьев на досках. Напомню, позиция называется архитектор, с такой помпой, что нужно будет думать над архитектурой, как там манипулировать сейчас пятабайтами данных. Видимо, пятабайт должно было впечатлить, хотя это пара сотен серверов в наше время. Неважно. На что я так реже ответил, что я все понял, но, вы знаете, я вам не подойду. Спустя какое-то время мне пишут рекрутеры, что, вы знаете, мы поняли, да, у нас слишком много этапов интервью, вам достаточно будет там один раз только прийти и поворачивать деревья. Вот, я говорю, ну, нет, вы не понимаете, в чем суть проблемы. Вы ищете архитектора, да, и ваши процессы, они многое говорят о вашей компании, то что, то есть, попробуйте поставить себя на мое место, вы говорите, нам нужен там архитектор, который разработает рычагуру, и так далее. Но при этом вы просите его прийти и поворачивать там деревья, то есть, возможно, несколько вариантов. Либо вы под этой позиции имеете в виду, не то, что я под ней понимаю, и реально, как бы, на этой должности мне придется вращать деревья на досках, и тогда мне этот вариант не очень подходит. Либо вы просите, ну, как бы, вот к вам приходит клиент, как бы, мы входим в мою роль, к вам приходит клиент, и чтобы доказать, что вы подходите на данную роль, просит продемонстрировать совершенно нераниматную навыку. Наверное, вы не захотите вести дела с таким клиентом. То есть, и мне хотелось бы, чтобы вот больше компания не мысли, как мыслит наш гость, что, ну, я еще понимаю, там, на разработческую позицию, ну, да, не грех, там, разок-другой, там, повернуть дерево на одной доске, разок на другой доске, ну, нормально. Вот, но если вы собеседуете архитектора, ну, зачем вы его, зачем вы задаете такие вопросы? То есть, я не понимаю. И здесь очень важно, смотри, Саша, здесь интеркомпания, это, опять же, два трека, инженерный и менеджерский, и зачастую позиция архитектора — это позиция, которая идет после, не знаю, став инженера, либо принципа инженера, и иногда это просто, ну, такая, ну, название лычки, что по факту у тебя будет некоторые обязанности, где нужно будет смотреть за архитектурой, но там также будут обязанности по написанию кода. Ну, опять же, я просто говорю, что такое может быть. Ну, меня больше поразило даже не это, то есть, нет, я совершенно не против временами написать код, я даже обеими руками за. Вот, ну, то есть, какие вопросы я ожидаю на позицию архитектора? Вот, я в компанию, еще архитектора, как бы, на что я буду смотреть? Ну, безусловно, да, там будет вопрос уровня, как бы выстроили Twitter, но это не самое главное, то есть, меня, то есть, эта роль, она предполагает общение с другими людьми, достаточно много общения, то есть, я буду пытаться узнать, как человек ведет себя в каких-то конфликтных ситуациях, конфликтных в не в смысле, когда там люди встают и на кулаках дерутся, а как он разрешает ситуацию, когда люди не согласны с друг другом, как он там аргументирует свою точку зрения и так далее, то есть, какие-то софт-скелы, не знаю, у меня был так длинный список вопросов, потому что я старался все-таки рекрусером дать достаточно развернутый фидбэк, опять же, как он будет формулировать задачи для, может быть, это не очень архитектурская тема на самом деле, но, опять же, люди иногда не понимают, что если ты общаешься с начинающими разработчиками, то им полезно дать задачу, возьми такой фреймворк, такую базу данных и сделай вот так, и у тебя будет результат. И начинающие разработчики, они рады следовать твоим инструкциям, потому что они получают результаты, они довольны, но это совершенно не работает со сеньорами, потому что, на секунду, чипы это эксперты в своей области, и они лучше знают, как нужно, если ты им говоришь, какие базы данных используют и так далее, то обе демотивируют, поэтому с ними разговор совершенно другой, ты приходишь и говоришь, ребята, если вот такая проблема, решите, вы эксперты, вы лучше знаете как, и от архитектора я ожидаю, что он все-таки должен это понимать, и много-много других тонкостей. Кстати, мне кажется, что то, что ты сказал про сеньоров, это с одной стороны правильно, конечно, с другой стороны, это должно определяться, наверное, не тайтлом и даже, наверное, не опытом, а скорее правильным положением человека в позиции, то есть, если вспоминать преслабутый ситуацион лидершип, то вне зависимости от того, какой человек тайтл, если он уже, в принципе, хорошо понимает и хорошо замотивирован в том, что он делает, то он примет решение лучше, чем ты, а если человек даже хорошо понимает, но плохо замотивирован, то что он делает, то он не сможет предать нормальное решение без какой-то помощи, и понимание вот этого, это на самом деле тоже большой момент в плане работы архитектором или менеджером, чем-то еще в этом плане. Я, кстати, хотел немножко вернуться к изначально вопросу Олега, я хотел немножко другу дополнить то, что Олег сказал, что с одной стороны, происходит комодизация каких-то подходов, то есть, действительно, если раньше, то есть, я даже так за другой, еще дальше зайду, есть такой ток у Хилля Вейна, мы про него несколько раз вспоминали в контексте TLA+, вот у него есть такой ток, What can we learn from software history, где он, кроме прочего, разбирается, то вот откуда пошел общий вопрос, например, про реверс, связанный с писком, и что ему удалось, по классным признакам, его трассировать во времена, когда людям приходилось memory management руками писать, носить, и это реально был вопрос из разряда, ну, как бы, вы вообще можете вашу работу делать буквально на месте, и тогда он как бы не был готов к библиотеке, не было ничего, и это спрашивали, потому что, ну, блин, человеку это придется делать на работе, а потом достал один из тех вопросов, которые, типа, ну, знаете ли, это вообще ваш компьютерсайенс, то же самое, там, типа, с красночерными деревьями, то есть, ну, не знаю, я, мне кажется, не смогу ответить на этот вопрос, у меня придется куда-то лезть, потому что, ну, не приходится сейчас уже ничего этого обычно писать вручную, потому что все есть в библиотеках, и по мере коммодизации у нас что-то уходит из каких-то подходов, которые нужно регулярно писать руками, в библиотеке, в какие-то готовые решения, если даже это не написано за вас, вы точно знаете где посмотреть, как написать, но, однако, у нас одновременно с этим, у нас scope систем, которую мы строим, он расширяется, то есть, если раньше, когда мы писали memory management носи, у нас, нам было достаточно сложно построить систему, которая работала на одном компьютере, мы не говорили о системах, которые, как одно целое, работают на масштабах типа планеты",
    "result": {
      "error": "API request failed: Error code: 429 - {'error': {'message': 'Request too large for gpt-4.1 in organization org-xSG9D345gtYlIKC330wCyrEG on tokens per min (TPM): Limit 30000, Requested 34090. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
      "stack_trace": "Traceback (most recent call last):\n  File \"/home/andrei/Projects/podcast-shownotes/scripts/build_search_eval_dataset.py\", line 157, in generate_search_query\n    response = await self.client.chat.completions.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 1927, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1767, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1461, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1547, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1547, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1562, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Request too large for gpt-4.1 in organization org-xSG9D345gtYlIKC330wCyrEG on tokens per min (TPM): Limit 30000, Requested 34090. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
    }
  }
]