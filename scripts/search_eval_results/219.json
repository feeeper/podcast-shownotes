[
  {
    "segment_id": "ebc9d33b-76a6-4de7-8e18-ed0ddafa100c",
    "episode_id": "3958bf0e-77a9-468d-8ec3-31a57e6eef52",
    "episode_number": 219,
    "segment_number": 3,
    "text": "Вот это основная идея, которая в Quake получила свой развитие, до того, что эта же штука идёт в первом пакете вместе с данными. И если сервер ещё помнит такое соединение, каждый раз, когда я говорю «соединение», я пишу в кавычках, потому что UDP, но с точки зрения Quake, это вполне соединение, то они сразу продолжают обмениваться данными. Вот, если нет, то сервер ему говорит, что давай-ка пришли мне полный пакет, и в следующем пакете идёт там всё, что нужно, и опять тоже начинаются сразу данные идти. Хотя, конечно, их там уже поменьше влезет в первый пакет. Я тем временем посмотрел немного логи на своём блоге и увидел, что там чуть ли не каждый второй запрос идёт с HTTP 2, и поэтому мне очень интересно было бы послушать про неоспоримые недостатки его реализации в Nginx, про которые было сказано. В Nginx-е? Ну, в Nginx-е теперь всё хорошо, кроме одного открытого ишью, который закрыли, потому что это не баг, но потом всё-таки открыли как enhancement. Вы знаете, что на практике HTTP 2 всегда идёт по ТВСу. Это связано там несколько есть на этой причине, потому что это безопаснее, потому что это privacy, потому что security, но основная причина, потому что это независимое HTTP 2, плейнтекст HTTP 2 будет ломать слишком большое количество роутеров посередине, которые пытаются делать инспекцию пакетов, видят какую-то фигню, вот эти как начинается HTTP 1.1, а потом что-то непонятное, какой-то парит глагол, какие-то там option фреймы пошли, вообще непонятное, и ломаются. Если ты всё своё окружение контролируешь, если у тебя нет такой проблемы, ты можешь захотеть использовать HTTP 2 per TCP без ТВСа. Ну, там, не знаю, если опять же это локальные сети, ты не думаешь, что тебя кто-нибудь взломает, начнёт слушать трафик, что, конечно же, не так, но вдруг, то ты можешь захотеть HTTP 2 per TCP, потому что это немножко быстрее, потому что нет ханчайка и так далее. Соответственно, HTTP 2 работает при помощи, там есть два механизма, один — pre-org knowledge, когда ты просто посылаешь открытый порт HTTP 2 запроса сразу. Второй вариант — это когда делается HTTP 1.1. Сначала запросы, потом через механизм upgrade он обрисовывается на HTTP 2. Ну, в случае с ТЛСом, третий вариант, самый распространённый, это когда именно в заголовке ТЛС идёт, что следующий протокол будет HTTP 2. И вот первые два варианта, они в Nginx не реализованы. То есть нельзя повесить... точнее, нет, первый реализован, то есть если ты посылаешь запрос на порт, он говорит, окей, я знаю, что это HTTP 2, всё хорошо, но ты не можешь туда посылать HTTP 1.1. А второй вариант, когда у тебя запрос посылается 1.1, а потом ты хочешь пробледить на 2.0, ну, на HTTP 2, такой сейчас не реализован. То есть, другими словами, нельзя сейчас на Nginx сделать на одном порту без ТЛСа HTTP 1.1 и HTTP 2. И это проблема. Стоп-стоп-стоп-стоп-стоп-стоп-стоп. Ещё раз, в Nginx нельзя сделать на одном порту, чтобы был HTTP 1.1 и HTTP 2, я правильно всё понял? Без ТЛС. Без ТЛС. А, ну, окей, fair enough, потому что нефиг. Ну, вот, да, то есть если у тебя интернет, если у тебя блок, то, конечно, у тебя ТЛС, в том числе ещё одна причина иметь ТЛС. Но у нас мы в это упирались больше, чем сейчас. И мы приняли решение, что мы делаем ТЛС везде. Несмотря на то, что у нас там локальная сеть и всё такое, но чисто из-за этой особенности Nginx нам проще сделать ТЛС везде, чем как-то пытаться её обойти, потому что нам нужно ехать 1.1 и 2.0 на одном порту. Ну, и по-другому, сейчас есть, наверное, HTTP 3, да, что вообще такое 3, и зачем они решили делать ещё 3 сразу после 2? На самом деле разница здесь уже сильно-сильно меньше, чем было отличие от HTTP 1. То есть, во-первых, семантика всё та же самая сохраняется, те же самые рулы, те же самые глаголы, те же самые заголовки, когтела. То есть там, не знаю, HTTP кода, всё точно такое же. Бинарный протокол, как в HTTP 2. Что изменилось? Во-первых, механизм сжатия заголовка изменился, то есть как у нас был HPACK в HTTP 2, теперь в HTTP 3 будет QPACK. На самом деле, эта штука очень пунктуальна, но она адаптирована для того, что у нас будет UDP. То есть мы не просто не посылаем одни и те же заголовки, но если раньше мы это могли делать уверенно, потому что HTTP-соединение было установлено, теперь нам нужно немного добавлять информацию, чтобы знать, что если у нас UDP пакет уйдёт, а предыдущий ещё не был принят, нужно эту ситуацию разурлить. Ну и второе изменение, то, что мы там в чате обсуждали, то что по сути, квик это такой протокол, который очень сильно разрывает границы вот этих вот моделей AC, моделей TCP, IP и так далее.",
    "result": {
      "error": "API request failed: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-xSG9D345gtYlIKC330wCyrEG on tokens per min (TPM): Limit 30000, Used 30000, Requested 2284. Please try again in 4.568s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
      "stack_trace": "Traceback (most recent call last):\n  File \"/home/andrei/Projects/podcast-shownotes/scripts/build_search_eval_dataset.py\", line 157, in generate_search_query\n    response = await self.client.chat.completions.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 1927, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1767, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1461, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1547, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1547, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1562, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-xSG9D345gtYlIKC330wCyrEG on tokens per min (TPM): Limit 30000, Used 30000, Requested 2284. Please try again in 4.568s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
    }
  }
]