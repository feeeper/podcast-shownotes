[
  {
    "segment_id": "90da88d8-a0c0-4d7e-85cc-741aec6952ea",
    "episode_id": "e0fe5fb6-1b4d-484a-9e0a-d5ec1a8623dc",
    "episode_number": 360,
    "segment_number": 7,
    "text": "И, кстати, в show notes вы найдёте ссылки на два пейпера. Первый про C-Store 2005 года оригинальный, а второй он от 2012 года называется вертик 7 лет спустя. Ну, вертика 2.7 C-Store 7 лет спустя. Почему мне кажется важным соус на C-Paper? Потому что вот все современные аналитические системы, они в том или ином виде унаследовали дизайн C-Store. То есть, ну, например, я услышал доклад, вот как бы вот что в вертике как используется, я такой, ну, получается, что это как клик хаус, руку закрыта, правильно? Вот, ну и оно более-менее так получается. После вводной про вертику и что она собой представляет, докладчик говорит немножко про то, как они хранят данные на диске. У них есть два основных типа файлов. Первый это PIDX файлы, а второй это FDB файлы. Первый это своего рода мета информация о данных или, можно сказать, индекс, но индекс плохое слово, потому что оно имеет клееря... короче, у него двоякая трактовка в мире с УБД, назовем это мета информацией. А второй это непосредственно данные. PIDX файл, он хранит информацию о блоках, их размерах до сжатия, после сжатия, значение min-max, потому что, напомню, это, поскольку речь про колоночное хранилище, то один FDB файл с данными, он хранит значение в колонке, поэтому в мета информации мы можем хранить min-max. Смещение блоков в файле с данными, его контрольная сумма. То есть это такая вот мета информация о блоках в основном файле. Основной файл хранит непосредственно блоки, пожатые. Блоки в распакованном виде имеют размер 64 килобайта и сжимаются Delphi кодированием, и вот этим вот всем. Чтение происходит целыми блоками, и вы договоритесь, это прямо не говорится, но я подозреваю, у них там есть какой-то кэшинг для этих блоков, к которым часто обращение происходит. Сообщается, что файл с мета информации имеет объем меньше одной тысячи от объема данных. Дальше всплывает такое понятие, как body проекция. Речь идет вот о чем. Представьте себе, у вас есть кортежи, которые вы нарезали на колонке. Кортежи должны быть как-то ассортированы. Но в смысле они хранятся в ассортированном порядке, притом вам не обязательно только... То есть представьте, что мы рассматриваем кортеж целиком. Мы сказали, что мы его ассортируем только по вот этому. У нас будет первичный ключ, мы по нему сортируем, или вот там по вот этим трем полям. Притом там первый по возрастающим, второй по убывающим. Задали какой-то правил ассортировки, и наши данные на диске в колонках будут по этому правилу ассортированы. То есть получается, что у тебя соответствующие строки в разных файлах колонок будут ассортированы не независимо. Я не слишком запутан на это объясню. Последнее предложение. Повтори. В соответствующие сроки будут по-разному сортированы. В разном порядке быть в разных файлах. Ну смысл в общем. У тебя есть файл колонки, есть файл соседней колонки. Они хранятся не так, что ты вот отдельно ассортировал, а потом вторую ассортировал. И они каждый ассортированы по своему значению. Они ассортированы по правилам, которые ты задал для кортежа целиком. Угу. Я понял. Мне казалось, что какие-то стори делают не так. Мне казалось, что какие-то стори могут как угодно сортировать, но видимо здесь не так. Здесь видимо не так. Здесь не так. А смещение внутри этих файлов, где хранится? Оно хранится в PIDX файлах, о которых я говорил две минуты назад. Это вот файл мета-информации. Да, вопрос. Не, я просто повторяю. Получается в мета-информации у нас хранится, где соответствующее смещение внутри файла, а потом каждый файл это отдельная колонка, данные для данной колонки внутри кортежа. Да, все понятно. Что возвращает нас к вопросу, что такое Body Проекция? Оказывается, ты можешь... То есть, твоя Проекция — это прям вот такая сущность в вертике, и ты можешь делать Create Projection, Order By, вот это вот все прям запросом. И ты можешь... Вот у тебя есть данные, ты можешь их ассортировать по одним полям, а потом эти же данные ассортировать по другим полям. И вот тот факт, что у тебя есть копия данных, но ассортированная разным образом, вот это они называют то, что у тебя есть одна Проекция и вторая Проекция, и они Buddies, понимаете, типа друзьяшки. В них одни и те же данные, просто по-разному ассортированы. Почему это важно? Потому что, как я понял с доклада, потому что докладчик как-то очень быстро перескочил на следующую тему, у них есть фича, называется K-Safety. И в случае, если у тебя недоступно... То есть, у тебя пришел запрос, ты глядя на этот запрос, понимаешь, что тебе нужно использовать вот какую-то конкретную Проекцию, потому что там выгодно отсортированы ключи, ты можешь эффективно исполнить запрос. Но оказывается, что у тебя эта нода недоступна и прилегла, и сеть порвалась. Вот в этих сценариях фича под названием K-Safety позволяет тебе обратиться к Buddie Проекции, в которой данная ассортированная может быть не очень выгодна для этого конкретного запроса, но они доступны. И вместо того, чтобы сказать, что этот запрос вообще не будет исполнен, ты его исполнишь просто медленно. Такая вот у них интересная фича. И мало того, что медленно, ты еще и в оффлайне это можешь сделать в теории. Ну в смысле, если у тебя она распределенная базой, или мы про это не говорили пока, то есть все данные хренятся на всех нодах? В смысле, в случае с Network Split, я вот это имел в виду. В случае Network Split, я не знаю как в вертике, но у тебя обычно попадают запросы или перевыполняется под дерево. Это короче, это домен, сильно отличающийся от хранения. Да, но если у тебя все эти данные уже есть локально в этом Buddie. Ну локально относительно чего? Относительно того, что у тебя storage. У тебя storage обычно, если это Buddie для storage, они будут там же, где типа, ну то есть, если у тебя есть нода с каким-то пусочек Relation, то Buddie этого кусочка будет на той же нодке. Ну то есть, Relation обычно наливаешь, помнишь, мы в прошлый выпуск обсуждали джойны. Вот как бы ты его наливаешь, ты его скорее всего стараешь налить или равномерно, или везде, если он маленький. Ну вот, кстати, что касается распределенной части в всяких фейловерах и так далее, эта часть в докладе не очень сильно освещена, поэтому вот, я не готов комментировать насколько то, что вы говорите, соответствует, действительно, или нет. И вот, повторюсь, вот этот момент про key safety, он вот буквально две минуты уделяет, как с носки можно сказать. Так вот, в какой-то, то есть, была у них такая фича, ну в какой-то, и она описана в пейвере, но в какой-то момент они выяснили, что вот эта идея с Buddie проекциями, когда ты хранишь койп, данные и их копию ассортированы по-разному, это очень хреновая идея. Оказывается, что в больших распределенных системах, типа вертики, достаточно завидной регулярностью возникает вопрос рекавери, когда у тебя либо диски умирают, и тебе нужны вот рекаверить эту конкретную ноду, либо у тебя нода полежала, ее нужно поднять, налить с реплик, возможно, повторюсь, они не очень углубляются, протоку у них распределенно работает, но так или иначе у тебя возникает рекаверия. И факт существования Buddie проекции означает, что тебе нужно одни и те же данные много раз пересортировать в рекаверисценарии. Чтобы митигировать эту проблему и сортировать меньше данных при рекавере, они придумали штуку под названием Data Target Proxy или DT Proxy. И вот дальше как-то докладчик, либо я глупенький, либо докладчик не очень хорошо объясняет, он говорит, что идея в том, что данные хранятся в одном месте, имеется ввиду вот эти PIDX и FDB файлы, они хранятся в одной ноде, а вот то, что раньше было Buddie проекции, которая по-другому эти данные сортирует, это реализовано через стриминг-репликацию, как я понял, стриминг-репликацию на вот этот Data Target Proxy, который, собственно, вот в них... То есть я был неправ, то есть все-таки, когда я, Ваня, сказал, что Buddie хранится там же, где данные, я был неправ. Я так подозреваю у тебя то, где физически расположен Data Target Proxy, он не обязательно на другой машине. То есть это отдельный процесс, но где он живет, об этом история умалчивает. Поэтому Ваня может быть как прав, так и не прав. Хотя показалось бы достаточно тупо стриминг-реплицировать данные с машином на нее же, так что скорее всего это другая физическая машина. Так вот, они говорят, что мы берем наши основные данные, наши основные FDB файлы и реплицируем их на вот эту ноду, которая должна их отсортировать как-то иначе, и тут я нифига не понял, а как это, собственно, помогает ускорять рекаврию. И после этого докладчик как-то переходит к другой теме. То есть где-то тут вот произошло небольшое лукавство, или опять я глупенький и не понимаю, что у меня прямым текстом говорят. То есть за счет чего выигрыш при рекаврии, почему данных внезапно нужно сортировать меньше, я не совсем понял. Кроме, может быть, того, что они распределили эту задачу, то что теперь у тебя на каждую проекцию по отдельной DTProxy, которая в параллель занимается этой фигней, которую ты раньше занимался на одной машине, может быть, но это не точно. Дальше мы рассказываем про то, как они клево умеют сортировать быстро строки. Для сортировки строк речь идет вот о чем, что помимо проекции у тебя еще бывают order by и group by запросы, для которых ты не построил проекцию. То есть ты выгреб какие-то данные. Допустим, у тебя результат запроса, он влез в память, да он большой, да его там 500 метров, но он влез в память. А теперь тебе вот то, что влез в память, нужно сделать order by, group by, вот такая вот задачка. Оказалось, что они себя сравнивали с другой системой, она называется Tencent. Честно говоря, я не знаю, что это за система, возможно где-то всплывало у нас в подкастах, но я забыл. По-видимому тоже какая-то аналитическая система. И оказалось, что Tencent работает быстрее на таких запросах, чем Vertica, для них это была большая проблема. Кто-нибудь знает, что такое Tencent? Звучит, что это китайская компания, которая всеми владеет, а не как база данных, поэтому я без понятия, если честно. Окей. Что-то знакомое, но я тоже не помню деталей. Так вот, и они придумали… Оказалось, что вот эти запросы медленнее работают в основном на строках, когда тебе нужно сделать order by для строчек, и для ускорения этого кейса они придумали… Вот какой алгоритм. Они называют EE5, точнее EE5-Encoding. Идея заключается в том, что они отображают оске строку в int64. Как они это делают? Они берут бинарное представление строк и запис… То есть это будет немного сложно сейчас словами описать. В докладе это было хорошо объяснено, мне просто сейчас будет сложно без картинок пересказать, но я попробую. Представьте, что вы берете строка из четырех символов, вы копируете эти символы как есть в будущей int64, а в конце добавляете пятый символ с длиной строки. Если строка была ABC, то у вас получается 5 байт. Это байт A, байт B, байт C, потом байт 5. Это длина строки. А потом все забит нулями. И оказывается, что вот этот их алгоритм, он позволяет отличать null от нинала, например, от пустой строки. И при этом int64 оказывается отсортированным в том же порядке, в котором отсортированы строки. Я не то чтобы много вот над этим думал, у меня есть подозрение, что в каких-то ограниченных случаях сортировка это будет немножко странной. То есть, например, у тебя бывают локали, у тебя бывает Unicode. Про то, что они делают со строками длиннее 7 байт, история вообще умолчивает. Ну, то есть моя интерпретация, что эта оптимизация работает только для оских строк и только имеющих длину не более 7 байт. Но это не точно. Также они поддерживают сортировку в обратном порядке, типа desk. Для этого нужно просто взять биты в int64 и перевернуть их. И тогда можно будет использовать тот же алгоритм сортировать строки в обратном порядке. Когда они запилили эту балалайку, то их order-by-запросы стали в два раза быстрее того, что того, когда это реализовано в Tencent, в предположении, что там что-то похоже на STRS-CMP. Напомню, а до этой оптимизации они были медленнее, чем Tencent. Также эта штука ускорила group-by-запросы, но как побочный эффект, их в основном интересовали order-by. Вот такая потрясающая история. Дальше докладчик говорит о том, что они раньше были не cloud-native, а теперь они cloud-native. Что под этим понимается, у их пользователей есть разные кейсы. Например, один из кейсов, что у тебя нагрузка почему-то на аналитическую систему зависит от времени суток, хотя, по моим представлениям, для аналитики это не вполне типично. И они хотят вот этот часть кластера, которая отвечает за вычисление, которое как бы по цепу нагружено. Вот эта часть должна ширинкаться и наоборот вебскелиться, а сами данные должны храняться в shared storage местного облачного провайдера. Это первый кейс, они так умеют. Второй это так называемый long-tail кейс, когда у тебя очень много данных, но в основном ты работаешь с горячими данными, а холодные данные тебе нужно просто хранить и временами окружать, но очень редко.",
    "result": {
      "query": "Vertica storage architecture"
    }
  }
]