[
  {
    "segment_id": "18af5878-765e-4693-a079-36ea68fe33e7",
    "episode_id": "07e8047d-e5fa-4b66-9586-575fb0b2630c",
    "episode_number": 351,
    "segment_number": 4,
    "text": "Мне кажется, это... Никто этого не предлагал, вроде. Ну, я не знаю, просто ты в какой-то момент так сформулировал, что я в эту сторону начал думать. Нет, я имел в виду, что надо мыслить шире и открыто, более открыто. Ну да, в целом, да, с thinking outside of the box, вот это все. Кто еще умеет мыслить вне коробки, это Брэндон Грэг, и в своем блоге он рассказывает небольшую кулсторию из далекого 2018 года. Вот, на тем не менее, кулстория клевая, возможно вы ее слышали в каком-нибудь из его докладов, поэтому, сорянчик, но постарайтесь быть краток. Кулстория о том, что на одном из них, из их сервисов, Netflix, надо думать, возникла проблема с производительностью после очередного апдейта, насколько я помню. Я по памяти рассказываю статью. И его попросили посмотреть, и Брэндон там зашел на сервер, пошуршал доступными тулами, и там статьи все это более подробно описаны, и, ага, и нашел проблему. Tupit, ZFS. Вот, и он пришел к разработчикам, которые просили его помощи, и говорит то, что ну вот у вас проблемы с ZFS. Я бы, если честно, хотел бы узнать, как он обнаружил, что это именно ZFS Tupit. Ну, открой статью там про... Не помнишь детали? Ну, там какие-то названия туллов, там стэктрейсы, которые, ну в смысле, там сказано, что они используют Atlas, с помощью которого ты можешь собрать стэктрейсы всех машин, в смысле, флайн графы. По флайн графам ты видишь, что у них была деградация производительности на там 30 с чем-то процентов, и ты видишь, что примерно 30 процентов времени CPU съедается в едре по такому-то стэктрейсу, и ты понимаешь, что это относится к ZFS. Вот. Он говорит, ребята, я вижу, что у вас проблемы с ZFS, а что вы с ним делаете? И вот с этого места начинается самая веселая, потому что ребята говорят, а мы не используем ZFS? Он такой, ну как так? Я вот, я смотрю во флайн графы, у вас там 35 процентов времени тратится в ZFS, и как вы его, блин, не используете? Вот так мы его вообще никогда не использовали, в принципе. Вот. Он решил рандом в этом убедиться. Есть в Catalog Proc, где модулю ядра отдают разную прикольную статистику, специальный файл, по которому можно посмотреть, как часто было обращение к ZFS, и называется ARC STATS. Точнее, ARC STATS — это статистика по ARC, расшифровывается как Adaptive Replacement Cache, и по сути это cache ZFS, который в памяти. И обратившись к... Ну, он только в памяти, там в этом и плюшка, но да. И обратившись к специальному файлуку в Catalog Proc, по этой статистике было видно, что действительно ZFS никто никогда не работал вообще, а временно цепь уже рется. Вот. В итоге анализ кода ZFS, насколько я упоминаю, показал, что там действительно в ядре накачали Update, и произошло следующее, что у тебя, когда система отъедает определенное количество памяти, ZFS это видит и пытается вот эту память, выделенную под cache file в системе, в ARC освободить. И суть изменений в ядре заключалась в том, что, короче, у тебя вот есть список страниц, тебе нужно из них выбрать кандидата, кого освободить, выгрузить. Эта страница выбирается случайным образом и освобождается. Вот. Есть две проблемы. Первая. Почему-то разработчики ZFS решили, что функция, которая вот эту случайную страницу выбирает, она должна быть криптостойкой. Поэтому там, типа, SHA считался. Вторая проблема, что вот эта случайная страница, она выбирается независимо от размера списка массива этих страниц, в том числе, когда размер этого массива 0. То есть происходило так. У тебя размер кыша 0, ты видишь, что система отъедает много памяти, ты считаешь криптофункции, которые тебе выбирают рандомную страницу из нуля, выбираешь никакую страницу, понимаешь, что у тебя больше не встало и продолжаешь считать хэши, выгружая страницу из пустого кыша.",
    "result": {
      "query": "Brendan Gregg ZFS ARC bug"
    }
  }
]