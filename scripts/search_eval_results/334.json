[
  {
    "segment_id": "3b7a8bfd-4221-40e7-92d0-eec3d70917e4",
    "episode_id": "3cce3396-bb23-4d46-8b1c-cf87df3cb4df",
    "episode_number": 334,
    "segment_number": 4,
    "text": "Это тоже и достоинство, и недостаток. Достоинство то, что система является гибкой и одновременно понимаемой. То есть можно удобно представлять, какая у нас конфигурация кластера, и особенно это важно, если у нас шардирование согласно каким-то правилам нашей предметной области. Это важно, когда у вас уже есть какая-то система, которая работает, и там своя схема шардирования. Ну, у вас рекламная сеть, и там, может быть, у вас вручную был шардированный MySQL, и еще вручную какие-нибудь шардированные фиговинки, куда какие-нибудь данные подкладываются. Вы хотите еще добавить ClickHouse, и допустим, у вас правило шардирования вообще лежит в метабазе, где ручное отображение клиентов на шарды. И вот если вы используете ClickHouse, то это вообще не проблема. Любая схема шардирования, пожалуйста. А если вы, например, используете, ну скажем, Кассандру, данные будут как-то сами куда-то там переезжать, и вы ничего про это не будете знать. Зато меньше настраивать. И вот мы хотим, чтобы, по крайней мере, было доступно в качестве опции, чтобы тоже можно было как можно меньше беспокоиться о шардировании данных, о тонкой настройке, если достаточно автоматического решения. Но сохранить при этом и возможности гибко все делать. Наброс номер два. Можно быстренько это руководить? Да, конечно. Прости, что я перебил. Если я ошибаюсь, вы некоторое время назад занесли Nu-Raft. Верно ли мое понимание, что это чтобы не нужно было зуокиперы настраивать и насколько вы все еще от него зависите? Да. Сейчас это решение на финальной стадии разработки. Оно еще не рекомендуется для продакшена, но оно уже проходит все тесты в джепсоне, и в общем-то все, что нужно, работает. Что интересно, это, во-первых, какая мотивация? Мотивация в том, что людям не нравится то, что есть зуокипер. Почему людям не нравится зуокипер, есть несколько причин. Одна из причин далеко не главная, но некоторым людям просто не нравится зуокипер, как бы это сказать. Потому что он не нужен вообще-то. Но он операционно специфичный, давайте так. Да. Ну то, что это отдельный компонент на джаве, другая технология, отдельно настраивать. Вообще зуокипер хорошая система, она работает корректно, это самое главное. Есть некоторые недостатки, в том числе то, что можно сказать как баги, но это не совсем баги. Это такие мелочи, особенности, которые не могут исправить. То, что, скажем, счетчик транзакции, это знаковый 32-битный Инд, и если транзакций много, он переполняется, и это приводит к перезапуску зуокипера, недоступность там в течение некоторого времени. У некоторых клиентов это несколько раз за день. А происходит и исправить это невозможно в принципе. Или, например, то, что размер пакета максимально должен сдаваться заранее по умолчанию 1 мегабайт, и это довольно много, но очень легко совершенно случайно его превысить, и в этом случае никак не будет работать, если только не увеличить вручную. А чтобы увеличить вручную, надо думать, насколько. Тоже неудобно. Или, например, то, что у зуокипера неподдерживаемая и важная сишная библиотека, зуокипер Си. У нее есть проблемы. Например, если при отправке мультитранзакции экспарилась сессия с зуокипером, то в библиотеке просто тегфолд. Библиотеку эту мы переписали давно. То есть клиентскую часть мы просто не использовали, у нас своя библиотека, и мы постарались сделать ее довольно абстрактной, чтобы вместо зуокипера можно было поставить другую систему. То есть эта библиотека, внутренняя зуокиперная модель данных, все зуокиперные, те же запросы зуокиперные. Первое, что мы сделали, это сделали МОК к зуокиперу под названием TestKeeper, который реализует ту же самую модель данных, но в пределах одного сервера, того же самого оперативки, данные больше нигде не хранятся, что подходит для тестов. Мы стали тестировать одновременно и с тесткипером, и с обычным зуокипером, но добились, что все одинаково, все работает, и продолжили это использовать для тестов. А дальше возникла очень заманчивая возможность. Можно ли взять и действительно вместо зуокипера подставить что-то другое? Был проект по тому, чтобы под этим интерфейсом реализовать ITCD, сделать поддержку ITCD. Ну тоже система координации, которая тоже работает, проходит тесты Джепсона, все прекрасно. Там правда другая модель данных. Если говорить про модель данных и протокол зуокипера, то можно отметить следующие важные вещи. Это вотчи, эфемерные ноды, мульти-транзакции по разным ключам. Ну и что-то там еще было, я уже не помню. Ну так, я, наверное, поясню, что зуокипер, то есть многие конфигурационные сторожи, то же самое ITCD, он по принципу ключа значения работает. У зуокипера у него именно модель данных с такой древовидной структурой с, как я знаю, с реестром в Windows. Наверное, в этом самое большое различие. Вотчи, если я правильно помню, в зуокипере можно подписаться на какую-то ветку и получать апдейты по всему, что происходит. В ITCD по-моему есть какие-то вотчи, но они работают по-другому, чем в зуокипере. Да, такие мелкие отличия. Ну в зуокипере на самом деле вотчи они не рекурсивные, то есть только на саму ноду, плюс на появление детей. Но в ITCD мы столкнулись с особенностями, например, то, что нет возможности автомарно создать ноду и навесить на нее вотчи. Просто навесить вотчи и создать ноду – это отдельная операция. Зато можно создать вотчи на несуществующую ноду. Так что мы решили, что мы сначала создаем вотчи, потом создаем ноду, а потом игнорируем все события, которые пришли до того, как мы создали ноду. В общем, примерно на этом моменте мы эту разработку прекратили и решили сделать кое-что другое. И кое-что другое состоит из нескольких шагов. Первый шаг – это сделать дроп-ин замену зуокипера. Представьте, тот же самый зуокипер работает так же, как зуокипер. Разве что другой формат логов и снапшотов, но точно такой же клиенсерный протокол, то есть зуокипер, но лучше. Довольно спорно, потому что этот шаг нам почти ничего не дает. Не нам, не клиентам. Но все-таки кое-что дает. Дает, во-первых, то, что не нужно устанавливать отдельный софт, отдельным каким-то образом. Просто тот же клик-хаус, у него режимы работы как зуокипер. Все есть. И еще возможность исправить несколько мелких недостатков. Это переполнение за xid, несколько уменьшить потребление оперативки. Что там еще было? Вот эта проблема с размером пакета. Но это все такие довольно мелочи, но довольно застаревшие такие проблемы исправят. Но это только первый шаг. Следующий шаг это то, что можно будет эту штуку иметь встроенной в сам клик-хаус сервер, который хранит данные, обслуживает запросы. Это тоже сопряжено с несколькими рисками, потому что зуокипер ставят на отдельные серверы не просто так, а из-за того, что он чувствительный на клетенсе дискового ввода-вывода. Если его ставят на те же машины, то будут проблемы. Тем не менее мы думаем, что вот наличие такой штуки позволит нам улучшить тестируемость этой конфигурации. И в конце концов позволит добиться того, что это будет работать вместе. И конечно следующий шаг, гипотетический, который можно было бы рассматривать это наличие кастомного протокола. Потому что делать на основе рафта модель данных зуокипера, а потом поверх этой модели данных зуокипера делать координацию, это довольно такой кривой способ. Но это нормально с учетом того, что это позволит нам мигрировать. Если мы просто попытаемся сделать на рафте кастомный протокол для всего, то у нас возникнет две проблемы. Первая проблема это то, что сложно мигрировать, а вторая проблема это то, что не очень удобно версионирование всяких команд. В общем всего, от чего поддерживают лог в рафте. Там откатывать неудобно, это отдельная головная боль. Быстрый вопрос завершающий про нур авто. А то, что его сейчас нельзя включить на Макаси и RebiSD, это ограничение самого нура авто или просто никто не тестировал? Уже можно, уже сделали, что можно включить и это просто потому что принцип такой. Подключаем библиотеку, там описываются файлы сборки. Ну и мы их описываем и проверяем на своей машине, все добавляем. И на альтернативных системах просто некоторые библиотеки сразу отключаются, чтобы не мучиться. Валер, моя серия вопросов про нура авто завершена. Хорошо, следующий вопрос. Все довольно плохо с вложенными структурами данных. Есть только массивы, не джейсонов, не какой-то мэп, как структура данных в клик-аузе сейчас нет? Не сколько-либо маленькой прямоугольника на плоскости с точечкой. И как геопользоваться? На самом деле есть. Сейчас расскажу подробнее. Значит про вложенную структуру данных. Во-первых массивы, массивы произвольной вложенности. Записываем, есть. В качестве вложенного типа данных, любой тип данных, то есть тоже там массивы, строки, числа, даты с временем. Дальше картежи тоже есть. Массивы картежей картежев массивов, пожалуйста. Дальше именованные картежи, которые как структуры получаются. И еще такая штука, nested type. Что такое nested type? Представьте у вас какая-то структура таблиц, которые вам надо часто джоинть. А теперь представьте, что вы хотите хранить их так, как будто они уже поджоинены. Скажем есть покупки в интернет-магазине, а в каждой покупке есть корзина с товарами. И вот все это можно в одной таблице хранить. Это будет лог покупок у нас. И в каждой строчке с логом покупок nested структура данных с товарами. Она представляет массив именованных картежей. Грубо говоря, как такая маленькая табличка внутри одной строчки другой таблицы. Вот это тоже есть. Это то, что относится к структурированным данным произвольной вложности. Кстати, совсем недавно nested структуры имели максимальную вложность только единицы, сейчас наконец-то уже больше. Но это считается как пока экспериментальное, оно только весной этого года стало доступно. Следующий вопрос, это чего с полуструктурированными данными? Скажем, словарики, мэпы. Недавно тоже появились, теперь есть. Что интересно, это то, что там в этих мэпах для использования просто линейный поиск, потому что, как правило, маленькие такие мэпки, а большие, это... Короче, с большими ничего особо хорошего не выйдет. Что дальше можно рассматривать? Можно рассматривать всякие джейсоны, например. Тут пока ничего особо хорошего нет, но не совсем. Джейсон можно хранить просто в типе данных стринг. Есть функция для работы с джейсонами достать что-нибудь по пути, просто по какому-то полю в объекте. Эти функции на самом деле довольно эффективны, они реализованы с использованием библиотеки SIMDJSON. То есть в принципе неплохо. Недостаток в том, что этот джейсон будет храниться в одном столбце. То есть если нам надо достать оттуда какую-то мелочь, то придется читать этот целый столбец. Как будто у нас строковая база данных, а не столбцовая. И поэтому сейчас у нас разрабатывается средство, чтобы джейсона автоматически раскладывать по столбцам. Это прямо совсем круто. То есть для полустатуруемых данных можно использовать преимущество столбцевой базы данных. Но это еще не готово, поэтому подробно рассказывать не буду. Про геогеографические функции. У нас вот как раз недавно смешивался полуреквест. Теперь можно создавать полигоны, линии и делать над ними всякие штуки. Это уже есть с версии 21.4. Но ведь не индексируется. Я напрасываю если что. Я понимаю, что ClickHouse не база данных для геопоисков. У нас если необходимо индексировать, у нас есть специальный тип словаря, географический словарь. И его реализация строит эффективный индекс по полигонам. И дает возможность находить данные. Ты будешь делать запрос условно по какой-то точке. И к какому-то полигону ты можешь крепить данные. И это будет довольно эффективно. Так как реализация уже тестировалась. И по бенчмаркам там хороший результат. Ты говоришь так скромно, вроде бы уже тестировалась. Я расскажу с другого ключа. Часто бывает так, что в ClickHouse какая-то функциональность реализуется одним каким-то частным случаем. Но вот этот частный случай это ровно то, что нужно для какой-то серьезной задачи в продакшене. Рассмотрим задачу поиска полигона или проверки того, что точка в полигоне. Сначала просто проверка того, что точка находится в полигоне. Для этого есть функция PointInPolygon. Ей можно передать константный или неконстантный полигон. Причем сначала мы реализовали только поддержку константного полигона. То есть точка какая угодно. Это скажем из лога с координатами пользователей мобильных приложений. Они ходят по городу, записываются координаты. И нам нужно по куче координат, проверить их вхождение в один константный полигон. Мы делаем следующее. Мы этот полигон индексируем очень простым алгоритмом. Во-первых, bounding box есть, а во-вторых bounding box разбиваем на сетку размером 256 на 256 или 16 на 16. Я не помню. В каждой ячейке сетки она либо целиком находится в полигоне, либо целиком не находится в полигоне. Либо ее пересекает одна линия и делит на две части. Либо ее пересекает больше одной линии. И во всяких случаях кроме последнего получается очень быстро, а в последнем более-менее быстро. То есть добиваемся, что там на типичных случаях получается 100 миллионов этих лукапов в секунду. Так что вот эта вот маленькая штука, но она сделана, когда нам потребовалось вот конкретную задачу решить, но решить ее хорошо. Следующая. Словарь полигонов. Эта задача называется reverse geocoding. То есть есть точка, а есть статический заранее подготовленный словарь, где скажем эти полигоны всех стран и городов внутри них. И вот этот словарь можно проиндексировать заранее, так что поиск был быстрым. Там получается всего лишь один миллион на одно процессорное ядро, но тоже неплохо. Это весьма впечатляет. Может быть вы и кайден умеете? Ну типа ответ на вопрос, найди мне 100 ближайших старбаксов. А вот тут, к сожалению, не особо. Это такая частая задача в ОЛАПе. Как же так? Очень хотелось бы сделать КНН и есть даже варианты как это делать. Очень интересные. Скажем, можно рассмотреть вариант с неким VP3. Вантажпоинт. Это возможность разбить пространство произвольной размерности, даже неизвестной размерности, задать в этом пространстве некоторую сетку координат. Таким способом, что мы пространство кидаем некоторый набор точек, скажем 10 точек случайных, а потом в качестве координат используем просто расстояние до каждой из этих 10 точек. А затем на этих координатах можно сделать индекс пространственный, типа binary space partition, с помощью, а дальше применять Z-кривую или кривую Гильберта. Это все пока фантазии, а фантазии у нас много. Еще только последний номер, по глупостям, это такое вот, что у вас с оконными функциями? Они есть. Недавно появилась поддержка, как раз в этом году, и сейчас они уже готовы для продакшена. Правда, экспериментальный флаг мы еще не убрали. И эта поддержка вообще честная. Ее делал профессиональный разработчик, который сделал прямо все по стандарту. Кстати, раньше этот разработчик работал в компании Cosgres Pro. Может я его знаю? Это Саша Кузненков. А, да, передавай Саше привет. Что ж, у меня закончился список набросов на клейкауз глупых. Я не знаю, мы в принципе уже час тридцать говорим. У нас, наверное, не знаю, может у Светки или Вани есть какие-нибудь вопросы. У меня иссякли, а Саша вообще хотел молчать. Я пытался, но не получилось. Мне интересно узнать, как вы собираете всякие метрики, телеметрию, и что вы собираете, что для вас важно, что для вас не важно, особенно в ситуациях, когда это находится у какого-то пользователя, и вы не имеете доступа к этой базе данных, и вы не знаете, что конкретно используется. Это супер вопрос, очень важный. То, что касается интроспекции, довольно развито. Для начала, просто маленький пример. Представьте, что в клейкаузе внутри есть встроенный отладчик. Есть системная таблица System.StackTrace. Вы делаете из нее Select, Select звездочка from System Stack Trace. И вам подается Stack Trace всех потоков. Если какой-то запрос долго висит, вы выполняете этот запрос, и вам видно, где он находится. Как это вы собираете? А вот это очень интересно, потому что мы это... сейчас я объясню механизм. Смотрим список потоков в процессе из Pro.cfs. Дальше в каждый поток отправим сигнал. Есть обработчик сигнала, а вот тут уже очень сложно. В обработчик сигнала берется Stack Trace. Почему это очень сложно? Потому что нет. Не существует сейчас... В обычных сценариях нет никакого способа в обработчике сигнала получить Stack Trace и сделать этот сигнал safe. Но для этого мы пропачкали... Прости, пожалуйста. Важный вопрос. Stack Trace чего? Вы же не однопроцессные. У вас же наверняка много трэдов. Stack Trace всех потоков. Всех сразу, я понял. А вы много трэдов, это правильно. Да. Пришлось для этого пропачкать либо Unwind. Есть три разные реализации либо Unwind, и они все не сигнал safe. Это очень большая проблема, мы учились примерно год. Я сейчас про это рассказывать не буду. Но теперь это работает. И это действительно работает, поверьте. Это значит просто одна мелочь про интроспекцию. Но это еще не все. Скажем, на основе того же самого есть сэмплирующий профайлер. Встроенный прямо в сам Click House. И кстати, он включен по умолчанию. Просто с маленькой частотой сэмплирования. Всего лишь один раз в секунду. Но теперь представьте на каком-нибудь кластере в Яндекс.Метрике, например. Там 600 серверов работают. И в течение недели после выкладки, потом мы смотрим, и у нас накопилось просто гигантское количество этих Stack Traces. Которые кстати собираются по двум таймерам. Таймер по CPU и просто таймер по реальному времени. С помощью таймера по CPU мы посмотрим в каких местах тратилась CPU чаще. А с помощью таймера реального времени мы увидим все ожидания, где один поток ждал другого. А еще увидим чтение по сети, чтение с дисков и какие-нибудь банальные слипы, если они есть. И тоже это все пишется в системную таблицу сам же Click House. Потому что Click House он подходит для хранения метрик, телеметрии как бэк-энд. И таким образом мы в Click House используем сам Click House для бэк-энда для телеметрии. И сможем посмотреть, что же у нас на продакшене занимало большую часть времени. Очень удобно. Но это еще не все. Каждый запрос логируется, конечно же. Причем логируется два раза. Когда начал выполняться и когда закончил выполняться. В этот лог запросов записывают еще классные штуки. Эти классные штуки это метрики на каждый запрос. Причем есть метрики наши, которые собираются внутри самого Click House. Например, мы открыли файл, прибавляем единичку. Значит там будет метрика количество раз, когда мы обучаем клик хаусу. А есть метрики, которые получаются из ядра операционной системы. Это, например, количество времени, которое мы провели, выполняя инструкции в user space. Ну просто user time, system time, eo weight есть. Есть данные о том, сколько времени мы провели в игре. И когда мы делали это, то мы получили, что мы делали в игре. Eo weight есть. Есть данные о том, сколько на самом деле данных было прочитано из блокчного устройства. То, что из user space никак не получить. Из user space можно получить сколько мы данных читали из файловой системы в байтах. Но это может читаться из PHP, а операционная система знает сколько реально читалось из диска. И это очень важно. И эти метрики сохраняются на каждый запрос. Клик хаус записывается в системную таплицу, и мы можем агрегировать по пользователям, по времени, по классам запросов. Кстати, функция формализации запросов тоже есть прямо для этой цели, чтобы по классам запросов автоматически все это проверить. Здесь отметим то, что клик хаус как backend очень часто используется для того, чтобы хранить логи запросов в другие системы. То есть логи запросов MySQL и Postgres очень часто сохраняют в клик хаус. Такой инструмент как Tercone Monitoring в качестве backend использует клик хаус. Но клик хаус тоже в качестве backend использует клик хаус. Это очень круто. Так, ну что-то я уже долго говорю, таких возможностей там еще куча. А не возникает проблем с тем фактом, что вы не совсем транзакционный storage? Для того, чтобы хранить данные интроспекции, конечно, не возникает. А в остальном это тоже интересный вопрос. Можно отметить, в каком именно смысле клик хаус поддерживает или не поддерживает транзакционность. Клик хаус не поддерживает полноценные ASET транзакции по стандарту SQL типа begin, commit и все такое. Но там есть некоторые свойства транзакций. Рассмотрим один запрос insert. Если этот запрос вставляет данные, которые попадают в одну партицию одной таблицы, там еще могут быть материализованные представления навешанные. В общем рассмотрим единицу ставки данных в одну партицию одной таблицы. Вот эта единица вставки данных обладает следующими свойствами. Во-первых, атомарность либо целиком вставлена, либо целиком не вставлена. Если целиком не вставлена, будет отправлена exception к клиенту. Клиент от exception либо получит, либо, там, понятное дело, клиент может не получить exception, если у него соединение разорвалось, это важно. Атомарность, дальше изолированность. Это значит, что селекты в то же самое время работают без блокировок, они видят некоторые snapshot таблицы, которые на момент времени, когда этот селект начался. Тоже некоторые свойства транзакционности. Дальше durability. Здесь durability надо рассматривать в распределенной системе довольно аккуратно. То есть можно рассматривать в пределах одного сервера, делаем ли мы, стараемся ли мы, чтобы данные попросить операционную систему записать данные на диск. Там есть настройка. Можно и так и так. А в распределенной системе гораздо важнее, чтобы данные попали на несколько реплик, пусть даже они не будут записаны на физический диск, а будут записаны файловую систему только. Но если они попадают на несколько географически распределенных реплик, это тоже следует считать таким способством durability. Здесь есть тоже специальная настройка, которая позволяет указать на какое количество реплик эти данные должны быть записаны. Получаются буквы A, I, D и C. Consistency. Consistency особенно сложно рассматривать в распределенных системах. Что мы будем рассматривать под словом consistency? Какие-то варианты, которые должны сохраняться. Я думаю, ты смешиваешь тепло с мягким. Ассет никак не связан с тем фактом, что система распределенная или нет. Да, к сожалению. Ассет рассматривает не в распределенной системе. А когда распределенная, приходится просто это усложнять, как-то натягивать. Дальше я не знаю, о чем говорить. Какие-то свойства должны рассматривать. И некоторые свойства есть, некоторых нет. Окей, я понял, ответ на ваших задачах не возникает. Ответ принимается. Валера, тебе слово. А чего еще не хватает в ClickHouse? Что такое частое, что вы слышите от комьюнити, например, мол, давайте вы скорее имплементируете. А то мы не можем использовать эту базу данных у себя. Что такое топ-1? Я расскажу про свою личную боль. Дело в том, что ClickHouse очень удобный. И в нем язык SQL, это свой диалект SQL, который в некотором роде лучше, чем стандартный SQL. Приведу пример. Вот, когда мы пишем в SQL запросе Select чего-то там, as чего-то там. Это называется как-то, по-моему, correlation name. И дальше есть особенности, что вот эти correlation name можно использовать в секции OrderBy. То есть можно писать SelectX as Y, потом написать OrderByY. Но в OrderBy нельзя использовать выражение, по крайней мере, по стандарту. То есть нельзя сказать OrderBy X plus 1. А вот где-нибудь в GroupBy или в Where по стандарту нельзя использовать эти correlation name. То есть нельзя сказать SelectX plus 1 as Y, GroupByY. Но это не во всех системах. В MySQL чуть больше поддерживается. В Postgres тоже как-то, но я не помню. А в ClickHouse мы изначально сделали это даже с точки зрения простоты реализации было, что алиасы можно задавать везде. И как алиасы, так и выражения тоже можно использовать везде. Можно даже написать скобочка открывается, X plus 1 as Y, потом еще plus 1 as Z. Такие сложные конструкции. Но из-за этого задача разрешения имен, поиска имен при анализе запросов у нас гораздо сложнее. И часто у людей возникают вопросы, почему мы не сделали какую-то функциональность простым и стандартным способом. Потому что есть ведь уже инструменты для разбора эсколь выражения, для их анализа. Скажем Apache Colcid. И некоторые системы его используют. А почему мы не используем Apache Colcid? Или почему мы не взяли Postgres на анализатор запросов? Почему мы его не встроили? Потому что мы с самого начала сделали, что у нас такой очень удобный для нас SQL, который теперь трудно поддерживать. И вот из-за этого довольно большое количество багов есть. При разрешении имен в случае, когда в запросе несколько джоинов. И вообще этот случай у нас сделан плохо и его придется переделывать. А у меня вот посетила мысль, а кто-нибудь из клиентов заявляет о том, что использует ClickHouse для AllApp и backup этот AllApp. Как у вас с backup и умеете ли вы в интерметальные backup? Или это никому не нужно? Все перезаливают данные, если что? Конечно, это нужно. И сейчас встроены возможности backup, в ClickHouse нет. Есть стороннее средство, называется ClickHouse backup. И еще внутри ClickHouse есть некоторые средства, которые как бы такие кирпичики. На основе которых уже сторонние средства могут сделать полноценные backup, включая инкрементальные. И это довольно востребованный фич-квест, это довольно важно. С другой стороны, в некоторых сценариях использовать backup затруднительно. Скажем в метрике, там получается где-то 60 или 70 петабайт сжатых данных с коэффициентом репликации 2. При этом возникают всякие вопросы, как делать с backup. И для backup довольно сложное решение, состоящее в том, что создается третья еще реплика. Но эта реплика создается на немного других машинах, а на этих машинах файловая система ZFS, причем на Linux. И в этой файловой системе иногда создаются снапшоты, а иногда эти снапшоты удаляются. Такое весьма странное решение. И в случае такого гораздо важнее не только backup, но и защита системы от человеческих сбоев, от ошибок конфигурации, от всяческих ошибок. Здесь довольно важно всякие защитные механизмы, которые сразу предусмотрены. Вот один защитный механизм я предусмотрел для того, чтобы я сам мог спокойно спать по ночам. Он очень простой. Если человек пишет drop table, а таблица больше 50 гигабайт, кидается exception. Это можно, конечно, настроить конфигурацией. В тексте exception написано, что именно настроить. Но понятное дело, что это может настроить только тот, кто имеет доступ к изменению конфигурации. А по умолчанию, пожалуйста, exception. Пишешь drop table, оно не работает. Все, я могу спокойно спать по ночам. А операций delete у нас вообще не было раньше. Не было возможности удалить данные из кликауса. Согласитесь, это классно. Давай сойдемся на том, что это весьма оригинально. А пока репликацию вспомнили, а отложенная репликация поддерживается? Не, не поддерживается. То есть репликация с фиксированным лагом. Ну да, да. Ах да, я еще забыл такой аргумент. Смотрите, если вы делаете drop table, у вас на самом деле удаляются не таблица, а реплика. Просто на одну реплику станет меньше. Потом делаете create table, и данные снова реплицируются. Давай проясним. То есть я делаю create table, а у меня там уже есть данные, потому что они среплицировались с существовавшей раньше реплики. Я правильно понял? Если написать create table и указать путь координации для уже существующей таблицы, то это значит создать новую реплику. Она нальется. Это очень удобно, если там мигрировать сервер нужно. На старом сервере создаем новый сервер, создаем там новую реплику, на старом удаляем. Кому бы вы не советовали вообще использовать кликаус? Интересный вопрос. Я могу сказать, что самые лучшие клиенты кликаус это те, кто использует кликаус, потому что очень надо. Как бы от безысходности эти клиенты используют кликаус, потому что у них просто ничего не работает, так же как у нас в метрике ничего не работало. И кликаус это как бы решение для их задач. А гораздо хуже, когда люди могли бы справиться и другими средствами, им нужна просто какая-то релиционная база данных. Если они используют кликаус ради хайпа, это не совсем правильно, хотя конечно очень польстительно. Да, даже часто говорим людям, ну вот для этой задачи просто возьмите Postgres и все. Ну я слышу, что если нужна просто релиционная база данных, это скорее не кликаус. Какие еще моменты, когда кликаус не нужно использовать? Это частые мелкие запросы. То есть сценарий как в киевалию базе. На самом деле есть сценарий, которые хотелось бы рассмотреть, в том числе в кликаусе. Я сейчас про них расскажу. Ну вот представьте, что вам нужно просто доставать данные по какому-то клиенту и делать это хотя бы тысячу раз в секунду. И в этом случае кликаус либо плохо справится, либо даже вообще не справится. Придется как-то масштабировать по-дурацки, потому что каждый мелкий запрос, он будет какую-то пачку данных читать, какую-то пачку обрабатывать, разжимать данные. И скорость работы того же редиса или даже не обязательно редиса, те же EMAI SQL и Postgres, они вполне себе тюнятся на киевалию сценарий, чтобы можно было чуть больше одного миллиона киевалию запросов таких мелких обрабатывать. В кликаусе это не выйдет. А сценарий, про который я хочу рассказать, это то, что, скажем, есть та же рекламная сеть, в ней пишутся логи. Логи надо писать в кликаус. Анализировать логи надо тоже в кликаусе. Здесь без вариантов. Это лучшее решение. А вот что делать, если мы в той же рекламной сети собираем профиль пользователя? Этот профиль пользователя будет использоваться для поведенческого таргетинга и для персонализации. Рекламных объявлений показываются в ронтайме. Когда пользователь заходит на сайт, ему надо сразу обратиться на сервер и достать эти рекламные объявления. А для этого надо достать профиль пользователя. И если это рекламная сеть нормального масштаба, то на каждый сервер может придется хотя бы 10 тысяч запросов в секунду. И уже возникает вопрос, где хранить эти профили, как их отдавать. И вот для этого кликаус, к сожалению, не подойдет, хотя он может подойти для того, чтобы эти профили, скажем так, варить. Чтобы их периодически по логам составлять. А вот выгружать эти профили на какие-то показывающие серверы придется уже отдельно. А для временных рядов, кликаус подходит? Подходит, причем вообще прекрасно подходит. Может быть даже лучше, чем таймскейл. Такой вопрос, почему вы пишите про таймскейл, вы там работаете? Ну, мы про таймскейл раньше рассказывали, но в данном случае, да, так совпало, что я уже две недели работаю в таймскейл. Ага, когда рассказываем про таймсириус базы данных, то всегда есть такие соображения, что таймсириус базы данных может решить какую-то задачу лучше за счет хорошей специализации. Вот такая система, как Victoria Metrics, кстати, недавно была в подкасте, где в ЗН три выпуска назад или что-то в этом роде. Рекомендую. Я тоже слушал и мне очень понравилось. А она внутри принцип хранения, то есть Merge3, там куски, все такое сжатие данных, похоже на кликаус, но за счет специализации в мелких деталях, то есть конкретных кодеках, и за счет поддержки там протоколов и нативной интеграции, типа, чтобы она сразу работала как бэкэнд для промитивуса и поддерживала этот язык запросов. Получается, таймсириус база может получить преимущество. С другой стороны, нету сейчас фундаментальных причин, чтобы кликаус не работал так же хорошо. Там есть все, что нужно. И некоторые результаты показывают. Вот, скажем, видел недавно статью от CERN. Это интересно, что это дэншмарк баз данных, но оформлен как такая пейпер, то есть сверстано в технике и все такое. Там сравнивали Influx, что-то еще и кликаус. Ну и кликаус все выиграл. У меня знаешь какой вопрос? С точки зрения компании и опенсорос продуктов, какие бы аргументы ты бы посоветовал тем людям, которые пытаются свои собственные внутренние наработки вывести в опенсорос внутри компании? Не обязательно яндекс, а вообще, что бы ты мог посоветовать таким людям? Меня часто спрашивают, как выводить, ну как часто, пару раз в год. Но между тем, это на самом деле больная тема для многих людей. Их труды находятся в закрытых источниках и мне разрешают ничего выпускать. Что можешь посоветовать? Есть несколько классов Open Source. Первый самый простой класс это какие-то мелкие утилиты, мелкие вспомогательные инструменты. И здесь я бы рекомендовал просто сразу их выкладывать без особых надежд, что там, не знаю, комьюнити какой-то будет. Просто заведите репозиторий на GitHub и всякие мелочи, которые у вас образуются, просто коммитите туда. Тут могут возникнуть некоторые вопросы, ну в общем, пишите эти мелочи из дома и коммитите. В конце концов, это мелочи. Это не так важно. Гораздо сложнее, когда какой-то крупный продукт, который делает продакшн в компании, как убедить, что его стоит выводить в Open Source. И вообще, стоит ли это делать? Я помню, что я для того, чтобы убедить руководство, написал список где-то из десятков пунктов, которые в общем-то помог, но основным здесь то, что и руководство тоже достаточно лояльно. То есть эти пункты оказались убедительными, они легли на правильную почву. Это важно. Что за пункт? Это очень интересно, заинтриговал. Да, есть прямо список, я его могу открыть, но сейчас, наверное, я буду долго его искать, несколько минут, поэтому я пытаюсь вспомнить. Например, такой пункт. Если компания большая, то если ваш продукт находится в Open Source, то даже люди внутри компании лучше про него узнают. Им самим интереснее то, что это Open Source. То есть в любой компании работают люди, они, если занимаются вороткой данных, они скорее всего слышали про ходу, потому что они интересуются темой. И вот возможность выпустить продукт в Open Source, это возможность стать частью этой темы для широкой аудитории, чтобы широкая аудитория тоже интересовалась. А иначе внутри компании у людей мало мотивации изучать внутреннюю технологию. Они будут думать, я изучу внутреннюю технологию, а как я потом пройду собеседование в Facebook? Или типа того? Какой смысл вообще? Получается это не актив знания внутренней технологии, а непонятно что. Это один аргумент. Еще один аргумент. Это то, что разработчики более мотивированы становиться. Более интересно делать Open Source. Пишешь код и ты знаешь, это сразу же твое портфолио. Ты его пишешь не только для компании, но и для себя. Получается это преимущество для разработчиков. Но это преимущество в том числе для компании, потому что разработчик лучше и больше делает. Допустим человек будет работать в нерабочее время, разрабатывает Open Source. Это приносит пользу компании. Причем никаких вопросов не возникает. Все это очень естественно. Так, что там еще было? Я бы открыл про естественное нерабочее время, но ладно. Но это как получится по-разному воспринимается. Может быть наоборот будет хуже, если будет из-за этого выгорание. Довольно часто как раз ситуация с Open Source. Такие риски имеются. Так что как-то надо балансировать. Если получается. Про выгоревших сотрудников это было в списке для менеджмента? Нет, это не было. Там был отдельный пункт риски. Тоже очень интересно. Там почему-то такого не было. Наверное не думал. Еще один пункт вспоминается. Это развитие технологического бренда компании. Иногда про какую-то компанию могут думать много стереотипов или таких странных вещей. Ну рассмотрим какую-нибудь компанию, которая занимается информационной безопасностью. И про нее идут разговоры. Она делает информационную безопасность и средства для того, чтобы эта компания находится под госструктурами. Про какую-нибудь компанию могут говорить. Про Касперского или про Positive Technologies. Может быть даже не без основательного. Хочется, чтобы кроме стереотипов говорили про что-то хорошее. Я захожу в GitHub-пропозиторий компании Positive Technologies и вижу, что там есть несколько классных опенсорс-продуктов. Это альтернатива для LMDB, встраиваемая от базы данных, под названием позитивные таблицы. Я думаю, ну классно. Интересная компания. Может там интересные задачи есть. И некоторых разработчиков это привлечет, а для некоторых просто... Да, в основном это для HR-бренда. Это то, с чем можно идти на всевозможные конференции и привлекать аудиторию. У меня только последний вопрос остался. Я работала раньше с Vertica. И в одной из компаний, где я была, я думаю, я представляю, приходит новый разработчик в компанию. И говорит, я послушала или послушала наш подкаст. И там говорили про такую замечательную базу данных как Clickhouse. Как этому человеку, какие аргументы нужно использовать, чтобы убедить компанию уйти с Vertica на Clickhouse? Стоит попробовать Clickhouse, загрузить какие-то тестовые данные, сделать proof of concept, если есть к этому интерес. В остальном спрашиваться, какие недостатки у существующего решения. Либо платите дорого. Тогда, конечно, стоит начать с proof of concept. Попробовать, может быть, рассказать несколько success stories. Вот есть success story компания LifeStreet, которая мигрировала с Vertica. Это целая реклама на сети. Миграция, она заняла немало времени, больше чем полгода. Но с другой стороны, миграция полностью успешная.",
    "result": {
      "query": "ClickHouse достоинства и недостатки"
    }
  }
]