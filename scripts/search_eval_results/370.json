[
  {
    "segment_id": "9855ae82-e400-46d5-bf84-2d8db758e5ac",
    "episode_id": "dc046ba4-8c04-4021-8f4a-673cf2753d42",
    "episode_number": 370,
    "segment_number": 11,
    "text": "Слушай, я, короче, я не смотрел в Pidgey Cat, чтобы понимать, как он делает шардинг, но я могу просто заранее сказать, что шардировать Pogress это, вообще говоря, возможно не самая тривиальная задача, просто потому что ты можешь сделать массу выборов того, какие ты хочешь компромиссы, компромиссить типа в духе мы делаем транзакции между шардами, и это вообще, короче, разные сервера, которые друг другу не разговаривают, или мы, может быть, и мы данные с него не хотим джонить и так далее, или, может быть, мы хотим представлять их всех большой дружной виртуальной базы данных, или, может быть, мы, например, пишем у них всех патроник, в дельности читаем как будто бы это одна база, и в общем, шардивание можно накрутить разными способами, и что-то, да, я как бы вот не готов, я бы скорее предпочел, чтобы Сталон оставался чисто решением про H.A., потому что шардировать нужна релационная база, нужна application specific, если они сами по себе не умеют быть прозрачно масштабированы, я скорее в этом плане верю в какой-нибудь зенит, который будет масштабировать на уровне тогда уже сториджа, типа как вору, без чем пытаться его как-то нарезать, потому что с нарезанием всегда сложности, если оно такое, через какую-то прокси. Шар. Шардирование сделать очень легко, только потом транзакции делать заебешься. Я же говорю, смотри, я как минимум могу придумать класса задач, то есть первая у нас пользователи своими там какими-то штуками, которые между пользователями мы не перемещаем объекты, потрясающе, можно просто нарезать и не делать транзакции, и все-таки может быть вообще ситуация, когда у нас какая-то общая общечан с всяким, мы пишем, может быть, даже независимо, но потом хотим вгребать все вместе, как будто бы даже в одну, может быть не в одной транзакции, но хотя бы чтобы запросы не делать несколько разных баз, и это тоже можно накрутить на пасгрессии, это будет совершенно по-моему накручиваться, это будет через partitioning и foreign data proper накручиваться, и я не знаю, что можно делать, короче, протекционировать, как это сделано в таймскейле, типа тоже какой-то гибрид из этих двух получается, то есть сильно задача будет ориентирована, ты будешь это сделать на пасгрессии, вот что пытаюсь сказать. Ну да, согласен. Мне, кстати, несколько сультаций, как большого ВХ, просто вот, pg bouncer написан на C или B венте, просто и понятно, здесь я смотрю в depende.pgcat и вижу Tokyo, Tokyo это то же самое, ну, решение для задачи, да?",
    "result": {
      "error": "API request failed: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-xSG9D345gtYlIKC330wCyrEG on tokens per min (TPM): Limit 30000, Used 30000, Requested 1407. Please try again in 2.814s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
      "stack_trace": "Traceback (most recent call last):\n  File \"/home/andrei/Projects/podcast-shownotes/scripts/build_search_eval_dataset.py\", line 157, in generate_search_query\n    response = await self.client.chat.completions.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 1927, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1767, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1461, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1547, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1547, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1562, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-xSG9D345gtYlIKC330wCyrEG on tokens per min (TPM): Limit 30000, Used 30000, Requested 1407. Please try again in 2.814s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
    }
  }
]