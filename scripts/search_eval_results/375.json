[
  {
    "segment_id": "ee77e2eb-d660-47e3-94e0-7c3f21d8dd41",
    "episode_id": "cbf3ad33-328a-49ba-b691-9033ab448ecf",
    "episode_number": 375,
    "segment_number": 6,
    "text": "Это не твоё воображение, это, видимо, у нас, я уж не знаю в силу чего, наверное в силу того, что появились новые сториджи, появилось больше возможностей распилённо, больше памяти стало, в общем, их стало более, наверное, просто сделать, потому что это вообще сложная задача, которую всегда пытались решать. Я помню, Лев Валкан ещё в 2013 году что ли сказал что-то в духе, что типа вот кто напишет нормальную графовую базу данных, заработает миллионы. Это когда ещё эхо было, вот и девзена ещё не было как подкаста, то есть как бы рынок был всегда и нужда была всегда, но не знаю сложно сказать почему снова все взялись за как-то почему все делают подход к как говорят подход к станку в данный момент, почему всех снова случилось. Ну как известно, святое место пусто не бывает, если есть рынок, то на него будут пытаться переходить игроки, но какие-то возможно уйдут с него ни с чем, но какие-то останутся вот занимая какие-то нижки, как вот StarDock с поиском например или вот Reload Shnalae пытается в ML. У меня по этой теме всё. Хорошу, у меня тогда такая не знаю, тема сама по себе скорее всего одной строкой, но мне наверное интересно будет не знаю Ваню возможно послушать по этому вопросу, есть такой точнее был такой продукт, скажу как он назывался, в общем хранилище для прометивса кажется называлось Cortex наверное или Вань я вру или не вру? Слушай я не помню был Танос и да да да, Танос мы обсуждали. А второй кажется Cortex назывался. Да и по-моему даже третий был. Да было несколько, Cortex я сейчас посмотрел, Cortex и в общем Cortex насколько я помню делала компания Grafana Labs, они его делали полностью в открытую часть cncf cloudnary foundation это всё и в какой-то момент всякие амазоны и прочие гуглы стали брать и хвостить то что ребята делали и в итоге очень то есть компания была вынуждена очень многие вещи перестать колбасить в open source и начали колбасить в какой-то свой закрытый Grafana Enterprise fork колбасили колбасили в какой-то момент поняли что им хочется они ощущают некую несправедливость вот происходящего что и в общем если я правильно понял что происходит в этом аналитике потому что я не слишком погружен именно в графанов графановскую экосистему если правильно понимаю что происходит они взяли свой код который недолго время колбасили в закрытую там типа возможно что-то переделали выложили это под лицензией hgpl3 и назвали это Grafana Mimir это насколько я понимаю fork Cortex который ушел от Cortex вперед он hgpl и как бы типа true open source лицензия но все еще с ограничениями и xtmp в чем как именно hgpl работает но если я правильно помню идея тоже в том чтобы ограничить использование программного обеспечения как это в облаках ситуация что в облаках да ситуация когда клава на этих за которыми будущее да в общем чтобы не получалось так что вот вы как компания вкладывает вкладывает потом приходит какой-нибудь клад провайдер на все готовая забирает ваш кусок и делает поднимает себя свой собственный сервис где опять же можно даже делать закрытые модификации которые не обязаны совершенно никак как-то и контрибьюте обратно смотно припоминаю мое мне не являясь греческой консультации что там принцип вот как типа если ты пользуешься чем-то основанным на gpl этом на роутере там что что-то под гп линк русис ты можешь прийти к производительской дай мне исходники вот здесь принцип такой же соблакам это и если пользуешься сервисом ты говоришь я считаю что у вас там а в gpl дайте мне исходники ты все еще может строить на этом бизнесе хочешь но ты обязан всем давайте в ходе в общем я так понимаю что это несколько менее противоречивая лицензия чем",
    "result": {
      "error": "API request failed: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-xSG9D345gtYlIKC330wCyrEG on tokens per min (TPM): Limit 30000, Used 30000, Requested 1917. Please try again in 3.834s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
      "stack_trace": "Traceback (most recent call last):\n  File \"/home/andrei/Projects/podcast-shownotes/scripts/build_search_eval_dataset.py\", line 157, in generate_search_query\n    response = await self.client.chat.completions.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 1927, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1767, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1461, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1547, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1547, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1562, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-xSG9D345gtYlIKC330wCyrEG on tokens per min (TPM): Limit 30000, Used 30000, Requested 1917. Please try again in 3.834s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
    }
  }
]