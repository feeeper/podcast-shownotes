[
  {
    "segment_id": "4562474a-6bc5-496f-a4d9-3e5da11efa1d",
    "episode_id": "e0d21f08-bd15-4ad4-a832-4afdc3f22a1b",
    "episode_number": 286,
    "segment_number": 4,
    "text": "И вот чтобы вот это ещё посчитать, блин, если они это всё берут во внимание, это реально круто. Оно действительно так работает. Но я… ну блин, мне как-то трудно в это поверить, я думаю, они как-то это проще решают. К сожалению, про это в пейпере ничего нет. Так, пошли дальше. Нагоризонтальное. Нагоризонтальное всё интереснее. Почему вообще нужно горизонтальное масштабирование? Потому что без него никак. Простите за банальный ответ. Чаще всего все задачи, они… ну не все, но многие задачи, они используют по максимуму какое-то количество ресурсов, например, у меня есть три ЦПУ, я их использую до конца по максимуму, но больше трёх ЦПУ я использовать не могу, ну потому что на одном у меня шедулер и два у меня вычислители какие-нибудь, ну к примеру, я говорю. Или, для всего это вообще one CPU task, то есть на одном ЦПУ накрутится, использует его максимум, но использовать несколько не может. Соответственно, нужно искать несколько и давать им разные ЦПУ, чтобы они там работали как-то. А сколько запускать? Это уже, соответственно, задача автопилота, он пытается определить, сколько нужно инстанцев, какое количество контейнеров данного типа нужно запустить для того, чтобы полностью покрыть все требования по этой задаче. И здесь, на самом деле, это довольно сложно. Сложно выполнить, когда у тебя есть… давайте я немножко вернусь к Кубернетусу и к своему горизонтальному HPA, horizontal pod autoscaler. Там работа гораздо проще, потому что у нас нет вертикального автоматического масштабатора, масштабиратора, как это правильно сказать. Есть какая-то метрика, например, ЦПУ, и эта метрика ЦПУ является отличным показателем того, насколько много инстанцев нам нужно. Если мы запустили 5 инстанцев и они все упираются в ЦПУ, скорее всего, нам нужно запустить 6 инстанц для того, чтобы задачи данного типа разгребать. Если мы добавили 6, 7, 8, 9, 10, и ЦПУ понизилось меньше 80, значит все, нам не нужно больше расти. А если внезапно оно начинает уменьшаться меньше, скажем, 60, значит нам надо уменьшать количество инстанцев, я не знаю, реплик, как это правильно назвать, реплик, наверное, для того, чтобы нам слишком просто не проедать ресурсы впустую. И мы уменьшаем, уменьшаем, уменьшаем, пока опять ЦПУ не вырастает в пределах от 60 до 80. Вот так работает автоскейлер в Кубернетосе, если на пальцах выяснять. Здесь же у нас получается хитрая штука, потому что мы не даем ему, мы еще вертикально масштабируем, и непонятно, сколько нам нужно запускать горизонтально масштабированных, потому что каждый из них будет использовать ЦПУ настолько, насколько ему дают. То есть автопилот будет устанавливать лимиты так, чтобы почти упираться в лимиты. И поэтому нам тяжело понять, сколько нам нужно запускать. И здесь они, отсюда они очень интересный выход нашли. Они следят за утилизацией ЦПУ, имеют статистику по памяти и они позволяют пользователю написать функцию размера кластера, называется target size. То есть человек, я так понимаю, разработчик пишет, а может оператор, я не знаю, пишет функцию для вычисления количества реплик для данной задачи. И соответственно, в дальнейшем, имея вот эту функцию, а точнее результат ее значений, в системе автопилот уже начинает изменять количество реальных реплик, уже ориентируясь на те метрики, которые у него есть, и на вот эту функцию, которую предоставил ему человек. Ну и конечно же начинает применять все те стандартные штуки, которые и в кубернетике, и в HPA мы применяем. То есть если внезапно у нас уменьшилось количество используемого ЦПУ в наших задачах, нам не нужно резко сразу уменьшать количество реплик, потому что это может быть какой-то спайк, и нам просто задачи не дошли по каким-то причинам, или, не знаю, где-то глюкануло, акула что-то перекусила, и на 5 секунд, на 5 минут нам резко понизилось количество запросов. Но через 5 минут оно может резко вырасти обратно, и поэтому нам надо плавно понижать количество реплик. Соответственно, ну там куча всяких есть параметров. Ну скажем, точно так же не надо быстро расти. Бывают иногда спайки, которые резко увеличивают ЦПУ, но при этом они так же быстро уходят. Надо плавно повышаться, плавно понижаться, смотреть на предыдущую историю, вводить какие-то, не знаю, взвешенные понятия там и так далее, взвешенные характеристики, параметры. Это всё стандартные алгоритмы, которые и в кубернезе точно так же работают, но вот самое интересное для меня было, как они это делают, это как раз с помощью функции, которую пишет сам человек. Вот, это, в принципе, всё, как он работает. Подожди, давай ты расскажешь, что это за функция, потому что я послушал, функция потрясающе клёвая, я не понимаю, как это работает. Легче разбирать это, наверное, например. Ну да, я хочу пример. Есть 2 реплики, 2 контейнера, которые выполняют свою задачу, они используют ЦПУ на 70%. Разбираем, например, на кубернетовце без вертикального масштабирования, так проще понять. В какой-то момент времени приходит резкий спайк, резкий скачок количества запросов, он увеличился, не знаю, в 3 раза. Не знаю, в твиттере где-то вышла ссылка на ваш ресурс. Вот. Прибегает толпа народу, которая теоретически может обработаться, я не знаю, 20 машинами, то есть у вас сейчас 2 машины, а вы можете обработать 20 машинами. Даже если вы сейчас прямо увеличите, внезапно запустите еще 20 машин, на этот пройдет, я не знаю, сколько, 30-40 секунд времени, эти запросы уже отвалятся, вы их не сможете обработать. Зато после этого спайка чаще всего оно упадет до предыдущего уровня. Поэтому количество реплик не нужно увеличивать мгновенно с 2 до 20, какие бы характеристики мы там не высчитывали автоматически, нужно плавно увеличивать. Вот это вот плавное уменьшение, это ограничение роста, оно в параметрах в Автопилоте и в Кубернетесе, оно есть. То есть вы это задаете, ну вот начиная с той версии с 1.18 в Кубернетесе, вы можете задавать, насколько можно быстро расти количество реплик. Вот. Соответственно здесь точно также есть какое-то ограничение роста. Аналогично со снижением, то есть как бы если внезапно перестало идти, не надо мгновенно снижаться. В Кубернетесе есть только один параметр, вы вычисляете регламентованное значение по новому количеству реплик, за последние N минут вы можете установить его любое, от 5 до 60 или не помню до скольки. И потом через эти 5-60 минут вы снижаете, если у вас было 10 реплик, у вас одну минуту он говорит, надо пожалуй 8, вторую минуту говорит надо 8, третью минуту говорит 6, четвертую минуту 8 и на пятую минуту снова 8 и вы такие ну ладно максимальное значение среди них 8, самое безопасное будет 8, давайте снижаться до 8. В Автопилоте тоже самое есть, и помимо этого есть еще параметр, который позволяет уменьшать в какое-то количество раз плавно, то есть он не уменьшает с 10 до 8 за 5 минут, он уменьшает по одной машине раз в 2 минуты грубо говоря, он примерно к этому же плавному результату стремится. Есть такая функциональность, когда вы запрещаете быстрому колебанию метрик, то есть например у вас нет вот этих смещений, нет запрета быстрой смены количества реплик, например вы можете подпрыгнуть с 2 до 10, но если вы подпрыгнули с 2 до 10, то обратно вернуться к 2 вам уже не получится, потому что есть такой запрет на вот эту вот вибрацию, то есть запрет вибрации. Там тоже есть какие-то параметры, которые характеризуют, что считается вибрацией, насколько быстро можно снижаться после вибрации, вот это вот здесь в Автопилоте тоже есть. Примерно понятно? Я все еще не понимаю, где здесь функция и как это решает проблему, как это помогает им избавиться от проблемы с вертикальным тускеликом. О, я понял, я понял твой вопрос, извини, сразу не понял. Функция вводится человеком, то есть я сейчас рассказывал в основном про кубернетическое решение, потому что на кубернетическом решении проще понять. На Автопилоте у тебя нет понимания, сколько тебе понадобится ЦПУ, потому что ты все время используешь 80-90% возможного ЦПУ, близкого к лимиту, и вместо этого есть функция. Например, функция привязана к метрике... да, это я вот это, наверное, не сказал. Функция может быть привязана вообще к любым метрикам вашей системы. Например, количество сообщений в очереди на обработку данного батча. Ну, то есть, я не знаю, в кавке у вас лежит 1000 сообщений, ты говоришь, что мне надо запускать n сообщений в очереди разделить на 100, вот значит у меня должно быть 10 реплик, если у меня там 1000 сообщений лежит. Понимаешь, да? Вот, и ты пишешь функцию n разделить на 100. Вот так вот девелопер сказал. Или, например, ты можешь сказать, что количество запросов в секунду к ресурсу bc разделить на 100 тысяч, потому что именно вот такое количество может выдержать мой данный таск, и так далее, и так далее. То есть, ты фактически можешь оперировать любыми понятиями, которые доступны в сторидже метрика. То есть, в примитиве или где-то еще. Я не знаю, кто у них используется. Вот, в дальнейшем алгоритм смотрит на этот показатель, и у него есть следующая формулировка. Этот показатель примерно то же самое, что в кубернетосе рассчитать количество, желательное количество метрик, исходя из того, сколько цпу используется. Вот, и он смотрит, так, у меня сейчас запущено 2 этих, 2 моих инстанца, 2 реплики, а вот этот показатель, который мне предоставил человек, говорит, что мне надо сделать 10. И он не будет делать 10, потому что от 2 до 10 слишком быстро растет. А у меня есть характеристика, которая говорит мне увеличивать больше, чем на 15% каждую минуту. Значит, в следующую минуту я сделаю там еще одну реплику, там или сколько процентов нужно. Вот. И вот эта функция, она используется для того, чтобы подсказать автопилоту желательное количество реплик при данной нагрузке и при данных метриках системы. То есть, я понимаю, что можно в этом случае стрельнуть себе в ногу и поставить эту метрику на загруженность цпу, и после этого, из-за того, что автопилот будет его теребить, у тебя неправильно будет работать автоскерлинг количеством машин.",
    "result": {
      "error": "API request failed: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-xSG9D345gtYlIKC330wCyrEG on tokens per min (TPM): Limit 30000, Used 30000, Requested 4746. Please try again in 9.492s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
      "stack_trace": "Traceback (most recent call last):\n  File \"/home/andrei/Projects/podcast-shownotes/scripts/build_search_eval_dataset.py\", line 157, in generate_search_query\n    response = await self.client.chat.completions.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 1927, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1767, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1461, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1547, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1547, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1562, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-xSG9D345gtYlIKC330wCyrEG on tokens per min (TPM): Limit 30000, Used 30000, Requested 4746. Please try again in 9.492s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
    }
  }
]