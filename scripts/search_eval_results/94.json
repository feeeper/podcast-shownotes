[
  {
    "segment_id": "91292993-414d-4473-af56-ced57c19e53c",
    "episode_id": "53fc2050-51d4-4466-b8be-5e6c031265aa",
    "episode_number": 94,
    "segment_number": 2,
    "text": "Начался момент, когда она просто отказывалась принимать новые данные на запись. По логам было видно, что у нее переполнились какие-то внутренние структуры данных, типа внутренних буферов, она начала заниматься постоянной их очисткой и сбросом, ей потребовалось несколько часов на это, потом она вернулась в строй. Но вот такой момент, что она может провалиться ненадолго, а потом вернуться, очень смущает. Вы брали прям из коробки InfluxDB, либо как-то его подстраивали под свои нужды? Я попробовал прямо из коробки, потому что это у меня один из критериев выбора, когда есть большой выбор, а я себе составил списочек, по-моему туда вошло штучек 8 баз данных, которые вроде хорошо должны работать именно с Time Series, что она должна требовать минимальной настройки, то есть чем меньше надо настраивать, тем лучше, по этому критерию. Поэтому первое, что я пытался делать, вот просто установил, запустил, поехал. Вот кстати по поводу пропускной способности, это у вас сможешь назвать цифры? Точных цифр я сейчас назвать не могу, потому что сам не всегда их знаю. Десятки тысяч в секунду. Неплохо, неплохо. Ты говорил, в каком-то формате хотите складывать. У вас есть что-нибудь по поводу формата, какие-нибудь предположения, либо желание конкретно использовать то и не использовать другое? По поводу формата не очень понял. Я недавно проводил эксперименты по выбору форматов стерилизации, для того чтобы перегонять эти данные по Message Passing. Там я тоже себе составил списочек из шести, которые мне показались самыми перспективными. Есть ли про это вопрос? Да, да, да, про это. В чем перегонять, потом в чем хранить? Ну я использовал свой стандартный прием, построил такую табличку, где по строкам это разные стерилизаторы, по столбцам разные метрики, которые мне интересны, и начал потихоньку их заполнять. По строчкам у меня, по-моему, получились... Ну первое, что пришло на ум, это Proto Buffers, дальше там возник Flat Buffers, был там вот такой интересный новый для меня протокол CapnProto, который мне очень понравился, был, по-моему, BSone, был просто JSON. А Avro не смотрел? Avro не смотрел. Avro я в этот список не включил, не помню сейчас точно по каким причинам. Может быть просто не смог объять все. И по столбцам, собственно, стандартные характеристики, время стерилизации, время десерилизации, поддержка необходимых фич, которые нужны, размеры сжатия, без сжатия данных, которые протекают. И потихонечку начал заполнять что-то микробетчмарками, что-то более сложными бетчмарками. И потом просто перевел скалярные критерии, и вот на удивление выиграл как раз вот этот новый протокол CapnProto. Очень советую посмотреть. Это работа над ошибками создателя второй версии Google Protocol Buffers. Вот, собственно, ведущий программист этого проекта как раз ушел оттуда и сделал свой новый стерилизатор, в котором, как он сам пишет, пытался исправить ошибки прошлого. На мой взгляд, получилось хорошо и интересно. А еще раз, как он называется? CapnProto. А что там с версионированием? Мне кажется, опять гусь пропадает очень сильно. Там поддерживание, в сторону расширения. Павел, ты у нас снова пропадаешь? Я предлагаю маленькую тему. Да, давайте маленькую тему. Я был вопрос спросить, вот же черт. Окей, давайте маленькую тему. Да, давай, Павел, ты переконнектишься, и мы за это время пообсудим другую тему. Маленькую тему. Ну, моя тема не маленькая, давайте... Ну, давай такую. У нас все не маленькие. Ну, окей, тогда я начну с этой темы. Собственно, большая новость, большое событие, то, чего я так долго ждала, — это выход в open source Херона. Это, напомню, если кто не слышал, не помнит, это фреймворк для потоковой обработки данных от Твиттера, как замена Шторму. И я этого события ждала порядка года, они рассказали про этот фреймворк где-то летом прошлого года, обещали, обещали его за open source, и вот, спустя год таки случилось. И теперь, пожалуйста, используйте под лицензией Apache 2.0. Так что без проблем можно тянуть к себе это в production или не в production. А Херон, он теперь Apache Херон или еще Твиттер Херон? Пока что Твиттер. Можно посмотреть, есть ли он в Apache какой-нибудь инкубатор или что-нибудь такого. Но лицензия Apache. Так что они у себя этот фреймворк уже проверили, так что, в принципе, можно попробовать переключиться, хотя бы несколько топологий. К слову, почему переключиться можно легко? Потому что есть совместимость на уровне API, там сохранены те же концепции в виде Spout и Bolt, это работает и в Хероне, в том числе. При этом есть поддержка таких систем, как Apache, Mesos, Aurora и подобных, и для того, чтобы можно было запускать ваши топологии на, скажем, этих ресурсовых менеджерах. Наверное, так их можно классифицировать. Так что я очень рада этому. Мне, к сожалению, не было возможности попробовать его, посмотреть на код, но интересно то, что он написан на Java, C++ и Python. И заметно то, что Твиттер, они же, впрочем, любители скалы, но этот проект был написан на Java, что говорит о многом. Я, кстати, немножко заглянул в исходник код, там, по-моему, все-таки ядро получилось и на C++, а такое ощущение, что Java нужна исключительно для совместимости со старым API. Ну, охотно верю. Написано, что в 2-5 раз больше производительности, но если они выпили Java и переписали на нормальный язык, то, ну, в смысле быстрый, не нормальный, но быстрый, то охотно верю. Вот, просто это как раз таки был вопрос, когда я читала paper про Херон, там были графики с производительностью, там графики были совсем не 2-5 раз, там были десятки раз, и мне было совершенно непонятно, как они это получили, за счет чего. Вот если это плюсы сыграли свою роль, то это довольно интересно и занятно. К слову, TwitterStorm был написан, ApacheStorm был написан на Clojure. В смысле, изначально? Да, изначально его писали на Clojure. Две, ты наверняка обратила внимание на то, что они позиционировали как одно из 10 переход от потока ориентированной к процессу ориентированной. Да, да, да, это что-то похожее на Actor, ну, в некотором смысле. И сталкивался с эффектом, что попытка сделать на потоках распределенные системы, она может давать худшие критерии по производительности. То есть, несмотря на то, что, казалось бы, там можно сэкономить на пересылке данных в пределах одной ноды, когда мы работаем в многопоточном режиме, но мы там можем начать сильно проигрывать просто из-за синхронизации на общих объектах. Ну, по тому же принципу, например, у нас Redis, он, как сказать, однопроцессный. Что еще у нас есть однопроцессный? Тарантул однопроцессный. Ну, то есть нужно много экземпляров, запускай на разных портах. Ну, и хочется сказать, что это не такой уж и плохой подход. Ну да, ну зачем людям эти лишние логи? Выпилим их все. Мне этот подход очень нервит, потому что я вот... надо мной, конечно, иногда смеются, но я часто говорю фразу, что в некоторых областях многопоточное программирование устарело, и его должно... давно пора заменить многопроцессорным. Не потому что я часто пишу на питоне, где очень все плохо с многопоточным, но очень все хорошо с многопроцессорным, а я вижу такие тенденции на других языках, в частности на C++. Слушай, вот возвращаясь к тому, где ты порвался у нас, тебе CapnPro-то по сравнению с FlatBuffers как? Очень был удивлен тем, что FlatBuffers невероятно проиграли во времени сериализации. Даже так? Вполне возможно, что я их неправильно варил, но для меня был важен вот какой критерий. Чтобы вот код... я просто прочитал туториал, я написал код, и тоже не, может быть, долго, проводя время с профайлером, я получил бы хороший годный результат. Возможно, с FlatBuffers там есть ручки, которые нужно аккуратно подкрутить и получить результат гораздо лучше, но, например, вот просто потратив минут 20 на написание кусочка кода сериализатора, я получил просто невероятную разницу по времени. Это для меня самого было удивление, потому что вроде в рекламных буклетиках FlatBuffers очень даже неплохо позиционировались. А вот ты начал рассказывать про версионирование, можешь поподробнее рассказать, что там в аналоге протобафа? Там исчезли опшонал поля, теперь можно только в сторону расширения, то есть нельзя поля удалять, это достаточно разумно, это вечная проблема, если с новой версией протоколов что-то удаляется. Можно идти в сторону расширения протоколов, то есть добавлением новых полей. А если нужно удалить поля, то есть что в такой ситуации вообще делать? Новое сообщение вообще придумывать? Да, есть такой подход, поскольку это Breaking Changes, логично представить это как новую версию со своими форматами, которые старые могут просто не воспринимать. Может быть там есть еще какие-то варианты? Я не смотрел конкретно на проблему версионирования, это то, что я буквально из сториала подчеркнул. Может быть если копнуть глубже, там есть какие-то обходные пути, или не обходные, а очень даже прямые. Наверное не так еще хорошо знаю этот сериализатор. Что правда приятно, на нем можно также крутить удаленный вызов процедур. Вещь, которую я очень тоже не люблю. Я всегда больше стараюсь смотреть как Message Pressing системы, но иногда бывает тоже необходимый удаленный вызов процедур, там есть такая возможность. Пойдемте может быть уже к мясу и Хаскелю? Да, у меня пока вопросов гостя больше нет. Давайте тогда к Хаскелю. И кто у нас главный по Хаскелю на этой неделе? Я думаю, Александр. Бесспорно же, это нет, это Ваня. Я сегодня главный по Хаскелю? Слушайте, отлично. Ну тогда самое вкусное, что сегодня можно сказать по Хаскелю, это то, что вышла версия 8.0.1. Насколько я понимаю, это первая версия в 8-ой, то есть не было 8.0.0, или я пропустил? Все верно, да. Это первый бранч в 8-ой ветке, да, первый релиз в 8-ой ветке, и очень-очень много вкусного. Я сам давно уже не пишу на Хаскеле, но вот после подобных изменений прям хочется вернуться. Кто-нибудь, кроме меня, читал релиз на WoT-се? Конечно. Отлично. Я даже не знаю, с чего начать. На мой взгляд, здесь вышло сразу две вещи, которые очень могут сильно помочь разработчикам. Первая вещь – это упрощение, усложнение, я даже не знаю, как это правильно выразить. Это новая директива –xtypeintype, которая позволяет календарь рассматривать точно так же, как типы, и это позволяет делать дофигища всего. Я даже, я не знаю, я прочитал парочку статей по зависимым типам в связи с этим. Я до конца не понимаю всей глубины всех этих изменений, которые позволяют теперь Хаскель-компилятор обрабатывать и помогать вам писать ваши программы. Одну из статей, я надеюсь, мы сегодня еще обсудим. И я не знаю, кто еще что-то здесь хочет сказать. То есть, основная фишка, которая здесь, я был на CodeMesh на предпоследнем, я уже не помню, и там было хорошее видео, женщина на пальцах объясняла, очень четко, здорово рассказывала, почему зависимые типы – это хорошо, и почему на Хаскель до сих пор неудобно с ними работать. И она в качестве примеров показывала вещи, которые в этой статье, в которой сегодня мы будем обсуждать, уже решаются. То есть, Хаскель реально шагнул вперед за полгода, за год, и сейчас уже зависимые типы могут на самом деле очень сильно вам все это помогать, и все это вот в восьмой версии компилятора. Добавьте кто-нибудь что-нибудь. Я не очень сильно разбираюсь в зависимых типах, это больше к Валере. А вот я на самом деле тоже, я же зависимые типы, я вот как бы, я имею представление, что это такое, но я ни разу с ними не работал, поэтому мне как-то... Я тоже не знаю, Макса Сахацкого надо звать. Надо звать специалиста, да. Ну вот да, похожая ситуация. Для меня это, знаешь, такая фича, которая окончательно сделает проект неподдерживаемым никем, кроме того, кто это писал, и я против, я осуждаю. Но то есть где-нибудь в библиотеке, там, где мне это не придется смотреть, может быть, это хорошо. Ну, в общем, я думаю, надо будет как-нибудь позвать человека, который сильно разбирается в зависимых типах, который на пальцах нам покажет, что мы не правы, и что сейчас весь мир перейдет на Хаскеля именно из-за этого. Вот. Вторая вкусная штука, которая появилась в восьмой версии, это расширение, которое позволяет говорить, что вот этот модуль у меня теперь полностью strict by default, то есть он не ленивый по умолчанию, а строгий. И это позволяет сильно оптимизировать некоторые вещи. То есть строгие можно было раньше делать какие-то отдельные типы, а в дальнейшем можно было вообще целые, как это, не типы, а часть вычислений с помощью некоторых конструкций языка. В дальнейшем можно было, по-моему, в 7.10 появлялось, то, что можно было сделать строгим сам тип, а теперь целый модуль можно сделать строгим. Вот это управление строгостью мне в свое время, когда я давно уже на Хаскеля не пишу, но в свое время, когда я писал, у меня очень-очень не хватало. Не всегда можно было все контролировать, и сейчас, если они это разрешают, это сильно улучшает гибкость языка и позволяет писать многие вещи, которые до этого не могли писать. Это основное, за что, может быть, не основное, но то, за что часто критикуют Хаскеля, что он ленивый, что он очень ленивый, думать сложно. На самом деле он не ленивый, а не строгий, и вот то, что эти расширения языка делают, это можно было делать раньше, просто тебе приходилось явно писать, когда ты объявляешь тип, нужно было во всех полях прописать высказательные знаки, при том это даже в самом языке, в стандарте Хаскеля, типа 1998 года, наверное. То есть это даже не расширение ГХЦ, а строгие поля. И точно так же ты мог делать строгие вычисления, тебе просто приходилось для этого дополнительно что-то писать, а эти расширения, они тебе экономят число символов, которые ты печатаешь. Но это очень удобно, это очень здорово. Вот, что там еще? Еще они обновились до LLVM 3C, и теперь аппликатив можно писать в дунотации. Ну, LLVM это вообще здорово, а аппликатив в дунотацию, мне кажется, это такая больше удобняшка. Я не могу сказать, что у меня такая необходимость постоянно возникала. А чем здорово то, что они обновили LLVM? Ну, стало лучше. Стало еще лучше? Ну, там типа новые оптимизаторы занесли, баги поправили, баги добавили. Поправили и добавили. Ну, это не одни и те же баги, это разные. Вот, ну я думаю, что на самом деле здесь много всяких еще полезностей. Тут на самом деле релиз ноут достаточно большой, и надо очень сильно вчитываться, чтобы все полностью в глубину понять. Вот мы самую верхушку айсберга собрали, все остальное читайте сами, кому интересно. Меня беспокоит в этой истории то, что сейчас все продвинутые хоскелисты сидят на стеккедже. Мы вроде обсуждали, да, в подкасте? Да. И сообщается, что в стеккедж LTS скорее всего GHC 8 попадет только где-то в сентябре. То есть релиз состоялся, но вот прям для продакта он будет готов, ну, не скоро. Связано это с тем, что не все пакеты еще перешли на совместимые с GHC 8. Ну, я честно говоря не уверен, что они ломали обратную совместимость, скорее у них есть верхняя граница в зависимости. И там написано, что нужен пакет бейс версии, не больше чем был в 7.10, поэтому вот не скампелиться просто. Нет, нет, там синтаксис поменялся временами. То есть там есть целая статья о миграции на 8.0, и там показывают, как синтаксис меняется. Там в GDT поменялся немножко синтаксис, они GDT немножко переработали. Там несколько вещей поправили, я не углублялся, но говорят, что сейчас работает лучше и приятнее, но из-за этого синтаксис пришлось поменять местами. Интересно. Вот, в Nightly уже, ну, в стекерш Nightly уже GHC 8.0, при том, FPComplex пишет, что они как бы заигнорировали вот эти верхние границы на пакеты, поэтому там чуть больше пакетов, чем на самом деле можно установить с Hackers для GHC 8.0. И они ждут пока пакетов станет больше, в них поправят баги и так далее. Ну и где-то по некоторым оценкам к сентябрю должно быть уже прям совсем хорошо. И небольшая релейтет-новость, что у него появился официальный пакет под FreeBSD. Вот, у меня пока все. Отлично. Тогда предлагаю к следующей теме по Хаскале, которая сильно связана. Как раз одна из статей, которую я упоминал, это про то, как использовать зависимые типы в Хаскале. И статья от человека, который мне очень понравился. Это Джастин Ли, PHD какого-то из калифорнийских университетов, я там в них не особенно разбираюсь. Но мне нравится, как пишет, он говорит, вы знаете, я не волшебник, я только учусь вот всем этим зависимым типам, давайте я вам немножечко объясню на пальцах, короче.",
    "result": {
      "error": "API request failed: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-xSG9D345gtYlIKC330wCyrEG on tokens per min (TPM): Limit 30000, Used 30000, Requested 7492. Please try again in 14.984s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
      "stack_trace": "Traceback (most recent call last):\n  File \"/home/andrei/Projects/podcast-shownotes/scripts/build_search_eval_dataset.py\", line 157, in generate_search_query\n    response = await self.client.chat.completions.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 1927, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1767, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1461, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1547, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1547, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1562, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-xSG9D345gtYlIKC330wCyrEG on tokens per min (TPM): Limit 30000, Used 30000, Requested 7492. Please try again in 14.984s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
    }
  }
]