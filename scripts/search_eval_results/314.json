[
  {
    "segment_id": "2c4bec9a-bf5b-45c8-a622-6d8c5a0bb940",
    "episode_id": "809d6413-0ae8-444f-9a0f-59f36e2f103a",
    "episode_number": 314,
    "segment_number": 5,
    "text": "Наверное, тогда мы, получается, как-то логично подходим к тому, что вопрос, с которого, возможно, стоило бы начать – зачем вообще нужна рэдпанда, причем такие отличия от кавки, кроме вот уже названного, и, не знаю, архитектурное отличие, идеологическое отличие, наверное, можно, наверное, начать с идеологических. Красная панда нужна, потому что мы делаем компанию и делаем продукты и хотим срабатывать деньги. Как мы это делаем? Мы пытаемся сделать продукт, который работает местами лучше, чем кавка. То есть, мы целимся на улучшение developer experience. Сейчас одна из основных критик кавки с точки зрения управления идет, что это сложно, нужно поставить отдельно зукипер, отдельно кавку, и получается достаточно большая система, которую сложно поддерживать. Мы стараемся разработать систему, которая требует минимального вмешательства, который удобно использовать. С другой стороны, мы хотим сделать ее более быстрой, ну здесь как бы слово правильное не «быстрой», а… Предсказуемой. Предсказуемой и больше, чтобы она выдерживала большую нагрузку. То есть, вот мы говорим, что красная панда быстрее, чем кавка в 10 раз. Ну, как бы на самом деле, если, допустим, на систему давать очень малую нагрузку, то у нас все опирается в айо и скорость красной панды сравнима с кавкой. Интересные вещи начинаются с ростом нагрузки. То есть, мы больше и больше даем нагрузку, и в какой-то момент кавка начинает уходить на экспоненту, данные по latency и for-put, а красная панда еще как бы держится. И это означает, что можно просто на одно и то же железо пустить большую нагрузку, и будет нужно меньше этого. То есть, вместо того, чтобы горизонтально масштабировать, можно масштабировать вертикально, и это более прибыльно. И получается, то есть, допустим, с кавкой... Я просто сейчас цифры беру на бум, но, допустим, если кавке требуется 300 нот в кластере, красная панда может держать эту нагрузку с меньшим количеством нот в несколько раз. И получается, нужно меньше платить за железо, и нужно меньше платить за поддержку этого железа. Мне хочется быть адвокатом дьявола и поадвокатировать за кавку. Просто, когда речь идет про эти бенчмарки, всегда очень важно понимать, какой именно профиль нагрузки. И когда мы говорим, что красная панда в 10 раз больше нагрузки выдерживает, хорошо, о какой нагрузке идет речь. То есть, что у нас есть какие-то цифры, вы можете назвать конкретику? Мы сейчас работаем над бенчмарками, и я не говорил, что она выдерживает в 10 раз большую нагрузку. Она дает в 10 раз лучше лейтензии при более высокой нагрузке, чем кавка. А является ли это проблемой для пользователей? То есть, там лейтензии, если она будет 10 раз меньше, будет ли это на самом деле существенной проблемой для пользователей? Для компаний, у которых большие инсталляции кавки, это проблема, потому что мы можем обеспечить то же самое за меньшее деньги. Я в основном еще хотел пошутить, что я вообще не уверен, что из кавки можно собрать кластеры на 300 машин просто потому что там зукипер может с этим не очень хорошо справиться. Давайте тоже продолжу свое наступление. Просто когда еще делится кавку, мы понимаем, что это такая, что бы стандартной индустрии, но в индустрии вступило довольно много опыта. На рынке есть люди, которые позиционируют себя как эксперты с кавкой. У нас есть такое большое комьюнити, и можно получить ответы на вопросы. И в целом, если я принимаю решение, умею технологию использовать, скорее всего, я пойду сразу на кавку, потому что есть люди, которых можно нанять. Я понимаю, как с этим жить в продакшене. Я понимаю, как пробушить какие-то проблемы. И даже если есть какие-то проблемы с масштабирностью, то это конечно далеко будет не факт, что мы с ними будем сталкиваться. А у нас есть, если что, там компания ConfluentIO, которая даст нам своих solution архитект, которые нам помогут с этим разобраться, если что. Вот убедите меня использовать ваш инструмент. Насколько я понимаю, компании разные, и некоторые предпочитают деплоить кавку у себя и поддерживать ее своими силами. Некоторые используют deployment в Cloud. Например, есть Cloud от Confluent, и мы сейчас работаем над собственным Cloud. И получается, что если компания использует Cloud версию mess.us с протоколом кавки, то для нее нет больших рисков переключаться между Confluent или нами. А если мы можем обеспечить какие-то лучшие показатели, то мы рассчитываем, что они будут использовать нас. Вставай на сторону света. Если все работает, то зачем куда-то шевелиться? Я могу представить, что кто-то услышал про красную панду, когда они ничего не написали, решили хайпануть и поставить красную панду. В идеальном мире ничего плохого не происходит, все продолжает работать здорово. Если у меня уже инсталляция кавки, в которой уже какие-то данные, которые придется мигрировать, возможно, и они в формате записаны на диске, то есть данных могут быть терабайт. И в принципе, скажем, как-то она работает. Почему, если она вроде работает, я должен пойти на красную панду? Или в каком месте у меня начать не работать, и я захочу пойти на красную панду? Это, кстати, Валера, это вообще килерфитча. Если у меня есть проблемы с кавкой, у меня большая инсталляция, и я вижу, как у меня график растут, я могу предсказать, что через шесть месяцев у меня будут проблемы серьезные. Если мне краспанда приходит и говорит, а у меня здесь есть классный ту, который поможет тебе смигрировать, прям все очень круто, быстро, безопасно, вот мы же это сделали другими компаниями, смотри, как он работает, используй нас. Это реально очень сильный аргумент в пользу красной панды. Насколько я понимаю, это именно так и работает. У нас есть, Денис, поправь меня, если я правильно, если я неправильно это все понимаю, но Panda Proxy как раз позволяет сделать так, что вы будете писать, компания будет писать данные в Kafka Cluster и одновременно Red Panda Cluster и вы можете поднять Red Panda Cluster, посмотреть, как именно это работает и увидеть, что это все требует намного меньше вложений и ресурсов и так далее, и просто в один прекрасный момент переключиться, когда у вас и там, и там будут одинаковые данные, так как вы не храните данные вечно в Kafka. Здесь же есть нюанс, подожди, получается, смотри, если у меня большая инсталляция Kafka, это довольно уже много машин и мы речь идет про то, как имеет смысл идти на Panda, когда у нас уже большая инсталляция, у нас масштабируемость становится проблемой, то есть если вы предлагаете сделать инсталляцию Panda и как бы параллельно запускать данные, это значит, что мне придется еще один большой кластер поднимать, что довольно дорого, вот так с этим жить, это тяжело продать. Вот как называется SalesPitch, в том, что он будет стоить сильно дешевле, чем основной кластер и в один прекрасный момент станет очевидно, что вам будет намного проще и лучше с красной Panda, ну либо не станет. А как быть ситуацией, ну просто с Kafka большой аргумент, это то, что ну в принципе добавлять железо это довольно дешево, это те, то есть я покупаю сервер, да, это дорого, но это единобременные сдержки, а если нанимать людей, то их нужно менеджить, это всегда такие постоянные затраты и когда речь идет про Panda, мы не знаем, как она поведет себя в продакшене, мы не знаем, какой у нас есть экспертиза на рынке для меня, а это большой риск, как с этим жить? Ну это на самом деле правильный аргумент, пока что в индустрии очень мало опыта использования красной Panda и именно поэтому, я так понимаю, что мы выложили все в Open Source, чтобы кто угодно мог попробовать, чтобы люди могли побенчмаркать и посмотреть сами и дать нам какой-то фидбэк, ну плюс мы стараемся улучшить дилевольский экспиренс, скажем такой пример, у нас есть инструмент по названиям RPK, который помимо своей основной функции он позволяет настроить машину для максимизировать производительность, отключить merging водо-вывода в ядре или перерывания по ядрам и все такое прочее, и это не нужно конфигурировать, то есть вам не нужно тратить человеческие ресурсы, есть инструмент, который это все делает автоматически, он знает, как быть, например, со стандартными инстанциями на Амазоне и делает все правильно из коробки, в общем, мы стараемся работать над этим тоже. Ну, так сказать, это одна история, потому что если вы будете заходить через девелоперов в компании, это вам нужен, ну то есть это реальный кейс, как можно продавать панду, потому что если девелоперам очень нравится, они приходят с горящими глазами к людям с деньгами компании, которые принимают решение, и это может хорошо сработать. В общем, я так понимаю, что это именно наш кейс, то есть наш CEO Алекс, он рассказал много раз, что мы хотим иметь как можно больше инсталляций и быть максимально user-friendly для комьюнити Java и Python-девелоперов, которые не хотят устанавливать, конфигурировать кучу Java с каких-то сервисов и так далее. И отсюда же идет инициатива с облаком. Света, ты еще спрашивала про Pitch и попытаться тебя убедить, у нас уже есть клиенты, с которыми мы работаем, по-моему, больше чем полгода, и они нам репортили данные, насколько у них скратились расходы на кластер по сравнению с Kafka. Мы как бы со временем будем публиковать эту информацию, но там в несколько раз упала цена. Очень, очень жду, мне кажется, это такие цифры, если вы будете какие-то кейсы от ваших пользователей показывать, это сильные аргументы. А расскажите подробнее, как вы работаете с комьюнити, то есть что вы делаете, чтобы сделать действительно классным опытом для инженеров. Вот вы сказали, что есть, то есть, ну понятно, это общение с людьми, это общение с разработчиками, у вас есть специальные инструменты, которые помогают сделать эту ту лучше и как-то приятнее для разработчиков. А что еще, как вы это вообще измеряете, как вы понимаете, что да, вот это то, что людям нравится, а не то, что вы думаете, что людям понравится. Мы смотрим на фидбек от наших кастомеров, плюс у нас есть цель в компании, например, цель к Developer Experience, что инсталляция красной панды должна занимать меньше минуты от момента, как ты скачал, то есть ты можешь, хочешь попробовать, как она работает, у тебя несколько команд, которые ты запускаешь, и простота",
    "result": {
      "error": "API request failed: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-xSG9D345gtYlIKC330wCyrEG on tokens per min (TPM): Limit 30000, Used 30000, Requested 4813. Please try again in 9.626s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
      "stack_trace": "Traceback (most recent call last):\n  File \"/home/andrei/Projects/podcast-shownotes/scripts/build_search_eval_dataset.py\", line 157, in generate_search_query\n    response = await self.client.chat.completions.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 1927, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1767, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1461, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1547, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1547, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1562, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-xSG9D345gtYlIKC330wCyrEG on tokens per min (TPM): Limit 30000, Used 30000, Requested 4813. Please try again in 9.626s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
    }
  }
]