[
  {
    "segment_id": "4528a637-671e-492e-b97f-95bec2e9a52e",
    "episode_id": "2dbe0e1a-cbbc-4005-92b4-c36949116dc6",
    "episode_number": 101,
    "segment_number": 6,
    "text": "Поэтому что делает Disk Scrabber? Disk Scrabber – он постоянно запущен на ноде, и он постоянно последовательно читает данные и проверяет контрольную сумму этих данных. То есть мы точно знаем, что то, что на диске есть, мы это прочитали, мы проверили контрольную сумму. Можно сразу, пока не уехали дальше, вопрос? Как часто, на какой объем записанных данных вы видите, что разъезжается… То есть, скажем, на каждые 10 терабайт, которые прошли через диск, одна чексума полетела. Есть понятие, от какой цифры? Одна чексума полетела. В смысле, мы пишем, что в среднем за такой объем записанной информации происходит один сбой на винчестере. Ты имеешь в виду, если какая-то статистическая информация происходит? Да, да, да. Как часто? Я, к сожалению, не скажу. Достаточно часто происходит. Ок, сори, что прибил. Да, но вот этот нижний уровень проверяет только реально на ноде записанные данные. Мы проверяем то, что чексумы сходятся. Эта проверка обычно завершается в течение недели. За одну неделю он полностью проверяет одну ноду. Одна нода — это около 5 байт данных, получается. То есть, там 96 жестких дисков по 10 терабайт. Если мне память не изменяет. В общем, там 5 байт данных. Соответственно, новые данные, которые приходят на эту ноду, они более часто проверяются. Потому что новые данные, скорее всего, если ошибки возникают, то возникают при записи этих данных. Поэтому они более часто проверяются. Второй уровень — это треш-инспектор. Что это такое? Перед реальным каким-то перемещением данных, то есть, например, если нам нужно сделать компактизацию данных, эти данные до того, как переместиться, помещаются в некую корзину. Соответственно, треш-инспектор ходит по этой корзине и проверяет, что эти данные действительно были удалены. И тогда из этой корзины можно их будет удалить в дальнейшем. То есть, он помечает их. Если эти данные были удалены в корзине, но где-то что-то пошло не так, и на самом деле они не должны были быть удалены, то этими данными будет заниматься система auto-remediation и как-то их восстанавливать или проверять, почему это произошло. Плюс в этой корзине данные лежат еще некоторое время на всякий случай. Вдруг что-то пошло не так, и мы захотим их оттуда восстановить. К сожалению, я не помню, сколько конкретно они лежат. От одного где-то до 30 дней. Там есть некая зависимость. Третий уровень — это так называемый Extend Referee. Собственно, он проверяет, что все операции с данными, например, перемещение файлов или unlink этого файла, они соответствуют реальным инструкциям. То есть, если какой-то левый процесс вдруг что-то запустит с файлами, то Extend Referee просто ему не даст. Там используется Tomoe Linux, который позволяет ограничивать различным процессам по-моему, сисколы и возможность работы с различными ресурсами. Я не знаю, Саша, ты знаешь про Tomoe Linux? Нет. Нет, а что это? Я другого Александра, который с Docker. Да, я слышал про Tomoe Linux. Это же, по-моему, просто опция в ядре. Да. Даже без патча, по-моему, можно сейчас его включить. Да, это достаточно старый патч. Собственно, здесь мы ограничиваем другие процессы. Дальше, следующий уровень — это сканирование метаданных. И по сути, что происходит. У нас метаданные хранятся в отдельных местах, поэтому мы ходим по метаданным и запрашиваем блоки. То есть мы смотрим, что все блоки есть для конкретных метаданных. И соответственно, как происходят метаданные, хранятся в отдельной базе MySQL. Есть процесс, который заходит в эту базу, смотрит, что есть, например, файл, он состоит из таких-то блоков с такими-то хэш-суммами. Соответственно, мы дальше с системы Magic Pocket запрашиваем эти файлы по их идентификаторам. И система не передает по сети эти файлы, она просто проверяет, что эти файлы есть, она проверяет их контрольную сумму и говорит, что все ок. Таким образом, мы проверяем, что все наши метаданные, они реально есть у нас, все эти файлы. Вот это более высокий уровень. Дальше есть еще один уровень, называемый Storage Watcher. Это система такого черного ящика. Здесь мы исходили из того, что все верификаторы, которые написаны разработчиками самой системы, они могут быть немножко слеповатыми, потому что мы знаем, как система работает, поэтому можем исходить из этих предпосылок. Вот эта система Storage Watcher написана совершенно другой командой, абсолютно левой, которая не знает, как работает наша система, а исходит из того, как она должна работать, то есть как описано, что она работает. И, соответственно, какой-то процент данных, который к нам приходит, он кладется в кавку, хэш-суммы файликов, и вот этот Storage Watcher проходится по этим файликам и проверяет, что действительно они правильно положены, в правильных зонах, и что все работает как надо. Ну и, собственно, самый высокий уровень – это Cross-Zone Verifier, потому что файлы лежат у пользователя на разных зонах. Есть, например, Западная Америка, Восточная Америка, Европа. Вот этот верифайер проверяет, что файлы лежат в нужной зоне, и когда они перемещаются, тоже проверяет, что файлы правильно переместились. Вот, соответственно, вот такое большое количество проверок нужно для того, чтобы мы точно знали, что у нас никогда не пропадет там ни один битик данных. Соответственно, это потому что, как бы, так как мы храним данные, нам очень это важно, чтобы у нас ничего не пропало, ничего не поменялось, никакие битики не испортились у пользователей. Ну и плюс ко всему, к этому добавляется то, что весь любой новый код проходит некие этапы. Сначала проверяется интеграционными тестами в течение месяца, например, а потом проверяется работа этого кода на стейджинге, который обслуживает реальный трафик, ну там какой-то процент этого трафика, вроде кэннери, но на самом деле нет, потому что реальные данные, они есть в продакшене, если у нас в стейджинге что-то случилось, то у нас дальше работает продакшен, а мы стейджинг чиним, например. Вот, и также очень интересная вещь, это различные дизастер рекавери, т.е. мы, так как у нас система такая вот отказоустойчивая, то соответственно все графики каких-то проблем показывают всегда ноль, ну т.е. что все отлично, ничего не пропало, система работает идеально. Соответственно нам нужно самим периодически вызывать какие-то катаклизмы и смотреть, как система реагирует на это. Вот, помимо этого есть еще некое слепое тестирование. Ты же самое главное не сказал, как вы в описанных тобой условиях делаете вид, что у вас диск потерял данные. Делаем вид, что диск потерял данные. Ну чтобы пропустить эти графики, чтобы они не ноль показали. Мы можем просто вынуть этот диск вручную, например, либо просто отключить его софтварно. Но это не интересный кейс, а побить сектор, как можете. Вот, для того, чтобы делать реально какие-то такие интересные вещи, у нас есть такое слепое тестирование. Это когда мы выбираем какого-то человека, который имеет полные права на систему, и этот человек делает в системе различные, ну такой же покляк человек, делает различные проблемы. То есть это уже не просто автоматизированная система, которая ломает, а реально умный человек, который тут, например, диск поломает, тут какие-то битики свернет. И, соответственно, только этот человек знает, что он поломал систему. Все остальные операторы и сама система об этом не знают, и мы проверяем в итоге в конце правильно ли мы все починили и все ли мы нашли. Такое слепое тестирование тоже периодически проводится. Тут уже более умная система происходит. Но, собственно, статья только про верификацию конкретно эта, она не особо много дает деталей именно технических. Наверное, более технические детали будут в следующих статьях этого цикла. Ну и плюс еще в этой статье есть ссылки на видео, где более подробно некоторые части системы рассматриваются. Вопросы, дополнения? Может, у чата есть вопросы? Пока чат думает, я предлагаю передать слово Старцу, который... Я так понимаю, это тема с прошлого выпуска, да? Про PocketWatch? Не знаю, но... Нет, PocketWatch, мы это уже обсуждали. Про верификацию системы? Да, продолжая дальше тему, был недавно доклад от Кэти МакГаффи про какие способы верификации в распределенных системах. И она рассматривала варианты, начиная от того, что ты сказал. То есть запускаем вот эти тесты, вносим, делаем такие фейлор инжекшн, и смотрим, что система реагирует как надо. Еще рассмотрение таких штук, как формальное доказательство и кейсы от Амазона. Вот, кстати, интересно, вот в Дробоксе у вас чем таким народ занимался? Утверждение такое, что Амазон в нескольких своих системах, там есть решительная система, какой-то протокол, по которому они работают, и писали для этого спецификации на TLA+, и проверяли, это прямо в модул-чекере, там всякие проперти, можно проверить лайвлнес и прочее, и находили такие баги, для реализации которых надо было сделать последовательность из нескольких десятков действий, и которые они видели в продакшене при этом, но не могли понять из-за чего они происходят. Проблема была именно на уровне того, что сам протокол неправильный. Интересно, в Дробоксе же система достаточно большая, и вот кто-нибудь в такое игрался? Да, конкретно в Сторидже у нас точно есть математическое моделирование, я, к сожалению, не могу ничего про него рассказать, потому что это не публичная информация. Ну да, там парень, который диссертацию защищал, по как раз таким системам он все это делал. Я не уверен, что есть ли какие-то опубликованные вайтпейперы, поэтому скорее всего нет, но возможно будет в будущем. Просто это реально такое, получается, не ноу-хау, но это технологический ресурс компании, я не уверен, что он будет реально раскрываться, и я не уверен, что Амазон раскрывает это все. Ну да, там самих моделек они, по-моему, не публиковали, ну и это достаточно такая система, даже из статьи было понятно, что она достаточно внутри, то есть это не какой-то продукт. Перекладывание из одного API в другое API, для того чтобы третье API могло работать. Вообще интересно в таком в историческом развитии на это посмотреть, потому что когда Лэмпорт создал, вообще изначально Лэмпорт, который создатель TLA приспособил темпоральную логику для описания систем, для формального описания систем, он изначально вообще не хотел создавать как язычок, как model checker, потому что никогда не пойдет в продакшен, и никто этим никогда не будет пользоваться, а просто плодить сущности рая, того, что плодить он не хотел.",
    "result": {
      "query": "distributed storage verification methods"
    }
  }
]