[
  {
    "segment_id": "f6df960b-c8ea-41d6-8c3e-a98287927041",
    "episode_id": "c1efeadf-3358-4b68-a5a1-5a3ffaf98789",
    "episode_number": 454,
    "segment_number": 5,
    "text": "В отличие от GAN, диффузные модели они итеративные. То есть они работают, но выполняются на изображении не один раз, а от 10 до 50. И здесь я бы хотел привести такую аналогию. Представьте распечатанную фотографию, вы кладёте его на стол, и сверху обычным песком, но тонким слоем покрываете гранату. Так что фотографию не видно. А потом итеративно, шаг за шагом, вы рукой смахиваете песок с фотографией. И она начинает проявляться. Так вот, у диффузных моделей изначально они имеют дело с белым шумом. То есть если мы хотим получить картинку 1024 на 1024, изначально мы имеем белый шум, разбитый на белый, и мы можем получить картинку, изначально мы имеем белый шум размера 1024 на 1024. Потом мы решаем сгенерить картинку из 10 итераций, то есть на каждой итерации мы удаляем 10% шума. То есть первая итерация, мы имеем 90% шума и 10% картинки. Ну и так все 10 итераций, пока не останется только картинка. Это чем-то похоже... Погоди, я не очень понял, а как шум удаляется? Это делает как раз нейронная сеть. Она, если это 10 этапов, ей ставится задача убрать из картинки 10% шума. Технически это выглядит, то что ей нужно предсказать эти 10% шума, которые вычитаются из изначального шума. Вот, не знаю, фактически на первом, после первого применения этой, после первой итерации у нас будет, у нас не будет как такой картинки, у нас будет какое-то 10% изменений. Очень, очень, очень зашумленная картинка, зашумленная на 90%. Там будут видны некие контуры проявляться, и это чем-то похоже, как рисуют художники. Сначала они рисуют контуры, эскизы, потом добавляют детали, кладут цвет, и постепенно получают свою картину. Погоди, погоди, а 10% шума в данном случае что означает? Что мы с каждого пикселя вычитаем какое-то значение, или мы 10% пикселей берем и их как-то изменяем? Да, вот первый вариант. Из каждого пикселя 10% шума. Для меня это. .. Другой вопрос. Давай. Если не ошибаюсь, то... У момента Ганны, они же тоже работают на основе шума. В смысле, начинают с шума. Да, верно. А какое архитектурное отличие? Отличие в том, что Ганны работают в один проход, в одну итерацию. А диффузные модели, они итеративные. Также Ганны, Ганны это соревновательная модель. Там учатся две нейронные сети, одна генератор, который старается сгенерировать реалистичную картинку. А вторая сетка, это дискриминатор, который пытается понять реальная это фотка или сгенерированная. И эти две сетки конкурируют друг с другом. Генератор пытается реалистичную картинку сгенерировать, а дискриминатор пытается определить, реальная она это или нет. Что тогда обучает, что используется для обучения диффузной сети? Да, это хороший вопрос. У диффузной сети там просто берется разница между тем, что мы должны получить, мы обучаемся на датасете, и то, что сгенерировал Ганн. А точнее, мы должны предсказать шум, который мы должны вычесть. Подожди, сгенерировал Ганн? Ой, тьфу! Мы должны... Сама сетка в диффузной модели, она предсказывает на самом деле шум, который мы вычитаем из... изначального шума. И вот мы сравниваем тот шум, который предсказала диффузная модель, с тем, который реально присутствует. Ну там довольно сложная математика. А можно совсем глупый вопрос? Ну от меня как полного дилетанта. А что является входом у нейронных сетей в этой задаче? Это зависит от того, какие задачи мы решаем. Есть сетки для распознавания. Например, нужно распознать, какой класс объекта изображен на картинке. Например, кошка, собака и так далее. Нужно предсказать просто класс, как правило, это число этого класса. Если мы говорим о генеративных нейронных сетях, то на вход поступает просто шум. Это как в GAN-ах и в диффузных моделях. Шум — это то, на что опирается сетка. Она шум превращает в маппет, в картинки. Видимо, кроме шума, должен быть еще один параметр. Как мы знаем, что про шум? Да, есть еще условия. Условия в тексту имидж этим условием является наше текстовое описание того, что мы хотим получить. Но, например, в GAN-ах такого условия не было. То есть в StyleGAN такого условия не было, и он просто генерил рандомные изобнулица. В таких нейронных сетях, как DALI, Milt-Shorten и Stable Diffusion, там действительно на вход подается, во-первых, это картинка, которая изначально является белым шумом, и текстовое описание, которому нейронная сеть должна следовать. Мы остановились на том, что мы имеем 10 стадий, на каждой стадии мы вычитаем шум из... В контекст мы подаем на каждую стадию? Извините, еще такой вопрос. Да, именно контекст на каждой стадии. На каждой стадии, соответственно, мы вычитаем шум из этой картинки, и постепенно начинает проявляться картинка все яснее и яснее. Мне непонятно здесь следующее. Во-первых, когда мы говорим, вот вычитается шум и появляются контуры, фактически получается, что после первой стадии мы уже обречены двигаться по вот тому направлению, которое было выбрано на первой стадии, верно? То есть как бы контуры уже нарисованы, мы не можем после этого... Вторая стадия не может другую картинку, она фактически продолжает первую стадию, верно? Да, именно так. И в этом сила диффузной модели, потому что на ранних этапах нейронная сеть ответственна за контуры, какие-то низкоуровневые черты, а на дальнейших она именно занимается детализацией и добавлением цвета. И да, есть свой путь, и она, начав что-то генерировать, не может переключиться на другую картинку. И чтобы уточнить, на каждом этапе, в том числе, на каждом этапе, всегда одна и та же нейронная сеть? Да, всегда одна и та же нейронная сеть, но еще ей добавляется параметр как этап. То есть если у нас 10 шагов, то есть первый, второй, третий... Она знает какой этап. Я раскрою, откуда у нас есть исходный шум, откуда мы знаем, какой шум должен быть. Потому что мы изначально... У нас есть датасет, а именно эти нейронные сети обучаются на датасете LINE, состоящей из более 5 миллиардов изображений в спаренном датасете, где у каждой картинки у нас есть текстовое описание. И мы берем картинку и последовательно, если мы берем 10 шагов, ну на самом деле берем 1000 шагов для обучения, и тысячу раз на одну тысячную долю зажимляем. И тем самым у нас есть шум на этом шаге. И сколько шума нужно удалить. То есть эта информация у нас есть из датасета. Так, вот здесь я потерялся. Ты можешь как-то перефразировать еще раз, повторить то же самое? Да, мы берем исходную картинку, берем 1000 шагов и тысячу раз добавляем шум. Размер пока и на тысячном шаге мы должны получить белый шум. То есть то, с чего мы потом начнем обратный процесс генерации картинки. Это во время обучения. И это обучение запоминает, какой шум приводит, из какого результата в белый шум. То есть фактически обучение шуму. Да. Да. Да. Интересно. Да. Интересно. Окей. Ты знаешь, звучит очень просто. Мне удивительно, что такая простая система дает такие удивительные результаты. Мне кажется, вопрос в объемах данных и размерах сетей. Кстати, есть какие-то... Ты можешь озвучить какие-то порядки, сколько нейронов в таких сетях, сколько слоев? Да. Да. Например... Есть open source решение и не open source. Open source решение это stable diffusion. Их есть две версии. Первая версия stable diffusion, там порядка миллиарда параметров у нейронной сети. А вторая версия, которая называется stable diffusion XL, у нее три... Ну, порядка трех с половиной миллиардов параметров. А для... Для параметра это float... То есть это множитель на входе нейрона или что мы называем параметром? Параметры это переменные... Ну, нейронная сеть моделируется с помощью... Там моделируется нейрон. У нейронов есть входы. И каждый вход там имеет параметр. Вход может усиливаться или... Ну... Уменьшаться. И это вот как раз описывается числом, которые есть параметры. Я понял. Для сравнения, вот последний stable diffusion XL, он имеет три с половиной миллиарда параметров. Например, у GPT-3 там 175 миллиардов параметров. Несмотря на то, что языковая модель работает с текстом. А... А... Stable diffusion работает с изображением, которое сложнее. Повтори еще раз. Еще раз повтори. Сколько-сколько где там? У stable diffusion три с половиной миллиарда. У chat GPT... У GPT-3 их 175 миллиардов. То есть на два порядка. Больше чем на два порядка. Больше чем на два порядка. Больше чем на два порядка. По исключениям масштаба, это обычная многослойная нейронная сеть. Это правильно? Под капотом, на самом деле, chat GPT-3 и stable diffusion, ну, я думаю, также и дальние mid-journey, там у них одинаковая архитектура. То есть они работают на трансформерах. На attention-слоях. Так. Неудивительно. Да. Да. Да. Когда ты начинал, ты рассказывал про лица и GAN сети, а сейчас мы говорим больше про DALI, то есть это уже трансформеры. Между ними по качеству большой разрыв? Визуально, ну, визуальные привлекательности, они соизмеримы. Но GAN-ы, они узкоспециализированные. Они могут... Ну, например, только лица. Или только транспорт. А диффузные модели за счет итеративности, они могут, ну, произвольно фотогенерировать и работают, ну, с условием текста. То есть текстового описания. Понятно. То есть они просто шире в применении, а так по факту и то, и то дает одинаковые результаты. Да. С точки зрения, да, привлекательности. Качество, да. Ну, плюс-минус. Слушай, объясни мне, почему проблема у DALI с пальцами? Ну, это связано с тем, что в датасете недостаточно представлены фотографии с пальцами. И эта проблема решаема. Решаема с помощью обучения, то есть сохранить датасет, где присутствуют пальцы в большом количестве. Да. Да. И, конечно, пальцы в большом количестве взять в нейронную сеть и дообучить. Да. Проблема с этим, да. Слушай, но... Извините, но пальцы в большом количестве звучат так, как будто бы мы в Петербурге. Окей. Ладно. Закончили с пальцами, закончили с DALI. Stable Diffusion это что? Stable Diffusion это также диффузная модель. Но ее отличительное свойство то, что она open source. И вокруг этой модели собрано большое сообщество разработчиков и дизайнеров. И оно активно развивается. Мы до этого говорили про ганы и про трансформеры. А диффузная модель это что? Диффузная модель это вот итеративный подход, где мы итеративно удаляем шум. Ага, да-да-да, понял. Да, сегодня вокруг этой модели большое сообщество. И на просторах интернета, чтобы пользоваться этой моделью, не обязательно иметь там... Уметь иметь знания в области машинного обучения или уметь программировать. Существуют программы, которые позволяют запустить эти нейронные... Ну, эту нейронную сеть на своем компьютере. Но слухой это три таких есть, которые активно используются. Это автоматик, наиболее низкоуровневый, но который предоставляет больше всего возможностей. Это также фокус, который старается быть простым, как mid-journey. Но при этом давать максимальное качество. И третий это Comfy UI. Это архитектура, построенная на нодах и очень легко встраивается в свои проекты. То есть очень приятный код для использования в своих проектах, так скажем. И благодаря большому сообществу... Мимо тех возможностей, которые предоставляет, например, mid-journey, это генерация с помощью текста, upscaling или вариация, есть много других возможностей. И я пройдусь по ним по нескольким штук. 8-7. Погоди, прежде чем мы пойдем, давай вернемся назад немножечко. То есть mid-journey это что? Mid-journey это тоже генеративная нейронная сеть. На базе диффузной модели. Она не open-source, но она доступна через Discord. Ее нужно зарегистрироваться. Насколько я помню, сейчас она платная. Да, она платная. А почему она так хорошо понимает текст? Она как бы... Ну то есть как бы я сравниваю ее по качеству понимания текста. Ну он с DALLA сравним. Да. Как они смогли этого добиться? Ну это закрытая модель. Там куча слухов наверняка ходит, расскажи нам их. Да, у них используется, например, обучение с подкреплением под reinforcement learning from human feedback. Ну то есть она на последнем этапе обучается от обратной связи человека. То есть что это значит? Если вы работали с Discord, то она генерирует не одно изображение, а как правило четыре. А потом юзер выбирает одно из четырех, он решает ее или сохранить, или там скейлить, работать с ней дальше. Эту информацию можно использовать. То есть человек из четырех изображений выбрал одну, то есть решил, что она самая визуально привлекательная. И как раз используется вот этот метод этого обучения reinforcement learning, который позволяет использовать вот эту обратную связь от людей. Напомню, что такое reinforcement learning. Это обучение без датасета, где агент, то есть наша нейронная сеть, обучается в какой-то одинаковом виде. То есть в какой-то одинаковом виде. И получается в какой-то динамической среде. И получает обратную связь от этой среды и вознаграждение. Это может быть игра в шахматы, это может быть игра в доту, это может быть какой-то манипулятор, который ставит задачу, например, открыть холодильник. И агент, исследуя мир самостоятельно, на первых этапах возможно случайно обучается этому без датасета. А в случае с Мидлёрной или же там ChatGPT этой средой является вот как раз обратная связь от людей. То есть получается, что они с пользователей берут деньги за то, чтобы получать результаты и одновременно на пользователях же учатся, чтобы улучшать качество своего продукта. Да, также самое, я думаю, и с ChatGPT. Который дает возможность выбрать продукт. Да, также самое, я думаю, и с ChatGPT. Который дает возможность выбрать продукт. Ну, кстати, там вроде есть лайк, выбрать ответ, который понравился из нескольких вариантов. Да. Никогда не пользовался лайками, поэтому от меня они не получают такого ФПГ. Ну вот, продолжая про Прим, те наработки, созданные сообществом вокруг... Ну, сообществом вокруг... Зддай, played it. Рассмотрим, что у нас есть внутри. Ну что ж. Что у нас на экране. Вот, neue baldusta получилась уже-таки, да. Не нужно откisyечь, скачать и про рекормных моделей в 2К. Это уже очень уровень безопасности. 7elim midnight,urg Press respond, я, наверное, буду в портфеле enjoyment, кстати. Просто, для чего бы Brockee не конкцентировал, это реально чё? Да,соро Straßenbl Vietnam и Buziered. Рекомендую освещ Mediaád программа protecting Australia, потому что мы как правило, все равно раны в Bien Flying, значит, почти бесконечно. Закрасить какой-то, ну, зарисовать какой-то объект внутри изображения. Например, вы сделали селфи, и нужно убрать людей с заднего фона. Очень бывает полезно. Или outpainting. Это изменить, ну, добавить, изменить как бы соотношение сторон. Например, у нас есть квадратное изображение, а сделать его вертикальным. Добавить сверху чуть-чуть и снизу. А далее есть такие подходы, как дообучение нейронной сети, fine-tuning, лора. Вот, кстати, предыдущая по удалить объектам и так далее, это ты говоришь про подход, верно? А где он уже есть? Это уже есть в такой программе, как автоматик и confine. Как я уже говорил, это программы, которые дают возможность пользоваться Stable Diffusion на своем компьютере. И эти программы дают веб-интерфейс, то есть вы работаете через браузер. Там фактически чат. Что извини? Там как чат общаешься, как в биджоне. Нет, нет, там есть браузер. Буи интерактивный, то есть можно загружать свои картинки, есть кнопочки, есть места, где отображается результат и так далее. А стоит еще отметить, наверное, тем, кто захочет использовать это, там можно использовать графические ускорители минимальные, это 3-4 гигабайта. Но можно и использовать. .. Использовать... Использовать... Использовать... Эти программы без графического ускорителя, но при этом генерация будет медленнее на CPU. Слушай, прикольно. Надо обязательно добавить ссылки на эти приложения, где это можно попробовать. А разница между... Ну, скажем, вот у меня дома стоит этот Mac Mini на M3. Он насколько быстрый по сравнению с какой-нибудь... А, и у меня есть видюха в этом 4060 в NVIDIA. В лэптопе. Какая разница производительности будет между ними? На вскидку. Не знаю, порядок, два порядка. А между CPU или между чем из чего можно точить? Между M3 и NVIDIA 4060. А, ну, насколько мне известно, M3, там они заточены под GPU и неплохо с ним работают. Ну, если брать обычный процессор без графического... Графического ускорителя и какой-нибудь простой графический ускоритель, там разница будет, ну, раза в 10, наверное. Понятно, то есть порядок. Окей, понятно. А, так что есть такие подходы, как Pintuning или LoRa обучение. Не все концепты может... Table Diffusion может снимерить не все концепты. Например, он не знаком с вами, потому что он не обучался на вас. И вы не сможете сгенерировать себя, потому что вас не было в депосете. Или какого-нибудь нового, не знаю, нового изобретения, которое не присутствовало в депосете. На этот случай можно модель дообучить на своих данных. А для этого также есть веб-интерфейс. Для этого не нужно. Нужно умение программирования или навыков компьютерного... Ну, навыков машинного обучения. Например, один из таких известных – это Кахия, где можно загрузить свои фотографии, например, можно загрузить себя, и дообучить нейронную сеть на себе. И тем самым эта дообученная нейронная сеть будет понимать, кто вы, и если вы подойдете... Если вы подойдете на вход какой-то, ну, текстовое описание с собой, например, если я применю Айнур на Луне, то он сможет... или Айнур в скафандре, он сможет это сгенерировать, потому что он понимает, что такое Айнур, мою концепцию. Да, также благодаря этому открытому сообществу вообще существуют большие онлайн-платформы, где люди делятся своими моделями, своими промпами. Например, один из таких – это Циви Таи, где уже тысячи, ну, сотни таких дообученных моделей под разные сценарии. Например, есть модели, которые обучены на реалистичность, то есть нейронная сеть будет генерировать очень реалистичные фотографии. Или, например, нейронная сеть, ну, в Stable Defusion обучена на логотип, или на определенных персонажей какого-то фильма, или на... какие-то концепты и так далее. Это можно найти через поиск, это можно использовать как для вдохновения и так далее. Слушай, вот ты перечисляешь много всего, но как бы... Если я установлю одно из этих трех приложений, о которых ты до этого говорил, там это все уже будет в виде плагинов? Или как это делается? То есть Photoshop, я понимаю, я открыл, вот у меня есть пункт меню «эффекты», и я пошел по эффектам, применяю что-то. А здесь все эти вещи, насколько я понимаю, они все-таки не в одном плаконе, не в одном приложении, не как-то... Да, хорошее замечание. Да-да-да, действительно так. Из всех трех — это автоматик, который поддерживает практически все, и любую модель можно будет запустить на нем, но одновременно он самый низкоуровневый. Ну да, чтобы все это потыкать, попробовать, я думаю, стоит начать именно с вот этой программы, с вот этой программы «автоматик». А есть вторая программа — это «фокус», где нету никаких расширений, то есть дополнительно к нему ничего нельзя будет добавить, даже с моделями она тоже ограничена. Есть «комфайл» — это что-то среднее. Ну, я думаю, стоит начать именно с «автоматик». Но при этом тоже скажу, если нужны максимальные качества изображения, то это «фокус». Они заточены на наиболее привлекательные результаты генерации. Понял, понял. Пошли дальше. Так, также освещу такой момент, в которых, например, нету «image-journey». Это с помощью такого расширения, как «control-net», мы можем управлять генерацией по пикселям. Например, с помощью текста мы не можем опускаться на уровне пикселей. Мы можем что-то описать абстрактно, словами, поверхностно, но мы не можем задать, в какой части изображения должен появиться такой-то объект, или, не знаю, какого оттенка. Это можем, но такие низкоуровневые вещи не можем. А «control-net» позволяет, например, сделать следующую вещь. Мы можем в редакторе на белом фоне нарисовать наброски, эскизы от руки и подать свои эскизы-наброски на вход вот этого «control-net». И генерация «stable diffusion» с «control-net» будет генерировать так, что результат будет соотноситься с нашими эскизами. Мы можем нарисовать свой дом, какое-нибудь животное или человека, и генерация будет совпадать с этим эскизом. Например, также есть алгоритмы, которые предсказывают контуры на исходном изображении. Это, например, «canny», «head» и так далее. Это мы тоже можем использовать. А «control-net» на вход может принять контуры и сгенерировать такое изображение, на котором будут точно такие же контуры. Например, мы можем взять фотографию нашей комнаты, посчитать контуры и подать на вход «control-net». А в «canny»... В качестве «pronged» задать дизайн, который мы хотим. Например, мы хотим обои определенного цвета, полы определенной текстуры, мебель и так далее. И можем посмотреть, как будет выглядеть наша комната в таком-то стиле. Также это относится к архитектуре. Тоже можем посмотреть на свой дом. Звучит вообще офигенно. Да, это действительно хорошо работает. Также по входной картинке, например, по входной картинке человека, мы можем определить абстрактный скелет. Например, есть такой алгоритм, нейронная сеть «OpenPos», который определяет скелет человека. А этот скелет человека мы можем отрисовать на белом фоне и также подать на вход «control-net». И при генерации «Stable Diffusion» с этим скелетом мы получим на выходе человека, который имеет... ну, такое же позиционирование, как и наш скелет, который на вход мы подавали. Это тоже может быть полезно, если мы, например, рекламуем для фитнес-клуба, где человек бежит, где можем точное положение человека задать, точное положение на картинке. Также есть алгоритмы, которые по фото сегментируют его на семантические классы. Например, здесь у нас речка, эти пиксели принадлежат речке, здесь у нас небо, здесь лес. И вот эту семантическую сегментацию мы также можем подать на вход «control-net», и выход генерации будет соотноситься с этой сегментацией. То есть мы можем управлять. Семантическая сегментация – это значит, что каждому классу у нас будет какой-то цвет. Например, за воду будет отвечать 200 грамм, а за воду будет отвечать 255, 0, 100, RGB цвет и так далее. Также по входной фотке мы можем определить такие параметры, как глубина, карта нормали и так далее. Есть там устройства, которые могут выдать нам глубину дев камеры. Эту информацию мы тоже можем использовать. Например, iPhone. Да, например, iPhone. Эту информацию мы тоже можем использовать и «control-net». Есть что-то для телефонов? Вообще, это кажется, такая самая логичная вещь. То есть у тебя на телефоне есть эта информация, там неплохие эти установленные CPU, и все это можно делать на телефоне, нет? Это можно сейчас сделать в автоматике, но на телефоне я на самом деле не встречал. Но это отличная идея на самом деле. Ну вот, собственно, наверное, это основные. Но «control-net» ты сам можешь обучить, то есть взять любое свое, ну, придумать сам какое-то изображение, которое описывает сцену, и обучить вот этот вот «control-net», чтобы он влиял на генерацию Stable Diffusion. Вот у меня в компании тоже порядка 5 таких обучений. Далее. Далее тоже хотел рассказать про варианты, ну, то есть Stable Diffusion. Ты можешь на вход Stable Diffusion подать картинку и сгенерировать вариации этой картинки. То есть у тебя есть какой-то уже результат, который тебя удовлетворяет, но ты бы хотел, а вдруг где-то поблизости этой картинки есть еще лучше, или можешь получить вдохновение. И да, есть такой подход как IP-адаптер, который позволяет нам, вот, также получить картинку, ну, готовую. Он по этой картинке как бы описывает ее текстом высокоуровнево. Этот текст подает вместе с нашим текстом, ну, вместе он конкатенирует его уже на вход Stable Diffusion, и он учитывает и текст, который мы написали, и картинку, которую мы подали. То есть он учитывает их в равных долях, и на выходе получается вариация. Ну, что-то среднее между входной картинки и нашего текстового описания. Далее есть такие подходы как Region Prompter. Он позволяет нашу картинку делить на части и для каждой части задавать свой промпт. Это позволяет более детально описывать картинку. Например, в верхней части мы один промпт используем, в нижней части другой промпт, и более подробно, детально, описали сцену. И последние два, которые я хотел бы посвятить, это в этих программах, Automatic, Confi и Focus, из коробки доступны алгоритмы улучшения качества Super Resolution, такие как Codeformer, Real ESRGAN и GFPGAN. И также из коробки доступны алгоритмы DeepFake, где мы меняем лицо, то есть мы сгенерировали человека и можем поменять лицо, идентичность человека на любой фотке. Ну, в общем, вот осветил такие продвинутые методы использования Stable Diffusion, которые, например, нет в Midjourney, но которые доступны с помощью, там, Automatic, например. Но это при этом надо иметь хорошую видюху. Midjourney это просто в чате спросила, и оно тебе ответило. Да. Ну, вообще, ограничение там 2-3 гигабайта, но я думаю, с этого уже можно работать, это можно рассчитывать на 5 секунд на фото, там, с 3 гигабайтами. Такой вопрос. Может быть, он не совсем в чем, потому что про видео, но мое понимание, что видео это последовательность картинок, поэтому, может, ты меня поправишь. Вот эти видео, Гарри Поттер носит Balenciaga, это какими алгоритмами генерируется? Это тоже диффузные модели, это генеративные нейронные сети, где на вход подается текст, а на выходе уже согласованное видео. На выходе это тоже череда изображений, но алгоритм подразумевает то, что они будут согласованы во времени. Видео можно также сгенерировать с помощью Stable Diffusion, то есть тоже открытое сообщество разработало решение, которое можно использовать в автоматике, но видео будут как бы там, согласованности не будет, будет как бы мельчайшее, мельчешение кадров. Потому что каждый кадр, он отдельно сгенерирован и не знает друг о друге. А в случае согласованных видео, что за это отвечаешь? Предыдущие кадры, предыдущие кадры подаются на вход к следующему, к какой-то меньшей размерности вход или как? Я не являюсь экспертом в этой области, но да, решение, про которое вы говорите, да, оно тоже используется. А еще есть вот эти видео, которые... У тебя есть видео, например, ты его переводишь на английский язык, то есть генерируешь какой-то звук, но при этом синхронизирует в видео, исходном видео синхронизируется движение губ под новое аудио, которое ты получил от переводчика. Какие алгоритмы занимаются вот именно lip-sync? Так... Ну, здесь можно использовать как ганы, так и диффузные модели. Ну, это зависит, как бы оба модели подходят. Единственное важное, что на вход подается... Наверное, аудио подается, и при генерации используем... Это учитывается, аудио соотносится с движением губ. Понял. И еще один вопрос от меня. Для заинтересованных слушателей, которые вот темой заинтересовались, что порекомендуешь почитать? Есть ли какой-то прям... Ну, в идеале прям совсем такой, знаешь, фундаментальный текстбук на тысячу страниц, где есть все? Ну, по поводу книг... Вот эта вот область, она вот куется прямо сейчас, и мне кажется, книги отстают. Почитать я бы... Советовал бы пройти курсы. Есть курсы Стэнфорд, есть курсы на Курсера. Ну вот. Да, наверное, так. Много обучающих видео в Ютубе. Книгу... Книгу по диффузным моделям. Я не сталкивался. Понял. Еще не написали. Ну, может, ты напишешь. Может, написано, может, я не знаю. Понял, спасибо. Также я могу опросить... Все очень здорово, и... Да, и этот, как его... Я, если честно, смотрел недавно несколько видео, как это можно использовать, ну, типа нарезки, как люди используют. И мне кажется, что большая часть людей, которые вот вне этого пузыря существуют, твоего пузыря, которого мы сейчас немножечко касаемся, они вообще не представляют себе, что сейчас можно делать. И для них это вообще полная магия. Расскажи твое... Ну, то есть, как бы я понимаю, что, когда такое происходит, какая-то новая технология приходит в мир, это может много чего поменять. У меня сейчас знакомый стартует компанию, ну, как стартует сейчас, он уже это делает несколько месяцев, когда они рисуют для разных брендов... Они делают приложение, которое ты, как... модельер или дизайнер, или еще что-нибудь такое, делаешь скетч там карандашом, а оно с помощью Stable Diffusion делает тебе готовые фотографии реалистичные, как эта модель будет выглядеть. И ты можешь сказать, так вот здесь вот это коричневым цветом, а вот здесь из кожи, а не из там предыдущего материала. И вот это все. И оно идет как-то, летит у него как-то этот стартап. Я все хочу его в гости зазывать. Вот, но я понимаю, что это все только начало. Какие большие... изменения в современных приложениях, в современных программах, в современных способах взаимодействия с компьютером там? Что ты видишь? Какие изменения это влекут за собой? Вот то, что ты сейчас рассказываешь. К чему это приведет? Я вижу аналогию. Например, если посмотреть в профилы, были большие изменения, например, когда был изобретен трактор. Мы получили... возможность вспахивать быстрее, качественнее. Человечество в количестве... начался экспоненциальный рост. При этом, наверное, ручной труд пропал и так далее. Сейчас искусственные нейронные сети претендуют на то, чтобы помогать заменить интеллектуальный труд. И также некоторые профессии будут... пропадать, менее востребованы. Появятся новые профессии, будет сильно расти производительность. То есть, я думаю, искусственный интеллект будет... будет присутствовать просто в каждом гаджете и станет повседневным помощником. Это вот как карвис в «Железный человек». Ну, собственно, вот. Ты знаешь, с одной стороны... звучит правдоподобно, с другой... Вот я вечерами сейчас... У меня есть Pet Project. В этом Pet Project нужно развести печатные платы. И в этом проекте их больше одной. Ну, то есть, у тебя есть... электрическая схема, электронная схема принципиальная. Тебе нужно ее перенести вот как... как рисунок, да, на двухслойное платье. И я этим вот занимаюсь вечерами. Понимаю, что... процесс достаточно такой рутинный, не сказать, что прям безумно творческий, и вообще-то мог бы быть прекрасно автоматизирован. Но вот по состоянию на январе 24 года, насколько мне известно, этот процесс, он нифига не автоматизирован. Нужно прям очень много ручной работы предлагать. Прям ты реально на одну плату у тебя можешь уйти там... ну, 8 часов в субботу. И... Мне это видится тоже как в некотором роде область искусственного интеллекта, хотя не связанной с нейронными сетями, поскольку... Это скорее, знаешь, из области поиска на графах. То есть у тебя состояние твоей печатной платы, это твое состояние на графе, и ты, когда делаешь изменения в печатной плате, это перемещение по... дуге. Как это правильно называется? Я уже все забыл. Дуг... Нет. Что у нас связывает узлы графа? Дуги? Эйч... Дуги, да? Вот, то есть... Или ребра. Или ребра, да, спасибо. То есть, в принципе, это тоже задача из области искусственного интеллекта, хотя не связанная с нейронными сетями. И... Вот... Было бы здорово, если бы кто-нибудь хорошо, качественно решил для начала более простую задачу, чем... Ну вот, например... Бытует мнение, что скоро профессия программистов вымрет. Я в этом очень сильно сомневаюсь, а вот... Профессия... Как это правильно называется? Он не... Не разводчик плат? У этого тоже есть название, но я под вечер очень долго соображаю. Вот я понимаю, что эту задачу, ее вполне можно... Ее решение автоматизировать. То есть это вопрос... Это будет ценный продукт, за это будут готовы платить. Не знаю, какой-то у меня... Просто из меня вырвался поток сознания на предмет, что неплохо бы, если бы люди для начала решили задачи попроще, которые тоже востребованы и точно имеют рынок.",
    "result": {
      "error": "API request failed: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-xSG9D345gtYlIKC330wCyrEG on tokens per min (TPM): Limit 30000, Used 30000, Requested 14049. Please try again in 28.098s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
      "stack_trace": "Traceback (most recent call last):\n  File \"/home/andrei/Projects/podcast-shownotes/scripts/build_search_eval_dataset.py\", line 157, in generate_search_query\n    response = await self.client.chat.completions.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 1927, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1767, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1461, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1547, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1547, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1562, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-xSG9D345gtYlIKC330wCyrEG on tokens per min (TPM): Limit 30000, Used 30000, Requested 14049. Please try again in 28.098s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
    }
  }
]