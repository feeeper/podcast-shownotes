[
  {
    "segment_id": "d7fe14f9-17c3-493b-8695-448c3ff52b4c",
    "episode_id": "549d427b-ee49-43d2-b570-b8d49923902f",
    "episode_number": 36,
    "segment_number": 5,
    "text": "Продолжая тему митапов, скоро в Москве должен пройти митап Go Lang, я так понимаю, с датой ещё не определились. Идёт обсуждение на эту тему, в том числе есть список рассылок Go Lang-Russian, там тоже можно поучаствовать. Вот, ссылка будет в Show Notes, там можно со всем ознакомиться. Продолжая тему митапов, то ли в комментариях на DevZone, то ли мне на почту написали организаторы конференции, называется conformato.org. Она такая своеобразная, это конференция для фрилансеров, для стартаперов и так далее. И она как-то связана с инвестициями, вот со всем этим. Если вы фрилансер или стартапер, то, возможно, вас эта тема заинтересует. Да, у нас можно пиариться за бесплатно в выпусках. И, наконец... Может, я тебя перебью? Конечно. Пиариться за бесплатно и Go. Если кому-то интересно, у нас в Интеле активно и серьезно занялись Go. Не знаю, про это публично много еще не рассказывали, но если кто-то следит за вакансиями в московском Интеле, то у нас есть вакансии, чтобы заниматься Go. Подожди, подожди. А что вы делаете с Go? Компиляторы. Так вы на Go пишете или делаете компиляторы на Go? Компилятор для Go. То есть вы не Go программистов нанимаете, вы нанимаете хардкорных сишников? Чтобы оптимизировать Go, нужно и компиляторщиком быть, и, с другой стороны, понимать, чего ты хочешь оптимизировать. И тесты писать? И тесты писать. И документацию. Да. Слушай, а цель какая? То есть это вы решили вскочить на этот поезд Go, то есть скоро надо ожидать того, что вы будете писать компиляторы для Rasta тоже? Нет, ну у Интел всегда одна задача, когда Интел начинает заниматься софтом, чтобы этот софт хорошо работал на интелском железе. Просто это некоторая активность в Open Source. Все, собственно, Go у нас же двигает компания на букву «Г». Ну, у нас ребята решили, почему бы не помочь ребятам из этой компании и заняться этим. Вообще, я был вовлечен слегка в эту активность и погрузился чуть-чуть в Go с точки зрения компиляторщика, и меня прям как будто бы холодной водой окатило. Я узнал, что есть совершенно другой мир, что люди смотрят на компилятор как на программу, которую они пытаются. .. То есть они смотрят не тем взглядом, которым я привык смотреть со стороны C++ разработчика, который оптимизирует программу до последнего. В Go же главный принцип, что мы компилируемся быстро. И если вы взглянете внутрь компилятора Go, вы поймете, что там оптимизации практически нет. Go сделан так, чтобы скомпилироваться в мгновение ока и ничего не оптимизировать. Это прям странно. Но это не странно, если у тебя проект большой, то действительно, почему бы не хотеть собрать его быстро? Ну да, это требование исходит от Google, у которого строчек на Go много-много-много нуликов, в этом числе, и они хотят, чтобы при каждом коммите они могли перекомпилировать полностью всю их базу кода и проверить ее. Отсюда, собственно, и требования к быстрой компиляции. Но это требование немножко удивляет, когда ты приходишь в этот мир со стороны, когда ты привык, что подумаешь, что пару минут покомпилируешь, а то быстро будет работать. Если честно, меня вообще удивляет альтруизм компании Intel, потому что вот есть компания Google, которая пытается максимально себе облегчить жизнь, и внезапно компания Intel решает, а почему бы нам не помочь Google облегчить себе жизнь? Я вот это вообще как-то не понимаю. Я думаю, тут дело не в альтруизме. Какие процессы будут покупать Google Arm или Intel? Совершенно верно. Если встает вопрос, какие мы хотим, какие чипы хотят покупать кастомеры для своих дата-центров, естественно, те чипы, на которых хорошо работают программы, которые они будут пускать. И если на Arm как-то работает так себе, то мы в безопасности. Если на Arm хорошо работает, тут возникают вопросы. А что, что вы тогда последние 10 лет не пилите GCC, для того, чтобы C быстрее работал? Да ты что не пилил GCC? У нас в Москве сидит н-ное количество человек. Но это количество намного меньше, чем разработчиков собственного компилятора, который не используется широкой общественностью. Это количество людей, которые пилят LLVM и GCC, в Intel увеличилось заметно за последние несколько лет. Так что я тут с тобой не соглашусь. Ну понятно. То есть в последнее время Intel поняла, что Arm может их догнать, и поэтому решила ускориться и улучшить пользователям свои ощущения на данной платформе. Вот кстати, Дима, что ты вообще думаешь по поводу Arm? Arm с точки зрения архитектуры, это тоже довольно чудная архитектура, скажем так вот аккуратненько. Мой бывший босс пошутил, что он знает одну архитектуру, которая не была поглощена x86, это Arm. Единственная причина, почему это не случилось, потому что референс мануал по Arm толще, чем по x86. Так что сама архитектура, она не такая прекрасная, на мой взгляд, как про это принято говорить. Например, там векторные инструкции реализованы из рук вон плохо. Если мы пытаемся какие-то тесты запускать на Intel векторном железе, то мы можем увидеть ускорение в 4 раза на SSE, это нормально. На Arm бывает все похуже. Еще интересная история с Arm против Intel в серверах. Arm грозился прийти в сервера, всех порвать, грозился, если я ничего не путаю, в 2011 году. Говорил, сейчас 64-битную платформу запилим и все. С тех пор прошло много времени. Я помню, что в конце прошлого года я смотрел публичные слайды для инвесторов компании Arm. Они говорили, откладываем, откладываем, 17-18 год планируем для победы на серверном рынке. И до сих пор не случается. Intel тут гораздо эффективнее. А не может это быть артефактом того, что Arm просто работает на тех, откуда у них деньги появляются, а деньги у них от мобилок. И за сервера прямо сейчас никто деньги не заносит, мобилки это очень такой vibrant рынок. И там очень много сейчас денег, поэтому они вынуждены свои ресурсы фокусировать на обслуживании того места, которое приносит деньги, вместо того, чтобы пытаться инвестировать в сервера, что может быть даст профит когда-нибудь. Здесь можно я, Дим, тебя немножко... Может быть, это не сама архитектура. У меня есть еще один гость на примете, который, возможно, сможет очень хорошо на эти вопросы ответить. Так что давайте вкратце сейчас ответим, но вообще я отдельно планирую поговорить. На каком-нибудь из следующих выпусков. Сейчас я тоже кое-что хочу добавить. Взять тот же базворд, который сейчас надувается, это Internet of Things. И, мне кажется, Arm там будет очень хорошо себя ощущать. Дим, скажи что-нибудь. Да, скажу. На мой взгляд, те исследования, которые я видел внутри компании, говорят о том, что именно в самой архитектуре не так много всего подходящего для low-power в Arm, по сравнению с x86. Это просто конкретные реализации чипов, которые лучше работают в Internet of Things, в Embedded, в Mobile. Но Intel очень активно двигается в сторону малопотребляющих чипов. Я тут даже, не скрою, удивлен тому, что мы релизим. Кюри недавно зарелизили, объявили, по крайней мере. Это такой маленький чип, который буквально в одежду можно вшивать с таким энергопотреблением маленьким. Замечание про то, что Arm фокусируется на мобильном сегменте. Да, это правда, но в мобильном сегменте векторные вычисления тоже очень актуальны. Когда мы хотим какую-то логику для игр считать, графику считать, не только на GPU, но какие-то вещи для графики считать и на CPU, это тоже важно, векторные инструкции. Они у них так себе. И завершая тему конференции, 15 апреля, то есть уже на этой неделе, дорогие слушатели, пройдет C++ пати, организует это Яндекс, организует в Минске. О, круто, на мой день рождения. Да. Тебе исполнится в 18? Угу, в 17. Начинается все это в 7 вечера, поэтому можно успеть после работы, конечно, если вы не в Москве живете. Будут интересные доклады про LLVM и про использование C++ для низкоуровневой платформозависимой разработки. Дим, ты по подобным вещам ходишь? Ты вроде как целевая аудитория... Я вообще целевая аудитория, потому что и C++, и LLVM, это вообще полностью про меня, и я бы там, наверное, как раз мог бы рассказать чего-нибудь даже. Я бы с удовольствием пошел, будь я в Минске, напомню, я в Москве. Ну, это недалеко. Ну, сколько там, часов 8 на машине, уже там. Очень близко, кстати, я ездил. 5 часов, если, ну, там, доехать до аэропорта, долететь самолетом и все такое. Ну, на машине, если лететь незренько-незренько, тоже недолго. А следующая тема Ванина? Это тема, то, что Амазон объявил, про то, что они выпускают эластик файл-систем. И я, если честно, немножко удивлен. И тут вопрос, наверное, больше на обсуждение. Валер, ты не читал эту новость? Нет пока. В целом они говорят, что мы делаем эластичную файловую систему, которую вы можете маунтить на большом количестве инстанциев. Они не сказали точно ограничения, но сказали, что счет может идти на тысячи инстанциев, где вы можете подмаунтить ее. Она поддерживает NFS версии 4 протокол. И они говорят, что прозрачно скалируется, очень-очень производительно и все такое прочее. При этом там цены, удобство и прочее-прочее. Я пока не пробежался полностью по спецификации, потому что это вышло сегодня, и я не успел почитать. Но в целом мне это очень напоминает уже фактически базу данных. То есть у тебя есть база данных, которую ты просто монтируешь по какому-то протоколу, а дальше ты гарантируешь какие-то свойства. В данном случае мне интересно, как они запись в один файл, скажем, будут конфликт решать и так далее. То есть NFS версии 4, он решает эту проблему или нет? Мне кажется, насколько я помню, в NFS было несколько вариантов разрешения всякого такого. В частности, по-моему, там есть и локи, или... Честно говоря, точно не помню. Но не на тысячи же серверов. В смысле? Там просто в протоколе предусмотрены способы недопущения. По-моему, там все-таки захват WriterLocker. А как ты захватишь на тысячу нод? В смысле на тысячу нод? У тебя каждый файл хранится... У тебя каждый файл, за каждый файл, в общем, есть строго один сервер. Нет, они сказали, что у них есть в том числе репликации. То есть ты можешь и какие-то копии хранить, и тоже в них ввести запись. Нет, смотри, запись ты можешь ввести. Смотри, как. У тебя обычно любая распределенная файловая система, там будет CF, люстра или что-то такое. У тебя есть Data Servers и есть Metadata Servers. Data Servers хранят просто блоки, к которым ты можешь иметь доступ, как хочешь. Координация при этом производится через Metadata Servers, которые могут быть реплицированы, например, с тем же самым Pactus, и у тебя за каждый набор, за каждый файл, каждый файл привязан всегда к какой-то группе реплик. То есть ты можешь совершенно классическим образом захватить там блокировку. Все понятно. Полноценная база данных, распределенная, Pactus, шардирование, и плюс еще реплицирование данных на каждой ноде для того, чтобы читать быстро. Вебскейл. Вебскейл, да. Нет, а я думал, что мы всякие базворды произносим, нет? Хватит издеваться, давай дальше. Окей, следующая новость, это моя любимая тема. Что в Европе стал продаваться смартфон Aquarius E4.5 Ubuntu Edition. То есть это в Европе продают смартфончики с Ubuntu на борту. Вот в ссылках будет ссылка на саму новость, на видео, где делается анбоксинг этого девайса. Но видео, оно чуть раньше появилось, потому что на какой-то конференции их продавали или раздавали, не знаю. У меня вот вопрос Валере. Сейчас ты платишь дань компании, которая не закрывает баги, которая закрывает open source базы данных на GitHub и так далее. Ты как живущий в Европе, не хочешь купить себе нормальный телефон? Конечно хочу, я же говорю, что если что-то такое бунтообразное выползет на свет божий, я скорее всего это куплю. У меня пока два консерна. Первый, это скорее всего будет лопата, я не хочу лопату. Нет, нет, нет, он такой, ну не совсем лопата. 4,5 дюйма, видно на экране. Это лопата, это нормальный размер. Ну, короче, посмотрим. Саша, а ты смотрел видео анбоксинга и там дальше первое использование?",
    "result": {
      "error": "API request failed: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-xSG9D345gtYlIKC330wCyrEG on tokens per min (TPM): Limit 30000, Used 30000, Requested 5488. Please try again in 10.976s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
      "stack_trace": "Traceback (most recent call last):\n  File \"/home/andrei/Projects/podcast-shownotes/scripts/build_search_eval_dataset.py\", line 157, in generate_search_query\n    response = await self.client.chat.completions.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 1927, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1767, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1461, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1547, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1547, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1562, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-xSG9D345gtYlIKC330wCyrEG on tokens per min (TPM): Limit 30000, Used 30000, Requested 5488. Please try again in 10.976s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
    }
  }
]