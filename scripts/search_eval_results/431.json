[
  {
    "segment_id": "5b1bcabe-7d7a-44b1-8947-d0b1055e1736",
    "episode_id": "2bcef5eb-a8a1-4017-b793-46fa64b56b7c",
    "episode_number": 431,
    "segment_number": 7,
    "text": "То есть там особо нечего контрибьютить, и если бы лицензия была другой, это бы особо ничего не поменяло. Ну, не знаю, не знаю, по-моему, как минимум кастомный GUI очень многие накидывают. Тот же вот летающий медведь, который у нас с тобой, у них кастомизированная прошивка, и они, кстати, выкладывают исходники. Так что... Медведи, котики... Некоторые компании ведут себя лучше других, и это хорошо. Медведи, котики... Ваня, тебе слово. Ты имеешь в виду по этой теме и по следующей уже? Ничего не знаю, но это ведущий, тебе можно все. Все понял, отлично. Вообще, мне кажется, что мы с нашей колокольни можем даже не всегда понимать, к чему приведут эти или иные лицензии. И я бы поэтому здесь поглядел на... эволюционно на все это. То есть, поглядеть через 50 лет, и там уже делать выводы. То есть, пусть существуют все лицензии, какие-то... Они же все все равно как-то понемножечку меняются, переосмысливаются, не знаю, оживают заново и так далее. И мне будет интересно на самом деле поглядеть лет через 20 хотя бы, к чему приведут все эти лицензии. Потому что мне лично тоже кажется, что JPL, он как-то чрезмерно усложнен. С другой стороны, они что-то пытаются делать, может они что-то сделают хорошее в конце концов. Ты знаешь, в исторической перспективе, что Postgres, что Linux, может, лет по 30, и звучит как будто Алекс прав, что JPL, что BSD нет особой разницы. И там, и там можно сделать успешный проект, и там, и там крутятся деньги, если на этом делается что-то полезное. Мне кажется, что JPL больше оправдан для проектов, когда у вендора есть искушение взять проект как есть, с минимальными патчами, и выдать его за свой продукт. Допустим, я не знаю, ты берешь Linux, добавляешь в него пару патчей и говоришь вот это моя уникальная прошивка, я все сам сделал. И у меня от этого коммерческая выгода есть. Таких для очень фундаментальных инфраструктурных проектов вроде ядра Linux JPL скорее всего более полезен, чем вреден. С другой стороны, если ты совсем маленький проект, в который очень нишевый, какая-то маленькая библиотечка, которая ходит в http очень умным образом, то скорее всего ограничения JPL просто отвратят твоих потенциальных коммерческих пользователей в сторону чего-то менее ограниченного или просто написания своего собственного. Так что... По сути ты можешь себе позволить JPL, только если ты такой же большой, как ядра Linux. Но как контрпример, если, например, ты теоретически сделал фреймворк для программирования на микроконтроллерах и сделал JPL, это большой фундаментальный проект, но ты все производные проекты сделал, ты их заразил JPL-ностью, понимаешь свои проблемы? Нет, понимаешь, он не станет фундаментальным, потому что его никто не использует. Под фундаментальным я имею в виду не просто что-то низкоуровневое, а что-то, что является или наверняка будет являться критической частью инфраструктуры интернета, грубо говоря. И понятное дело, что опять же, когда ты начинаешь этот проект, ты не знаешь. И Linux, скорее всего, занял ту роль, которую он занял, потому что ну... Он был относительно рано в относительно уникальной позиции, и ему повезло получить достаточно много контрибьюторов, чтобы стать вообще жизнеспособной операционной системой. Я думаю, Linux повезло то, что он поддерживал все железо. Он в себя все-все-все драйверы принимал. Ну, да-да-да. Вот. А BSD так не делали. СММ много контрибьюторов. Ну, я думаю, мы хорошо обсудили эту тему, поэтому... Пойдем дальше. Согласен. Согласен. Тогда пойдем к следующей теме, а следующая тема это снова новый Paper. Paper называется The Tale at Scale. Даже не знаю, как правильно это перевести. Хвост при нагрузке. Вы слушаете, как я бумагу перелистываю? Это у меня, конечно, замечательный Paper. Нагруженный... Сейчас. А-а-а... Ну, нет, нагрузка здесь не то. Это масштабированный хвост. Да-да, хвост под масштабом. Это мы постепенно... Это Paper телег недавности. И я читаю Paper, подбираясь к более свежему и современному Paper. И просто читаю все интересные ссылки, которые там были указаны. Вот. И поэтому как бы... Вот мы сейчас будем разбирать Paper 2013 года. Идея статьи заключается в том, что... На самом деле это базовое понятие. Сейчас большинство людей, которые работают с высоко нагруженными приложениями, они все это, понятное дело, знают.",
    "result": {
      "error": "API request failed: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-xSG9D345gtYlIKC330wCyrEG on tokens per min (TPM): Limit 30000, Used 30000, Requested 2201. Please try again in 4.402s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
      "stack_trace": "Traceback (most recent call last):\n  File \"/home/andrei/Projects/podcast-shownotes/scripts/build_search_eval_dataset.py\", line 157, in generate_search_query\n    response = await self.client.chat.completions.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 1927, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1767, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1461, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1547, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1547, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1562, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-xSG9D345gtYlIKC330wCyrEG on tokens per min (TPM): Limit 30000, Used 30000, Requested 2201. Please try again in 4.402s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
    }
  }
]