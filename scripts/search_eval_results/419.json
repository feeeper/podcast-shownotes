[
  {
    "segment_id": "6e4db80d-566a-44f2-a28b-e94dd4692218",
    "episode_id": "10841c48-8e8c-4d20-842a-484db16b3ce4",
    "episode_number": 419,
    "segment_number": 7,
    "text": "Я рассказал, в принципе, зачем это нужно. Рассказал, какие примеры и как это сделано. Тоже довольно интересно. Идея в том, что компьютер в Go собирает некие каунтеры. Технически коннектится к серверу. Сервер ему говорит, какие каунтеры ему нужны. И дальше сам тулчейн определяет, хочет он их слать или нет. И хочет он их слать, ну, в первую очередь, понятно, от настроек. То есть это можно просто все выключить. Но дальше есть сэмплинг нескольких ходов. То есть он может вообще их не слать. Может слать, если пришло сейчас хорошее время. Может слать только часть данных и так далее. То есть с точки зрения статистики все хорошо. Выяснить, естественно, никого, ничего нельзя. Все это полностью анонимно. И вообще никаких персональных данных там нет. Потому что нифига они нужны. И, соответственно, за счет сэмплинга и так называемой дифференциальной приватности, differential privacy, все никого выяснить нельзя. Все замечательно. Написаны три статьи, как я сказал. Создана дискуссия. Открыты комментарии. Ну и, конечно, 99% людей, боже мой, боже мой, Google хочет всех захавать. Мы никогда ничего не дадим. Все мы форкнем Go. Все мы будем ставить его из пакетов операционных систем, которые все это выпили и так далее и тому подобное. При том, что все максимально прозрачно. Вся реализация, понятное дело, open source. Все можно посмотреть. Файл, который отправляется, сохраняется локально, его тоже можно посмотреть и так далее и тому подобное. Вот. Дискуссия, соответственно, сразу ушла не туда. Ее через какое-то время закрыли. И долго внутри команды, судя по всему, и частично где-то в публичных местах, но я так понимаю, что в основном внутри команды, думали, как бы, что с этим делать. Думали, думали. В итоге решили сменить систему с opt-out на opt-in. Это примерно через месяц произошло. И, соответственно, в итоге решили сменить систему с opt-out на opt-in. И, соответственно, обновили некоторые ожидания по сэмплингу и так далее. За счет того, что теперь это opt-in, параметры должны быть немного другие, немного больше. Поэтому, если вы сделаете opt-in, то больше информации вы будете слать. И текущий перпозал вроде как всем понравился. Но, с другой стороны, в нем практически нету никаких белых паникеров, которые прибежали с словами про окно овертона в предыдущем сообщении. Возможно, просто волна схлынула. И поэтому пропозал, судя по всему, будет принят. И следующие версии в Google войдут. Вот такая история. Но для меня самое интересное, что... В статье я про это не очень сильно говорил, но там есть интересные расчеты, насколько можно посчитать аккуратность этих данных. И, конечно, если opt-in, есть большие перекосы в этих данных. И в итоге, если мы выйдем перекосным, мы все равно, конечно, не отказываемся от всяких наших опросов. И багов, заведенных в ищу трекера. Но интересно будет в будущем посмотреть, насколько эта статистика будет коррелироваться с реальностью. И как они вообще это будут вычислять. Вот такая история. То есть сейчас они сделали opt-in, а не opt-out, да? Ну прямо сейчас этого нет. Но текущий перпозал, который еще не принят, но я думаю, что скорее всего будет принят. Потому что там большинство за. Он будет opt-in, да. Но я вижу несколько наблюдений имею. То есть сейчас... Во-первых, часть проблемы, вероятно, заключается в том, что действительно в компиляторах так как-то не принято. По крайней мере, вот в open-source компиляторах, мне кажется, Go такой первый, который хочет слать телеметрию. Но тем не менее... Есть точно в .Net, по-моему, есть в Java. Хм. Тем не менее... Просто многие люди, наверное... Мне кажется, они не знакомы с проблемой, когда ты пишешь вот не просто приложение, там, текстовый редактор, да, например, а вот именно компилятор, база данных, что-то такого уровня. Насколько это большая проблема не знать, чем люди пользуются и как часто. То есть, например, вот если мы Postgres возьмем. В Postgres есть от всей души ненавистный мной механизм наследования таблиц. Он туда... Это огромное историческое недоразумение, что он туда попал на волне ажиотажа вокруг объектно-ориентированного программирования. И теперь в Postgres ты можешь реально унаследовать одну таблицу от другой таблицы. И это будет работать вот похоже на то, как работает наследование в OOP. Но чтобы жизнь не казалась малиной, там есть множественное наследование. И... И в несколько уровней. Вот. И было бы... Ну, в итоге большинство людей либо счастливо не знает об этом функционале и живут нормально. Многие использовали его для партицирования таблицы до тех пор, пока в Postgres не появилось именно отдельное партицирование. Теперь там есть наследование таблицы, а есть партицирование. Вот. И было бы здорово в третьем тысячелетии избавиться от наследования таблицы. Потому что, скорее всего, оно... Ну, объективно, для решения практических задач нафиг не нужно. Ты можешь наследование изобразить без встроенного механизма наследования, если тебе прямо учителено припекло. Вот. Но теперь проблема. Ты как open-source-проект без телеметрии, ты вообще не знаешь, а этим кто-то пользуется, не пользуется, а как много, а в процентном соотношении от пользователя. Вот если мы выпилим наследование, то сколько процентов пользователей это зафиксит. Вот. И это касается не только наследователей, это касается любой фичи. У тебя нет никакой возможности узнать, кто, для чего, как, какие места в Postgres используют. Это усложняет и развитие проекта, и его поддержку, потому что, ну, никто не знает, кто и как, и для чего использует Postgres. Вот. Хорошо, что ты привел нам поздравленных, потому что у нас есть много интересных вопросов. Да. И мы их получаем. Да. Потому что у нас, собственно, в RDB есть такая же проблема, да. Нам нужно реализовать довольно большой-большой функционал того, что делает MongoDB. Но, как ни странно, именно в базах данных телеметрия появилась довольно давно. И примерно во всех. Ну, то есть там с Cockroach, начиная... Ой, с Cockroach. А, простите, с... Как бешено. Couchbase, вот этих вот ребят еще там в эпоху NoSQL. Да. Но при этом баз данных — это как раз то место, где еще больше паникеров, чем, наверное, в компьютерах, потому что, боже мой, боже мой, вы хотите все мои данные заполнять. И там тоже приходится прикладывать очень много усилий, чтобы людям объяснить, как это работает, как это выключить, как... что шлется и так далее. И опять же тоже интересный вопрос, да, какую статистику будем получать, если вдруг решим сделать это opt-in. То есть мы, скорее всего, не будем. Скорее всего, у нас будет opt-out, как оно сейчас. И для нас эта статистика была неоценима просто, да. То есть весь наш roadmap за последние несколько месяцев, что мы вообще будем делать, он был под воздействием этой статистики, да. То есть мы видим, какие самые популярные команды не реализованы, и мы их реализовываем. Ладно. У нас с другой стороны, да, вот как бы есть люди, которые просто... Ну, как бы видят Google, видят Telemetry. И... Видят Telemetry, и все. Как бы все вместе вот так сразу. Вот. А второе наблюдение, что действительно opt-in, он примерно бесполезен. Ну, то есть у тебя будет там маленький процент компаний, которые вот прям напряглись, пошли и сказали, да, вот мы вот такие молодцы, хотим снять Telemetry. И вся твоя статистика, она будет перекошена. Да, вот это как раз самое главное, да. То есть если бы она была не перекошена, было бы вообще нормально, да. Особенно если у тебя большой install base, как сейчас в Go и пока еще не как у Feral TV, то можно очень эффективно делать сэмплинг, и все будет хорошо. Который opt-in, надо сидеть и рассчитывать, вот как разработчики Go рассчитывали, какие должны быть числа, чтобы не было перекоса. Но посмотрим, как это будет на практике. С другой стороны, если... Ну, то есть ты можешь как команда разработчиков, вендор какой-то создать. Команда разработчиков, вендор, какое слово тут более применимо. Ты можешь акцентировать внимание пользователей на том, что если вы хотите, чтобы наш продукт, наш компилятор был для вас удобен, имел правильные фичи, а и лишние фичи не выпилил, которыми вы пользуетесь, то не забудьте включить статистику. Потому что иначе вы вот не участвуете, потом не жалуетесь, если мы что-то поломаем. Ну да, потом ты включал статистику. Они увидели, что этой фичи никто не получил. Да. И она не пользуется. Пользуется один человек, который включил статистику. То есть ты ей выпилил. Так они видели ноль и предполагали, ну, наверное, эти люди паранорики не включили. И они говорят, а кто увидел одного и выпилили фичу. Ну, в общем, да, это, конечно, это не защищает ни от чего. Это не серебряные пули. И все равно, да, как всегда, там есть закон, как он называется, на H, закон какого-то гуглера. То, что если у вас есть какое-то поведение, то у него точно кто-то где-то завяжется. И когда вы его измените, вам придет баг репорта. Вот, но, да. Вот, но в то же время хоть какая-то телеметрия может быть перекошена. Это лучше, чем вообще никакой телеметрии. Я вот уверен, если честно. То есть, предположим, у тебя есть issue, есть report, есть как бы community, где ты с людьми разговариваешь. Есть телеметрия, да. И дальше у тебя все указывается. Все указывает на то, что вот там у людей есть такие-то проблемы. Возможно, ты не видишь очень большое счастливое большинство, например. Да, у которых просто все работает, и поэтому они даже и в кабинете не приходят, и баги не ставят, потому что не все хорошо. И телеметрию не включают, потому что зачем. Вот. Или, допустим, что у тебя есть люди, которые приходят и говорят про реальные баги. И есть огромное количество, там, допустим, ошибок в телеметрии про вещи, которые ты не хочешь делать. Ну, не делаешь, точнее. Потому что никто не просит. Просто никто не просит. Но по человеку есть данная телеметрия. И как это сравнить между собой? Как скоррелировать? Скоррелировать и так далее, да? Я не уверен, что когда наличие телеметрии, которая плохая, лучше, чем никакой. Так или иначе, я считаю, мы неплохо обсудили эту тему и предлагаю двигаться дальше. Дальше у меня тема одной строкой.",
    "result": {
      "error": "API request failed: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-xSG9D345gtYlIKC330wCyrEG on tokens per min (TPM): Limit 30000, Used 30000, Requested 4713. Please try again in 9.426s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
      "stack_trace": "Traceback (most recent call last):\n  File \"/home/andrei/Projects/podcast-shownotes/scripts/build_search_eval_dataset.py\", line 157, in generate_search_query\n    response = await self.client.chat.completions.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 1927, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1767, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1461, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1547, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1547, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1562, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-xSG9D345gtYlIKC330wCyrEG on tokens per min (TPM): Limit 30000, Used 30000, Requested 4713. Please try again in 9.426s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
    }
  }
]