[
  {
    "segment_id": "fb322b59-bce4-40e7-9396-594328152824",
    "episode_id": "98000153-80a0-43bc-8186-928ed567de9b",
    "episode_number": 215,
    "segment_number": 7,
    "text": "И я хотел поделиться впечатлениями о... Такой... Я даже не знаю, как это правильно сказать. Это типа SDK, наверное. Давайте начнем издалека. Когда вы работаете с Kubernetes, вы работаете чаще всего с готовым API, с готовыми типами, объектами, которые уже есть в системе. И все, казалось бы, хорошо. Есть огромное количество... Какое огромное? Десяток. Удобных вещей, на базе которых строятся практически любые сервисы и любые приложения. Поды, диплойменты, сервисы. Пяток еще небольшой. Дополнительных вещей. Но чаще всего, когда у вас есть своя какая-то хитрая бизнес-логика, вам нужно что-то дополнительно свое. И для того, чтобы это реализовывать, в Kubernetes есть такая вещь, как поддержка дополнительных... Как расширение API Kubernetes. То есть вы говорите, ребята, я хочу расширить API Kubernetes с помощью добавления еще одного, скажем, типа объекта. То есть этот тип объекта добавляется в Kubernetes, вы к нему обращаетесь, к Kubernetes, я имею в виду, обращаетесь через стандартный API, говорите ему, дайте мне список API, и ваш новый тип объекта, он появляется в кубернетском списке API. Вы можете сказать, а вот дайте-ка мне объекты вот этого типа. Отлично, а потом покажите мне статус вот этого объекта, а потом покажите мне спецификацию вот этого объекта. При этом эти объекты, это вы сами написали. Эта гибкость очень огромная, и это очень интересно. Интересно с точки зрения, что есть очень много use cases, когда вы хотите свою бизнес-логику сделать не на основе стандартных типов, которые есть в Kubernetes, а на основе дополнительных типов, которые вы сами можете захотеть сделать. И есть большое количество вещей, тулов каких-то, гайдов, как правильно это разрабатывать, и оказалось, что есть интересный проект, который был реализован как кубернетовская группа интересов. Там есть такая вещь, как SIG при разработке кубернетов, если вы общались и смотрели, то знаете. Один из SIG-ов, это как раз подобный вот этот лза, который называется куббилдер. Куббилдер – это SDK для разработки как раз новых типов данных, а контроллеров на основе этих новых типов данных. Вот этот новый тип данных в кубернетской терминологии означает Custom Resource Definition, и вам нужно написать, соответственно, CRD, Custom Resource Definition, плюс контроллер для него. Это довольно удобно сделать в куббилдер. С помощью двух команд вы сперва инициализируете проект, потом создаете новый API, создаете группу API, создаете новый ресурс. Фактически вы выполняете две команды, и у вас уже есть готовый проект, который умеет делать make, который уже делает какие-то тесты на базовые типы, который уже запускает автоматический контроллер, который следит за вашим типом и создает по умолчанию это создание нового диплоймента. На основе... То есть это некий аналог, я даже не знаю, диплоймент контроллера. То есть, наверное, так проще, правильнее всего выразиться. Это довольно удобная тулза, потому что иначе вам бы пришлось весь этот огромный набор кода писать самостоятельно, начиная от тестов и заканчивая правильным определением контроллера, каким образом его правильно надо зарегистрировать, каким образом надо определить права доступа через РБАК, каким образом... Ну, там куча всего делается. И я как раз с помощью куб-билдера построил нам тоже внутренний CRD, который делает кастомный горизонтальный... Хоризонтал по дата-скеллях, который с помощью внутренних конфигураций каким-то образом скалирует количество подов. Очень интересный опыт. Это для меня первая поделка на Голанге, и я не скажу, что мне не понравилось. Я прям был приятно поражен, как легко окунуться в огромный чужой проект. Для справки, для того чтобы все это заработало вместе, я подключил кучу библиотек внутри кубернетеса или вокруг кубернетеса и общий pull-request с нуля до рабочего состояния контроллера и имплементации CRD 16800-900000 файлов. Вы должны понимать, что при этом я написал из них десяток файлов, а все остальное было импортировано в папочку Vendor, которая создается автоматически за вас, когда вы делаете импорт из гошных файлов. Общий объем получившегося контроллера у меня порядка 200 мегабайт, и я не скажу, что это было очень сложно. Но это огромные вещи, огромный проект кубернетес, который вы должны понимать примерно как работает. Куча библиотек и куча подключаемых модулей. И мне понравился Go. В общем, Алексей, я понимаю, почему ты топишь за него во всех конференциях и подкастах. Подожди, как ты без генериков обошелся, Ваня? Ты знаешь, это первая версия. Мне нужна будет вторая, третья, четвертая и пятая. Я так четвертый, пятый, я созрею до генериков и приду плакать об этом. И найдешь ты место, где они все-таки нужны, и без них жить нельзя. На самом деле, скорее всего, нет. Не найдешь. Подожди, подожди. Это давай до четвертой серии доживем, не будем эту интригу портить. Вообще, где-то внутри кубернетеса есть подделка, которая делает тебе генерик через кодогенерацию очередная. О, да, кодогенерация, да. У меня уже есть куча кодогенераций. Я туда даже не залазил, куббилдер все построил за меня. То есть ты говоришь, я вот хочу вот этот CRD вот такого типа, а он там код генерирует, как правильно копировать этот тип, как ему... Ну там куча всего генерируется, я даже не сильно смотрел. Прям, в общем, советую всем куббилдер, если кто хочет делать кастомные ресурсы и контроллеры вокруг них. Советую. Интересно. Для меня тут такой идеологический конфликт в том плане, что кубернетес уже очень удобная платформа, чтобы свои сервисы поднимать, которые про кубернетес по идее не должны знать. Но с другой стороны, свой use case с автоскейлером для кубернетеса, наверное, имеет смысл прям как кастомный ресурс заливать. Я к тому, что можешь ли ты провести параллель, что если бы ты вот этот автоскейлер разрабатывал как обычный сервис, который как side effect можешь деплойть на кубернетес, ну и он там ожидает какого-то кубернетеса API, но это не значит, что к нему надо достукиваться тоже через кубернетеский API. То есть насколько это было бы сложнее для тебя? Ну смотри, для того, чтобы достучаться к кубернетесскому API, ты должен понимать, как оно работает. То есть фактически тебе надо либо подключить к кубернетесской библиотеке, либо самому имплементировать через REST API всю эту машинерию. Верно? Да. В дальнейшем я прямо сейчас скорее всего буду туда запихивать либо свой собственный внутренний API, либо какую-то дополнительную логику, потому что там нам нужна сложная логика по управлению проектом. Там не только на основе того, насколько сильно греет CPU, нам надо количество подов раскидывать, а на более сложных логиках вроде, я не знаю, среднего потока клиентов за последние пять минут. Понимаешь, да? То есть это будет сложнее сделать, но для этого мне скорее всего не придется делать API внутри моего контроллера. Или внутри этого... Ну, внутри, в общем, мне не придется делать дополнительный API, мне придется использовать машинерию, которая уже есть в кубернетесе. То есть кубернетес настолько гибкий, позволяет настолько много вещей делать, что мне... Кстати, это, наверное, даже сложнее становится. То есть мне разработать свой собственный API у своего сервиса гораздо проще пока что, чем разобраться в правильной настройке всей этой гибкости кубернетеса. И твой API сейчас — это просто кастомный ресурс, в который ты подкидываешь настройки, по сути? Да, да. Или другие вещи какие-то? У него есть внутренняя логика, а эта логика частично копируется из стандартного ванильного горизонтального HPA. Но я ее усложняю и планирую усложнять еще дальше, потому что я буду увеличивать количество конфигурационных параметров, которых нет в ванильном. Соответственно, он уже будет дергать метрики, он уже будет дергать список диплойментов подов, он уже будет сам все это делать внутри себя. Я просто буду конфигами управлять его работой. Ну это понятно. Но при этом получается, что чисто теоретически это мог бы быть обычный диплоймент, не через вот этот куб пилтер и не там какой-нибудь конфигмап. Я мог бы это сделать как обычный диплоймент и внешним сервисом, который каким-то образом лезет в кубернетес, менять его конфигурацию. Ты про это имеешь в виду? Ну, типа того, да. Да, да, да, конечно, мог бы. Но просто не имеет смысла это делать снаружи кубернетеса, потому что у кубернетеса уже есть куча удобных ручек. Это первое. А второе, то, что я сейчас делаю внутри своего CRD, я планирую добавлять в кубернетеский стандартный API. То есть я уже разговариваю с разработчиками, чтобы коммитить мои хотелки изменения туда в дальнейшем. Потому что мне не хочется обновлять мои файлы по мере того, как будут изменяться, добавляться, улучшаться кубернетеский API. Чтобы это было частью кубернетеского API. Абициозно. Ну, желаю удачи. Да, спасибо. Ну, интересно. Интересно, интересно. Да, ну, просто они там достаточно странные. Ну, как, может и не странные, но use cases, которые они расписывают, они типа во всяком случае упоминают. Например, тебе надо задеплоить какую-нибудь сложную хрень, типа кластеры MySQL, и ты хочешь, чтобы оно как-то более нативно на кубернете сложилось. И для этого, да, теоретически ты можешь написать свои контроллеры и все вот это. С другой стороны, есть системы, которые к кластерам типа кубернетеса имеют адаптеры просто. В том плане, что, например, если ты подключишь вот этот флажок, то мы попробуем через кубернетес API сделать Art of Discovery и так далее. То есть мне очень интересно, как это пойдет дальше. В том плане, что люди будут настолько сильно привязываться к кубернетесу и будут использовать что-то типа куббилдера, либо они будут билдить что-то более абстрактное, и кубернетес это будет одним из вариантов, на что это можно раскладывать. То есть вот этот момент интересует. Да, да, да, это интересно, потому что это фактически два разных вектора развития, и куда повернет все сообщество пока непонятно. Плюс я хотел бы сказать, что есть еще альтернативный вариант. Это как раз в терминах кубернетеса называется оператор. Когда у тебя есть какое-то стейтфул, большое приложение распределенное, ты хочешь управлять не только тем, как оно работает, но и его состоянием. Ну то есть MySQL и его состояние, правильное развертывание, это больше к оператору относится. И для работы с операторами есть отдельный проект, который аналог куббилдера, но куббилдер это больше как контроллер CRD, но не... Хотя на самом деле тут очень зыбкая и тонкая грань. Я думаю, что они пересекаются в том числе по применению, по возможным юзкейсам, и может быть в дальнейшем они объединятся в один. Пойдем дальше? Ну видимо да. Ну дальше Саша на тему, я предлагаю им тогда за них взяться. Я не могу на них плавно перейти, там что-то можно реверсить, можно не реверсить. Хорошо. Ранее я говорил, что последний тем про железо, она была последняя на этот выпуск. Я лгал, ну точнее немного лукавил, потому что эта тема как бы сопряжена местами. Серия постов называется Reversing ESP, скажу по-русски 8266 Firmware. Мы его вспоминали, как там называется, ESP32, который предыдущий в нем больше цифр, вот он называется ESP8266.",
    "result": {
      "error": "API request failed: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-xSG9D345gtYlIKC330wCyrEG on tokens per min (TPM): Limit 30000, Used 30000, Requested 5195. Please try again in 10.39s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
      "stack_trace": "Traceback (most recent call last):\n  File \"/home/andrei/Projects/podcast-shownotes/scripts/build_search_eval_dataset.py\", line 157, in generate_search_query\n    response = await self.client.chat.completions.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 1927, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1767, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1461, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1547, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1547, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1562, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-xSG9D345gtYlIKC330wCyrEG on tokens per min (TPM): Limit 30000, Used 30000, Requested 5195. Please try again in 10.39s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
    }
  }
]