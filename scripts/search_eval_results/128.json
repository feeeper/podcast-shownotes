[
  {
    "segment_id": "e6268555-a10d-42d0-9681-c0963779cc66",
    "episode_id": "68bc8d37-85ef-4757-920f-2a6d8ec2ea5f",
    "episode_number": 128,
    "segment_number": 5,
    "text": "Однозначно они ловят, однозначно они доказывают только следующее, что если у тебя есть тип, который они назвали fenced, то есть это типа огражденный, ограженный, ограниченный, то есть мы знаем, что программа, если состоит только из fenced типов, то она обладает конечным набором шаблонов взаимодействия между актерами. И в этом случае мы можем хоть какую-то логику построить на базе этого, о том, как работает эта программа, на базе своих вот этих вот рассуждений. Вот. Конечно же, вторая, вторая, то есть вот у них вот этот тул состоит из двух частей, первая это go infer, она написана с помощью этого пакета ssa, а вторая написана Haskell, вот, и тут, конечно, ничего удивительного нет, как пишет сам, как пишет сам Адриан, вот. И так, что-то я еще хотел сказать здесь по этому поводу и забыл. Ну, короче, давайте пока вопросы, я сейчас вспомню и сейчас скажу. Вот так вот. А в каком все это состоянии находится, это уже как-то пощупать можно, да? Это пощупать можно, это все выложено в open source, это уже есть paper или даже два на эту тему. Они с помощью этой штуки показали работоспособность и неработоспособность нескольких программ, которые они нашли на гитхабе, то есть там количество строчек, типы и что они там нашли. Вот, ну то есть мне кажется, что если эта вещь будет развиваться, то это, конечно же, можно интегрировать с госообществом, я прям уверен, там уже толпа народа носится с этим, потому что, на мой взгляд, это просто идеальная штука. Я на самом деле считаю, что это, мне кажется, совершенно офигенный девелопинг, потому что в мире Erlang были такие похожие тулы, там кое-что было впилено даже в текстовый диалайзер, еще были Conqueror и был MC Erlang и еще чего-то похожее. До этого Erlang был практически единственным языком, который не создан специально для верификации, где были подобные роды тулы. И сейчас вот у нас ура-ура, есть второй язык, который является и языком имплементации, и языком, на котором можно строить какую-то верификацию, а в котором таки можно строить какую-то верификацию. Ну пока она ограничена, но вполне возможно, это первый шаг, и причем, на мой взгляд, это очень большой и сложный шаг. По крайней мере, доказательство того, что это все будет корректно, у них занимает не одну страницу чистой математики и теоретиков. Я не врубался туда даже. Это очень здорово. Не знаю, мне кажется, скомканно объяснил, не донес ничего. Я ожидал услышать возгласов «Ура-ура, переходим все писать на GO, потому что здесь хоть какая-то будет». И только Валера согласился. Мне на Erlang неплохо писалось. Честно признаться, я Conqueror в реальности не пользовался ни разу. На практике я в итоге пользовался просто, ну я уже много раз про это рассказывал, QuickCheck'ом, ну не QuickCheck'ом, Propper'ом. Ну и был, по-моему, похожий тул от той же группы людей, что и Conqueror, делали Dialyzer. Но Dialyzer реально иногда может находить race condition или прочее безобразие. На самом деле, в GO это немножко интереснее, потому что в GO кроме каналов, я не знаю, учитывается ли это в пейпере, но в GO кроме каналов есть еще Mutex'ы и прочие интересные вещи. То есть, на мой взгляд, ну и да, опять-таки, в GO можно каналы передавать через каналы, и в GO каждая горутина может быть больше одного канала, поэтому, мне кажется, анализировать GO немножко сложнее, чем Erlang, больше написать фигню в этом смысле в GO проще, чем в Erlang. Вот, поэтому, мне кажется, что это получится какое-то развитие. Ну и плюс GO, в принципе, похоже, что популярнее Erlang. Не может быть. Это к чему говорю, потому что, да, такие тулы, они... В любом случае, на GO уже сейчас пишется много систем, которые в ряковце использовали, в хосте в гриву у кликчека есть такой модуль пульс. Вот он не делает это статически, статически он просто прогоняет кучу-кучу разных скедулингов. И я думаю, что если будет какой-то, сколько-нибудь рабочий на приличной кодовой базе, приличного размера кодовая база тул, то людям вполне возможно всякие etcd или консул, или похожего толка программы, там, опять-таки, куски кубернет, то есть у нас тоже есть... даже если мы общаемся с etcd, у нас могут быть нетривиальные проблемы с конкуренцией. Даже в принципе в любом программе, в которой есть какой-то нетривиальный конкуренции, ну то есть что-то сложнее, чем погрузить на пользователя, у нас непременно может вылезти что-то такое неочевидное, что даже не обязательно вылезет в продакшене сегодня, оно вам может выстрелить через год. И такой тулинг, он будет однозначно помогать. Больше того, наличие такого тулинга лично мне придаст больше уверенности чем, скажем, помянулся в теме etcd, консул и т.п. Я считаю, что тема себя исчерпала, и предлагаю переходить к теме слушателей, но тут я вижу, Ваня добавлял конференцию, давай ты про неё расскажешь, потому что она станет, возможно, неактуальной. О да, меня попросили рассказать про конференцию, это DevOps Days, это конференция международная, но она проводится будет в Москве, и поэтому это делают российские разработчики, соответственно. Я не уверен, что всё будет по-русски, есть подозрения, что будет частично на английском, по крайней мере я точно знаю о одном или двух выступающих, которые приезжают из-за рубежа, возможно, их будет больше, программа уже известна, но я не рассматривал глубоко. Проходить она будет, секундочку, я сейчас подлежу, я не подготовился. 11 марта в Москве, уже можно всё покупать, я имею в виду билеты покупать, смотреть программу и ехать. Что ещё станет, возможно, неактуальным к следующему выпуску, это вакансия, которую нас никто не просил пиарить, но она как-то сама к нам прилетела. Оказывается, в Москве есть вакансия программиста на Хаксе. Напомни, что это? Это тот, что транслируется во всё, и по-моему на Хаскаль похож. Нет, он похож на C-Sharp, мы про это говорили в прошлом выпуске. Да, значит, я перепутал с чем-то другим, что похоже на Хаскаль, но он транслируется во всё и с виду нормальный язык. На нём обычно пишут игрульки всякие. Зарплата вполне себе достойная, и ознакомьтесь, если вам интересно Хаксе. Да, и мы переходим к темам слушателей, и первая тема, она на самом деле наполовину слушателя, наполовину наша, потому что мы её тоже добавляли, про… нет, тут опять какие-то печатные платы. Про миллион запросов в секунду на бетоне. Это ты, наверное, сам добавлял. Нет, нет, её к нам из слушателей принёс Содиан, но у нас её добавлял я. Вот, давайте так, кто кроме меня… а, тут Ваня отметился, ты читал, правда? Так, я пропустил, про что читал? Миллион запросов на бетоне. Да, да, я читал. Но это же обман. Это обман, многим от этого бомбануло, да, это беспредел какой-то. Но всем на самом деле наплевать. То есть расскажу краткую версию. Чувак запилил, ну, кстати, неизвестно, может быть, девушка, запилил на СиАХ сервер, очень быстрый веб-сервер, там асинхронные все дела, и программируется под этот сервер на бетоне, потом он сделал бенчмарк. Бенчмарк тестирует исключительно накладные расходы этого самого сервера, то есть без шаблонизатора, без хождения в базу и так далее. И получил там умопомрачительный график, то что там типа го в 10 раз медленнее и нод.js в 10 раз медленнее, а вот СиАХ сервер он всех порвал. Он только то, что это СиАХ сервер, говорил чуть попозже, так сначала он говорил, это питон всех играет. Вот, но если вот там на секунду отвлечься от того, что заголовок он такой желтый, там называется миллион запросов в секунду на питоне. В остальном статья, она довольно неплохая, я считаю, и в ней описываются интересные техники оптимизации, то есть тот же HTTP пайплайнинг, асинхронный вывод, как это сделано через, что он там использует, я не помню, на lib.ua, или на чём он написал. Я не помню. Или напрямую на сисколах линуксовых. Плюс тут утилизируются такие вещи, что, а вот вы знаете, что можно парсить заголовки с использованием инструкции ssh4, и вот всякие такие прикольные вещи. То есть смысл в том, что чувак реально написал очень быстрый HTTP сервер, который реально удобно программируется на питоне, и который реально быстрее всех других HTTP серверов, которые он смог потестить. И там, учитывая наличие асин.io и вот этого всего, то есть я считаю, что hello, да, весь надо на питоне писать. Давайте теперь там в репортаж спрашивают, почему обман. Почему обман? Во-первых, сама статья была про то, как миллион запросов в секунду на питоне, а внутри он рассказывает, что на самом деле на питоне я писал только обвязки, а всё, что надо, и дальше список того, что надо, я написал на C. Я, кстати, не вижу, в чём обман, но это настоящий дзен программирования на скриптовых языках, то есть сначала пишем, например, на ирланде, потом смотрим, где тормозит, и переписываем на C, тут то же самое. Но тогда не надо было заголовок писать. Слушай, если у меня Nginx в продакшене, у меня делает ло миллион запросов в секунду? Да, да, да, всё так. Это, кстати, под вопросом, потому что ло, он там не джиткомпилируется в Nginx, джиткомпилируется она не бесплатно. Кто бы его знал. Вот, да. Дальше, в чём обман ещё? Обман ещё в том, как он это всё тестирует. То есть, во-первых, он возвращал чистый Hello World в статическую строчку, во-вторых, он делал всего 100 соединений, и из 100 соединений делал пайплайнинг по куче запросов, по 12, по 24, я не помню точно. То есть, в реальном мире это вообще неприменимо. Ну да, можно, конечно же, сделать, накрутить так, что у тебя будет миллион запросов, только ты нигде это применить не сможешь, а если будешь по-настоящему мерить, то, конечно, циферки будут значительно поменьше. Обратил внимание, он не позиционирует это как веб-сервер, который вот там типа джангу на нём поднимается, он это позиционирует как веб-сервер для микросервисов, и я не вижу, почему здесь пайплайнинг не засчитывается. Потому что пайплайнинг, как ты будешь его использовать, пайплайнинг твой? Пошлю 100, 500 запросов, получу 100, 500 ответов. То есть, один микросервис другому микросервису или с клиента ты будешь его спрашивать? Ну, в микросервисы клиенты обычно не ходят. Ну, ходят, почему же? Ну, хорошо, допустим, с одного сервиса, да, мне нужно по 10 пользователям получить данные, вот я 10 запросов в пачках послал. Во-первых, во-вторых, получается, что ты будешь это посылать внутри одного коннекта, друг за другом, верно? То есть, ты будешь получать ответ на первый, на десятый только после того, как ты получишь ответ на все предыдущие 9. Да, но мне же ничто не мешает иметь больше одного коннекта, у меня там, разумеется, есть полер, вот это все как положено. Но я к тому, что в реальной жизни ты не будешь делать пайплайнинг из 20 этих запросов внутри одного соединения пока… Ну, понимаешь, может быть, буду, а может быть, не буду, его поддержка, она не вредит. Если тебе CES, JavaScript и, собственно, HTML приезжают с одного сервера, уже минимум 3, а потом еще картинки, может быть, с того же сервера. Тогда это не Hello World. Ну, короче, здесь слишком много натянуто за уши, мне вообще желтизна меня прям под горой дала. Понимаешь, я согласен, что желтизна, я согласен с тем, что бенчмарк синтетический, но, по-моему, тем не менее, вообще зачетные проекты и зачетные серверы и данные питания. Да, не на каком питании, на C. Имеется в виду, да, для программистов на питании. Да, он так внизу и написал, что, ребят, вы можете мой код использовать из Руби, ну, то есть, можно было поставить из Руби. Он не так говорит, он говорит, что ту же идею можно портировать для Руби, но его сервер не поддерживает Руби, насколько я знаю. Ну, скорее всего, он же на питании писал. Не, я к тому, что это просто чувак пишет на C, он хорошо знает C, он хорошо понимает, какие процессы, как внизу там всё идёт, то есть, для примера, он получает все эти запросы, а потом, для того, чтобы минимизировать количество обращений к операционной системе, он всё складывает в один большой пакет, там 20-30, сколько штук влезет, и потом одним райтом всё это отпихивает. Отличная оптимизация. Да-да-да, я к тому, что, ну, не питон это, не Руби, не Ло, не это, это чистый Nginx, ты на Nginx сколько статических запросов сможешь с Hello World выфоткать. А вот не факт, что Nginx именно так написан по ряду причин, во-первых, он сильно более фичастый, и от этого тяжелее, во-вторых, он сильно более кроссплатформенный, а здесь чувак прям под Linux запилил, насколько я понимаю.",
    "result": {
      "error": "API request failed: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-xSG9D345gtYlIKC330wCyrEG on tokens per min (TPM): Limit 30000, Used 30000, Requested 5692. Please try again in 11.384s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
      "stack_trace": "Traceback (most recent call last):\n  File \"/home/andrei/Projects/podcast-shownotes/scripts/build_search_eval_dataset.py\", line 157, in generate_search_query\n    response = await self.client.chat.completions.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 1927, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1767, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1461, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1547, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1547, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1562, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-xSG9D345gtYlIKC330wCyrEG on tokens per min (TPM): Limit 30000, Used 30000, Requested 5692. Please try again in 11.384s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
    }
  }
]