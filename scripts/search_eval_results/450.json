[
  {
    "segment_id": "fa6f69e6-8d11-4c47-82ed-4ad8a44b3acf",
    "episode_id": "fec239f3-5942-4a20-84f3-863ec8251770",
    "episode_number": 450,
    "segment_number": 5,
    "text": "И это получается сложно, потому что у тебя надо понять, какой там был байт, вытащить его, сделать какую-то арифметику, вычесть, потом сделать какую-то арифметику, арифметику с новым байтом сложить, вот. И суммарно это не очень быстро. Точнее, это очень быстро, если мы используем это в качестве своего там подпроджекта, но это можно сделать гораздо быстрее, если это у нас огромные массивы данных, и у нас нам надо кучу кучу хеширования делать. Вот. Оказалось, что вот этот JIR, JIR, новый алгоритм, они назвали, не они назвали, они начали его использовать. JIR, JIR hashing. Вот. Он делает намного проще всё. Он делает за... Он не делает вычитания. Он делает фактически смещение на один бит влево на каждом шаге значения. То есть, по факту получается, мы берём старые значения хэша, сдвигаем его на один бит влево и прибавляем к нему новое значение байта с какой-то битовой арифметикой от какой-то константы. Ну, там константа там всё сложнее, но для простоты. Итого получается, что после того, как... Или, может, не на один бит, а на несколько бит в зависимости от того, какое у вас окно. Вот. Но для простоты давайте один бит. Итого получается, что со временем, когда вы берёте один байт, второй байт, третий байт, четвёртый байт, у вас постоянно предыдущее значение байтов, которое включено в ваш хэш, оно сдвигается всё левее-левее-левее и уходит из области. Вот. Но за счёт того, что сдвигание на один бит влево, оно намного-намного эффективнее, чем дополнительно делать вычитание и сложение, получается все намного более эффективно. И после этого у них оказалось, что вот это вот хеширование, оно там что-то, я забыл цифры, ну больше, чем в 10 раз быстрее, чем предыдущее хеширование. Им пришлось допиливать другие куски системы и принимать какие-то дополнительные изменения, в том числе по дизайну. Одно из изменений по дизайну, которое они приняли, это было отрубить слишком короткие кусочки. То есть когда вы делаете разделение на чанки, предыдущий алгоритм, как бы вот у тебя маска сложилась, что у тебя столько-то бит, все, мы берем и отсекаем чанк, даже если он очень маленький. А новая система, они поняли, что после того, как мы отрубили чанк и начинаем делать сравнение, вот следующая часть этого сравнения, она тоже очень, очень тяжелая с точки зрения вычисления. И поэтому для маленьких кусочков это неэффективно, это невыгодно. Гораздо проще сказать, блин, у нас маленьких кусочков не будет в принципе. И давайте мы будем меньше какого-то кусочка, если меньше какого-то размера кусочек, то мы его не будем отсекать. Это позволило сделать два изменения. Во-первых, не отсекать, это значит, что можно несколько бит пропустить в начале вычисления. То есть как бы просто это бесплатно, бесплатно мы сдвигаем, байты пришли, мы сдвигаем, байты пришли, мы сдвигаем, мы ничего не делаем. А второе, это то, что не надо делать вторую часть вычислений, сравнения и так далее, потому что маленькие кусочки неэффективно сравнивать. Они это просто статистически вычислили. Вторую часть, они пришли к выводу, что слишком большие куски системы, слишком большие куски данных на сравнение, тоже показывают низкую эффективность, потому что если бы мы развивали чаще, то мы бы больше находили дубликатов. И поэтому вот это как бы всегда есть какое-то нормальное распределение, есть очень длинный хвост. Они отсекают этот какой-то длинный хвост, они показывают, каким образом они это ввели в математический аппарат для того, чтобы если у вас чуть больше какого-то значения длины, то у вас вероятность отсечения гораздо выше. Они отсекают его на каком-то уровне. Они нашли оптимальный уровень, я сейчас уже не помню, там в килобайтах, там 32 что ли килобайта. И вокруг этого 32 килобайта, или от 16 до 32, или от 32 до 64, я уже сейчас не помню. Они как раз делают вот эти чанки. Они показали, что это самый оптимальный уровень определения дедупликации. Эффективный для вот этого фаз CDC. То есть они изменили математический аппарат, алгоритм и математический аппарат внутри для того, чтобы отсекать слишком маленькие кусочки, отсекать слишком большие кусочки. И как следствие, вот на средних кусочках, где чаще... чаще всего попадаются дубликаты, они действуют наиболее эффективно. Короче, было интересно почитать вот эту часть. Наверное, самое интересное там про то, какие они вводили изменения, про то, как они собирали статистику, графики, вот это все. Там пояснение достаточно понятное, то есть как бы математика несложная. Так, так, так. Что-то дальше я тут листочками двигаю. Погодите, я погляжу. Ну, наверное, самое интересное я показал. Вот. Дальше они рассказывают более глубоко про предыдущие методы, показывают, где их не была неэффективность, показывают, как они пришли к идее, что надо отсекать, показывают, каким образом они как бы принимали решение, где отсекать. Но я в это не хочу углубляться. Вот. Итого они сказали, что по сравнению с вот этим вот классическим, который считался open-source бесплатным и использовался в большинстве систем, они сделали ускорение в 10 раз. Вот. По сравнению с платным, который использовался в каких-то подсистемах, они сделали ускорение в 3 раза. И при этом, как бы, если вот не делать изменений вот этих вот дизайна, что маленькие-большие кусочки отсекать и так далее, у них гораздо хуже определение дубликатов. Но за счет того, что вот они ввели изменения в этом дизайне, оказалось, что производительность и вот эта дедубликация, уровень дедубликации примерно такой же. То есть без ухудшения качества они сделали ускорение, от 3 до 10 раз, в зависимости, с чем сравнивать. Вот. Короче, интересная система. И становится понятно, как это все работает. И я прям советую к причине. Она так хорошо читается. Классно написанный пеппер. Вопросы? Правильно понимаю, что по сути просто быстрее стало и точнее разбивка? Да, получается, что... Тут сложно сказать точнее, не точнее. Они просто... Они просто немножечко поменяли правила для того, чтобы их быстрый алгоритм с высокой эффективностью работал. То есть предыдущий алгоритм, он тоже определял дедубликаты. Но на этих, как его... На больших кусках они происходили реже. А на маленьких кусках они происходят чаще, потому что эффективнее работает. У тебя какой-нибудь байт затешится, ты уже не замечаешь там и так далее. Вот. И получается... И получается, что да. Они ускоряли и сделали все быстрее. Вопрос, как это, конечно, дальше использовать. Я понимаю, как это использовать в маленьких системах, типа, я не знаю, там, баз данных или какой-нибудь хранилище, когда у тебя все по-другой. Но если у тебя огромные кластера и, я не знаю, петабайты данных, я плохо себе представляю, как это можно эффективно использовать. Ты же не можешь просто взять и выкинуть файл. У тебя кусок этого файла дубликат. И что ты дальше с этим делаешь? Ну, в смысле, ты при передаче, при репликации, при капах, вот это все ты хранишь по кускам. Ты обычно как... Ты хранишь файл, как... Ты представляешь файл, как ссылки на куски в определенной последовательности. Вот. Поэтому, когда тебе нужно запятить копию этого файла с небольшим изменением, ты на самом деле просто пишешь новые метаданные, что типа вот по такой вот куске, короче, вот те, по такому-то сдвигу, вот тот новый кусок, вот этот кусок. Я понимаю, о чем ты говоришь. Но это получается, мы убираем сразу всю идею файловой системы? Почему убираем? В смысле, вот посмотри, если ты возьмешь какой-нибудь, не знаю, у нас нету прям точных, точных, точной информации, как EBS внутри устроен, или, или, ладно, EBS неправильный пример, S3, внутри устроен, но, скорее всего, скорее всего, эта штука оперирует членками фиксированного размера, реальные объекты, особенно если они большие, они, скорее всего, представляются как раз-таки записью с метаданными, из того, как их... Это все понятно. К примеру, я вот говорю, например, S3 хранит это чанками по 100 килобайт. А тут мы определили, что внутри этих 100 килобайт вот эти 32 килобайта дубликат. Смотри, мы тогда храним по 32 килобайта. Ну, то есть, короче, они, опять же, не обязаны быть фиксированными, они там, скажем, не больше, чем сколько-то. Но тогда, получается, ты привязываешь свое хранилище тому, как у тебя работают алгоритмы дубликации. Ну, ты, скорее всего, привязываешь к тому, что он существует. Тебе важно просто, что вот это разделение, что у тебя есть какие-то физические чанки, которые ты сохранишь и реплицируешь, есть большое понятие объекта, которое видно пользователю. И... Я ровно про это имею в виду. Ну, к примеру, объект большой какой-то блоб, да? Он пользует... И он состоит внутри себя из чанков, я не знаю, там, 32. А тут внезапно мы обнаружили, что если взять чанк 33, потому что при передаче мы разбили его не на 32, а от 16 до 64 оно разбилось на 33, оно дублирует вот тот кусок. И тогда тебе, получается, твои куски по 32 нужно переразбить и сказать, у меня есть на самом деле не два по 32, а 33 и 31. Вот. И на лету сказать, а, кстати, вот этот 33, он дубликат какого-то вот того дальнего куска. Блин, это столько сразу изменений по всем движкам. Скорее всего, тебе не нужно менять алгоритм сразу везде. Почти. Вот ты сейчас другую проблему притаскиваешь, ты притаскиваешь по сути проблему миграции с одного алгоритма на другой. Скорее всего, в системе будет жить несколько алгоритмов какое-то время. Скорее всего, у тебя будет рядом с метаданами написано, какой версии алгоритма они разбиты. Да без разницы, какие алгоритмы они разбиты. Я к тому, что у тебя твоя дедубликация, она плавающая, она не фиксированная в длине. А, я понял, что этот конкретный алгоритм позволяет себе плавать дедубликациями. Да, и получается, что у тебя, ну вот определилось у тебя, что кусок этого чанка у тебя дубликат. Но для того, чтобы это использовать, ты должен у тебя внутри на лету менять структуру твоих данных каким образом. Я тут что могу сказать. Если оно ведет себя более-менее предсказуемо на одних и тех же данных, то, скорее всего, оно ну, типа да, ты, возможно, какие-то куски переразобьешь, возможно, больше, чем нужно переразобьешь. Но я так понимаю, что поинт в том, что предыдущий алгоритм в принципе хуже работали, и ты не в одном месте там, типа, лишний кусок переразобьешь, а ты в целом более крупной чанке будешь не считать дубликатами, когда они дубликаты. Да, да. Ну, и, кстати, сама вот эта переразбивка, она не такая сложная на самом деле. Чаще всего это какое-нибудь дерево, и у тебя ты просто вместо одного узла будет другой узел, вот и все. Ну вот да, то есть, как бы, у тебя же, ты сейчас, например, понимаешь, о боже мой, там где-то съехала дедубликация на байтик. Ну и, типа, черт с ним, потому что у тебя, если файл не гигабайт, и ты там, не знаю, вместо того, чтобы дедуплицировать строго один чанк, ну, ты, типа, два новых чанка запишешь с немножко другой границей. Это все равно может оказаться лучше, чем, типа, если оно в десяти других местах найдет ненастоящую корочку. Да, да. Ну, тогда тебе единственное, что сложность, которую я вижу, это нужно иметь какое-то хранилище ID тире данные, которые у тебя есть. Ну, у тебя почти типа в тех системах крупных у тебя большой объект стоит с кучей маленьких, и этот большой объект на самом деле хранится просто как список маленьких объектов. Или дерево маленьких объектов. Просто для того, чтобы понять, что вот мы сейчас передаем дубликат, ты должен очень быстро уметь по этому списку искать. Да, ну, слушай, это же может быть банально offset внутри файла. Согласен. Просто очень большой файл. Такой, на пятобайты. Нет, в смысле offset внутри файла, я говорю. То есть ты, например, когда ты представишь, что ты пишешь дропбокс, который, ну, дропбокс подобное хранилище, которое пишет в S3 подобное хранилище, с алгоритмом дедупликации, который мы рассматривали выше. Вот ты, когда ты читаешь из файла локально на системе, с которой ты пытаешься загрузить файл, ты же его сканируешь сначала до конца в любом случае, и смотришь как бы на дупликаты, на чанки, на каких offset, по какому offset каждый чанк живет. И ты, соответственно, потом берешь и посылаешь по такому-то offset вот такой-то хэшек, по такому-то offset такой-то хэшек и так далее. А принимающая сторона берет, сравнивает, считает у себя то же самое. Возможно, у нее потом даже эти хэшки где-то уже заранее посчитаны и сохранены, и даже не нужно пересчитывать хэшек. Конечно, так и должно быть. И она просто будет смотреть offset, хэшек, offset, хэшек, offset, хэшек, вот здесь не совпало, давай-ка мне вот этот чанк загружай. То есть тебе в любом случае нужно будет эту операцию производить. Я понял, ты говоришь про синхронизацию а-ля Dropbox, а я-то говорю про синхронизацию уровня вот у нас пользователь загружает файловую систему, а у другого пользователя есть, я не знаю, такая же файловая система, ну я не знаю. Если это вся файловая система, то у нее уже есть вот это дерево с этими хэшами. Да, да, да. И таких пользователей куча, и таких данных петабайт, и на все есть хэши, и вот поиск по этим хэшам может быть тяжелым, я только это думаю. Слушай, опять же, в зависимости от того, почему ты загружаешь целую файловую систему, и что это за странный юзкейс такой? Я новый пользователь Dropbox'а. Ну, тогда да, у тебя изначальная загрузка в любом случае будет большая и долгая. Да, но я просто к тому, что скорее всего, вот если я приду и буду загружать свою файловую систему, у меня уникальных данных там, я не знаю, полпроцента. Все остальные данные когда-то кем-то были загружены, можно просто сказать, а, это у него... Когда-то кем-то, скорее всего, так нельзя делать, просто если ты про такой юзкейс, скорее всего, это нельзя просто потому, что типа GDPR и всякое такое, я не знаю, может какой-нибудь Dropbox таким занимается, если есть какие-то прям очень популярные файлы, которые у всех одинаковые. Да, я не знаю, файлы операционной системы у всех одинаковые. Потому что никто не загружает операционную систему. Ну, а если мне надо, я не знаю. Ну, короче, anyway, не факт, что вообще так можно делать, даже если так можно, скорее всего, у очень популярных файлов, скорее всего, опять же, это, скорее всего, какая-то отдельная оптимизация, которая будет накручена в духе, ага, смотрите, это популярный хэшик. А вообще, при чем тут GDPR? Что, он неужели такое запрещает? GDPR сам по себе такое конкретно не запрещает, я просто, например, вперед, что у тебя может быть какой-то Data Privacy Regulation, который скажет, что вообще-то нельзя так делать. Нельзя делать что именно? Нельзя пытаться матчить... Чужой контент. Так я же не матчу контент. Я матчу хэш-контент. Именно что? Если такая регуляция есть, то у всех видео-аудио стриминговых платформ будут очень большие проблемы. Большие проблемы. Я имею в виду, в смысле, стримный дробок сделаешь. Аудио-видео стриминг, да, они по-любому так делают. Anyway, конкретно большие популярные файлы, скорее всего, у всех прям целиком одинаковые, я не думаю, что им там вообще у них есть задача разбить начальники популярных файлов. Я думаю, что у них просто есть полный хэш популярного файла, и, скорее всего, он просто у них один раз везде хранится. Это, скорее всего, правда. То есть, типа, вот та задача, которую мы решаем, разбивая начальники дедупликацию, она, скорее всего, даже не... Ее даже не нужно решать для популярных файлов. Но я вдруг понял, почему так нельзя делать на самом деле. Все напряженно думают, почему я не могу взять хэш от своего файла и сказать, что ага, он уже есть в облаке. Ну, хэш это... Ладно, а почему ты не можешь это во время передачи на чанке побить и сказать? Подожди со своими чанками. Давай рассмотрим простой кейс. Почему я не могу взять хэш от файла и сказать, что ага, у меня уже есть такой хэш в облаке, все окей. Ты боишься коллизий? Да, есть коллизия. Коллизий нет в идеальном мире. Немножко поможет, если хранить хэш и размер файла, это будет уже прям сильно-сильно лучше. Или два хэша, как гид делает. Ну, нет, два хэша не обязательно, достаточно просто взять ша-512 какой-нибудь. Вот. Два разных хэша просто дадут тоже очень маленькую вероятность коллизий, на которые можно забить. Пока все не завется. Да, я к тому, что в лоб посчитать хэш и сказать, что Саша, вот этот твой файл, это на самом деле вот тот файл, так будет достаточно неловко, когда у меня, понимаете, да, чужой файл появится в облаке. По-моему, даже где-то такое было, по-моему, что-то подобное я уже слышал. Более того, а теперь подумаем о том, что зономерный пользователь может обузить протокол и искать чужие файлы. Понимаете, да, просто генерируя случайные хэши. Абьюз, он все тяжел в установке. Я всегда, как бы, мне кажется, можно ради интереса попросить пользователей притащить любые системы, которые они считают невозможно абьюзить, а мы все вместе здесь придумаем, как же их можно абьюзить. Итак, мы обсудили еще одну интересную тему. Нам тут по секретному каналу писали карточку, ты не забыл подвигать? Я забыл карточку подвигать. Представляешь? Ага, типичная ошибка. Да, да, да. Придется там что-нибудь поправить. Так, и тогда мы переходим к следующей теме. Про Милвус. Милвус. Да, придется мучительно вспоминать, о чем доклад. Доклад из серии MLDB. Доклад читает господин по имени Ли Лиу. То есть у него имя Ли, а фамилия Лиу. Милвус это векторное СОБД, распределенное. А компания, которая занимается разработкой Милвус, называется ЗИЛИС. Это международная компания с разработчиками и сотрудниками из США, КНР и Германии. Насчет Германии я не уверен, есть ли там разработчики, но как минимум я видел там девелопера адвокатов. Характерно, что докладчик, он является выпускником CMU. С Система открытая, лежит на гитхабе, написано на ГО, лицензия Apache 2.0, а зарабатывать пытаются на Клауде, как это сейчас модно. Судя по сайту, у них прилично клиентов из всяких там IBM и подобного рода. Ну, прям много клиентов. Еще у меня выписано, что на сайте ЗИЛИС, если зайти в раздел вакансий, как будто бы у них нет удаленки, но там есть, специальная вакансия под названием «Если вы не нашли подходящую роль, тогда вот откликнитесь на эту».",
    "result": {
      "query": "JIR hashing algorithm efficiency"
    }
  }
]