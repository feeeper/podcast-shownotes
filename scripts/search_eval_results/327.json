[
  {
    "segment_id": "268311c6-1a06-4948-bb9e-1bc75e6ebe6b",
    "episode_id": "b54da985-5836-43c7-85f9-d41e25604c9e",
    "episode_number": 327,
    "segment_number": 9,
    "text": "Я был тогда человеком, который просто пришел в новую, как это, в совершенно новую для себя область. Я перешел из компиляторов в эти стартапы, и соответственно для меня это было вообще что-то новое откровение. Я с открытым ртом смотрел на все эти новые премудрости и пытался вникнуть, почему мы делаем момент так и не иначе. Но сейчас, когда я читаю эту статью, я понимаю, что мне в тот момент очень бы ее не хватало. Поэтому я прям советую тем, кто пытается сейчас понять идеи асинхронных каких-то коммуникаций, пытается понять, каким образом можно строить масштабируемые системы с отложенными какими-то заданиями, и каким образом, на какие проблемы вы натолкнетесь. Вот можете прочитать эту статью, она, кстати, есть даже на русском языке, они молодцы, они сделали перевод статьи на многие языки. На русском статье читабельная, есть несколько мест, где перевод не очень точно, но в целом разобраться можно. И прям вот советую всем, кто не знает и хочет понять, вот, пожалуйста, заходите и читайте. А я сейчас немножко вкратце пробегусь. Рассказывается про то, что очередь это в целом, способ асинхронной коммуникации, что можно посылать какие-то сообщения, можно даже задания посылать, чтобы вы посылали какое-то задание на другую ноду, она исполняет потом вам посылать через другую очередь обратно ответ. И все это удобно и хорошо, и, казалось бы, когда вы начинаете переходить на подобный способ коммуникации, вы получаете огромные фишки, огромные плюсы и кучу всего хорошего, из-за того, что вы никак не связаны, вы послали сообщение, смотрите на размер очереди, добавить обработчиков и таким образом получить масштабируемость, и все счастливы и довольны, но потом оказывается, что минус-то все-таки есть. Ну, например, один из минусов, который здесь хорошо освещается, это про то, что если у вас внезапно выросло количество скорость производства сообщений, и, ну, скажем, у вас идет там, не знаю, 10 тысяч сообщений в час, и в какой-то момент времени вам идет в два раза больше, 20 тысяч сообщений в час. Если у вас та же самая скорость потребления, то, соответственно, вам надо увеличить точно так же в два раза количество подписчиков, и тогда точно так же за час вы это сможете разобрать. Если вы не увеличиваете количество подписчиков в два раза, то вы сможете разобрать эти сообщения, которые нагенерировали сейчас, только, там, не знаю, в ночной период, когда PhotoMaker у вас генериров noirится меньше, потому что у вас же рассчитано на то, чтобы вы потребляетесь их с такой же скоростью, но возможно, вы можете делать чуть побыстрее, чтобы вы ничего не потеряли. Большинство очередей в современном мире, они куда-то синхронизируются на жесткие диски, и даже если что-то сломается, вы можете потом это восстановить, бэкапы, как бы, все это нормально. Но получается, что вы откладываете ответ на каждое отдельное сообщение на какой-то непонятный промежуток времени, вот как мы сейчас говорили, скажем, на час. Будет ли актуально сообщение через час? Ну, например, нантификация с моего старого проекта, нантификация курьеру, что, пожалуй, можете забирать заказ, вы стоите рядышком, так зайдите и заберите его, через час она уже совершенно станет не актуальна. Нужно ли в этот момент посылать эту нантификацию? Это очень сильно зависит от вашего приложения, от того, что вы собираете сделать. И оказывается, что в современном мире откладывание, выполнение заданий очень часто не имеет вообще никакого смысла. И, как говорит Amazon, многие клиенты при работе с очередями просят использовать не FIFO-логику, то есть First In, First Out, то сообщение, которым первым пришло, но первым работает, а LIFO, то есть Last In, First Out, для того, чтобы читатели сперва читали самые свежие сообщения, потому что то, что там лежит дальше, по стеку внизу оно, возможно, уже давно протухло и нам вообще ни скольчика не нужно. Конечно, все это сильно зависит от приложения, нужно в любом случае обработать все сообщения именно в том порядке, в котором они были посланы и так далее, но как бы это все зависит от вас, каким образом распорядиться с этим. Я тут случайно замьютился. Рассказывается немножко в качестве рекламы про SQS, про Лямбда, каким образом это используется в Лямбдах. Так-так-так-так. А, да. Вот. Еще здесь рассказывается про то, что вы должны... Здесь они говорят так, что как бы в синхронных системах вот эти сообщения, когда посылаются, вы должны для себя определить какой-то порог и, скажем, все сообщения должны быть разобраны там до этого порога, а иначе они считаются протухшими. То есть они вот, скажем, в какой-то системе они советуют, что задержка должна быть хотя бы меньше секунды. То есть здесь не объясняется, почему они выбрали секунду, почему не минуту или не час, но в целом идея понятна. То есть как бы вы определяете, что есть корректное сообщение. Возможно стоит отбрасывать сообщения, которые слишком старые. Но мы про это сейчас только что говорили. И очень часто приходится измерять с помощью метрики, графики выводить, возможно выводить аллерты, если у вас появляется внезапно большая задержка. Но опять задержками бывает такая проблема, что скажем, когда вы делаете сообщение просто один к одному, например, один читатель разослал, один писатель разослал сообщение, один читатель прочитал его. Ничего страшного не произойдет, если задержка... В смысле, если появляется чуть больше сообщений, но читатель будет точно так же их прочитывать. И иногда бывает такая логика, что вам надо делать фанаут. Фанаут – это когда вы посылаете одно сообщение, а оно расходится там, не знаю, тысячу подписчиков. Или, например, оно расходится миллиону подписчиков. Очевидно, что чем больше вот этот вот фанаут фактор, тем больше будет задержка. Скажем, разослать это десяти подписчикам – это не то же самое, что разослать это миллиону подписчиков. И какой-то из подписчиков может быть в этот момент вообще недоступен. Скажем, интернетов things, который тоже в AWS есть, и здесь они часто приводят в качестве примеры, в том числе, наверное, ради рекламы. Часто бывает так, что какое-то сообщение послано к какому-то устройству, а в интернетов things очень часто бывает устройство экономит электроэнергию, и поэтому подключены к интернету не все время, а подключается раз в какой-то промежуток времени, и он будет ждать следующего подключения – это сообщение для этого устройства, будет ждать следующего подключения этого устройства для того, чтобы послаться ему. Вот, соответственно, задержка между отсылкой и получением она может быть очень большая, но это зависит опять же от устройства, это может быть не являться проблемой. Каким образом эти метрики строить тоже не до конца понятно. Они советуют здесь метрики типа от отсылки до первого прочтения, это будет показывать, что когда начинают рассылаться. Последнее прочтение тоже, наверное, имеет смысл, но тут тоже такая непонятная логика, потому что очень многое зависит от вашего приложения, а не от архитектуры, инфраструктуры этого очередей. То есть, надо понимать зачем и что вы делаете. Так, так, так, так, так. А, да, здесь они очень много размышляют про то, что многие сообщения, многие системы сообщений делаются для большого количества пользователей, и здесь вот очень интересное размышление на тему того, как это правильно делать. Скажем, у вас система платформы какая-то, в которой есть 20 клиентов, и все эти 20 клиентов получают сообщения не знаю, скажем, тот же самый Clubhouse или Podcasts или еще что-нибудь такое. То есть, кто-то генерирует эти сообщения и рассылает это во все каналы какие может, и подписчики подписывают и слушают именно этот стрим, там, эти сообщения. Вот. Использовать нужно для всех клиентов одни и те же очереди или использовать разные очереди? Если разные очереди или если одни и те же очереди, надо как-то пытаться разделить их по серверам для того, чтобы у них были независимые каналы и клиенты не влияли друг на друга. Или нужно делать какие-то выделения. Здесь статья не приводит ответ, что лучше всего использовать Multi-tenancy, то есть, как бы, когда у тебя все каналы для всех рассчитаны, и это позволяет очень сильно экономить с точки зрения ресурсов. Вот. Но одновременно вам нужно придумывать какую-нибудь систему защиты. Например, если какой-то клиент внезапно в десять раз увеличивает нагрузку, вам очень тяжело будет справиться с эффективностью вашей системы, чтобы у всех остальных клиентов, которые не увеличили нагрузку, latency внезапно не увеличился из-за вот этого жирного клиента, который начал сильно нагружать вашу систему. Каким образом делать эту защиту, каким образом делать ограничения, надо где-то вставлять rate limiting или выделять его в этот момент в отдельные сервера, чтобы он никак не влиял на остальных клиентов. Вот это все становится очень сложной задачей. Это не очевидно в тот момент, когда вы проектируете систему. Но это такие базовые вопросы, которые точно вас коснутся. Я не хочу пересказать всю статью. Мы как бы немножечко зацепили. Сейчас я посмотрю, может еще какая-то есть интересная тема, но в целом я прям очень советую статью. А, да, здесь они размышляют на тему мертвых очередей. То есть очереди сообщений, которые не смогли обработаться, или, скажем, очереди сообщений, которые отстают сильно от графика, и вы можете таким образом с помощью очередей для мертвых сообщений хранить долгое сообщение, которое не смогли обработаться для того, чтобы их человек разобрал, или, скажем, очередь, которая не успевает обработаться из-за того, что потребители недостаточно быстрые, или по каким-то другим причинам вы можете часть сообщений отсылать в другие очереди, чтобы они, чтобы подписчики получали, в том числе и актуальную информацию, а не слишком старую. Вспомним про нотификацию для курьеров. Это все примеры как вы можете пытаться бороться с перегрузкой вашей системы, при том, что с одной стороны, эта перегрузка неизбежна, даже если вы попытались полностью защититься, вы все время что-нибудь не учтете и наткнетесь на какие-то возможные сбои в виде большого количества сообщений, посылаемого откуда-то, куда-то, а с другой стороны, про это все равно надо заранее думать и придумывать заранее все эти механизмы. Наверное, как-то так. Если вкратце к другим статьям, здесь есть хорошие отсылки на разные другие материалы, в частности, зацепляется закон Литла, что если у вас есть 100 сообщений в секунду, а среднее время обработки составляет где-то 100 мс, означает, что для того, чтобы обрабатывать этот поток сообщений, вам нужно около 10 или плюс потока, плюс потока в зависимости от. Потому что 100 сообщений, 0,1 мс на каждое сообщение, автоматически означает 10 потоков. При этом становится очевидно, что если внезапно вы как-то так переписываете логику, чтобы у тебя возрастает empowered время обработки в два раза, значит в два раза увеличивается количество потоков, или если внезапноlanookаение подписки по 어�ению Mercury по количеству потоковowe то есть, конечно, тоже возрастать. Такая логика, это базовый подсчет, который позволяет посчитать плюс-минус, как вам надо правильно обрабатывать. Мне понравилось. И здесь хорошо обыгрывается. И еще я хотел что упомянуть в этом контексте. Здесь тоже зацепляется этот вопрос. У нас в компании, когда я писал автоскейлер для Kubernetes, очень часто приходили, и помню я это даже здесь уже рассказывал, приходили и спрашивали следующую логику, что, ребят, как нам правильно сделать так автоскейлер для Kubernetes, чтобы масштабирование сервиса происходило в зависимости от количества сообщений в очереди. Например, у нас там есть какой-нибудь RabbitMQ, который какая-то очередь, нам есть кто-то, кто посылает сообщение в эту очередь, а нам нужно количество подписчиков на эту очередь, обработчиков этих сообщений увеличивать, если внезапно у нас возрастает поток этих сообщений. И казалось бы, логично, чем больше сообщений в очереди, надо соответственно увеличить количество обработчиков, количество обработчиков станет больше, они смогут быстрее разгрести этот поток, меньше проблем у пользователей. Логично? Нелогично. Нелогично это потому, что когда вы запускаете новый обработчик какое-то сообщение, ну там, было 100, вы запустили еще 100, а стало 200, конечно, они схватят эти 200 дополнительные 100 новых сообщений из очереди, и конечно, они начнут их обрабатывать. Но вопрос, что они будут дальше делать с этими сообщениями? Ответ такой, что, скорее всего, они пойдут ломиться с запросами в другие части системы. Готовы ли эти другие части системы к в два раза увеличенному количеству сообщений, запросов, или там, генерации новых сообщений? Скорее всего, нет. То есть, как бы, или если да, то это тоже будет спустя какое-то время, и у вас вот эта вот волна перегрузки каких-то разных компонентов, она будет гулять по вашей системе, создавая довольно большое количество проблем, потому что, во-первых, это не очевидно, откуда идет это перераспределение нагрузки. А во-вторых, ну, как бы, вы чаще всего об этом не думаете заранее. То есть, ну, когда вот к нам приходили с запросом, а давайте мы сделаем такую штуку, чтобы вот у нас сейчас стало в 100 раз больше сообщений. Мы хотим в 100 раз, всего лишь в 100 раз количество обработчиков увеличить. Мы такие, а кто вас потребляет? Ну, как бы, вы спрашивали тех, кого вы используете, они смогут в 100 раз больше увеличенную нагрузку сделать? Он говорит, ну, нам как бы это не очень интересно, у нас главное свою проблему решить. И это довольно стандартный шаблон, то есть, как бы люди думают, думают, как решить свою систему. Кто врывается? А, в смысле, потому что я кликаю, соряно, обладен аминометией, а случайно обладен аминометией. На самом деле, я уже рассказал все, что я хотел рассказать. Здесь 12 страниц всего статьи, она не очень большая, ее можно прочитать за часик, я с комментированием и так далее за часик ее прочитал. Вот. Я прям ее советую тем, кто в этой теме не очень пока разбирается, но хочет погрузиться, вот заходите, читайте, смотрите, интересно. Хотя деталей хотелось бы побольше, конечно. Иногда они маловато дают, и общими словами много рекламы еще дают своих сервисов. В тему ты сказал, Ваня, про вот эту волну, которая запускается и постепенно так или иначе показывает, какие части системы могут справиться с нагрузкой. И вот мне это напоминает в поток обработки данных. У нас есть такая вещь как Backpressure, и оно частично помогает справиться с этим. И вот я думаю, есть ли что такого плана, но для более масштабных систем. Кстати, да, это прикольно, тема масштабировать кубинетиз сервис на основе того, сколько Backpressure он дает в обратную сторону. Поясни идею, что-то я пока не очень понимаю. То есть, как только у тебя сервис начинает создавать Backpressure в обратную сторону, добавлять ему Worker, потому что, очевидно, что если он создает Backpressure, значит он не справляется. И Backpressure, то есть тут возникает вопрос в том, как именно снимать метрику Backpressure, потому что разные системы ее по-разному отдают. То есть, они по-разному это производят. То есть, это обычно в протокол, вводится или какое-то негативное подтверждение, или, не знаю, как в кавке, там Backpressure создается просто из-за того, что ты, как потребитель, ты будешь высасываться своей скоростью. Давайте я поясню для слухов, которые не в теме. Backpressure – это механизм, когда у вас есть несколько подсистем, ну скажем, подсистема A, есть подсистема B. И приходит запрос от пользователя подсистему A, она очень быстрая, такая классная, четкая, быстро разгребает запросы и спрашивает одновременно подсистему B. А подсистема B медленная, тормознутая, и она ничего не может сделать. Это означает, хотя подсистема A в целом быстрая и хороша, но подсистема A без B не может сделать ничего. И поэтому, пока подсистема B не сделает все, что надо сделать, подсистема A бесполезна. Поэтому вводится такая система, что B говорит, а, слушай, ты меня тут спрашиваешь с слишком большой скоростью, давай-ка замедлись. И подсистема A начинает медленнее брать запрос от пользователя для того, чтобы каждый запрос можно было обработать корректно и корректно отдать пользователю. Вы получается таким образом разустреняете давление обратно к точке, откуда приходит это давление. И с той точки, которой не может обработать достаточное количество давления. Поэтому эта штука называется обратное давление. Выпускаете волну обратного давления. И в синхронной системе понятно, как это сделать? То есть, если A вызывает B синхронно, то пока бы не ответила A, пока A не сможет взять новый запрос. И это все очень легко сделать. В синхронной системе A просто послет сообщение в сторону B, а B, когда сообщение обработает, оно теоретически послет обратно. Но если B очень медленно, это значит, что количество сообщений перед системой B будет бесконечно расти и в конце концов убьет систему. То есть, нужно мерять количество сообщений в очереди для подсистемы B и говорить, ка если я не знаю, что делать под систему B и говорить, ка если эта очередь растет быстрее, чем под систему B успевает обрабатывать, нужно говорить системе A, типа «хей, давай ты замедлишься» или автоматически добавлять чего-то для подсистемы B. Да. Но иногда под систему B нельзя увеличивать. То есть, она может быть не масштабируемая. Ну, не знаю, один жесткий диск и все, вы больше не сможете обработать. На смысл, что я имею в виду, обычно бэкпрэшер ничего автоматически не добавляет, я бы сказал. Бэкпрэшер просто чтобы замедлиться, но можно сделать кубернетис-оператора, который детектит наличие бэкпрэшера в протоколе, что он начал случаться, и добавляет страдающей системе воркеров, я вошут имел в виду. Да, это я просто немножечко на шаг назад сделал, чтобы более простую концепцию объяснить. Теперь эта концепция, я думаю, слушателям станет более понятной. Есть ли что-то в кубернетис, которое умеет это делать из коробки? Есть API, и ты можешь написать все, что хочешь. Тут проблема именно в том, что у тебя в каждом протоколе будет какой-то свой собственный уникальный бэкпрэшер, и наверное очень тяжело его захватить в общем случае, что он происходит. Ну, в кубернетисе есть этот метрик с API, как бы вы можете любую метрику посылать в качестве сигнала для любого другого как это, оператора, скажем. Но это конечно все придется как-то писать на коленке. Это не сложно, но это надо делать самостоятельно. Надо сказать, что многие, мне кажется, продакшн системы должны иметь бэкпрэшер как какую-то метрику, которая видимо человеческому оператору, значит, наверное, ее можно и кубернетис оператору скромить. Да, но я, сказать по правде, не видел большого количества систем, которые бы это делали на уровне дизайна всего проекта, всей системы целиком. Хотя вещь очень полезная. Так, ну что, и мы переходим к следующей теме, которая, казалось бы, вообще никак не связана с кубернетисом. Я отмолчусь, потому что у меня машина делает звуки... стиральные, а не стиральные, это пустымочные.",
    "result": {
      "query": "message queue scalability challenges"
    }
  }
]