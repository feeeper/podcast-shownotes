[
  {
    "segment_id": "ccffae41-f7e1-4729-9c83-f539eb3c2b23",
    "episode_id": "ff2102a7-b54a-47ae-866f-24f39b7bbbc0",
    "episode_number": 186,
    "segment_number": 4,
    "text": "Ну то есть да, большой такой дисклеймер, это слухи. На Bloomberg.com появился слух про то, что к 2020 году Apple откажется от процессоров Intel. В пользу ARM своего. И да, я не могу передать словами, как я презираю Bloomberg.com, потому что уже в пятый раз, наверное, я открываю эту страничку с новостью, и они мне в уши включают видео, которое я не просил включать мне в уши. Ненавижу такие сайты. Между прочим, ну, проблема в том, что один из этих сайтов – YouTube. Ты как бы открываешь вкладку, и он тебе включает видео, хотя ты его не просил. И это не настраивается. Подожди секунду, я открываю вкладку с видео. Единственный контент, который есть на этой вкладке – это видео. Вот здесь я жду этого. А знаешь, что происходит у меня? Я, например, после ребута открываю браузер с вкладками, и у меня включается 50 видео. Ну, хорошо, не 50, 10. И они все начинают говорить одновременно, и они в разных вкладках, и тебе нужно по очереди их все замьютить. Знаешь, как это неприятно? Это, кстати, странно. Там вроде же есть API, чтобы понять, активная ли вкладка, и только тогда у меня YouTube, во всяком случае, что в Firefox, что в Chrome. Кстати, у меня тоже, да. Это работает, когда ты открываешь новую вкладку, это не работает, когда ты запускаешь в браузер, и у тебя запенены вкладки. Так что это API немного сломан. Это баг. Баг – то, что у YouTube нет настройки, которая вообще выключает это. Я хочу прям жмакать в клуб, мне нетрудно. Ну, кстати, если кто-нибудь из слушателей знает, как это починить, я был бы очень признателен. Да, но новость, даже получается, не новость, это слушок. Ну вот вы бы одобрили или осудили отказ от Intel в пользу ARM? У меня смешные ощущения. То есть в чем-то типа вот этих мелких MacBook, я бы скорее одобрил, потому что, ну, если хочешь, ты все еще можешь тот же самый Unix софт собирать под ARM, и на самом деле сейчас уже много чего под ARM крутится, не знаю, та же самая Raspberry Pi. То есть мне нет сомнений, что большая часть софта более-менее как-то будет работать. Особенно там они успешно раньше делали переход с PowerPC на Intel, я думаю, что они, у них есть опыт это делать. С другой стороны, на чем-то типа машин, которые вот прошки, которые специально люди покупают, чтобы заниматься чем-то таким, какой-то профессиональной деятельностью, там это, скорее всего, будет губительно, я почти уверен, что там, если они вообще там будут менять процессоры, это будет долгое время, что какая-то история из разряда, вот у",
    "result": {
      "error": "API request failed: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-xSG9D345gtYlIKC330wCyrEG on tokens per min (TPM): Limit 30000, Used 30000, Requested 1391. Please try again in 2.782s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
      "stack_trace": "Traceback (most recent call last):\n  File \"/home/andrei/Projects/podcast-shownotes/scripts/build_search_eval_dataset.py\", line 157, in generate_search_query\n    response = await self.client.chat.completions.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 1927, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1767, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1461, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1547, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1547, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1562, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-xSG9D345gtYlIKC330wCyrEG on tokens per min (TPM): Limit 30000, Used 30000, Requested 1391. Please try again in 2.782s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
    }
  }
]