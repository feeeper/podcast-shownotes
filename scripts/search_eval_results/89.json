[
  {
    "segment_id": "3c8f1aad-37f9-4e8a-8ce9-73c19c99de29",
    "episode_id": "202e1585-e5e6-4c24-bf42-8921e076d044",
    "episode_number": 89,
    "segment_number": 7,
    "text": "Или не надо уже? Расскажи, надо? Надо, обязательно. Там идея была в том, что у тебя есть 64 ядра в твоём процессоре, и они разделены на 8 нуманод по 8 ядер. И когда они запускали свою математику на R, у них это попадало на 2 разных нуманоды, а вся остальная логика по сборке, make, она попадала на остальные 6 нуманод, равномерно распределяясь между ними. И получалось, что средняя нагрузка между всеми, у всех нуманод была примерно одинаковой, так вот совпало. То есть те ядра, которые делали make, получается 8 ядер в одной нуманоде, и 6 на 8, 48 ядер делали make, и соответственно ещё 16 ядер делали R. Но R делался в один поток, и получается в нуманоде, внутри одной нуманоды использовалось только одно ядро, а остальных пустовали. Для того, чтобы на них перекинуть задачи, мы должны были на верхнем уровне балансировки, когда мы балансируем между нуманодами, должны были определить, что у тебя где-то есть много работы, а где-то есть мало работы. Но мы, когда сравниваем среднюю загрузку, средняя загрузка получалась одинаковой, потому что R жрёт много CPU, а make потребляет много IA, и поэтому они по CPU примерно столько же занимают. И получалось, что у тебя 7 ядер в нуманоде, который делает R, они никак не получали нагрузку. После того, как они сделали FIX, FIX был сравнивать минимальное количество нагрузки на какую-то единицу. То есть внутри нуманоды ты смотришь все ядра и берёшь то ядро, которое делает минимальное количество нагрузки. И аналогично ты берёшь ядро в другой нуманоде. То есть, например, это нуманода, которая простаивает и ничего не делает, она меньше, чем любое другое ядро в соседней нуманоде. После такого у тебя получалось, что у тебя на одном ядре делался R с математикой, а остальные ядра немножко подгружались задачами make с других нуманод. Вот как-то так. Все уснули, похоже. Как страшно жить. Ты просто знаешь, я так слушать у тебя и думаю, боже. Боже, ну есть правильно даже потрачено. И на самом деле получается так, что я не знаю сколько точно, они может быть и делали анализ, но здесь не говорят сколько. Нет, ну просто смотри, извини, я перебиваю снова. Ну в общем, представь, у нас есть сотня HPC-кластеров по миру, да? И почти все из них работают на Линуксе. И тут мы говорим, что банальная математика и запускалка математики. То бишь типичная задача HPC-кластера. Я тебя все-таки поправлю, а на большинстве серверов пока, насколько я знаю, не нумы и далеко не 64 ядра. Я говорю про HPC. Ну там и не везде нумы, про HPC. Нет, а давайте, здесь в конце есть такая табличка, это вот анализ этих багов. И вот тут получается 4 бага, которые они исправили. И, значит, статистика. Первое, во-первых, ядро, версии ядра, на которых этот баг был добавлен, ну, появился. То есть в первом это 2.6.38 и максимальное увеличение производительности после фикса 13 раз. Второй баг 3.9 ядра, максимальное увеличение производительности 27 раз. Третье 2.6.32 и вот как раз 22%. И четвертое это 3.19 и там 138 раз. Вот так вот. Какая жесть.",
    "result": {
      "error": "API request failed: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-xSG9D345gtYlIKC330wCyrEG on tokens per min (TPM): Limit 30000, Used 30000, Requested 1631. Please try again in 3.262s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
      "stack_trace": "Traceback (most recent call last):\n  File \"/home/andrei/Projects/podcast-shownotes/scripts/build_search_eval_dataset.py\", line 157, in generate_search_query\n    response = await self.client.chat.completions.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 1927, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1767, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1461, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1547, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1547, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1562, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-xSG9D345gtYlIKC330wCyrEG on tokens per min (TPM): Limit 30000, Used 30000, Requested 1631. Please try again in 3.262s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
    }
  }
]