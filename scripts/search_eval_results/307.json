[
  {
    "segment_id": "95d98f3d-de04-44ff-a917-36437d138401",
    "episode_id": "b74fe6c2-9576-4c08-a4ab-4f15370fd33a",
    "episode_number": 307,
    "segment_number": 2,
    "text": "И если компания только открывает в штатах, далеко не всегда она может себе позволить там нанять и обучить, короче, кого-то, кого можно допустить к продакшену. Ну, и понятие ночи, это такая штука субъективная. Моя вот смена начинается в 6 утра, и для меня 6 утра это ночь. Поэтому... Полностью согласен. Вот, ну, мы все знаем, что машинное обучение решает все проблемы, и мы можем просто научить модель, значит, подавлять ложноположительные срабатывания аллертов и спать крепко, и у нас все будет хорошо. Вот, а правильное аллерт оно, конечно же, не подавит, и мы будем разбужены и все починим. Ну, или не иметь ложноположительных. Ну, это сложно, это думать надо, когда аллерт пишешь, там, шумные данные, сложные метрики. Мы об этом тоже поговорим. Вот, но прежде чем погубиться... Нам, в общем, думать не хотелось бы. Да, думать не хочется, хочется думать о более приятных вещах, там, строить новые системы, а не поддерживать старые, ну, и все такое. Вот, поэтому чем меньше времени потрачено на мониторинг и построение мониторинга и обслуживание мониторинга, тем лучше. Правда? Так вот, прежде чем мы погрузимся в детали машинного обучения и Operations, такое краткое напоминание о том, какое бывает машинное обучение. В принципе, можно поделить на четыре типа. Есть обучение с учителем, это то, что все мы знаем и любим, когда у нас есть много примеров, что должно быть на входе, что должно быть на выходе, мы их кидаем в алгоритм, алгоритм их жует, жует, жует, и научается предсказывать более-менее адекватный выход по входу. Вот, проблема, конечно, в том, что надо достаточно много примеров, насколько много зависит от того, насколько сложная проблема решаем, и насколько сложный алгоритм. Там есть алгоритмы, которые требуют поменьше данных, чтобы обучиться вроде деревьев принятия решения, а глубокие нейронные сети, например, любят очень много данных. Ну, классический use case для такого обучения, это распознавание образов на картинках, он очень хорошо работает, берем, размечаем картинку, говорим, нарисован на ней котик или единорожек, и, значит, распознаем фотки либо Света, либо Саши. Вот, вариант номер два – это обучение без учителя, это такой немного более редкий случай, но, как правило, он сводится к тому, что у нас есть куча данных, о которых мы не очень чего знаем, у нас они не размечены никакими классами, но мы хотим, чтобы робот взял и сам их как-то на классы поделил по принципу похожести. Это, в принципе, неплохо работает для того, чтобы находить какие-то аномалии, выбросы статистические, и, например, активно используется для таких вещей, как fraud detection, потому что мошенники ведут себя не совсем как настоящий пользователь, ну, или, по крайней мере, плохие мошенники. Вместе с этими двумя типами их можно немножечко друг с другом пересечь, и получится такое полуавтоматическое обучение. Когда у нас есть датасет, мы размечаем небольшую часть этого датасета, потому что размечание – это, как правило, ручная работа, требует людей, люди дорогие, мы не хотим много времени тратить, поэтому мы размечаем небольшой кусочек датасета, потом используем кластеризацию, чтобы распространить, эту разметку на всю оставшую часть датасета, и кидаем это уже в обычный алгоритм для обучения с учителем. Ну и, наконец, самый последний модный молодежный способ обучения – это обучение с подкреплением, в английской литературе известный как reinforcement learning. Дико популярный с тех пор, как DeepMind выиграл все игры на свете с помощью reinforcement learning. Работает он, грубо говоря, так, что есть наш агент, который обучается, он совершает действия в виртуальном или реальном мире, и мы в ответ ему даем положительное или негативное подкрепление, чтобы он знал, делает он правильно что-то или нет. Вот. Этот алгоритм классный, он хорошо работает с задачами, когда не очень понятно, как там достигать, как он может быть в этом агенте, но его проблема в том, что он требует еще больше данных для обучения и в целом довольно дорогой. А если мы хотим оперировать в реальном мире, то обучаться мы можем только со скоростью реального времени, и это требует очень большого количества времени. Вот. Поэтому применять такой подход довольно сложно для практических целей. Вот. Случит пока понятно, только непонятно звучит количество. Вот многое это сколько? А очень многое это сколько? Ну смотри, обучение с учителем, это сильно зависит от того, сколько у вас входных сигналов, сколько у вас там вскрытых переменных вот этого всего, но грубо говоря, для обучения с учителем надо там начинать 10 в 6, до 10 в 12 датапонтов. В некоторых случаях можно меньше, но для большинства задач в реальном мире, типа миллион точек данных это начальная точка. Обучение с подкреплением, как правило, считается, что типа на три порядка больше для похожей задачи. Но если мы будем обучать, то мы будем обучать, то мы будем обучать, то мы будем обучать. Я слабо представляю, как в нашей компании собрать количество аллертов для того, чтобы ее научить. Спойлеры, спойлеры, спойлеры. Отлично. Ты на самом деле прав, потому что сборка и разметка данных это одна из наибольших проблем для применения машинного обучения к Operations. Во-первых, проблема состоит в том, что в некоторых случаях просто недостаточно примеров. Я потом расскажу про небольшой эксперимент, который я делал на работе несколько месяцев назад, и скажу, сколько датапонтов у меня получилось и за какой период времени. Но мы до этого дойдем. Другие проблемы с данными для обучения состоит в том, что машинное обучение в целом неплохое для задач интерполирование, то есть когда наше обучающее множество оно полностью покрывает множество точек, на которых мы будем делать предсказания. Но оно совершенно неадекватно ведет себя, когда надо обобщить и экстраполировать на случай, который не похоже ни на что то, что видел алгоритм перед этим. В принципе, это... Подожди, возможно, спойлер, а нет ли какого-то такого очевидного подхода для того, чтобы алгоритм хорошо интерполировал, но если он не может здорово интерполировать такие пейджи? Чтобы он что, прости? Ну, чтобы он пейджил только тогда, когда не может интерполировать. То есть если в какой-то простой способ определить, что это какие-то очень странные данные, мы не можем тут интерполировать, давайте... Извал человека на помощь. Теоретически да, однако проблема в том, что другая часть проблемы сбора данных это в том, что продакшн это сложная система, и у нас очень много входных сигналов. Там сотни, если не тысячи, не десятки тысяч разных метрик, там не знаю, сотни или десятки под систем, все они как-то друг с другом взаимодействуют. Если у вас 10 тысяч входных переменных собрать такой датасет, чтобы он хорошо покрывал все вменяемые значения 10 тысяч переменных во всех адекватных комбинациях, это сложно. Можно попытаться читерить и генерировать искусственные примеры, но это тоже... У этого есть свои проблемы в том, что зачастую такой датасет может на самом деле не быть похож на реальное поведение системы, и ваша модель научится какой-нибудь ерунде. В общем, проблема номер один репрезентативность датасета, проблема номер два его размерность, проблема номер три состоит в том, что когда люди размечают датасеты, относящиеся к продакшену, они на самом деле не очень консистентно размечают. Если вы возьмете двух экспертов и посадите их распознавать, я не знаю, текст в книжке, они, скорее всего, более-менее одинаково его переведут. Если вы посадите двух инженеров определять root cause для инцидента, скорее всего, они придут к разным ответам. Может, не совсем разным, но немножко разным. И из-за этого очень сложно получить высококачественный датасет, потому что если вы поделите работу между несколькими инженерами, то каждый сделает ее немножко по-своему. Если вы отдадите ее одному инженеру, он уволится, потому что кому охота сидеть и три года размечать какой-то дурацкий датасет. Ну и не говоря уже о том, что мы в то время, пока мы не онкол, мы пишем новые системы, и соответственно наша обученная модель ничего не будет знать про новые системы, плюс меняются окружения, меняются версии ядра, это тоже как-то влияет на метрики, поэтому проблема оказывается в том, что, во-первых, данные устаревают, а во-вторых, их нужно периодически перегенерировать. Это дорого. Вот. Другая проблема состоит в том, что большинство алгоритмов машинного обучения непрозрачны, то есть очень сложно понять, а почему, собственно, он выдал вот именно такой результат. Сейчас есть довольно много всяких исследований на тему того, как мы можем интерпретировать нейронные сети, и в некоторых специальных областях, типа распознавания образов, начинают появляться некоторые концепции о том, как таки нейронные сети себя ведут, но в общем случае мы не умеем интерпретировать нейронные сети, а нейронные сети это, естественно, то, что все больше всего хотят использовать. Вот. Умные датасайнтики... Подожди, подожди. Мне кажется, вот тут как-то ты так перепрыгиваешь. Мне кажется, задача не просыпаться, а нет задачи, у нас вроде не было задачи использовать непременно нейронную сеть. Или все такое? Правильно. Нет, не обязательно нейронную сеть. Я как раз хотел сказать, что умный датасайнтист тут же возьмет и скажет, что зачем нам нейронная сеть, нейронная сеть отстой, мы используем там, интерпретируемый алгоритм, типа линейной регрессии или логистической регрессии, или дерево решений. Проблема в том, что все они на больших размерностях входных данных тоже быстро становятся очень непонятными и неинтерпретируемыми. Вот. Одно дело, когда ты там даешь доклад, рисуешь красивое дерево из трех узлов и говоришь, ну вот абсолютно понятно, что мы посмотрели на переменную здесь, и это означает A или B. Ура, мы получили наш ответ. Когда у вас в дереве там тысячи бранчей, и они все как-то странно ветвятся, непонятно, как это дерево вообще построилось, можно нейронную сеть натренировать с тем же успехом. Вот. То есть, опять же, чем сложнее прибентная область, которую мы пытаемся моделировать, тем сложнее модель, тем сложнее модель понимать, а продакшн это сложная область. В общем, помимо чисто теоретической сложности в интерпретировании моделей, есть проблема в том, что из-за того, что их сложно интерпретировать, люди имеют тенденцию не доверять таким системам. Даже хотя люди делают ошибки сами по себе, мы люди, в принципе, довольно неплохо можем предсказать ошибки других людей. Предсказать ошибки моделей очень сложно. Они могут ошибаться совершенно безумными способами, и у них нет здравого смысла, который есть у нас, чтобы сказать, ой, постойте, я что-то фигню несу. Вот. Соответственно, даже если мы натренируем модель, нам все равно придется написать кучку евристик, чтобы перепроверять ответы нашей модели. Мы начали с того, что мы не хотим думать и хотим, чтобы машинное обучение за нас все сделало. Правильно? Далее. Третья проблема с непрозрачностью состоит в том, что машинное обучение по природе своей находит корреляции между входными данными, но эти корреляции далеко не всегда имеют причинно-следственные связи. В качестве упражнения погуглите Spurious correlations, и вы найдете прекрасный сайт, который показывает очень убедительно, что выручка с игровых автоматов очень сильно коррелирует с количеством PHD, которые были выданы в Соединенных Штатах. Если вы мониторите выручку игровых автоматов, я не уверен, что вы будете себя хорошо чувствовать, зная, что ваша модель может тупо смотреть на количество PHD, которые выдали в Соединенных Штатах. Ну и последний, более экзотический случай. В некоторых случаях просто есть юридические причины, когда нам нужно точно иметь возможность объяснить, почему система так себя повела. Это достаточно редко встречается в контексте продакшена и онкол, но встречается. Вот, это общая проблема. Теперь можно поговорить о том, как мы все-таки могли бы попытаться решать разные проблемы и вообще какие проблемы с помощью машинного обучения мы можем решить. Первая проблема, которая приходит на ум, это, как я уже говорил, у нас в каждой хоть сколь или быть сложной системе есть куча метрик, у нас есть куча реплицированных задач, каждая может там давать десятки метрика себе, использование процессора, памяти, количество попаданий в кэш, количество промахов в кэш, latency, RPC вызовов, все что угодно. Когда у нас происходит какой-то аутуч, нам хотелось бы, чтобы кто-то за нас подумал и сказал, ну вот эти графики, наверное, тебе скажут, что, собственно, сейчас не так. Вот смотри на них и не смотри на все остальное. Жуткая экономия времени очень хочется иметь. Давайте попробуем подумать, как это сделать. Звучит как задача классификации, правильно? Мы хотим сказать, что для каждой метрики, грубо говоря, есть один или более классов, что при таких входных данных эта метрика интересна или неинтересна. Хорошо, что нам нужно сделать? Назначаем каждой метрике класс, назначаем, идем, просматриваем нашу полную историю инцидентов, если она у нас вообще есть, если нет, то о каком машинном обучении речь. Просматриваем инциденты, размечаем, какие метрики были интересны при каком инциденте и обучаемся. На этом этапе мы моментально наступаем на все проблемы, которые я описал раньше. Очень много метрик, которые нужно разметить, не очень много инцидентов, которые можно разметить. Обучение в итоге получается не очень качественное, и в целом даже простецкие ивристики, написанные человеком, работают лучше. И чтобы окончательно закопать эту идею, вспоминаем, что состояние мира меняется, месяц спустя мы должны будем пойти и переделать эту работу заново, потому что у нас добавились новые метрики, новые системы и так далее. На данном этапе оказывается, что простые ивристики в духе, я не знаю, у нас сбоить система, давайте посмотрим графики, которые приходят из этой системы, они работают гораздо лучше и надежнее, чем наше любимое машинное обучение. Грустно, но что поделаешь? Может быть... Будущее это за машинным обучением, правильно? Будущее, безусловно, за машинным обучением. Возможно, даже это будущее наступило, и может оно работает с аллертами, правильно? Потому что с чего мы начали, мы не любим просыпаться по ночам, давайте используем машинное обучение для того, чтобы улучшить наши аллерты. Более того, давайте сделаем еще шаг вперед и скажем, если уж мы занялись проблемой, давайте машинное обучение вообще будет за нас эти аллерты триггерить. Потому что, ну, какой смысл писать ивристику просто для того, чтобы ее потом робот подавил, если мы можем сказать, робот, ну ты и пошли тогда аллерт, с самого начала. Опять же, как это можно сделать? Берем весь список наших метрик, опять та же самая проблема, очень много метрик, но допустим мы с этим справимся. И для большого количества точек во времени инцидент, соответствующих инцидентам, а, пардон, перед этим мы еще должны определить список, какие аллерты мы в принципе хотим получать, потому что зачастую мы не хотим просто получить сообщение, что что-то во вселенной не так. Мы хотим получить что-то более вменяемое, у нас у юзеров большой error rate, или у нас latency поднялся, или у нас кончился storage в какой-то системе, что-то более конкретное. Соответственно, мы должны определить список аллертов, которые мы в принципе хотим получить, а дальше классическая проблема, идем и размечаем все предыдущие инциденты, когда каждый и говорим, должен был аллерт сработать в этой ситуации или нет. Кроме того, мы должны, чтобы наша система умела не только триггерить аллерты, но еще и не триггерить, мы должны дать ей достаточное количество примеров, когда аллерты не срабатывают, потому что в противном случае наша система может выучить, что всегда хотя бы один аллерт должен сработать. И это не то, что мы, наверное, добиваемся. Ну, хорошо, допустим, мы пошли, разметили, дали там примеры, что вот в тот момент у нас все было спокойно, в тот момент у нас сломалась система A, соответственно, мы хотим аллерт про систему A и так далее. Ну, зная потребности машинного обучения, нам нужно примерно тысячу примеров для каждого аллерта, желательно больше, но тысячу, наверное, хватит, и это много работы. Но, допустим, мы это сделали, мы сразу после... Допустим, мы это сделали, мы обучили модель, мы ее задеплоили в продакшн, и она посылает нам аллерты. Поскольку машинное обучение по своей природе оно вероятностное, оно может делать ошибки и будет делать ошибки. Соответственно, в какой-то момент оно поднимет нашего несчастного инженера посреди ночи нечаянно и напрасно, и более того, инженер не сможет понять, а почему, собственно, аллерт сработал, смотри выше, потому что интерпретировать эти модели очень сложно. Вот. И, возможно, это не вполне рациональное поведение со стороны людей, но люди очень не любят, когда они не понимают, почему их разбудили. Вот. И тут уже возникает чисто маркетинговая проблема, что люди просто не захотят работать с такой системой аллертов, они перестанут ей доверять, они скажут, а что если она еще и не срабатывает, когда должна бы срабатывать? И давайте-ка мы все-таки лучше подумаем головой и напишем иуристики. Ну, пока что все довольно мрачно, но, может быть, мы можем использовать машинное обучение для того, чтобы понять при выкатке релизов, типа там, канареечное обучение, а, ссори, канареечное тестирование, когда мы там выкатили наш релиз на немножечко юзеров, посмотрели, ведет ли он себя хорошо, посмотрели на метрики и продолжили. Ну, понятное дело, что если мы смотрим на метрики вручную, это не идеально, потому что людям сложно посмотреть на все метрики, но даже если мы напишем какие-то автоматизированные правила, всегда будут какие-то потенциальные дыры, которые мы не покроем, в которых может произойти развал, но наши автоматические канарейки их не заметят. Казалось бы, опять, машинное обучение, бросим туда все метрики, в принципе, здесь даже с разметкой данных, наверное, не нужно спариться, потому что мы можем просто посмотреть историю предыдущих релизов, и когда мы делали окаты, и на всем этом обучиться. И даже, допустим, мы сумеем обойти ту проблему, что у нас не было миллионов релизов за время существования продукта, в чем проблема? Проблема, на самом деле, в том же самом. По сути, это частный случай проблемы с вертами, только за того, чтобы поднимать человека, мы откатываем релиз. Если мы будем откатывать, ну, несмотря на то, что мы не будем никого или не отвлекаем никого из людей, собственно, с событием отката, это все равно вредит нашей скорости доставки новых фич в продакшен, это значит, что, скорее всего, после каждого лажноположительного срабатывания какой-то релизинженер будет вынужден пойти и проверить, а почему мы, собственно, не выкатили в этот раз, и у нас возникают все те же самые маркетинговые проблемы, что и с аллертами, люди перестанут любить и верить об этой системе, и все плохо. Люди просто скажут, ну ее нафиг, мы напишем наши собственные правила. Ну, и последний случай, который статья рассматривает, это так называемый root cause analysis, выявление первопричин для того или иного инцидента. В принципе, по случайному совпадению, мы обсуждали проблему problem detection по мотивам совершенно другой статьи в прошлом выпуске, в котором я был, это Divzen 297. И одно из наблюдений из того обсуждения было в том, что есть большая разница между экспертами и новичком, и эксперты лучше справляются с нахождением сигнала в шуме, потому что они видели много примеров, и потому что они в целом лучше понимают, как работает система. Соответственно, это тоже звучит, на поверхности это звучит как что-то, что хорошо должно быть заменимо на искусственный интеллект, потому что было бы здорово, если бы искусственный интеллект мог подсказывать нашим новичкам, а что может быть причиной инцидента. И дальше уже новички сами пытались подтвердить или опровергнуть эту гипотезу. Опять же, в какие проблемы мы упираемся в этом случае? Самая сложная проблема, пожалуй, отличная от всех предыдущих случаев, о которых мы разговаривали до того, это в том, что количество первопричин практически бесконечное, то есть есть тысячи разных способов, как система может сломаться, если у нас системы взаимодействуют друг с другом, у нас происходит комбинаторный взрыв, потому что сложные комбинации поломок в двух системах могут приводить к совершенно неожиданным поломкам системы в целом, и так далее. А чтобы наша система производила какой-то root cause analysis, мы должны для начала составить список всех возможных первопричин. Соответственно, уже на этапе составления всех возможных первопричин получается сложно покрыть все интересные случаи, потому что интересных случаев слишком много. Подожди секунду, можно здесь опроверивать? Давай. Из предыдущих работ я писал систему, и она оказалась достаточно успешной. Не сказать, что она всегда работала, но достаточно неплохо показывала, когда она находила просто корреляции между проблемной метрикой, на которую сработал alert, и какими-то другими метриками, которые у нас есть для этой системы или для соседних систем. И, соответственно, первую причину не обязательно искать руками, не обязательно ее описывать, не обязательно найти, что имеет полную совпадение, но как бы имеет очень большую корреляцию, но имеется для какой-то базовой, не знаю, для чего-то другого, но чуть-чуть пораньше срабатывает. Понимаешь, о чем я говорю, да? Да, я понимаю. В принципе, это валидный подход. Я думаю, что тут есть два принципиальных отличия. Отличие номер один состоит в том, что тут скорее подходит не машинное обучение, а различные алгоритмы anomaly detection, которые больше статистические, чем, собственно, машинное обучение. Проблема номер два состоит в том, что, как я уже говорил раньше, корреляция – это не всегда причинно-следственная связь. Смотри, выше выручка игровых автоматов и количество PHD. В-третьих, далеко не всегда даже есть очевидная корреляция.",
    "result": {
      "query": "machine learning for operations"
    }
  }
]