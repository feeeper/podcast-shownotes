[
  {
    "segment_id": "f3765437-6461-45a1-96d4-de473d572c28",
    "episode_id": "a4e0a078-25d7-4b69-823b-e2752c79470c",
    "episode_number": 200,
    "segment_number": 2,
    "text": "Машинное обучение – это достаточно большая область. Есть ли у тебя какой-то вектор, который тебе наиболее интересен? Есть же и какие-то поисковые алгоритмы, и я хочу, чтобы у меня ИИ носил мне продукты из магазина, и алгоритмы на видеоизображениях и так далее. Что именно тебя больше всего интересует? Ну вот я могу сказать, что меня интересует то, чем я занималась в бакалавриате. Возможно, это эффект цыплёнка, утёнка. В общем, то, что первое взяло, то и влюблён. Ладно, не важно. Но в общем, я этим занималась в бакалавриате, и меня это прям очень захватывало. Байсовское машинное обучение – это подход к машинному обучению, где мы в модели настраиваем веса. Мы настраиваем не веса, но в общем у нас есть какая-то параметризованная модель, и вот мы каким-то методом обучаем эту модель, получаем какие-то чиселки, параметры модели, и потом применяем её. Вместо этого в байсовском подходе мы настраиваем распределение. Мы учим апостриорное распределение. Можно опустить слово апостриорное, чтобы не пугаться, если вдруг кто-то незнаком. Мы настраиваем или обучаем оптимальное распределение на веса модели. И это там имеет некоторый ряд преимуществ в дальнейшем применении. Например, мы можем делать оценку неопределённости. Это очень важно в каких-то приложениях типа self-driving cars, или в чём-то, что связано с медициной, или в чём-то, что связано с финансами, какими-то рисками. То есть мы можем понять, насколько модель вообще уверена в том, что она предсказывает, а не просто она что-то предсказала, и вот всё. Ну вот да, но последний год я занималась байсовским подходом к регуляризации нейронных сетей и к сжатию нейронных сетей. И меня очень, на самом деле, привлекла, захватила тема сжатия моделей, потому что сейчас нейронки все применяют, ну так, как какой-то чёрный ящик. Просто берём много слоёв, фигачим, ура. Мне интересно понимать, почему именно эта модель работает, и вообще может... А вот можно я тебя здесь придорможу? Я видел ровно, по-моему, одну что ли статью по теме, и сейчас я попробую вспомнить, как она называлась. Её небезызвестный Эдриан Коллер у себя, по-моему, даже обозревал. По-моему, там был цикл из двух статей. Opening the black box of deep neural networks via information. Я её не читал, я знаю, что он её просто обозревал. То есть мой вопрос к тебе. Знакома ли ты вообще с этим ресёрчем то, что там уже есть? И если да, то куда вообще смотреть, есть ли там какие-то результаты? Так, ты спрашиваешь меня конкретно про information bottleneck для объяснения deep learning или про что? Скажем так, я привёл information bottleneck просто как пример, что это единственная работа, на которую я вообще натыкался в Twitter. Как вообще выглядит ландшафт? Исследование в этой области, почему оно работает? На самом деле, вот сейчас я потерялась немножко в таймлайне. Полгода назад или год назад-то было, вышла статья, которая якобы вообще объяснила, почему работает deep learning как раз с information bottleneck. Я вообще не хочу переводить этот термин, потому что он звучит ужасно. Информационное горлышко. Информационный боттленк.",
    "result": {
      "query": "information bottleneck deep learning"
    }
  }
]