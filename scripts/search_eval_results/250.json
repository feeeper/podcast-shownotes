[
  {
    "segment_id": "3a94038a-a9eb-418f-97ec-8edbdbc2a04a",
    "episode_id": "196dc153-c3ee-450c-a1f9-845eb613d7dd",
    "episode_number": 250,
    "segment_number": 2,
    "text": "Накидают тем действительно в бэклог, и потом такие ненадежные не приходят. А знаете, что еще ненадежное, но за вами обязательно придет? Это потрясающая звонилка. Не придет, а пришло. Пришло, да. За мной, кстати, ну я не знаю, может быть меня хакали, может быть не хакали, надеюсь не хакали. Вот, но в общем, у меня был Zoom, у меня частично эта уязвимость была, но я думаю я спойлерю, и Ваня сейчас нам все расскажет. Да, Zoom ненадежен. К сожалению, я не могу отказаться от Zoom. Вот давайте я еще расскажу, что такое Zoom, ну потому что я не уверен, что прям вообще все знают, что это за хрень. Я думаю, что те, кто пытался участвовать в онлайн-веб созвонах на большое количество людей, не имеют других альтернатив, кроме как Zoom. Потому что это единственная звонилка, которая устанавливается на все платформы, и вы можете делать очень большие эфиренции с головами, говорящими, со звуком, и очень легко прямо из клиента сделать записи. На самом деле очень удобное решение, за исключением того, что она ненадежна, совсем ненадежна. Кстати, если отрезать головы, то вполне сойдет и Mumble. Кстати, согласен. А если их не отрезать, то есть Zoom? Там еще широкое экрано, это очень важная фича Zoom, широкое экрано, и даже не просто широкое экрано, а можно взять своим курсором и начать рисовать на чужом экране. Согласен, нет, ну я просто про то, что если отрезать головы, то в одной компании мы реально ставили Mumble и проводили конференции, ну как бы стендапы, ну не стендапы, плохо, но митинги, планирования с разными офисами, это неплохо работало вполне. Мит не работает, если у тебя, во-первых, не у всех есть Google аккаунты, по-моему, там надо по умолчанию иметь какие-то Google аккаунты, во-вторых, вот, к примеру, мы хотим созвониться в Питером, и у нас нет платного Google аккаунта, мы не сможем сделать мит. Да, нужен оплаченный аккаунт. Да. И более того, он мало того должен быть оплаченный, он должен помочь, если я не ошибаюсь, я могу, конечно, сейчас обманывать, но там какой-то минимальный должен быть порог, сколько ты платишь для того, чтобы большие конференции устроили. Итак, вернемся к Zoom. Zoom оказался ненадежен, и об этом вышла статья, сейчас я посмотрю точно дату, число вышло на 8 июля. 8 июля вышла статья за авторством Джонатан Лейшуч, да, я правильно понимаю? Лейтшух. Лейтшух. Я, извините, прошу прощения, я всегда говорю неправильную фамилию, вы помните. И она написала статью, в которой по случаю, по какому-то случаю она начала копать, как работает Zoom, я не помню точно причину, но что-то вроде любопытства. И она внезапно увидела, что Zoom для того, чтобы облегчить и улучшить юзабилити своего клиента, устанавливают веб-сервер на машину, на Mac. Подожди, почему она? Это молодой человек? Я аббатарку приблизил, это молодой человек. Прошу прощения, тогда это он. Я думаю, фамилия читается Ляйцух. Ляйцху... Ну это немецкая фамилия, я прочитал бы Ляйцух. Точно, это парень. Я читал всегда, был уверен, что это девушка, я где-то видел, может быть там кто-то ошибся и назвал её, в общем, без разницы, его. Не так важно. Важно то, что устанавливается на локальную Mac веб-сервер, который слушает все обращения к себе, и, соответственно, если вы открываете какую-то страничку в которой или нажимаете на ссылку, которая должна обратиться к этому серверу или для того, чтобы... Или, например, в i-Frame у вас она открывается, у вас автоматически открывается Zoom, и что самое страшное, автоматически открывается микрофон и аудио. При этом настраивать... Вы можете локально настраивать своего Zoom-клиента для того, чтобы он не включал видео и аудио, но на самом деле тот, кто создавал конференцию Zoom, он устанавливает в настройках этой конференции, этого созвона, что я хочу, чтобы звук был у всех и видео было у всех, и, соответственно, по умолчанию, когда открывается этот клиент, окошко клиента, без разницы, что вы до этого в нём настраивали, у него всё включено так, как организатор захотел. Это была фича, это было намеренно сделано разработчиками, они посчитали, что так будет удобно. И суммарно получается такая вещь, что вы можете загрузить какой-нибудь сайт, у которого там в фоне есть i-Frame, и у вас автоматически откроется Zoom и сразу начнёт передавать в сеть то, что у вас показывает ваша камера. По этому поводу у нас в компании... Внезапно у всех всё подгорело и начали раздавать такие шторки, которые автоматически приклеиваются на экран ноутбука и делают закрывание камеры. Представляете, насколько сильно у всех всё подгорело? У меня подгорело настолько, что как только я увидел, я пошёл сносить. Что самое страшное, что если вы сносите Zoom по умолчанию, этот клиент остаётся...в смысле, веб-сервер остаётся висеть. И это сделано для того. . . Это, опять же, удобство пользователя. Это сделано для того, чтобы, когда вы в следующий раз нажмёте на зумовскую ссылку, оно само вам поставило автоматически зумовское аппликейшн и автоматически его сразу открыло. То есть вы ничего не делаете дополнительного, вы просто нажимаете на ссылку, она загружает вам с каких-то серверов, которые прописаны где-то у вас там внутри этого веб-сервиса, устанавливает оттуда приложение, запускает его, настраивает на этот созвон. Вообще это страшно. Ну то есть вот лично мне вообще страшно. Во-первых, я не знаю, что это за приложение, насколько его легко хакнуть. Во-вторых, оно скачивает...был сделан реверс-инжиниринг, и там было там 3 или 4 разных урла, которые... У одного из них заканчивается сертификат в мае 2019 года. То есть исследование было в апреле, а к мае он должен будет закончиться. То есть нет нигде гарантии, что, во-первых, это правильные урлы, во-вторых, что там правильно работает сертификат настроенный, и что они не утекут, и что домен продлится и так далее. И вообще нет никакой защиты от закрытых исходников, которые где-то у вас там запускаются. То есть вполне возможно, что через пару дней нашлась бы уязвимость, которая позволила бы, я не знаю, запускать что угодно на вашем компьютере. Что еще веселее, для того, чтобы полностью соединить этот веб-сервис, нужно еще очень сильно постараться. Потому что он... То есть там есть последовательность шагов, и начал по сети ходить башевский скрипт, он у нас в шоу-моутах ссылается на гисте, на гитхабе, как правильно снести полностью весь Zoom. Для того, чтобы это сделать, там... Сейчас я открою, посмотрю, что он в какой последовательности делает. Секундочку. Он удаляет Zoom application... А, да, говори. Пока ты делал паузу, мне стало интересно, что за фамилия такая. И в предположении, что она немецкая, это читается Leitschuh. Да, я уже тоже проверил, просто не стал обрываться, да, Leitschuh. E-E читается как I, а S-H как S-H. А, там, кстати, T есть, поэтому Leitschuh. И этот скрипт по полному удалению Zoom'а 100 строчек ровно. Он называется «Вакцина от Zoom'а». И там надо кучу всего удалить. Кучу всего... Включая Kernel Extension. Которого у меня, к счастью, не было. Ну, вот там есть такая строчка, кекст, ну, то есть это, по сути, модуль ядра. Kernel Extension — это сленг маковский для, как я уже сказал, модуля ядра. Он там за каким-то чертом ставится, у меня его в системе не было, к счастью. Но то, что какая-то хрень, когда она вам в систему заезжает, она вам ставит веб-сервер, который слушает в каком-то порту и может на ваш Mac ставить всякое говно. И Kernel Extension, который... Какой-то проприетарный код крутится у вас внутри... Ну, проприетарный и подписанный EPLOM крутится у вас внутри ядра. Это прям просто, не знаю... Я раньше-то стремался всякие софты не из Mac App Store ставить, если это не что-то, что я хорошо знаю, типа, не знаю, какой-нибудь RADIUS'а. А теперь я буду совсем параноик, и я думаю, что теперь мне только журналку всякую такую говно ставить. Потому что, блин, это пипец как стремно. Ты, не знаю, поставил себе звонилку, а она тебе просто... Она какой-то троинский конь. Давайте более научно подойдём к подгоранию от этой новости. Вот, и посмотрим, сколько это в микротонске. Мне кажется, это минимум на 15 микротонски тянет. У тебя, Валер, сколько показывает тонский метр? Сейчас все ссылаются на то, что к нам занесли в виде серии постов от тонского... Не от тонского. А, ну, да-да-да, ну, то есть отзывок это оригинально на твит. Как беж господина зовут? Вастрек. Но там были микрокозули, так что не надо. Это империал единицы измерения, а в системе все измеряются в тонске. У меня подгорело очень сильно, и я, если честно, очень сильно расстроен. Я расстроен, что единственная звонилка, которая позволяла Хрышу звонить, она настолько плоха. Но давайте послушаем, что было дальше. То есть, когда вышла вот эта вот оригинальная статья, она вышла, сейчас я точно дату скажу, где-то в апреле, по-моему, или в марте даже. Точнее как вышла. Об этой уязвимости, конечно же, надо было сразу сообщить в Zoom. И в Zoom, сейчас я вот даты пытаюсь найти. Даты, даты, даты, здесь были даты. Вот, 8 марта. Вот. 8 марта, соответственно, он соединился с Zoom, ему не ответили по твитру. 26 марта он написал официальное письмо и сказал, что, ребята, вы мне не отвечаете, а через 90 дней я открою это в паблик. 27 марта ему ответили с какими-то пояснениями. 1 марта запросили подтверждение того, что он написал. 5 мая написал ему Security Engineer, подтверждающий, что да, так, взаимость у нас есть, и давайте мы начнем что-нибудь придумывать. Вот. И вот, в общем, здесь полный, я не буду все перечислять, 20-25 пунктов. В итоге они ничего не сделали. То есть, вот реально, за 90 дней Zoom не выпустил ничего для того, чтобы это починить. Zoom предложил несколько вариантов, как можно это пофиксить. И при этом сам человек, который нашел уязвимость, сейчас, подождите, я вспомню, как его зовут, Джонатан. Вот. Джонатан тут же на их предложение нашел несколько вариантов уязвимости уже их предложений, поэтому сказал, что это неправильно, не будет работать, и в итоге за 90 дней они не сделали ничего. Когда уязвимость вышла и у всех все взорвалось, внезапно Zoom начал делать. Итак, вышел вот этот скрипт для того, чтобы удалять все подряд. 8 числа, как вы помните, вышла сама уязвимость, и 8-го же числа или 9-го числа Zoom выпустил обновление своей программы, в которой он сказал, ребята, мы поняли, что внезапно мы что-то не так сделали, ну, может, они опоздали, конечно же, но я подозреваю, что это просто из-за того, что они раздолбае. Они выпустили обновление, в котором сказали, вот у нас есть кнопка, чтобы полностью удалить все. То есть, фактически они повторили скрипт, который в сети уже гулял, у себя через локальное приложение. Плюс они сказали, что мы планируем сделать удаление веб-сервиса и никогда его больше не пользовать. И после этого, сразу 10-го уже июля, Apple выкатил тихое обновление, которое автоматически установилось всем без предупреждения, которое у них есть по специальному критерию, там, security patch, которое выключает зумовский веб-сервис и не разрешает ему работать. По заверениям Apple и Zoom, они разрабатывали это вместе, и Zoom очень благодарит Apple, что Apple помогает устранять излишности в наших продуктах. Но, как вы видите, подгорело не только у меня, и возможно, тут все-таки больше 15-ти, Саша. А на твоем приборе шкала какая вообще? Имперская. Нет, нет, я имею в виду максимум, он сколько показывает? Вообще, я считаю, что, скажем, если по 10-бальной шкале судить, то я думаю, что это где-то в районе 8-ки, 9-ки. Потому что, скорее всего, кто-то уже нашел этот веб-сервис, ну, как бы, теория заговоров, и Бритва Акама говорит, что если кто-то что-то мог найти, то, скорее всего, он его нашел. А теория заговоров говорит, что, наверное, уже есть какие-нибудь способы это использовать для того, чтобы взламываться и ломать все подряд. А? Может, из-за этого и Кеннеди убили? Ну, как взламывать все подряд? Ну, в примере с тем же самым Mumble, да, вот представь себе такую ситуацию, что ты его установил для проведения конференции, да, чего-то. Потом, ну, на сервера, да. Потом, допустим, кто-то имеет доступ к этому серверу, да, может быть, это ты же, ты уже работу сменил, может быть, кому-то, ну, кто-то получил этот доступ, неважно. Ты же в Linux, ты можешь, сейчас я пытаюсь сообразить, можешь или не можешь. Через пульс аудио можно ли пробросить звук так, чтобы ты в Mumble не видел, что ты его захватываешь? Ну, я имею в виду без бота на хост-системе это возможно, как вы думаете? Я вот пытаюсь прикинуть, у меня пока нет ответа. На уровне пакетов по-любому может заморочиться. Я к тому, что ты, если очень сильно постараться, захочешь сломать альтернативное решение, ты всё равно найдёшь способ, ты согласен с этим или нет? Я не против того, что у них решение будет захватывать звук, если кто-то вломится в систему. Ну, то есть, я к тому, что вот я установил Mumble на Linux-системе, я ушёл, у меня креденшелсы теоретически после того, как я ушёл из компании или ещё откуда, не должны быть заходить на этот сервер. Теоретически, да-да-да, конечно. А мы говорим здесь про общий случай, общий случай такой, что эта программа работает на моём компе, она работала здесь годами, я не уверен, что у неё нет каких-то уязвимостей. А теоретически любой сайт с зумом запускал эту программу, как-то общался с этой программой на моём установленном компе, а эта программа работала с каким-то модулем ядра, который работает под рутом. Ну, звучит совершенно безопасно. Согласен. Интересно, обновление Apple удалило кекст или только выключило веб-сервер? О, кстати, я вот про кекст не смотрел. Надеюсь, что да. Пока я смотрю про кекст, я предлагаю перейти к следующей теме, но вообще вот пугающие перспективы, пугающие, просто пугающие. Лёша Палашченко пишет, что в шоу-ноутах должна быть реферальная ссылка на шторку для камеры. Да, хорошая тема. А ещё пластилин для микрофона, вот это мне особенно понравилось. И как это, огнеупорный стул. Ну, мне во всяком случае часто нужно, когда ты про такие новости слышишь, у меня не счётчик зашкаливает, у меня сразу как бы первая космическая, а тут вся... Стул сгорает. Да, я просто вынужден их часто менять. А что ещё надо часто менять? Это настройки графаны. Потому что реально, вот я не знаю, вы с графаной много работаете? С графаной, сейчас, с графаной, это которая новая, модная, современная, правильно, да? Которая браузерная. Да, да, много. Я с графайтом периодически путаю. А, всё, понял. Графайт тоже браузерный, если что. Ну, в смысле графайт, он всё-таки на сервере рендерится. А, ну окей. График на сервере рендерит, он тебе уже отдаёт NGPG. Отвечаю на вопрос Вани. Ну да, приходится. Ну, графан, это самая нормальная штука для рисования графиков. Из тех, которые я пробовал. Да, логично. И вы наверняка знаете, что когда графиков много, а когда ваш сервис, которого всё это закачивает за все эти графики, не самый быстрый, то получается всё так себе. Так себе. Поставишь больше 20 графиков на страничку, открываешь страничку, и она не открывается нифига. Потому что он не всё ещё закачал, не всё ещё может отрисовать. Так вот, с этим покончено. С этим покончено, потому что, начиная с версии 6.2, все графики в бане теперь стали ленивые. Они будут отображаться только тогда, когда сам график будет виден на вашем экране. То есть, вы открываете страничку, там на экране видно первых два графика. Будут закачиваться и отображаться только первые два графика. Ну, это не лепота. У меня обычно, мои страницы, то есть как бы у нас обычно юзкейс такой, что мы на каждый сервис в системе создаём графа на дешборд, на котором там куча графиков. Кит открыта по умолчанию, там можно закрывать, как это правильно называется, панели. Там можно панелями закрывать, чтобы их не было видно по умолчанию. Но всё равно куча открытых графиков по умолчанию. Их там может быть 10, 20, 100 бывает, когда сложность многих компонентов состоящая. А сейчас ты загружаешь её, первые два графика видно, там обычно самые важные какие-нибудь с количеством ошибок, там запросов, ну, не 2, а 4. И они быстренько отобразятся, покажутся, а всё остальное не показывается. Когда ты скроллишь, они начинают подгружаться и показываются. Ну, да, там появляется такая крутилка, которая тупит в твою сеть. Да, да, да. Я бы предпочёл всё-таки, чтобы у меня 100 графиков крутилось. Но у меня бывает самая проблема, знаешь, когда начинается какая-нибудь проблема на сервере Outage, ты заходишь в эту систему, а она такая, подожди 2 минутки, я сейчас подгружу всё. Идите к чёрту, дайте мне мой график, который я хочу. Вот. И, короче, я вот, если честно, прям счастлив. Ты понимаешь, да, что теперь твой график, который ты хочешь, он будет как бы не просто загружаться, а ты будешь, у тебя загрузится 2, потом такой проскроллишь, у тебя крутилка, проскроллишь, крутилка. Ну, по-моему, не сильно лучше стало. Во-первых, я самые важные ставлю в начало, чтобы было их сразу видно. Поэтому, когда я открою, мне будет всё видно сразу. А во-вторых, а ты думаешь, лучше будет, когда ты... Ну, то есть ты можешь прокрутить до нужного, ты можешь не пытаться ждать, пока он открутит на втором, тебе второй не нужен сразу. Я просто считаю, это, ну, в том описании, которое ты дал, это полумера, потому что они должны загружаться не лениво, они должны загружаться асинхронно. Ну, то есть, как бы загружаются 2 и параллельно, да, они сразу рендерятся, и в то же время, да, загружаются остальные. Просто, как бы у меня первые 2 из них не тормозят. Вообще удивительно, что они в первую очередь тормозили. Удивительно, что они это сделали в версии 6. А я обычно по прямой ссылочке ходил на графики, которые нужно было быстро посмотреть, у меня были букмарки или отдельные дэшборды. В целом, это работало. Слушай, у меня 20 сервисов, и в каждом из них куча показателей, мне сколько надо иметь прямых ссылок? 20. Ну, может, 15. Микросервисная структура, вот это всё же, это же сейчас самое модное, молодежное. А вот, на самом деле, интересный вопрос. Сейчас издалека начну. В компании, где я работал, там была самописная такая мониторилка с древовидной структурой. И элементы дерева, они примерно такие, что микросервис, его какие-то внутренние компоненты, и там у них статус, условно говоря, зелёный, жёлтый и красный. Притом, жёлтый не приветствовался. Идея в том, что ты заходишь на эту панельку и сразу увидишь, что в таком-то микросервисе, в таком-то компоненте, то-то сломалось. И там прямо текстом написано, что именно. Ну, чтобы долго не разбираться. Мне интересно, если это не секрет, если у вас что-то похоже, и если нет, то как вы диагностируете проблему? Я к тому, что по графикам это же неудобно. Ну, холстчек и метрики – это же разные вещи несколько. То есть если у тебя есть холстчек, который смотрит, какая компонента отвалилась и репортит статус её, то это одно дело. Это какой-нибудь там Nagios, вроде в Prometheus тоже можно то же самое делать. Да и в Grafana можно. У Grafana есть холстчек? Там же только Алёр. Какая разница, ты можешь Алёр сделать на какой-то график, который, по сути, холстчек. Если я правильно помню, Grafana – это просто фронтенд к чему-то, например, к Прометеусу, а в Прометеусе уже это настраивается. У каждого графика есть такая вкладочка Alerts, и ты можешь какие-то значения на графике выставить, или какие-то пороги, или какие-то… Даже там типа, если график вылез за порог больше, чем на сколько-то минут, тогда, не знаю, пульнуть Alert в какой-то слегк канал, например, так можно сделать в Grafana. Но это всё… Я к тому, что вот эти графики, они обычно показывают какие-то косвенные причины, ну, в смысле, косвенные явления. То есть, например, если у меня порвался где-то Connect, то вряд ли у меня есть график типа 1.0, там есть Connect, нет Connectа, потому что это расточительно, такие графики иметь. И скорее всего я увижу, что у меня где-то, не знаю, время ответа поползло вверх, а почему оно поползло, охрененно не узнают. И нужно курить логи, ещё что-то. И у меня просто немножко резонул слух, когда Ваня сказал, что когда что-то случается, я иду смотрю в графики. И отсюда мой вопрос, а нет ли у вас какой-то другой системы, которая прямо говорит, что именно сломалось, а не косвенно. Саш, вот то, что ты сейчас сказал, у нас в VOOGIE был график про состояние сокетов, ну, типа, сколько открыты сокеты, в каком они состоянии, и был график типа потери пакетов. Ну хорошо, твой график говорит, что у тебя два порвавшихся сокета, дальше твои действия. Ну, блин, как бы обычно они в контексте каких-то ещё других графиков. Обычно, если два порвавшихся сокета с одной машины, ну, они обычно не два, они там обычно, ну, иногда все на одной машине отваливаются, и там только понятно, что сетка отвалилась. Если у тебя пакет лосс попёрся, значит, у тебя там тоже, не знаю, можно пойти и рот рупнуть. Ну, то есть там редко бывает так, что у тебя какое-то совсем дикое непредсказуемое поведение на графиках, обычно у тебя эти графики, потому что ты при помощи них видишь какие-то проблемы довольно хорошо. Давайте я ещё уточню. Количество возможных вариантов проблем, оно не такое большое. Это сеть, это I.O. на отдельно взятой машине, это CPU, memory, это я имею в виду с точки зрения ресурсов. С точки зрения сервисов твоих, у тебя у каждого сервиса могут быть какие-то проблемы. Вот проблемы сервиса, это очень сильно зависит от сервиса и от бизнес-требований и от кучи всего, поэтому заранее сделать здесь универсальный метод не получится. У нас есть куча дешбордов, которые делают агрегацию по вот этим ресурсам, которые я говорил выше, и дают какие-то алерты вида. Вы знаете, вот здесь попёрли большое количество, не знаю, CPU ушёл за 100% и не хватает ни на что, кроме какого-то одного процесса, ну чего-нибудь типа такого. С точки зрения отдельно взятых сервисов, там свои метрики, которые показывают свои специфичные вещи, и алерты, соответственно, для каждого сервиса по-своему выставлены. Но самые простые, это я не знаю, в микросервисной архитектуре это всегда количество запросов и количество ошибок. То есть если ты видишь, что график количества запросов резко упал, значит, что тебе просто не доходят какие-то запросы. Или если график запросов точно такой же, а количество ошибок резко выросло, это что-то у тебя в сервисе случилось, что он не может обрабатывать эти запросы. Ну какие-то такие простейшие вещи, они универсальные. Ну вот именно что-то случилось. Ну то есть для меня график, который говорит, что что-то случилось, это совершенно бесполезная информация, я уже знаю, что что-то случилось, если я туда пришёл. Я хочу открыть экран, и чтобы мне там было написано, чувак, сломалось вот это. Ну совсем так не бывает, обычно как бывает. Ваня уже перечислил всякие разные ресурсы, какие вообще в природе есть, с которыми может быть проблема. Ты смотришь на какой-то очень высокоуровневый график, ты понимаешь, что проблема с сетью ЦПУ или чем, и обычно понимаешь, где. Дальше там есть какой-то дрилл-даун обычно доска, например, там сеть всех машин, как она себя чувствует, и ты можешь пойти глубже. И уже когда ты понял, на какой машине или на какой группе машин как себя ведёт сеть, ты можешь пойти там, не знаю, или в админку конкретно конкретной машины, или в админку конкретного роутера. Ну то есть я не знаю, как тебе, ну как что-то еще хочешь, чтобы у тебя просто была магическая система, которая, не знаю, говорит, что проблема точно вон там, но она тогда может сама починить, может. Такая, не знаю, AI-админ. Нет, но бывают же хеллс-чеки, которые дёргают сервис и спрашивают у него, ты здоров? Сервис говорит, я здоров или я болен. Вот поэтому-то. И это как бы не отрицает графики и метрики, просто их дополняет. В мультисервисной архитектуре, например, я работаю на последних работах, где мы всегда работаем, когда куча сервисов. Я видел очень маленький процент того, что человек по-настоящему проверяет здоровье данного сервиса. Чаще всего это просто, я не знаю, заглушка, которая всегда отвечает 200. Ну то есть он запущен, он работает, но он не смотрит внутреннюю статистику. Почему-то не пользуется поправностью идея делать именно полноценную проверку. Ну еще потому, что мало что из них, короче, то есть не знаю, даже если бы статус-гид возвращает, ну там, не знаю, говорит, что все окей, только если там соединение с базой, все нормально и так далее, это еще не значит, что с сервисом все по-настоящему хорошо, потому что он там, скажем, сервис сидит там за двумя слайдами балансеров, почему там, или за какой-нибудь фигней, и с точки зрения клиента все плохо, но халтчеки показывают, что все прекрасно. Ну как бы и безусловно. Или вот последний outage был, у нас закончились файловые скрипты. В смысле, процессы закончились, он уперся в лимит. И он соединился со всеми, ему вообще все хорошо, по его внутренней статистике, по логам он себя отлично чувствует. Но только что-то иногда сбоит. Нет, безусловно, халтчеки и метрики, они друг друга дополняют, они отрицают друг друга. Вот как бы Саша просто говорит, что ему хочется какой-то дешборт, на который ты заходишь, там написано, какой сервис работает, какой нет. Заходишь на дешборт халтчеков, и там видно, что это отвалилось, это не отвалилось. Понятное дело, что там будет не все всегда, а что-то будет только в метриках. Потому что, как правильно Валер сказал про лонбалансеры и все такое, топология сети у клиентов у всех разная, и понятное дело, что такие вещи с халтчеками не проверишь. Так что нужно просто и то, и то. А еще очень сильно помогают алерты. То есть, если правильно расписаны алерты во всех системах, которые у вас есть, и мы вот за это очень сильно боремся, чаще всего хорошо настроенный алерт тебе четко покажет, где происходит проблема. То есть он не смог ответить на какой-то запрос, у тебя возникает, увеличивается счетчик необработанных запросов, после того, как он превысит какой-то допустимый уровень, он начинает сигнализировать, э-э-эй, 10 последних запросов от меня не были обработаны, вернулись с ошибкой, здесь какая-то беда. Или там, я не знаю, я не смог выделить себе новую память, или что угодно, ты фактически можешь загнать в алерты. Все это делается через метрики, через какой-нибудь Примитиус, Нагиус, я не знаю, Графайт, что угодно, везде это можно настроить. А вот ты сказал волшебную фразу «правильно настроенные», то есть которые, во-первых, не будут тебя, когда всё относительно нормально, во-вторых, когда не будет того, кого надо, а не каждый раз себя. Может ты поделишься небольшими инсайтами на тему, как их правильно сделать? Ваня тоже так говорит «нет», а я уйду в закат. Это очень больной вопрос, потому что мы прямо сейчас пытаемся его решать, и похоже, что ничего кроме жёсткой диктатуры не работает. Короче, надо бить людей. Надо бить по рукам за ненастроенные алерты, или есть у меня альтернативное решение, но почему-то бизнес наш не готов делать хаос инженеринг. А я думал, что Ваня сейчас скажет «есть у меня альтернативное решение, но почему-то бизнес наш не готов бить людей». Я на самом деле считаю, что хаос инженеринг – это очень классная штука. Это чрезвычайно классная штука. Но я почти не знаю компаний, которые его пропагандуют. Напомню, что хаос инженеринг полагает, что вы добавляете хаос в свою систему, вы добавляете себе агента, который имеет полной картой всей вашей инфраструктуры, всех ваших сервисов и систем, и наугад регулярно выбирает какую-то подсистему или большую часть системы. Вообще он может выбирать что угодно, любого скоупа, до какого-то количества, конечно, все подряд он не может выключить, и нафиг убивает. Например, он может выключить какую-то, зарубить сеть в какой-нибудь дата-центре, или, например, убить какой-то сервис, или что угодно он может сломать, и ваша система должна быть настроена так, чтобы она продолжала работать при этих проблемах. А вот смотри, интересный вопрос. Вот последние три года в предыдущей компании я делал коробочный продукт, и сейчас я работаю в компании, которая делает open-source платформу, которая в душе тоже коробочный продукт, то есть ее ставят клиенты у себя. Как ты предлагаешь там устраивать хаос инженеринг? Хаос инженеринг должен быть на вашей тестовой инфраструктуре, которая будет проверять ваш коробочный продукт. Есть маленькая проблема с тестовой инфраструктурой, когда ты делаешь, во-первых, революционную базу данных, которая может использоваться, не знаю, может быть не десятком, но там как минимум пятью очень разными способами основными, и как бы платформу, которая затачивается под каждого конкретного клиента, то есть по сути движок. Как ты сделаешь тестовый стенд для движка? Я же не говорю тебе, что ты... Нет, подожди, давай так. Хаос инженеринг, он отключает какие-то подсистемы целиком. У тебя они либо настроены, что они будут продолжать работать со сбоями в каком-нибудь деградационном режиме, или они не настроены. Без разницы, как клиент его использует, без разницы, какой запрос, маленький или большой, тысячи запросов или два запроса. У тебя это либо есть в системе, либо нет. Да, грубо говоря, у тебя система начинает симулировать, затупляет часть диска или еще какая-нибудь такая фигня, или у тебя посыпался индекс. Или кэш выключился, или еще что-нибудь. И кэш начали вымывать. Поэтому теоретически ты можешь сделать эту систему, конечно, сложнее, чем в той системе, где Netflix это делает в своем виде, но мне кажется, что в каком-то виде это все равно можно сделать, особенно если ты делаешь коробочный продукт, устанавливаемый куда-то, который состоит из большого количества компонентов, ну, микросервисных. Ну хорошо, а теперь представь себе такую ситуацию. У тебя 10 клиентов, они все используют твою платформу 10 разными способами, и под них еще и там написано 10 вариантов кастомного кода. Ты предлагаешь поднять 10 стендов? Нет, нет, нет. Ну никто же не выйдет в 100% покрытие. Мы говорим о том, чтобы приблизиться к какому-то более хорошему состоянию, чем не делать ничего вообще. Я по-другому скажу. Вот смотри, например, у тебя клиент номер 1 делает все совершенно по-другому, чем делает клиент номер 2. И заметь, у них кода базы еще и разные. Но оба из них используют твою систему, в которой есть подсистема А. Представь теперь, что это хаос манки убивают эту подсистему А. Должны пострадать теоретически и первый клиент, и второй. Они могут пострадать разными способами, потому что у них разный юзкейс. Но если у тебя система подготовлена к убийству этой подсистемы А, и она запустит что-то новое, или там перейдет в какой-нибудь деградационный режим, теоретически в обоих случаях клиенты не должны пострадать сильно, по крайней мере. А лучше вообще не должны быть. Хорошо. Тогда еще такой вопрос. Чем это радикально отличается от найтли-тестов различных, в том числе джебсон-подобных? Потому что ты в найтли-тестах не делаешь фазы тестирования по отключению подсистемы. Если это джебсон-тесты, то еще как дела? Если это джебсон... Нет, нет, нет. Это совершенно разные вещи, это подход с разных сторон. Ты джебсоном, но у тебя, видимо, слишком общее понятие, что такое джебсон. Продолжай. И что такое конкретизационный тест. Хаус-манки, хаус-инжиниринг в полноценном виде. Конечно, он бывает часто не в полноценном виде. Хаус-инжиниринг предполагает, что у тебя, у хауса, как у обезьяны, есть полная карта всего, и она получает ее в динамике. То есть она в динамике обращается, не знаю, к Амазону и говорит, дай мне все инстансы, на которых я работаю. Тот дает ей все инстансы, она может вырубить любой. Когда ты пишешь джебсон-подсистему, ты говоришь, я предположу, что мне надо обезопасить себя вот здесь, вот здесь и вот здесь. Ты не будешь думать, а я в динамике вытащу все, а потом попробую все это перебором исключать или подключать. Она точно так же может хаус-манки удалить, скажем, диск в какой-нибудь отдельно взятой машине. Она может вырубить память, я не знаю, или еще какую-нибудь фигню сотворить. У нее очень богатые права, и она эти права, в смысле, она список того, что она может сломать, определяет в динамике. В отличие от джебсон-тестов, которые ты пишешь, исходя из какого-то юзкейса. Хорошо, понятно, но ты не заметил, что пришел к противоречию сам себе. Я сейчас объясню, в чем. В том, что в твоем определении хаус-инженеринга, ну и видимо вообще принятым, ты должен дублировать инфраструктуру клиента. Нет. Как же так? Ты только что сказал амазон, инстансы, убивать, все, что видно. Так. А почем я знаю, где клиент это деплоит, и как? Смотри, Саш, ты берешь, делаешь какую-то модельную деплой свою систему, ну скажем, тестовый стенд, который проверяет, ну не знаю, делает нагрузочное тестирование. Который ты клиентам показываешь, например. Да. Там свой демо-стенд, например, да. Ну или там, да, на котором ты так же нагрузочное тестирование производишь, какой-то вариант тестирования. Тогда заводишь такого, не знаю, механического пизданутого админа. Назовем это так, механический пизданутый админ, он берет тебе, начинает, короче, нажимать все кнопки. Говори просто хаос обезьяны, пожалуйста. Окей, сори. Хаос админ. Продолжай, продолжай. Ну если мы говорим про модель, вот как ты описываешь, то я все еще не понимаю, чем это отличается от джебсон-тестов, которые там случайно на моей модели что-то включают-выключают, например. Джебсон это, короче, не знаю, мне кажется, ты просто словом джебсон называешь все, что какой-то делает в фазе ломания чего-нибудь. На самом деле это не так, джебсон-тесты это конкретный фреймворк конкретного чувака, а фигура используется очень определенным образом. Хорошо, хорошо, хорошо, давайте называть это джебсон-подобными тестами. Даже не джебсон-подобные, смотри, джебсон-подобный тест, ты берешь, у тебя вообще сама, ну то есть вся серия джебсон, она про гарантии баз данных. Ты берешь какую-то гарантию баз данных, какой-то invariant и говоришь, окей, вот есть клевый вариант, давайте пробуем invariant, давайте пробуем его сломать. И начинаешь какие-то nemesis придумывать, которые будут именно этот invariant ломать. Как Ваня сказал, очень такие, не знаю, ты знаешь, как этот invariant можешь сломать, ты пытаешься его сломать. Как Ваня пытался объяснить, хаос манки, а также админ, нажимающий все кнопки, что такое, это просто какая-то хрень с почти рутовыми правами. Это фазе, фазе достижения. Это фазер инфраструктуры, давай так. Я все еще считаю, что вы сами себя противоречите, даже теперь уже по двум пунктам. Что, во-первых, инфраструктуру вы уже начали называть моделью, и она уже не представляет то, где клиент деплойт, а во-вторых, сейчас я постараюсь сформулировать аккуратно, вот этот хаос манки тестирования, оно предполагается, что его надо делать на реальной боевой системе, под реальной боевой нагрузкой, на реальной инфраструктуре, а пускай, ну, ломай ее как-то в разумных пределах. Ну не так, что ты как бы сразу пол дата-центра нахрен вообще вырубил. Например, вырубим вот эту машину, поднимем обратно, да, ну как-то, ну более-менее, чтобы продолжал работать, да, то есть ты ломаешь случайные компоненты. А теперь, внимание, если у меня стенд, на котором, ну если есть какая-то нагрузка, то она симулируется, там нет реальных пользователей, то зачем мне себя так ограничивать? Зачем мне по одной машинке ломать, когда я могу делать еще более агрессивные тесты? Можно я отвечу? По поводу первого. У тебя немножко другой случай, потому что, поэтому мы и говорим про какой-то тестовый стенд, потому что, как ты ответил во втором вопросе, теоретически надо ломать свой продакшен. У тебя просто нет продакшена, потому что ты его продаешь клиентам. Поэтому у тебя немножко другой случай. Вот. Касательно того, что ограничивать или не ограничивать, это как сравнивать фазы тестирования и юнит тестирования. Ты понимаешь, что ты теоретически с помощью юнит тестов можешь всегда повторить все фазы тестов. Но в реальности, сколько бы ты ни пробовал, чаще всего ты не будешь думать про все возможные варианты. Мне нравится выражение одного моего знакомого, что неудачная аналогия – это как кот, помещенный под фальш-потолок. Ты понимаешь, что такое фазы тестов? Ну, давай ты мне объяснишь. Фазы тестирования – когда ты на вход в свою программу подаешь случайным образом сгенерированную последовательность. Фазы тестирования чисто теоретически можно покрыть с помощью юнит тестирования. Вся проблема, найденная с помощью фазы тестирования, можно покрыть с помощью юнит тестирования. На практике получается, что фазы тестирования… Мы звали несколько гостей, которые искали с помощью фазы тестов ошибки в SysCall, помните? Мы звали людей, которые фазы тестирования применяли при каком-то, в каких-то этих, ну типа, что-то типа REST API своего они тестировали, где у них была богатая функциональность, я сейчас детали уже не помню. Я фазы тестирования несколько раз использовал. Сейчас есть куча фазе пакетов, которые фазе, куча библиотек, которые используют фазы тестирования вместе с анализом покрытия кода, например. Это все вместе настолько богатый пакет, настолько богатое тестирование, которое чаще всего, находит такой результат, который никогда бы не пришли тебе в голову. У нас на фазе тестирования находили такие ошибки, которые просто диву даешься, там не должно такое быть. Он должен покрываться, мы это делали специально тест. Нет, не покрывается, потому что вот после этого, этой единички подается ноль, а мы всегда предполагали, что да, потом будут подаваться единички. Такое в голову не пришло. И поэтому, хотя все предполагают, что юнит тестирования достаточно хорошо, для того, чтобы покрыть все возможные варианты, нет, не достаточно хорошо. В фазе тестирования находят еще проблемы. Поэтому, отвечая на твой вопрос, да, конечно же, когда ты пишешь полноценное 100% покрытие джебсон-тестами, джебсон-подобным тестированием своей системы, скорее всего, ты покроешь большое количество тестов. Но нельзя сделать 100% покрытие, и в фазе найдет тебе еще каких-нибудь проблем. Хаусманки, в данном случае, в фазе на инфраструктуре, как правильно сказал Валерий. И оно найдет тебе еще тестов. Запустите его, и я готов поспорить на что-нибудь. Тут есть, знаешь, в чем проблема? То есть я понимаю твою мысль, я с ней даже процентов на 70 скоро не согласиться. Проблема в том, что хаусманки тестирования, хаусманки подхода, хаусманки, что-то такое, оно не имеет определенного инварианта, который тестируется. То есть он говорит просто, я буду творить фигню, а что сломается, ну жалкие людишки уже разберутся по логам, поймут, что что-то произошло, что пошло не так. Когда у тебя нету... Поясни, пожалуйста, еще раз, я не очень понял, что значит жалкие людишки по логам. У тебя, у хаусманки, у него есть выход с типом boolean, который тебе скажет, да, тестирование пройдено, нет, тестирование не пройдено? На самом деле тут много подходов бывает, но чаще всего у хаусманки нет задачи протестировать, хаусманки убивают твою систему. Вот, я к этому и подвожу. Это не очень полезно на стенде. То есть я не могу, если я хочу делать агрессивное тестирование, мне нужен выход, что тесты пройдены или тесты не пройдены. Почему? Потому что я не буду... Я не понимаю, что мешает, если есть стенд, на который просто регулярно раскатывается свежий код, который просто всегда бежит, на котором бежит какая-то симулируемая нагрузка, просто всегда. Ну да, это Саша и говорит, то есть у тебя должен быть всегда нагрузка, большая. Во-вторых, у тебя должен быть постоянный хаус-чек включен, который будет все части системы анализировать и проверять, что она работает как надо. Ну да, это не так, что хаусманки, они не такой подход, что у нас, как бы не знаю, мы прогнали все наши тесты и они не нашли никаких проблем. Нет, это просто, что если этот тест когда-то что-то найдет, ну окей, в следующем релизе починим. Это не что-то такое, что... Это другой подход к тому, как ты находишь проблемы. Ты знаешь, типа как мы... Если понтестер ничего не нашел, это не значит, что проблем нет. Это значит, что понтестер ничего не нашел. Согласен? Можно я не буду отвечать на этот вопрос, а отвечу на другой? То, что вы описываете, там, система работает, не работает, вы начинаете вводить инварианты на систему. Например, что у меня идёт нагрузка, и ответы возвращаются не с ошибкой. Или что у меня есть хаус-чек, и он там типа окей всё время горит. И те тесты, которые я натравляю, ну те выключения-включения, которые хаус-манки делают, это мои немезисы. И которые я запускаю в случайном порядке и проверяю, что инварианты держатся. Это не хаус-манки тестирования. Хаус-манки – это не тестирование. Хаус-манки... Ну в смысле на стенде. Это не хаус-манки подход на стенде, это джебсентоподобные тесты. Нет, подожди, а почему ты не готов сделать то, что Валера хочет? То есть я бы точно так же сделал. Я бы запустил туда постоянное... У вас же есть анализ, у вас же есть тесты, давай так. Системные тесты у вас есть? Да, конечно. У вас есть наверняка бенчмарки какие-то, которые тоже проверяют результаты, верно? Ну да-да-да. То есть я бы на вашем месте запустил, если у меня были бесконечные ресурсы и куча инженеров готовых заниматься этой ерундой, я бы запустил тестовый стенд, на котором бы запустил бенчмарки одновременно со всеми системными тестами и запустил туда хаус-манки, которая постоянно пыталась бы что-то сломать. Если бы я хотел получить бесконечно живучую систему. И если какой-то из тестов внезапно падает, я такой, оп, очки, что-то у меня работает не так, дай-ка пролежу по логам у хаус-манки, что она пыталась убить, что у меня всё сломалось. Именно это мы и делаем. Но мы спорим о терминологии. То есть я утверждаю, что это не хаус-манки подход, это джебсон-подобное тестирование, потому что я написал сам немезиса, я сам определил инварианты. Тогда нам надо определить, что такое джебсон-тестирование. Для меня джебсон-тестирование – это когда ты вообще, у тебя есть распределённая система, у тебя есть какие-то инварианты, и ты ручками пишешь тесты для того, чтобы проверить свои инварианты. Ну да-да-да. Ну и там прикол в том, что мы выясняли, что у тебя количество инвариантов и немезисов перемножается. То есть, например, ты говоришь, что при NNED сплите у меня не теряются данные A, там B у меня все запросы возвращают ОК. Это как бы у тебя был один немезисный сплит, потом ты говоришь второй немезис, там не знаю, полное падение одной ноды. И ты говоришь, что все те же самые инварианты держатся. Ну в этом смысле они, конечно, близки, вопрос только, что ты не ручками это пишешь. Ручками пишу что? Тесты, которые будут что-то делать. Ты даёшь это власть хаосманке, она в случайном порядке это делает. Я думаю, большая разница в том, что как раз-таки... То есть это реально, чисто с точки зрения имплементации, это похожие подходы. Я понимаю, примерно, о чём говорит Саша, но, наверное, разница в философии, что в случае джебсона у тебя первичный инвариант, и ты его придумал, ты придумал какую-то сложную программу, которая по логам пытается проверить. И там этот инвариант, он очень нетривиальный для проверки. Это, наверное, ключевой момент джебсона, что там, например, проверяется лениризуемость. Там проверка лениризуемости, это гораздо больше, гораздо больше контрибьюшен подхода джебсона, чем, собственно, немезисы. Потому что немезисы, ну да, это похоже на хаос-подход, и я думаю, что, возможно, даже хаос-подход, по-моему, был до того, как джебсон стал популярен. В то время как основная идея хаос-подхода, потому что есть какой-то элементарный инвариант, типа работать не работает, health check. Но у тебя вот эти способы что-то сломать, они настолько произвольные, нетривиальные, что это прям отдельная какая-то, какой-то кусок софта, а не какие-то элементарные отрубалки сети или ломалки фаервола. Они на самом деле нетривиальные, Саш, может быть, ты сейчас недооцениваешь, но у хаос-манки, вот представь себе API у AWS, и представь, что она может выключить любую штуку, которой она может достучаться по AWS API. Там бесконечное количество того, что она может сломать. То есть она может выключить реально, я не знаю, кэш на твоей машине, кэш у CPU на твоей машине. Ну я утрирую сейчас, потому что нету такого API у AWS. Но она могла бы это сделать, если бы это было возможно. Ну смотри, если я пишу магазинчику, у которого традиционная схема типа мастер-постгресса с репликами, и если хаос-манки выключат мастер, то разумеется, у меня магазинчик навернётся. Я не хочу, чтобы он выключал вообще всё что угодно, понимаешь? Я не хочу, чтобы он выключал... Значит, ты не хаос-манки уже. Я не хочу, опять же, если у меня перед моим магазинчиком стоит ELB, я не хочу, чтобы он пришёл и удалил мой ELB. Я всё равно определяю, что он может, чего не может. Ну, ты зря это делаешь. Классическому подходу, да. Он может выключить реально что угодно, и ты должен это пережить. Ну тогда, если вы топите за такой подход, и если бы я был бизнесом, я бы послал вас нафиг. А вот Netflix не послал, и поэтому у них чрезвычайно стабильная система. Но при этом всё одновременно выключить всё равно нельзя. Так что какие-то ограничения всё-таки есть, и они выходят из инварианта бизнеса, что что-то должно продолжать работать. Поэтому ELB выключать нельзя, весь Netflix сразу выключать нельзя. С точки зрения ELB я бы не согласился, по-моему, у них есть дублирующие каналы, поэтому они как-то могут обеспечить жизневособность. Насчёт того, что всё подряд выключить нельзя, это конечно, да. Ну у Netflix наверняка есть дублирующие каналы, а вот у магазинчика, Саша, наверное, их нет. Ну с продюсером-дядей магазинчику едва ли нужен хаос-инжиринг. Ну почему же. Нет, я хочу, чтобы у меня был хороший магазинчик, чтобы он приносил мне много денег. У тебя бизнес в разработке софта или в том, чтобы товары продавать? В чём у тебя бизнес? Ну представим, что я azon.ru. Ну это уже не просто магазинчик, давай так. Ты так озвучил магазинчик, как будто, не знаю, какой-то личный небольшой магазин azon.ru, но там уже это начинает иметь смысл, наверное. Опять же, зависит от того, насколько часто ты катишь. То есть подходы типа хаос-инжиринга, они тем более оправданы, чем у тебя больше программистов в команде, и чем чаще ты катишь куда-то какие-то апдейты. Но при этом, ну ты согласен, что я не знаю, как на самом деле выглядит инфраструктура azon, но я подозреваю, что он вполне помещается в AWS, там чуть ли не в один постгрес. Тем не менее, у них есть своя команда разработчиков и всё такое. Вот, а оригинальный мой пойнт был в том, что если у меня нет реального прода, то есть я делаю как бы коробочный продукт, то мне нет смысла ограничивать себя хаос-манки подходами, я могу сделать куда более агрессивный подход. Так, подожди, а куда агрессивнее-то? То, что я называю, как бы озвучил, как джебсон-подобные тесты. Ну мы же разбираемся, что джебсон, там как бы основная сложность, это сложные-сложные варианты, и сравнительно подобранные, но не слишком адовые немезисы, а хаос – это как раз просто инварианты, но очень замороченные немезисы, и как раз-таки более агрессивный – это как раз хаос. Не-не-не, подожди, я хочу агрессивные, очень агрессивные инварианты и очень-очень агрессивный немезис, ну то есть смысл мне проверять замороченные инварианты на лайтовых немезисах? Я хочу на всех. Как бы смысл такой, что, я не знаю, ты, возможно, плохо, не то что плохо давно, или не очень на этом концентрировался, когда читал статьи Эфира, проблема произвольно-сложных мемезисов в том, что ты потом никогда сложный вариант не проверишь. У тебя проблема в том, что у тебя вариант линейной изуемости, нужно проверить всю историю и найти путь внутри истории, который говорит, что вот здесь нарушен вариант линейной изуемости. Чем больше у тебя более сложных мемезисов, тем более у тебя нетерминированная, офигительно разбросанная, сломанная история, тем дольше твой детектор нарушения линейной изуемости работает. Если ты начинаешь делать агрессивные ломания, твой джебс никогда не терминируется. Я же не комбинирую ломания, я один вариант тестирую против одного мемезиса. Тогда это не хаос, тогда ты недостаточно зло всё ломаешь. То есть в этом-то и прикол, в хаос ты агрессивно ломаешь, но вариант простой. То есть давай так, здесь мы отключаем кэш, здесь мы ломаем диск, здесь мы рвём сеть, ага, у нас там загорелась красная лампочка. А джебсон у нас сложный вариант, но мы ломаем что-то по одному разу, потому и говорю, что это не очень злое ломание инфраструктуры, потому что там у тебя какой-то один небольшой мемезис делает что-то в одном или паре мест. А джебсон вообще, кроме ломания сети и удаления инстанса, что-то ещё может делать? Он ещё часы умеет ломать, там на самом деле под каждым системом писались специализированные вещи, то есть ты хочешь проверить вариант, не знаю, про часы, то ты начинаешь ломать часы. Хорошо, часы я согласен, да, тоже, а что ещё? У нас сеть замедляется. Задержку сети, да, задержку сети делать, то есть её можно постировать. А, понятно, рвать, замедлять и, возможно, ещё дробь пакетов. Ранять машину, да. Ну, получается, не такое большое количество. Там ещё, на самом деле, одна из причин, почему мы готовый фреймворк не использовали, потому что если ты можешь заточиться под конкретную систему, ты можешь делать вещи, которые специфичны для твоей конкретной системы. То есть, например, как бы так аккуратно сформулировать, чтобы ничего лишнего не спалить. Представь себе, что у тебя распределённая система, да, и у тебя есть ноды, и нода тебя как-то аутентифицирует. Ты можешь проверять странные вещи, типа у тебя подключается нода с неправильным вот этим credentials, да, или две ноды с одинаковым credentials. То есть, вот такие вещи тестировать. Хотя, ну, к генерик-системе это нафиг не нужно. Ну, это как вот один пример. Но я не очень понял вот этот переход на то, что, ну это же не хаос, потому что, опять же, мы говорим о том, что мы хотим получить такой сценарий ломания системы, который легко воспроизводится, легко анализируется. И как раз хаос, подход его трудно анализировать, трудно воспроизводить. И если мы определили инварианты, инвариант держится на простом ломании. И он держится на двух простых ломаниях и на трёх. Ну в смысле, если он держится на одном, то он... Я с трудом представляю сценарий, при котором инвариант держится на одном ломании, держится на втором ломании. Но если я скомбинирую их там последовательно, то инвариант, вдруг, перестанет держаться. Ну, потому что ты сейчас говоришь, скорее всего, про одну систему, 🔔 iPhone,したкlihood, который учёный добирает 😇 данных. А мы с вами говорим про микросервисные системы, где типа у тебя есть сервис, зависящий от сервиса, зависящий от сервиса, и там бывают каскадные сбои. У меня буквально вот из практики сравнительно недавней каскадный сбой, просто, ну, потом, можно сказать, даже в общем-то в одной системе, просто, ну, типа между нескольких компонент. Есть PASGRES. PASGRES есть вакуум. Есть система, которая там читает таблицу PASGRES, пишет, ну, читает две таблицы PASGRES и пишет одну, потом тебе другие удаляет. Есть еще другая система, которая читает таблицу PASGRES, пишет в новую таблицу, удаляет из старой таблицы. И есть четвертая система, которая, или какая там, почему-то пятая система, которая ходит за предыдущей системой и те таблицы, которые удалили, но не удалили саму таблицу, запускает на них автовакуум. Вот, короче, был такой потрясающий рассинхрон работы системы этих, вот, как бы даже так, система автовакуума, не в автовакууме, а система обычного вакуума, которая ходит и вакуумит, прежде чем пришел в автовакуум. Она начала не поспевать за системой, которая удаляет из таблиц. В это время запустилась та система, которая читает две таблицы, делает одну, но на более больших таблицах и начала, короче, делать свое дело, что в итоге начало еще сильнее отбрасывать вакуумщиков, потому что вакуумщики в таком случае не могут удалять, они не могут помечать строки, которые видит другая транзакция, пусть они даже и в другой таблице, как, типа, почищенные. И то есть, как бы у тебя вот такой снежный ком, когда одна система начинает помечать другой. И, типа, не знаю, там буквально в одном месте нужно было добавить доминошку, чтобы это запустить. А что самое интересное, это очень хороший юзкейс. Я как раз хочу на его примере показать, что интеграционные тесты сложно было бы придумать, как эту систему сломать. Ну, то есть, можно было бы, конечно, об этом специально намеренно подумать, но вероятность того, что инженер подумает, очень низка. А хаусман пришел и сломал. Давайте заведем цитатник с фразами Вани, вырванные из контекста. Вероятность, что инженер подумает низка, да. Типа, зен, вероятность, что он думает низка. На футболке это напишу. А Джепсон, за счет... Ой, Джепсон, всё-то я уже заговорился. А хаусманка, она просто не знает, что ей можно и что ей нельзя. И она потыкает сюда, потыкает туда и такую штуку найдёт. Валер, я тебя перебил, прости, продолжи. Да я всё закончил, в общем-то. Ну, просто как бы это был, не знаю, как бы в одном месте шатнуть систему, но не так, что мы что-то где-то ей отключили, но просто в одном месте диск чуть медленнее начал работать и понеслась. И я даже не уверен, что... Я могу представить, как это же могло случиться, если диск начал работать медленно на одной группе машин, а тормозить начало на другой группе машин. Потому что типа там из кого-то внешнего сервиса мы данные вовремя не забрали и там отстало в другом месте. То есть как бы у нас в конкретном случае такого бы не было, но я могу представить, если бы это было немножко по-другому написано, как можно было бы триггернуть, типа затормозив диск вообще в другой группе систем. И это, блин, ну... Никакой Джепсон такое не начнёт ловить, это именно, знаешь, из разряда... Потому что никакой-то вариант ломается, у нас система просто выходит из-под контроля. Я хочу добавить ещё немножко другую сущность в хаос инженеринг этот. Если посмотреть, подняться на уровень выше, с точки зрения отдельно взятых систем на методологию разработки систем, мы приходим к тому, что если люди, пишущие систему, знают, что у них работает хаос инженеринг, и то, что их систему будут постоянно выключать, вырубать, я не знаю, диски, отдельно взятые элементы, пытаться сломать базу данных и так далее, получается уникальная вещь, что люди начинают работать лучше. Прям вот в этом уверен. Может, их просто надо бить? Придётся назвать так выпуск. То есть у вас его не практикуют, но ты уверен, что люди работают лучше, когда практикуют этот подход? У нас есть какие-то части этой методологии, и когда мы эти части включаем, становится намного лучше. Знаете, что ещё становится намного лучше? Нет, что? Айоу, когда вы для него сделали свой собственный специальный DSL. Вот, Facebook, например. Я просто тему перевожу, потому что мы о ней уже, мне кажется, полчаса разговариваем. Извините. Вот. Так вот. Facebook в своём блоге, в блоге своей видеоинжиниринг-команды написали про то, какую они интересную штуку делают. К сожалению, очень мало технических деталей, но есть пара интересных таких чисто консультальных моментов, и, наверное, попробуем о них поговорить. Система, ну, статья называется Oil, как масло, плюс Vcache, файлы абстракции для распределённых систем. В общем, они вводят две штуковины. Первая называется Oil, они называют, что это какой-то output input language, но дальше по контексту больше похоже, что это не какой-то прям язык, а скорее, какой-то framework, который имеет какой-то API фиксированный, но, если честно, не очень понятно. И есть, собственно, Vcache, это такая, не знаю, иерархическая система как бы кэширования, которая глубоко интегрирована с этим Oil, ну, и, то есть, там, цитесно строятся на этом Oil какие-то даги, ну, циклические графы зависимости по IEU, и они, типа, прозрачно транслируются в то, откуда как читать, где кэшировать, и там, не знаю, например, скажем, нам нужно прочитать данные с четырёх узлов, ну, по шарику, там, типа, наивно можно просто там со всех из них прочитать и ждать, а можно сделать там, типа, чуть менее наивно, мы кидаем запрос в самый близкий, а тот с более к себе близких подтягивает, и это, типа, в итоге меньше делательности получается, ну, или во всяком случае меньше загружения, ну, если даже не делательность такая, что во всяком случае меньше saturation CT. Такого рода всякие оптимизации, а также там, типа, сколько дублирования, сколько чанкинга, там довольно много у них наворочено, если честно, и без технических деталей тяжело говорить, что именно они делают. Но, в общем, ключевые, как мне показалось, идеи первое в том, что у них весь IEO описывается как такой, не знаю, декларативный граф зависимости, который потом интерпретируется какой-то системой кэширования и хранения, то есть получается такая, как бы вот как у нас есть в случае с базовыми данными, у нас есть SQL, для которого потом есть Planner, который сам планирует как-то лучше выполнить на тех данных, которые есть, как они лежат, и выполняет. Здесь получается похожая идея, видимо, что инфраструктуре как бы виднее, но там можно давать, в отличие от вот в SQL не принято давать хинты, там с другой стороны можно индексы построить там, где нам нужно. Здесь, соответственно, можно сказать, что мы хотим вот для этой нагрузки, мы хотим приоритизировать latency, а для этой мы хотим приоритизировать, наоборот, надежность хранения. Такого рода вещи можно делать. Второй интересный момент, что они немножко расширили классический POSIX API файловый, и у них read, когда вы читаете за границей файла, он может себя вести одним из двух образов. Он может или вернуть конец файла, и сказать, ой, перестань, что ты что-то делаешь, а может вести себя, как будто вы искатки читаете, то есть начать ждать, пока там еще появятся данные. Это очень круто работает, видимо, для видеостриминга, потому что у них, собственно, эта вся история из видеостриминга переехала. И, в принципе, это, наверное, очень удобно, ты просто продолжаешь читать из файла, а если это просто какое-то полностью загруженное видео, ты просто управляешься в конец и заканчиваешь поток, если это вещание, ты продолжаешь просто выгребать. В принципе, клевая идея. Ну вот, на мой взгляд, это две самые ключевые идеи. Там есть еще куча всяких довольно абстрактно написанных деталей, которые мне тяжело пересказывать, потому что я их не понимаю по очевидным причинам. Но если вас интересует такая рода вещь, то, наверное, попробуйте почитать статью. Мне интересно, что вы по этому поводу думаете, нужно ли нам, как это, даги, планировщики для наших нужд распределенного хранения данных. Мне кажется, что есть пять компаний в мире, кому это надо. Зачем это может понадобиться озону? Озону, да, наверное, не нужно. Но с другой стороны... С другой стороны, озон уже достаточно большой по сравнению вообще со всеми остальными. Ну да. Нет, я, скорее, думал, что если бы я делал, не знаю, твич уже есть, но если бы я делал твич, то может быть, такое было бы нужно. Видео, все-таки видео. Ну, конечно, видео, но что? Почему за все время существования подкаста, если меня память не подводит, нам к нам никогда никто не приходил из-за озона? Не знаю, мы к нему никого там не знаем. Вот, но если нас кто-то вдруг слушает, вы заходите, расскажете, что там у вас и как. Хотя вот на самом деле, Валер, мне кажется, чем дальше, тем больше будет сервисов как-то связанных с большими потоками данных, в частности, там, видео какое-нибудь, дополненная реальность и прочее, что нам пророчат уже давно, что к нам придет, но пока все никак не приходит. И, соответственно, все больше будет необходимости в каких-то подобных вещах. Может быть, мы просто недооцениваем важность этой фичи. А еще может быть, что такая штука позволит сделать какое-то, так как там декларативное описание, можно будет декларативно описывать, чего ты хочешь из системы, как е-роутить данные и все такое, она сама будет поднимать киши, базы данных, настраивать кассандры, там, какую-нибудь с соответствующими replication rules и все такое. Декларативность – это сила и будущее. Императивщина могила. Я бы сказал, прошлое. Я вижу некоторое противоречие в двух последовательных предложениях, типа поднять кассандра и декларативность – это будущее. Ну, я имел в виду, что поднять кассандру не вручную, а автоматически, но и кассандра – это такой, просто generic база данных, можно заменить любой другой. Главное, что ты говоришь, хочу, чтобы мои данные хранились надежно, а вот тут их надо кешировать, а вот тут надо, чтобы они были быстрые и желательно геораспределенные с геороутингом. Я так понимаю, что там даже, по-моему, ты не очень говоришь, что здесь нужно кешировать, ты говоришь, что вот здесь хочу быстро, а здесь хочу надежно, а здесь хочу, типа, не знаю, скажем… Дешево. Ну да, здесь хочу дешево, например, так, да. То есть дальше она сама думает, где кеш поставить, где… Притом, у них там эти vcache, он может быть right through, может быть right back, может быть, типа, вообще просто записать в кеш, а дальше неважно, то есть там вот еще разные такие, такого рода гарантии и… Как это? То есть это гарантии консистентности разные и, как бы, соответственно, разные latency и, если это еще намазать на геораспределенность по-разному, это может быть еще и разные гарантии не просто консистентности, а именно такой устойчивости к авариям, типа, дата-центров, вылета дата-центров. И все это смог настраивать просто менеджеры озона, им говорят, хочу быстро, дешево, и чтобы кешировалось, им говорят, система, выбери два.",
    "result": {
      "error": "API request failed: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-xSG9D345gtYlIKC330wCyrEG on tokens per min (TPM): Limit 30000, Used 29119, Requested 27314. Please try again in 52.866s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
      "stack_trace": "Traceback (most recent call last):\n  File \"/home/andrei/Projects/podcast-shownotes/scripts/build_search_eval_dataset.py\", line 157, in generate_search_query\n    response = await self.client.chat.completions.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 1927, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1767, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1461, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1547, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1547, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1562, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-xSG9D345gtYlIKC330wCyrEG on tokens per min (TPM): Limit 30000, Used 29119, Requested 27314. Please try again in 52.866s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
    }
  }
]