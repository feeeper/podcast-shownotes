[
  {
    "segment_id": "ced1cf00-fc14-4b8c-a383-08d37e3c5ba4",
    "episode_id": "53fc2050-51d4-4466-b8be-5e6c031265aa",
    "episode_number": 94,
    "segment_number": 3,
    "text": "И начинает объяснять на пальцах. Первая половина на пальцах вообще все легко понятно, хорошо, а потом он начинает углубляться, и мне, человек, который с зависимыми типами, мало чего работал, я понимаю, что он пишет, но если мне скажут, повтори, попробуй то же самое, я пойму, что еще месяца два, короче, мне это надо статью перечитывать внимательно под лупой, чтобы разобраться до конца. В целом, статья про то, как он делает нейронную сеть, и на примере нейронной сети, реализации нейронной сети он показывает, насколько удобны зависимые типы. В частности, он делает многослойную нейронную сеть, и при просчете всех, как это, backpropagation, когда он просчитывает результат нейронной сети, он делает умножение матриц и все такое. И вот он как раз одно из больших преимуществ зависимых типов, он показывает как раз то, что вся эта математика, когда вы умножаете одну матрицу на другую, получаете результат и так далее, вы должны очень внимательно, наивная версия он показал тоже, наивная версия очень внимательно должна быть написана, потому что вы можете перепутать местами два аргумента, на что вы умножаете, и вроде как у вас все скомпиляется, но на самом деле скомпиляция в runtime она упадет, потому что вы неправильной размерности матриц там где-нибудь что-нибудь поставили. И зависимый тип, он приводит как раз эту наивную версию в нормальный вид, когда у него компилятор гарантирует, что при умножении двух матриц они будут в нужной размерности стоять. Это гарантируется с помощью того, что у тебя каждый тип внутри себя показывает, какую размерность он, скажем, если у тебя есть нейронная сеть из двух слоев, у тебя есть описание размерности этих слоев прямо в описании типа. И в дальнейшем, когда ты, например, добавляешь еще один слой слева, у тебя получается, что матрицы весов совпадают. Первый слой у нейронной сети имеет размерность B, а тот слой, который ты еще добавляешь спереди, он имеет слой A. И получается, что у тебя должна быть матрица весов AB, и это все прямо в типах прописано, и в дальнейшем вся математика проверяется компилятором, и ты не можешь прошибиться. Я не буду углубляться в детали, но что мне больше всего понравилось. Во-первых, я заметил, что я понял, что я никогда не писал, при описании типов никогда не использовал kind signature. Я всегда говорил, что у меня есть функция такого типа, данные такого типа, или даже когда я использовал GDT, у меня не было никогда kind signature, а он всегда его использует, он говорит, что это намного удобнее. Кто-нибудь из вас использовал его? Ты использовал GDT, я вообще… ну, точнее, разу у меня не было такой необходимости. Все понял. Нет, я использовал, но kind signature не использовал никогда. Во-вторых, как я уже говорил, там нельзя было делать арифметику в типах, то есть он говорит, вот, к примеру, у нас там есть тип, который внутри себя прописывает, что у тебя внутренние слои имеют такие размерности, и это у нас получается массив int в описании типа. И мы добавляем слева еще один слой, и мы в этот массив int добавляем еще один int фактически. То есть, если я не ошибаюсь, как раз на коде mesh на докладе было показано, что в тот момент еще нельзя было это делать, ну, если я правильно помню. И особенно тут, конечно, весело получается, что когда у тебя есть четкое описание типов, какая размерность есть у тебя в твоей нейронной сети, ты говоришь ему, слушай, у меня вот тут есть реализация заполнятеля рандомного, и я хочу, чтобы ты… ну, через черепоманаду, конечно, всеми любимые. И, пожалуйста, сделай-ка мне, пожалуйста, тут… как это… напечатай-ка мне экземпляр типа с рандомным заполнятелем этих величин, весов и так далее. Из-за того, что у тебя тип четко известен, из типов ты можешь прямо генерировать на лету реализацию заполнения, и у тебя фактически сразу появляется случайным образом заполненная матрица всей нейронной сети. То есть как бы Haskell в этом смысле очень сильно помогает, и ты можешь сразу все это показать прямо в GHCI, это вообще четко. Надо, конечно, попробовать это все самостоятельно поделать. Ну и плюс автор пишет, что компилятор очень сильно помогает писать код, то есть, скажем, ты в какой-то точке не знаешь, что сюда подставить, и запутался, и вообще даже думать об этом не хочешь, ты можешь там подставить знак подчеркивания, и тебе компилятор подскажет, слушай, ты вот здесь, наверное, вот этот имел в виду, сразу покажет варианты, какие ты можешь использовать, чтобы у тебя типы начинали совпадать. Это все, конечно, здорово. И в качестве одного из выводов статьи автор говорит, что сейчас современное программирование на каких-нибудь плюсах, или вообще всех тех языках, где компилятор не подсказывает по типам и не содержит подобные вещи, как зависимые типы, все привыкли, что надо держать в голове очень много вещей, и они так сильно переплетаются, что если вы, не дай бог, отвлечетесь на секунду, на мгновение, то скорее всего где-нибудь баг допустите, и это окей, это стандартное положение вещей.",
    "result": {
      "query": "Haskell dependent types neural network matrix multiplication"
    }
  }
]