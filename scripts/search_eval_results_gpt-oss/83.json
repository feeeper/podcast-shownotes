[
  {
    "segment_id": "7847ec8f-b8a3-4134-b45b-018ec4d491cd",
    "episode_id": "fa4e299e-f603-49f4-8851-950c024a5592",
    "episode_number": 83,
    "segment_number": 3,
    "text": "Но тем не менее, вот просто виртуалки. И с большим расходом на менеджмент, ну в смысле на оперейшн, на саппорт. Да, и при том у тебя там проблема, у тебя получается разгуча проблем. Как раз ты не даёшь по статье пройтись, они там называют, почему оно именно так задизайнено. Хорошо, давай, я расскажу. То есть, смотри, там называются следующие пункты. Во-первых, у тебя получается, что поскольку у тебя один процесс приложения, это один контейнер, то у тебя получается, что единица управления совпадает с логической единицей, которая интересна разработчику. То есть, грубо говоря, мне интересны логи такого вот инстанция приложения. И это будут не логи с такой-то виртуалки, с такой-то машины виртуальной, где там может быть куча всего. Это будут вполне конкретные логи, вполне конкретные логические единицы. То же самое касается всех метриков этого всего. Вторая проблема связана с тем, что у тебя как раз хочется, если ты деплоешь где-то аппликейшн какой-то, хочется, чтобы все его инстанции, которые ты запустил с некоторой версии, имели бинарный идентичный код. И очень может быть, что у тебя один инстанций приложения может на одном сервере быть поднят больше одного раза. Особенно типично, если у тебя какие-то не очень многотрейдовые приложения, типа Ruby on Rails. Это вообще очень типичный вариант, или даже типа Redis. Очень типичный вариант на многопроцессорном сервере крутить много контейнеров, каждый из которых потребляет максимум одно ядро. И если у тебя такая ситуация, ты запускаешь множество виртуалок или даже контейнеров с изменяемой файловой системой, ты в итоге приходишь к тому, что у тебя один и тот же кусок данных скопирован много-много раз. И даже если он у тебя везде бинарно одинаковый, в первых, тебе это нужно как-то еще подтверждать каждый раз, а во-вторых, у тебя просто расходуется место ненужным образом. Вот место меня совсем не беспокоит, потому что оно как раз очень дешевое в нашем смысле. И усложняется управление. Нет, подожди, в каком смысле оно усложняется, если все автоматизировано? Как ты это автоматизируешь? Мы говорим сейчас концептуально, пока что. Что-то вроде кубернетеса поверх виртуалок а-ля LXE или KVM, или что-то вроде кубернетеса поверх докера, как он и сделан. Ты говоришь, что усложняется поддержка. Я говорю, что она ничем не усложняется, потому что и там, и там все автоматизировано. Понимаешь? Нет, не очень. То есть, когда ты говоришь, что я контунизирую, это значит, что у тебя есть 20-100-100 тысяч без разницы сколько одинаковых юнитов, которые, во-первых, взаимозаменяемы, во-вторых, полностью одинаковы в управлении. Когда у тебя есть виртуальная машина, на которую ты хочешь поставить 100 разных вещей, ты должен их вставить хотя бы в разные папки. И ты должен этим как-то управлять. Нет, подожди. То есть у тебя сложность добавляется. Ты сейчас мне пытаешься доказать, что она не добавляется, но она хоть как-то добавляется. Ты про какую сложность еще раз? Управление. Еще раз. Управление полностью автоматизировано. В чем усложнение? Ну вот смотри, ты хочешь в одну виртуалку по одному процессу вставить или много процессов на виртуалку? Я хочу, скажем так, по одному сервису на виртуалку. Притом сервис, я поясню, сервис может включать в себя, например, несколько, ну хорошо, допустим, одно приложение для простоты, EngineX настроенный перед ним, с SSL, с сертификатами и так далее. Возможно, какой-то агент для сборки метрик конкретно этого приложения, потому что, ну, я сомневаюсь, что Кубернет знает, как из моего постгресса, моего приложения собрать правильные метрики. И, ну, пока что ограничимся этим. И вот теперь смотри, тебе нужно перезапустить или обновить агенты во всех таких виртуалках? Я делаю новую виртуалку. Кубернетс и Борг это решают? Прикинь, снапшоты в любой технологии виртуализации тоже это решают. А без вообще трогания и перезапуска работающего сервиса? Ну да, мы так и катили в Амазон. Мы использовали реально Амазон, реально на виртуалках. Ты, чтобы выкатить апдейт, ты берешь виртуалочку, ну, которая не под нагрузкой, раскатываешь туда то, что тебе нужно один раз, создаёшь АМИ, то бишь образ, потом катишь его в автоскейлинг-группу. Всё. Ну и как ты потом внутри автоскейлинг-группы без остановки сервиса, без остановки каждого отдельного процесса обновляешь только часть имиджа? Зачем мне обновлять часть имиджа? Я обновляю целиком весь имидж. Ну, затем, что если... Ну, то есть, возьмём реальный пример. У нас, например, достаточно highly available set-up ряда, и в принципе мы переживаем падение одной машины вообще без проблем, но мы его не замечаем. Но на самом деле это кратковременный всплеск кладности для пользователей. В принципе, если мы можем этого избегать, мы будем этого избегать. Если мы хотим просто обновить агенты везде, то было бы хорошо иметь ручку, которая просто обновит агенты, а не вообще всё, если особенно реверсия ряка не менялась. Например, кубернетс как раз это решает, но я это подозреваю в Борг тоже, потому что в кубернетс единица сервиса, вот того, что называешь сервисом, называется под, а единица пакетирования и старта называется контейнер. И у тебя в поде, как бы, это такое, не знаю, что-то вроде такого общего спейса, они там друг другу могут ходить по сети, они там друг другу могут ходить по диску, но при этом это всё ещё разные имиджи, и это разные единицы управления, это разные хеллсчеки. Вот, соответственно, ты можешь вполне себе пойти просто и во все поды, которые содержат вот такую-то вот штуковину, то есть конкретно там вполне агент, пойти и обновить, перестартануть только агентов, не трогая работающий сервис вообще. Ну хорошо, то есть это примерно как, ну хорошо, пример с... я твой пример понял? То есть это примерно как я на своей виртуалке буду ходить не так, что обновлять целиком образ их, да, а буду ходить в них энсиблом и ставить новые депакеты агентов. Да, ну только как бы, да, после этого получаются типичные проблемы энсибла или шефа, или чего угодно, что тяжело гарантировать идентичность того, что там внутри. Ну, во-вторых, у тебя... А можно ли вообще гарантировать это? Да, нужно. Во-первых, ну, сложное то, что Валера сказала, а во-вторых, у тебя нет гарантии, что оно будет однозначно работать заранее. Ну, то есть я правильно понимаю, чтобы решить вот эту проблему, чтобы гарантировать идентичность, вы предлагаете накрутить вот всю эту схему с подами и указанием путей до файлов и так далее? Ну, то есть как бы да, потому что, смотри, у тебя чем больше машин, то есть у тебя, если тебе нужно сервис раскатить на 30 машин, ты начинаешь ощущать боль с шефом, ну, так, ну, как бы такую перебарываемую. Если у тебя сотни машин, то боль с шефом становится невыносимой. Когда ты гугл, у тебя тысячи машин, то боль настолько невыносима, что тебе проще, как бы, не знаю, один раз пути написать где-то в конфигурационном файле, и потом использовать этот факт, что у тебя бинарно идентичный образ везде катится, и вообще не думать о том, как бы так обновить шеф-рецепт, чтобы ничего не сломалось. Ну, хорошо, а если у меня нет такой проблемы, что, ну, если я могу свои сервисы ресортовать, и пользователи от этого не сильно страдают, ну, потому что у меня типичный вебчик на самом деле, то на самом деле мне кубернетс не нужен. Ну, как сказать, ты все равно от него не проиграешь. Пока что я... Тебе же самому не нужно писать кубернетс, согласись. Мне еще не нужно писать проксмокс с квм. Ну, проксмокс с квм этого точно не решает, он решает гораздо меньшую часть задач, чем решает кубернетс. Ну, он решает те же задачи, что и решает амазон, а поверх амазона мы вполне успешно катили вот образами, как я рассказал. Там есть репликашн менеджер, например, то есть он умеет поддерживать заданное количество сервисов, и если у сервиса обломался хеллсчек, и он даже не помер, но половина обломался от хеллсчек, еще один. Честно, не знаю, надо узнать. Второй момент, автоскейлинг, я подозреваю, там есть. Позволь, я тебя сразу поправлю, в амазоне что автоскейлинг, что вот это поддержание, которое ты назвал, работает из рук вон плохо и не всегда. Ну, как бы, у меня нет опыта работать с автоскейлингом амазона, но утверждается, и как бы, судя по счастливым пользователям, в кубернетисе это вполне себе работает. Я подозреваю, в амазоне это с ним очень хорошо работает, именно потому, что тебе автоскейлинг нужен не виртуалок, а числа процессов. Оно там по-разному. То есть, как бы, стартануть N процессов, это сильно более простой и быстрый процесс, чем стартануть N виртуалок, согласись. Я знаю, что я скажу, что я из вашего рассказа... что мы пришли к какому-то консенсусу, давай так скажем. Что я начал лучше понимать вашу эту идею с этими вашими странными контейнерами, у которых файлики изменяемые торчат наружу и вот это всё. Хотя я всё ещё склонен считать, что это довольно сложно. Но я готов допустить, что оно действительно решает какую-то боль в некоторых задачах. Оно решает боль при больших масштабах, в первую очередь. А потом уже заодно, из-за того, что Google не хочет иметь только решение одной проблемы, он решает сразу все свои проблемы, которые может, имея уже позади и Borg, и Omega. Из-за этого ты можешь иметь большое количество плюсов, потому что за тебя уже решили много что. Ну то есть да, на самом деле чисто если брать уже Kubernetes, то есть в Borg это решено каким-то костылями, судя по статье. Но вообще глобально там Kubernetes решает довольно интересные и удобные штуки. То есть ты можешь ему сказать, что мне, пожалуйста, нужно вот столько-то живых процессов, которые у которых HealthCheck отрабатывает. HealthCheck может довольно кастомно задать, и он будет стараться поддерживать это число живых процессов. Ну там еще, вот сравнивая как раз с предыдущими системами с Borg, они показали, что у них увеличилось количество абстракций. То есть мне очень понравился пример, мы его уже использовали у себя даже. Когда работает ReplicationController, это не сущность, которая внутри себя содержит 5, скажем, подов. А это сущность, которая ищет по заданным критериям все поды и смотрит сколько их. То есть он говорит, что должно быть 5 подов, которые подходят под данный LabelSelector. А ты лейблы навешиваешь на нужный тебе контейнер, поды, как хочешь. И в случае, если у тебя что-то поломалось, ты можешь сказать, вот этот контейнер как-то странно себя ведет, я снимаю с него лейбл и буду его инвестигировать. Как только ты снял с него лейбл, ReplicationController смотрит, у тебя стало вместо 5-4, запускает еще один, а твой остается работающим. Ты просто начинаешь в нем investigation какое-то проводить, решать какие-то проблемы. Да, а потом трафик пойдет мимо. Да, да. Трафик пойдет мимо, потому что у тебя есть еще дополнительная сущность, которая точно таким же LabelSelector, скажем, обладает. А можешь еще иметь другую сущность, которая обладает другим LabelSelector, в который он попадет. То есть у них добавились уровни абстракции с помощью вот этих лейблов. Они очень здорово добавили себе кучу фич, которые, возможно, даже сейчас до конца еще не раскрыты. Я имею в виду, что они не догадываются еще, как их можно использовать, потому что реально очень большое поле. И еще очень интересно следить за развитием. То есть мы там в позапрошлом, кажется, выпуске переняли, пеналик убранается за то, что он не умеет, например, нормально работать с, так скажем, тяжелым стейтом, типа вот Cassandra и всего такого. Он его вайпает, либо его нужно хранить где-то в не очень эффективном, то есть не в локальном сторидже. Они сейчас работают над соответствующими решениями этой проблемы. И вот тут просто интересно смотреть, как они достаточно сбоку добавляют просто подсет. Ну то есть они, конечно, не просто это делают, это довольно сложный процесс, но в любом случае это как бы вещь, которая случается довольно сбоку. То есть это не такая штука, ради которой нужно... Кубернетс чего-то не поддерживает, нужно поменять теперь весь Кубернетс. Они довольно сбоку просто это приделывают, это довольно здорово выглядит. Да, они как раз в пейпере очень хорошо описывается, что они ушли от orchestration, когда у тебя есть централизованная вещь, как монолитное ядро. Они перешли к концепции как-то хореографии, когда у тебя большое количество маленьких кусочков взаимодействует между собой и решает твою проблему. Это прям хорошая аналогия, потому что на самом деле в Кубернетсе много маленьких кусочков, которые вместе делают то, что надо делать. Я, кстати, эту идею слышал больше одного раза, по-разному названную. Например, проект RamCloud, из которого в своё время вышел Raft, я помню даже к какому-то ключе его обсуждали. У них есть то ли секция пейпера, то ли смежная под статья о том, как они научились писать офигенно код для вот этого всего, чтобы не сильно жертвовать усилиями распределённых систем, чтобы не сильно жертвовать производительностью и не сойти с ума. И вот они как раз говорят о том, что они заменили стейк-машины на некую сущность, когда у тебя влетают эвенты, меняют стейк, потом есть набор правил вида pattern matching по стейку, если pattern matching сработал, выполнен такой только кусок кода, дальше следующее правило и так далее. И то, что описывает Google, на самом деле, и этот набор правил более-менее стейтлес, т.е. там есть какой-то отдельный живущий стейт, но в данном случае кубернетс это стейт, живущий в ETC или где-то там ещё, и соответственно все сервисы довольно стейтлес, они просто могут пойти, посмотреть на текущий стейт, посмотреть",
    "result": {
      "query": "kubernetes vs virtual machines management"
    }
  }
]