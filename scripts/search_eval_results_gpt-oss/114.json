[
  {
    "segment_id": "e2ad9931-66aa-46b1-bdf9-df9b304e79eb",
    "episode_id": "fe6e8d44-6dae-4f56-aaab-9d789dbabf16",
    "episode_number": 114,
    "segment_number": 7,
    "text": "У меня очень напряженные недели, в последнее время мы большой-большой виз. Я занимаюсь телефонией, поэтому у нас все очень-очень интересно. В частности, следующая статья будет рассказывать о альтернативах Heroku. Она недостаточно подходит, как и многие такие пары срешения. Потому что в телефоне все очень-очень грустно. Все хотят реальные белые пишники. Очень часто просто так автоскейлить какую-то инфраструктуру не просто, как нам бы хотелось. Ну, наверное, Витя больше расскажет о статье на блоге Bitmatik. Да, на самом деле не я ее добавил, но я прочитал. Я думал, что тут будет Ваня Валеров, с которым мы ее обсудим, а не те еще кубернеты. Но ладно, может, обойдемся. В общем, суть такая, что человеку нравится Heroku, но Heroku даже для маленького масштаба получается довольно дорого. И он начал искать, чтобы можно было взять такое, чтобы было попроще, но и не так не било по карману, как Heroku. Вот, они вначале подумали про Chef, что, вообще, кажется, имеет очень мало отношений. Почему-то он написывает, что Chef получалось бы оверкилом. Ну и, соответственно, он говорит, что там не было многих моментов, которые должны были быть в такой системе. Они предоставляли бы автомасштабирование, которое поддерживали контейнеры, ну и были более простые в эксплуатации, чтобы даже разработчики могли довольно просто сконфигурировать их приложения. А не быть готовыми писать Chef рецепты для своих нужд. Из того, что они нашли более очевидного, был Kubernetes, но Kubernetes показался сам по себе более таким сложным. И они подумали, что было бы клево, если бы что-то было поверх него и с более простым API. Оказалось, что OpenShift работает поверх него, но OpenShift тот еще монстр, и для них это был оверкил. Дальше они попробовали Dock, но, как говорят, это работало только на одном хосте, и им не подошло. Ну и вот, и статья подводит к итогу, что их взгляд пал на проект, который называется Flynn, который в тот момент им очень понравился, и в принципе решил все их нужды. Возможно, они делают фидбэк какой-то, ссылаются на более старую версию Flynn, потому что, хоть и Flynn работает на Kubernetes, но они пишут, что он не поддерживал Persistence Volume, то есть они не могли запускать свой Elasticsearch, ну, соответственно, любое решение, которое нужно, чтобы хранило данные, и при перезагрузке ничего бы не теряло. Но потом я посмотрел на сайте Flynn, и вроде они заявляют, что они запускают по крайней мере MySQL, Postgres, запускают их даже в каком-то HA режиме, то получается, возможно, в последних версиях эту проблему решили. Но вообще, из вот этого двухминутного видео на их сайте, у меня задерживалось впечатление, что это для очень-очень простых проектов, когда ты рассчитываешь, что у вас один репозиторий, одно приложение, и вот вы им как-то управляете. Поэтому вполне может, наверное, подойти для каких-то таких простых проектов, для proof-of-concept, но не знаю, как этим получается управлять в масштабе. Сама статья, я бы не сказал, что сильно интересна, она вообще поднимает вопрос, что нужно ли городить пасс для Kubernetes, и есть для этого рынок, потому что существует уже много решений, непонятно, неуспешен ли Heroku из-за того, что просто нет рынка или из-за того, что он дорогой. В то же время есть Google App Engine, который я не слышал, что много людей его используют. Так что тут непонятно. Но зато очень много отзывов непосредственно от Kubernetes. Поэтому, может, получается, все дело в том, что гибкость в Kubernetes — это как раз то, что людям нужно, чтобы быть способными запускать свои приложения различного рода. Это, могу сказать, непосредственно работает для нас, то есть мы на голову к Kubernetes бежим. Не скажу за Google App Engine, а чуваков из Heroku я видел на одной конференции, у них дела нормально идут. То есть они для каких-то просто довольно своеобразных проектов. Не для телефониев. Да, на самом деле Heroku очень успешный. Он не захватил рынок так, что все идут на Heroku по умолчанию, но довольно много туда народ идет. То, что ребята показывают из серии, мы взяли Flynn, или там вместо Flynn есть Dais, или еще что-то. Или там ChromeVox, еще там какие-то. Их реально много, таких проектов, которые пытаются повторить Heroku Deploy на своем железе, или на своем контейнере. Heroku — это не только вот это Deploy и автоскейлинг. Там есть куча всяких приятных штук. Например, вы делаете pull request в своем приложении, и для pull request можно поднять то, что называется review version. То есть он просто на отдельном железе поднимает вам реакцию вашего приложения, так как она будет выдадить из этого pull request. Можно сразу потестить и всякое такое, погонять автотест, если хотите. Или там потрогать руками, и потом жить с ним. Я хотел сказать, что на самом деле удивляет, что люди пошли что-то там искать. В то же время, допустим, тот же Heroku купится на Амазоне. На Амазоне давно есть Elastic Beanstalk. В отличие от Google App Engine, Beanstalk почти никаких требований к приложению вообще не предоставляет. То есть из серии у вас приложение работает по HTTP. Допустим, это Docker Container, или там Docker Composer можно контейнер собрать. И оно вам его автоскейлит, и все, и там общается с базой данных, и всякое такое. Меня удивляет, что так мало людей смотрит в эту вещь. Конфиг — это из серии один раз настроил AWS CLI Tools, и после этого просто говоришь EC Deploy. Он деплоит текущую версию того чекаута, который у вас в проекте есть. Посмотрите в сторону Elastic Beanstalk. Я бы, наверное, еще единственное добавил, что Kubernetes в каком-то робе вроде движется тоже в этом направлении. Не в сторону Beanstalk, но в сторону именно вот этих более высокого уровня решений. И как раз Ваня, по-моему, в прошлом выпуске или позапрошлом выпуске упоминал про Helm и его поддержку. То есть там как раз люди описывают, допустим, рецепты для запуска MySQL или Postgres, например. То есть все эти компоненты Kubernetes, которые нужно для этого запустить. И фактически вы выполняете одну команду, и все это у вас получается. Ну, раз этого насъясняется... Я только хотела сказать, ты меня перебил. Да, если у нас есть какие-то дополнения, то не стесняйтесь говорить, слушатели, вы можете задавать свои вопросы. Мы переходим к следующей теме, это про закон Мура. Давайте я наброшу, а вы мне скажете, почему я не прав. Статья привлекла мое внимание про то, что некие учёные... Сейчас скажу, тут не уточняется, какие британские, не британские. Я, по крайней мере, так быстро не могу проглядеть. Из Берки. При помощи углеводородных или углеводных нанотрубок придумали, как сделать транзистор размером 1 нанометр. Напомню, что сейчас процессоры делаются по технологии 20 нанометров, а это получается в 20 раз меньше. И напомню, что есть такая штука, как закон Мура про количество транзисторов и их количество на единицу площади в зависимости от времени. И принято считать, что закон Мура уже не работает, что у нас не будет становиться больше транзисторов в ЦПУ в ближайшее время. Но оказывается, что работы в этом направлении идут. Новость на этой неделе про транзисторы размером 1 нанометр несколько лет получается. Около года назад была аналогичная новость про учёных, которые работают в IBM, что они, по-моему, не 1 нанометр сделали, но тоже сильно меньше, чем делается сейчас. То ли 3, то ли 5 нанометров. И поэтому ура-ура. У вас, кстати, интересный вопрос. Насколько я понимаю, частота процессоров тут ни при чём. То есть у нас не будет 10 гигагерцовых процессоров в обозримом будущем, у нас будут такие же процессоры, просто очень маленькие. Это так? Ну, наверное, количество ядер как-то можно масштабировать. То есть вроде говорили про частоту, что уже нет смысла дальше её увеличивать. А количество ядер, как можно больше транзисторов запихать на определённый объём, с этим, наверное, лучше может быть. А есть ли в этом большой смысл? Ну, ты в один сокет положил много ядер, а можешь в несколько сокетов. У тебя всё равно, когда их очень много, то приходят нумы и все дела. То есть ты начисляешь сокеты, то имеет смысл их экономить? Нет, ну тут вопрос же в том, что ты можешь параллелить вычисления, правильно? Или, например, увеличивать размер кэша, допустим, первого уровня. Кэш первого уровня, когда ты, например, в нём ищешь значение, ты читаешь значение параллельно. То есть вот какие-то такие вещи. То есть вот вещи, которые мы сейчас, допустим, когда мы думаем с вами о бинарном поясе, мы думаем последовательные операции. Можно же как? Можно себе большой кусок массива загрузить в кэш процессора и сразу найти нужный нам элемент. Потому что compare, операция сравнения не сработает сразу с ранее ячейками памяти. То есть вот какие-то такие идеи, которые можно делать для того, чтобы получать выбеж производительности именно не за счёт частоты, а за счёт того, как именно мы свои программы строим. Возможно, в каких-то местах нужно будет языки программирования менять, а в каких-то местах и не нужно. То есть здесь больше интересно, в данной конкретной новости, она на трубке и всякое такое. Вопрос в чём? Вопрос в том, что мы можем сделать процессоры маленькие или мы можем сделать процессоры большие. Но большие процессоры нужно охлаждать, и поэтому мы не можем запихнуть там хороший большой процессор, там телефон, допустим. Извините, вот интересно, вот слушатель сербукс, он пишет, что он пишет о энергоэффективности тепловыделения. И ты заговорил про тепловыделение. Я, конечно, не особо физик, но мне интересно, от того, что ты сделал транзистор маленьким, он что, меньше греется? Наоборот, он больше греет. То есть если ты делаешь, ну вот текущие, если брать кремниевые транзисторы, то есть чем меньше у тебя размер транзистора, тем больше у тебя как бы удельные теплопотери. То есть в целом монтируется меньше энергии, но больший процент этой энергии уходит в виде тепла. И поэтому, даже уменьшая, уменьшая, уменьшая те процессы, ты приходишь к тому, что в какой-то момент ты врезаешься вот в эту тепловую стену теплопотерей. Когда теплопотерей настолько много, что ты не можешь за этими потерями тепла делать какие-то вычисления достаточно быстрой скорости. И вопрос сейчас обычно не raw performance, а именно производительность на единицу энергии. Потому что у нас есть два вида вычислений. Первое это вот наши вычисления в телефонах, в каких-то носимых устройствах еще где-то. И тут важно именно энергопотребление для того, чтобы брейк и дольше жила устройство, становилось маленьким. Альтернатива это сервера. Тоже, допустим, если тебе нужна большая производительность, никто не мешает тебе просто вместо того, чтобы иметь один дата-центр, иметь десяток дата-центров. И то же самое с точки зрения дата-центра, самая главная величина, которую все учитывают и оптимизируют, это вот производительность на единицу энергии. Давайте в слухом остатке. Закон МОРы еще не до конца умер, то есть количество транзисторов оно может быть увеличено. И насчет энергопотребления вот то, что сказал Андрей. Так, ну, Света пишет, что отошла. Если вопросов нет, то я передаю слово Витю. Расскажи нам про развитие AI-стартапа. Попалась недавно довольно интересная статья про то, как можно сделать свой стартап в области artificial intelligence, искусственного интеллекта. На самом деле у меня нет ссылки на оригинальное видео, но есть фидбэк по этой статье и какая-то более краткая сводка. Презентовал человека, которого зовут Питер Бродский, он работает CEO в HyperScience. И насколько я понял, вот он как раз выстроил этот стартап в области AI, как компания, которая в консалтансе, что ли, то есть предоставляет эту компанию другим, решает проблемы других компаний, которые к ним обращаются. И он определяет несколько моментов, которые, он считает, основные, которые нужно учитывать в создании такой компании. Это 5P, на английском positioning, product, petabytes, process and people. И если кратко пройтись по ним, то есть в рамках позиционирования он упоминает, что уже есть другие довольно большие устоявшиеся игроки, типа Twitter, Google, Amazon, Facebook, с довольно большими ресурсами, которые могут позволить себе все таланты, которые обладают очень большим количеством данных. И вот за счет чего вообще вам, как маленькой начинающей компании, можно выехать. И он говорит, что один из путей, с которым вы можете отдалиться от них, это так как они в основном фокусируются на пользователях, на консумерах, то вы можете таргетировать энтерпрайза, то есть заниматься автоматизацией каких-то компаний других больших, помогать им с этим. И поднимает вопрос, что им предлагать. Короткие технологии, библиотеки продавать нет смысла, потому что может случиться так, что такой игрок как Google опенсорсит свой, допустим, TensorFlow, и все, и вы в пролете, все начинают его использовать, ваш продукт никому не нужен. А именно больше сделать уклон в сторону сервиса и решение непосредственных проблем заказчиков. Дальше поднимает вопрос про продукт, то есть что не всегда нужно 100% точности, то есть есть области, которые супер критичные, допустим, как автономное вождение машины, self-driving cars, то есть там цена ошибки довольно высока, но много приведенных областей, где это не так. Он говорит, что вы можете чувствовать себя довольно комфортно, просто работая в них, включая какую-то погрешность. Но говорит, что нужно иметь в виду цикл в вашем продукте, в котором вы улучшаете модель в течение времени, то есть из нескольких частей, где вы тренируете модель, деплоите ее, получаете какой-то фидбэк от пользователей, приходят новые пользователи, генерируют данные, дальше эти данные вы поглощаете и генерируете новые, более лучшие модели. И вот так вот работает этот цикл, то есть это то, что он говорит про продукт. Дальше третье P – это петабайты данных, и тут опять же тяжело конкурировать… Извини, а в начале точно были петабайты на третьем месте? По-моему, да. Окей, мне показалось, что там было что-то более приземистое, но хорошо, извини. Вот, это, по-моему, петабайты. Ну, то есть в любом случае он говорит, по-моему, на третьем месте про объем данных, и опять же у Амазона и у Google их много, то есть учитывая специфику их сервисов и число пользователей, которые их используют, маленьким компаниям нужно как-то решать вот этот вопрос. И несколько подходов – это один из них, возможно, получится использовать открытые источники данных, которые в опенсорстве, по-моему, тоже в выпуске как-то освещали этот вопрос. Можно кролить данные с других сайтов. Например, этот подход работал у нас с рецептами. То есть мы, когда занимались классификацией рецептов, то есть часть рецептов приходили с сайтов с какими-то лейблами. Их, конечно, все еще нужно было уничтитировать с другими сайтами, но у нас уже, получается, был тренинг-сет для алгоритмов. И фактически мы вот эти знания распространили на всю свою базу рецептов. То есть для нас непосредственно кроллинг данных с работой. И говорит про Data Capture Networks – это какие-то платформы, которые предоставляют анонимную статистику по пользователям. И он упоминает, что вроде даже они могут генерировать порядка 100 миллионов таких Data Points, информации о пользовательском поведении, которые вы тоже... Ну, вам не нужно знать, кто именно что-то сделал. Вам в целом нужно знать поведение. И фактически за какую-то ограниченную стоимость вы можете использовать вот этот подход. Дальше он упоминает, что называется Data Traps. И он в эту классификацию вводит слаг-ботов других ботов. То есть что-то вы выкладываете как можно раньше для пользователей. И как пользователи начинают с ними взаимодействовать, опять же, ваш объем данных начинает расти. И система становится более умная. И один из последних вариантов – это Human Labeling. Когда вы кого-то занимаете и, соответственно, занимаетесь с Supervised алгоритмами, и кто-то вам готовит данные. Это непосредственно вот сработало для нас. У нас получается доменная область гораздо проще, чем распознавание лиц и все такое. И даже ограниченное количество данных, то есть, скажем, 50 тысяч Data Points, могут дать хороший boost для начала. То есть вполне достаточно для тренировки моделей и дальше вот уже развивать, развивать, развивать. То есть у меня цикл был такой, что у нас вот… Я раньше упоминал, что команда была 10 человек, когда не было все автоматизировано. Сейчас команда из одного человека. И фактически на ранних этапах процесс состоял в том, что этот человек один день тратит на Labeling данных. Потом я тренирую модели, подготавливаю ему новый Data Set. С результатами от этой модели. Он по ним проходится, говорит, что верно, что неверно. Дальше я беру эти данные, опять тренирую новую модель на них. И вот так вот повторяется. И фактически boost был от каждого дня. Сейчас делаем это гораздо реже. Сейчас, наверное, раз в неделю это делаю, потому что вот просто уже масштаб порядка 200 тысяч. И мы не так быстро увеличиваем вот этот Data Set. Но метод как бы работает. Это касательно данных. Четвертая часть – это процесс. То есть, говоришь, 50% успеха – это вообще, может быть, не связано с технологиями. То есть, неважно, насколько умны ваши алгоритмы, насколько умны ваши ученые в компании. А тут вопрос социальной инженерии и вообще, насколько то, что вы строите, нужно конечному пользователю. И вообще, как вы готовы это довести до продакшн-системы. И он говорит, что как раз AI-инженеры, не Data Science, а AI-инженеры – это новая веяние в Enterprise, вообще в этой области, и довольно критично получить именно этих людей. Это частично сказалось на том, что я упоминал про ВИСК. Потому что человек, с которым мы работали, он был отлично вообще в построении нейронных сетей и знания дипленинга были хорошие, но для запуска это все в продакшн, в Москве, допустим, были, мягко говоря, недостаточно. И в маленьких компаниях с этим получается еще гораздо сложнее. Позвольте мне немножко дополнить. Мне кажется, не 50% не связано с технологиями, а, наверное, процентов 90. Потому что я приводил пример с одной компании, которая зарабатывает на продаже сладкой воды и зарабатывает на этом гигантские деньги. И при условии, что сладкая вода довольно бесполезная вещь, а искусственный интеллект и стартапы, которые на его базе решают реальные проблемы, это довольно полезная вещь. И 90% успеха зависит от того, насколько клевых продажников ты нанял и насколько они способны найти себе клиентов. Да, кстати, я вот тоже добавлю здесь, потому что здесь речь идет как раз-таки про B2B-компанию. И в B2B-компании, ну, вообще в предпринимательстве есть два направления. Вы делаете компанию либо там B2B, либо B2C. Есть, конечно, и разные пограничные случаи, но обычно вот два таких направления отдельных. Так вот, B2B считается более таким, не то чтобы легким, но более, скажем, безопасным путем, потому что в B2B можно найти себе, как ты сказал, заказчика, но обычно заказчик — это не самое правильное слово, это скорее ваш клиент, ваш кастомер, кто готов платить за вашу услугу. И вы, по факту, уже с него начинаете и дальше пытаетесь как-то расширить клиентскую базу. И для B2B самое главное — это продажники. Вы будете там, это очень такой часто долгий цикл продаж, вы будете в кучу митингов иметь встречаться, договариваться, передоговариваться. И по факту возникает тоже интересный момент, что вот вы... Это не сказать, что это вот свобода, то, что вы делаете какой-то продукт, вы на самом деле делаете какой-то сервис для другой компании. И, не знаю, это разница. То есть, вот, например, что делает Twitter — это в чистом виде для B2C. Ну, конечно, они потом уже повернули, у них уже есть сервисы для рекламы и для других компаний, но они стартовали как сервис чисто для людей. И да, вот они делали вот такой продукт в таком абстрактном вакууме для людей, чтобы люди пользовались, чтобы люди общались, обменивались информацией. Там, конечно, нету вот этих вот кучи людей-продажников. Вот я просто, да, по своему опыту скажу, что это занимает очень много времени. И вот сейчас фактически Никс коммуницирует. И то, что мне кажется, должно занимать довольно мало времени. То есть мы как-то быстро определяемся, мы подходим друг к другу с другой компанией или нет, они работают с нами или нет. Но тут ужасно, мне кажется, это настолько неэффективный процесс решения, больше, естественно, из-за сторонней компании, из-за их неповоротливости. Но да, фактически Ник сейчас этим в компании занят. И у нас еще одна женщина нам помогает, фактически даже она за бесплатно сейчас работает за опционы, иначе мы бы вообще не смогли платить. Но да, это такой довольно тяжелый получается процесс. Ну и мы тоже переходим в сторону больше B2B, но надеемся, что это повлияет и на продукт, в том, что улучшая коррект технологии, мы улучшаем качество данных для конечного пользователя. Просто вот предыдущий эксперимент, получается, провалился, что мы рассчитывали на привлечение инвестиций из-за большего энгейджмента, которого мы успешно добились с последним приложением. Но текущее состояние вещей показывает, что даже если мы пользовательскую базу увеличим в 2, в 3, в 10 раз, наверное, инвесторы в UK продолжат задавать тот же вопрос, откуда будет браться ревень. И учитывая то, что такого однозначного ответа на это нет, то есть есть догадки, то они разворачиваются, уходят. Но эта модель с B2B более всем ясна и понятно, откуда приходит все. Именно интерес к нашей компании от сторонних людей связан именно с этим направлением, с предоставлением API, с анализом данных и так далее. Частично, что упоминается еще в процессе, тут человек говорит, что некоторые подходят к этому так, что они говорят другим бизнесам, вы дайте нам все данные, а мы с ними сами разберемся и вам потом какой-нибудь результат выдадим, то есть что-то улучшить как-то так абстрактно. Он говорит, что во многих случаях, в подавляющей большинстве случаев это не работает, и вам непосредственно нужно с самого начала что-то предложить кастомеру, и с ним довольно близко работать, тем самым помогая в решении проблем.",
    "result": {
      "error": "API request failed: Error code: 400 - {'error': 'Trying to keep the first 5821 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}",
      "stack_trace": "Traceback (most recent call last):\n  File \"/home/andrei/Projects/podcast-shownotes/scripts/build_search_eval_dataset.py\", line 157, in generate_search_query\n    response = await self.client.chat.completions.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 1927, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1767, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1461, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1562, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': 'Trying to keep the first 5821 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}\n"
    }
  }
]