[
  {
    "segment_id": "2789b691-2c32-4113-9661-17d229b61740",
    "episode_id": "faf4c502-75b8-4450-8d53-45083af7fe74",
    "episode_number": 79,
    "segment_number": 2,
    "text": "Не, я слышал эту новость, но про бомбило, наверное, нет. А что там бомбило-то? Вы слушаете политсцен подкаст, да, о том, кто чего сказал в Сбербанке, и как интернет отреагировал. Не, просто это очевидная идея, логичная мысль, почему нет, я вообще не понимаю, что бомбило. Ну, там просто людей порвало на две части, одни, которые, а, сейчас нам, значит, agile-agile with production с Безудуба банк, и другие, которые, да вы что, надо же все-таки в 21 век перемещаться. Полярное мнение, да. У меня такой вопрос. Есть ли у вас DBA, или это мертвая профессия, и нет таких людей больше? Ну, почему же DBA? На базах, которые прям enterprise-enterprise, типа Орла, конечно же, есть очень мощные люди, и благодаря им у нас все тихо, мирно и спокойно при работе с базами. А вы запросы и хранимки сами пишете, или это занимается этим DBA? Мы пишем это все сами, и во время теста DBA может сам посмотреть, он даже может во время продакшна понять, что у тебя там что-то работает неправильно, и сам что-то подтюнить, индекс тебе создать. Ты даже об этом не узнаешь. А вот, между прочим, неплохо ли это, что ты об этом не узнаешь? Потому что, ну, как-то обычно довольно полезно знать, что вот там нам индекс подтюнили, и стало лучше, потому что, ну, потом, может быть, как-то об этом сам подумаешь. Конечно же, они обо всем сообщают. Я просто хотел, как-нибудь сказать, похвалить, что ли, о том, что у нас вот все так красиво с ними, обоюдно, что они сами знают, где можно так аккуратненько подтюнить, чтобы ничего не сломалось, и сами все это разбирают, и делают нашу жизнь проще, чтобы мы не отвлекались на саппорт, а делали новые продукты. Я думаю, нам нужно как-нибудь позвать крутого ароклового DBA, чтобы он нам рассказал, как у них там сделаны распределенные транзакции через RAC, и как они делают фейловер в двухнодном кластере. Я хотел узнать, раз уж пошли про то, как все построены процессы, а вот у вас Operations — это чисто Operations, или вы все-таки эти новомодные, эти все SRG и так далее используете? Есть у нас DevOps, которые занимаются инфраструктурой. Они занимаются диплоим, и автоматизацией примерно половины той инфраструктуры, которая относится к диплоию. Я вообще боялся про DevOps спрашивать, а у вас и DevOps есть, вообще классно. Да, очень удобно. Мы вместе с ними разрабатываем и диплоискрипты, и диплои-механизмы. Там мы как-то интегрируем DCI. А чем вы диплоите? Какие иммунштабы используете? Мы TeamCity. TeamCity у нас как основная CI используется. Она и pull-requests собирает, чтобы нельзя было смержить несобирающийся код, который не прошел тесты. Вот это поворот для банка. Говорю, мне кажется, вот это поворот для банка. CI? Нет, что у вас есть тула, которая не дает смержить pull-requests, которые не собираются. Потому что я даже в более модных и прогрессивных стартапах это не везде видел. Ну, благо есть Stash и вообще атласиновский стек, который нативно интегрируется в TeamCity тем же. И вот два клика можно на все это дело настроить. Мне кажется, это была тема для статьи на Хаббар сейчас. Ну, вообще, на самом деле, да. На самом деле, я скажу, что видел, не уверен, это или нет, но я вполне в себе в стартапе видел, люди берут IntelliJ IDEA, берут к ней плагин для TeamCity, и они, когда пишут код, когда они его коммитят, в TeamCity автоматически через этот плагин запускается сборка. И если она не проходит, то им обратно в плагин прилетает об этом Notify. И если она не проходит, то ты ее не можешь замерзнуть. Нет, ну просто, когда у тебя это есть в стартапе, это воспринимается как типа норма. Когда у тебя этого нет в стартапе, ну, ребят, вы что-то не так делаете. Я не воспринимаю это как норму, потому что я на самом деле за то, чтобы запускать, ну, что тесты должны проходить локально. Я должен лететь в самолете, я должен запустить regression-тесты вот где угодно, а не так, что через на каком-то сервере. На самом деле, с моей точки зрения, нужно оба, потому что у нас такое бывает, что у нас довольно разношерстная, скажем, локальная среда у людей бывает. Кто-то на Маке, кто-то на Linux, кто-то на Винде. Ну, в целом, Винды у нас, наверное, нет, у беконтрразработчиков, но, тем не менее, на Маке и на Linux точно бывает разница. Вот, и у человека, например, он написал, у него локальные тесты пробегают, у коллеги тоже локальные тесты пробегают, потому что у обоих, например, Мак, а у третьего коллеги и на CI они не пробегают, потому что там Linux. Нет, ты по-любому прогонишь тесты на CI, перед тем как это катить, согласен? Согласен, но мне кажется гораздо интереснее, когда у тебя приходит бот и под pull request там либо ставит звездочку, либо не ставит. Я, впрочем, не стану спорить, что да, нужно и так, и так. А еще лучше, когда есть ферма и прогоняется сразу по-разному. Так, вопросы гостю? Лес рук. У меня есть вопрос. Рома, ты говорил, что у вас есть множество наработок, касающихся кэшей, различных лип и вот этого всего. Расскажи, скажем, чем вам не понравились существующие решения в плане тех же кэшей, и вот что вы такое интересное сделали? Ну, про кэши это отдельная большая тема. Думаю, можно про нее рассказать. Есть такой паттерн проектирования, называется Dynamic Proxy Cache. Это когда вы в коде описываете кучу фасадов. В каждом фасаде у вас есть какие-то методы, которые вы хотите кэшировать. Вы в этом кэше дефините эти методы, и вот эта реализация позволяет еще очень по-хитрому инвалидировать эти кэши. То есть вы делаете мапу, в которой у вас описан кэшируемый метод, и сигнатура другого метода с маской ключей, вызов которого по этим же ключам, по совпадающему инвалидируют первый метод. Вот, под это все дело. У нас сначала была живая реализация, чтобы было все быстро и красиво. Потом мы переписали все это дело на скалы, внедрили макросы, и получилось что-то вроде небольшого DSL. Небольшой DSL, который в compile time проверит, что это все дело нормально скомпилировалось и будет работать. Потому что там Java реализация, это все работает гораздо сложнее. Там куча рефлекшенов. Но макросы нас спасли, у нас стало меньше проблем с этим всем делом. Я надеюсь, немножко осветил тему. А как вы вообще с макросами? Нормально у вас проблем не возникало? С точки зрения, вы же наверняка в идее разрабатываете, и она там не подчеркивает все красного. Благо, с этим стало чуть получше в последней идее. До этого, конечно, было сложно, но это того стоило. По сравнению с ситуациями, когда ты готовишь релиз, все задачи уже сморжены, необходимые тестеры начинают тестировать, и оказывается, что ты где-то не инвалидируешь кэш, допустим, при платеже. Тебе возвращают задачу, тратят время на то, чтобы это все пофиксить, найти причину и так далее. Не дай бог, если релиз переносится на день, ты выбираешь свои графики, и начинает все ехать некрасиво, неприятно. Но немножко помучались, и стало хорошо. Так, ну смотри, Ром, если ты хочешь еще что-нибудь нам поведать, то поведай, или мы перейдем к новостям. Ну, пожалуй, наверное, все. Ну хорошо, я смотрю, ты других тем не добавлял, но присоединяйся к нашему обсуждению. Вань, вытягивай тему, а то ты чего-то молчишь. Да, да. Ну, тут как раз гость говорил, что надо разное окружение, тестирование и так далее. Вот мы все же знаем, кто решает подобную проблему. У нас обновился докер для версии 1.10. Кто это? Докер, докер, докер, докер, докер, докер, докер, докер, докер, докер, докер. Норма докера выполнила. Кто о чем, а Ваня все там про докер, про докер. Значит, докер классная штука. Они выпустили версию, и меня бомбит. Это подделка. Подделка для записи LXE. Подделка? Не правда, Ваня. Подделка постепенно становится production-ready, надо сказать. Собственно, новость как раз об этом. Они постепенно улучшают security, они постепенно улучшают юзабилити всего того говна, которое нам вокруг навернули, и оно перестает быть говном. Отлично, теперь все два слушателя, которые делают паа, смогут радоваться. Так нет, ничего, в том-то и дело, что вот, короче, смотри. Вот то, что вокруг него в итоге навернули, то есть тот же самый компост, он приходит в то состояние, когда тебе паасом становится, собственно говоря, компост. То есть ты накатываешь некоторое множество машин... Компост как компостная куча, да? Не компост, а компоуз. Вот не надо грязи, пожалуйста. Не надо компоста. Ну то есть, как бы, компост, компоуз, был компост, например, полгода назад он еще был вполне себе компостом. И вот как раз тема о том, что... Слеженькое. Перестает. Версия больше так не пахнет. То есть ты накатываешь себе некоторое количество машин с токер-демоном, а дальше компоуз, он... То есть сейчас, если выбирать между текущей версией с ворм, в связке с компоуз и кубернетами, я бы уже задумался, потому что кубернеты до сих пор нормально не позволяют делать, например, развернуть тебе, например, кассандру, то же самое, которое мы уже обсуждали сегодня. Потому что в кубернетах нет понятия, что у тебя диск, он должен быть локальный, но не отваливаться после смерти контейнера. Вот там до сих пор этого не запилили. До сих пор это как-то ищут. Я вот, Валер, тебя слушаю и вспоминаю эту статью, а почему бы мне не захотелось рубить приложение в хероку, да? Да нет, просто возьми компоуз и кубернетас, и докер, потому что сейчас это все очень нужно. Я имею на это ответ. Смотри, если тебе хероку достаточно, тогда хостится на хероку. Больше того, ну, как бы я лично сам так и делаю. Но если по какой-то причине у тебя есть свое железо, и ты делаешь как бы селф-хостинг, например, потому что ты перерос хероку, и у тебя цены на хероку уже неадекватно дорогие. Ты, помнишь, сам примерно месяца, наверное, 4 назад заратывал за то, что а нафиг нам дайты док, мы сами у себя графит захостим. Ну вот из тех же соображений ты понимаешь, что на твоей нагрузке хероку тебе уже просто слишком дорого обходится. У тебя появляется необходимость строить свой хероку. И вот в какой-то момент тебе становится это удобно. Ты же, наверное, не будешь с ним спорить. Я точно не возьму для этого докер с собой неизменяемой файловой системой и вот с всей этой... А вот как бы... С всем этим компостом. Много чего проспал, потому что вот ты докер не любишь, но тем временем там постепенно улучшение. То есть, ну, неизменяемая файловая система, но там всегда были волюмы, но раньше они не очень хорошо работали. Сейчас они существенно лучше работают. Неизменяемая файловая система стала существенно лучше работать. Ну то есть, как бы к докеру раньше была поделка, на которой можно было тесты запускать. Он постепенно превращается, вот как бы... Я его ругал еще, ну, меньше месяца назад, наверное. Он постепенно превращается, доходит реально до состояния, когда им реально начинает можно быть пользоваться в продакшене. Я напоминаю, что тема Ванины. Валер, замечательно рассказываешь. На самом деле мне не очень понятно, почему ты любишь докер почему ты так не любишь докер, при этом так готов пользоваться LXC. То есть, у докера реально много удобств, которые сейчас он... Я поправлю. Почти продакшен. Я не готов пользоваться LXC. Я им пользуюсь каждый день, и у меня контейнеров десятки. В продакшене или для теста? У меня нет продакшена. Он у меня для разработки, но как бы в основном локально. А если для разработки, чем тебя докер не устроил? Ну то есть, ты мне предлагаешь постгресс запускать на неизменяемой файловой системе в докере? Ну, в принципе, я соглашусь, потому что конкретно в случае с LXC... Я на самом деле LXC пользовался еще до появления докера, как такового. Там самая боль была настроить сеть. Но на самом деле в докере она долгое время была такая же боль. Справедливости ради. И сейчас докер, в текущий момент времени, он удобнее. Ладно, Вань, чего там получилось-то? Случилось и добавилось? Добавилось кучу всего. Вышла версия докера, докер-энджайн, как у них называют. Это swarm, докер-машин, registry. Из самого полезного, что мне кажется полезным, это то, что у них айдишники образов теперь стали, как это сказать, зависеть от содержания. Не-не-не, это не самое полезное. Самое полезное, это все-таки починили то, что вот юиды. Это было просто больно. Представляешь, у тебя нулевой юид, в контейнере нулевой юид, это рут, а снаружи нулевой юид, это рут. И приехали. И, насколько я понимаю, это починили наконец. Подожди, что такое, как у юид может быть нулевой? А, как у рута, в смысле, я понял. Ага. Но эта бага тебе полезна, когда кто-то пытается снаружи тебя сломать, ты имеешь в виду такая шняга. Ну в смысле, блин, у тебя не бывает систем, про которые можно считать, что их снаружи не пытаются сломать. Бывает, конечно, вот у Саши, например. Нет, ну да, девиаторские системы. Ну тогда зачем тебе вообще, казалось бы, докер в продакшен тащить? Мы же все-таки говорим о том, что докер почти становится готов к продакшену. Да, да, да, конечно, конечно. Иначе зачем тебе сформ? Почти. Почти готов или почти становится? Почти готов. Но в целом, вот мне больше всего радует то, что они наконец допилили. То есть для меня декорегистририе с имиджами, которые просто случайным образом генерятся айдишники из образа, ну это вообще фактически ноль использования той возможности, которая заложена в самой идее. То есть сейчас айдишник привязан к тому содержанию, которое у тебя хранится в этом образе. И фактически, если он смотрит, что у тебя образ с подобным содержанием уже есть, он не будет его качать. Он будет лучше использовать место. Ну короче, в целом становится такая полезная хорошая штука. То есть они значительно ускорили скорость загрузки образов на регистре с помощью этой штуки. Конечно, надо там осторожно делать апгрейд, потому что докер по умолчанию пытается переиндексировать все образы и присвоить им новый секьюрный айдишник. И это может быть опасно. У них есть специальная толза на этот случай. Если вы хотите запустить в бэкграунде сперва толзу, чтобы она переиндексировала все, а потом обновить версию докера. На мой взгляд, самое полезное это уже вышеупомянутые секьюртификсы, потому что это просто был стык позор. Ну то есть мало того, что оно работает одним толстым дервном под рутом, что уже смог себе стык позор, так оно не разделено на подпроцессы и суидные бинарники. Так оно еще и, извините, такую дыру имело, ширину я не знаю, со что. Вторая вещь это то, что они еще в прошлой версии сделали нормальную сеть, а в этой версии нормальная сеть стала возможной использовать из композа и сформа. То есть теперь этим можно пользоваться, а не просто оно есть. Не знаю, я как-то с формом вообще не пользуюсь, мимо меня это пришло. Я с формом вообще не пользуюсь, просто мне недавно было интересно посмотреть на кубернаты. В рамках того, что у нас не секрет, что я уже озвучил, что у нас селфхост приложение. И мы задавались тем вопросом, что у нас на самом деле кластер процентов на 60 простаивает, если у нас нет пик часов. Возможно, мы могли бы как-то свое железо поужать и сэкономить на хостинге дополнительно. Ну или просто как бы менее бездарно расходовать ресурсы. И я начал смотреть на кубернатс, и я понял то, что я выше по тексту озвучил, что кубернатс не способен хостить ничего, что стейтфул. То есть сейчас последние два релиза, они его довели до состояния, когда он может хостить софт стейт.",
    "result": {
      "error": "API request failed: Error code: 400 - {'error': 'Trying to keep the first 4577 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}",
      "stack_trace": "Traceback (most recent call last):\n  File \"/home/andrei/Projects/podcast-shownotes/scripts/build_search_eval_dataset.py\", line 157, in generate_search_query\n    response = await self.client.chat.completions.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 1927, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1767, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1461, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1562, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': 'Trying to keep the first 4577 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}\n"
    }
  }
]