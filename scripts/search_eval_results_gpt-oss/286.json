[
  {
    "segment_id": "cf730a1e-07dc-46ce-81e2-ec7255cc698f",
    "episode_id": "e0d21f08-bd15-4ad4-a832-4afdc3f22a1b",
    "episode_number": 286,
    "segment_number": 3,
    "text": "То есть, вы должны раскидывать сервинг задачи туда, где у вас есть свободное место, и бетч задачи, если что, выкидывать оттуда. Этим всем занимается внутри себя Борг, но идея в том, что когда вы запускаете задачу, вы же должны ей переписать какое-то количество ресурсов или определить ей какое-то количество ресурсов в памяти и ЦПУ. И вот это определение размера вашей задачи, оно очень важно с точки зрения планировщика. То есть, планировщик посмотрит, что вам требуется 10 ЦПУ, и он не будет вас закидывать в машину, где мало ЦПУ. С другой стороны, когда он вас закинул на машину, где 10 ЦПУ, он вам дает эти 10 ЦПУ, и если там всего 16, то он считает, что осталось 6. То есть, такая вот простенькая логика, но работает на самом деле она очень плохо, потому что в общем случае вы не знаете заранее, сколько будет пользовать ЦПУ ваша задача. Чисто теоретически вы можете примерно почитать, но, скажем, какой-нибудь Erlang-приложение, там Java, они любят выедать все, что им доступно, то есть они смотрят, у нас там есть несколько ядер, давайте их загрузим в работу, если работа есть, то мы будем ее выполнять быстрее. Пошло больше, дали больше нагрузку, используем больше ЦПУ, все в порядке. Пошла меньше нагрузки, используем меньше ЦПУ. И получается такая хитрая ситуация, что вот вы, например, раскидали куда-то задачу на 10 ЦПУ, а она там крутится тихонечко, использует 2 ЦПУ. И вроде как все хорошо, и если что, она стрельнет до 10 ЦПУ, но проблема в том, что вот эти оставшиеся 8 ЦПУ, они же никем больше не будут никогда расходоваться. И получается, что при полезной нагрузке в 2 условные единицы, вы оплачиваете 10 условных единиц, то есть 8 условных единиц вы не используете. И ладно, если ЦПУ, то есть ЦПУ – это немножко неправильный пример, потому что мы используем меньше ЦПУ, мы меньше энергии съедаем и вроде как дешевле платим за кластера, а представьте с памятью. То есть вы сказали, что я буду использовать 10 ГБ памяти, а использовать 1 ГБ. И хотя мы ничего больше на эту машину положить там не можем, в эти 9 ГБ, считается, что она используется. И вот это все, для того, чтобы это все правильно делать, это получается у нас такой трейд-офф между тем, как правильно использовать ресурсы с одной стороны, а с другой стороны, чтобы не перезажимать слишком сильно эти лимиты. Потому что если вы зажмете лимиты слишком много, то задача, которая вы зажали ЦПУ, она будет троттлиться и, соответственно, медленнее выполняться. Если вы зажали память, она вообще может быть убита, там, out of memory killed. И получается, что, ну, как бы вот этот трейд-офф, сколько денег вы готовы потратить и сколько, при этом, насколько быстро и хорошо вы хотите это все сделать, это все сложно определить. Вот. И Google решил сделать специальную систему, которая будет оптимизировать это. Назвали его они Автопилот. То есть, как работает, ну, например, как вот работает большинство компаний, в частности, как я работаю с ресурсами Кубернетика. Я смотрю, сколько у меня задача в исторической перспективе занимает. Я примерно умножаю на 2 плюс-минус количество памяти, которое она использует в пике, вот. Но не на 2, там на полтора. Вот. И устанавливаю вот это количество памяти для того, чтобы моему контейнеру выдавалось, был лимит именно в это количество памяти. И так чаще всего делает большинство разработчиков. Таким образом, получается, что количество неиспользуемой памяти на машине достаточно велико, как минимум 50% в пике, а во время не пика, как бы даже увеличивается. Вы же не будете приходить ночью и переставлять этот лимит. Вот. И никто не будет это делать. Поэтому получается, что чаще всего разработчики не идеально используют свои системы. Это не только касается Кубернетов, это касается вообще чего угодно, любого планировщика ресурсов. Вот. И Кубернет с Google решил это сделать автоматизированным способом, с помощью системы, которая будет автоматически менять лимиты даже, я не знаю, как-то на лету, без влияния человека. То есть это будет полностью автоматическая система. Как показала практика, у них есть такой параметр, как слэк. Это зазор между тем, сколько поставлен лимит, и между тем, сколько использует система на самом деле. В ручной установленных лимитах зазор обычно составляет 46% от текущей надрузки. В Автопилоте у них 23%. То есть они фактически в два раза уменьшили зазор, а на самом деле цифра выражается в громадных величинах, если посмотреть, но на размеры Google и на размеры их дата кластеров. Сейчас я пойду вкратце по бумаге, она у меня распечатана, лежит, и зелёным фломастером отмечена. Что здесь интересно было? Да, это я всё рассказал. Здесь даётся краткое описание Борга. То есть мы Борг уже рассматривали, я не знаю, сколько раз. Борг – это система, которая крутится в Гугле, и которая, соответственно, является планировщиком, запускальщиком, и отслеживальщиком всех задач, которые работают в Борге. У них есть очень сложная, ну, с внешней точки зрения, иерархия. Олег, ты что-то хочешь добавить? Тебя чуть слышно стало. Сорян. У них есть, ну как, она не сложная, но у них есть определённые, что такое задача, что такое таск, что такое джоба, как их правильно запускать, как правильно определять лимиты. То есть это, как это, внутренний сленг, внутренняя архитектура самого Борга, которая влияет на то, как надо правильно разбивать на задачи. В частности, они предполагают, что все задачи, они ограничены по размеру. То есть вы должны даже большую задачу размять на кучу маленьких для того, чтобы их можно было запускать, независимо друг от друга, чтобы они друг другу могли результаты пересылать, для того, чтобы падение любой задачи не приводило к тому, что у вас все результаты потерялись и так далее. И в частности, по памяти и по ЦПУ, по ресурсам, они предполагают, что вы тоже будете разбивать её на какие-то примерно более-менее равные чанки, чтобы не было супер микро маленькой задачи и супер большой какой-то задачи. То есть всё будет примерно более-менее ровно для того, чтобы легче было управляться ресурсами. Соответственно, они рассказывают, как работает Борг, что там есть Борг Леты, что там есть Мастер. Я всё это не буду пересказывать, мы это смотрели, уже рассматривали в Каисе. Интересная тема, которая мне здесь понравилась, это то, что если у вас задача перекидывается на другую машину из-за того, что есть какая-то задача, которая выполняется, в этот момент приходит на эту машину задача с более высоким приоритетом, у них есть система приоритетов. Если у задачи более высокий приоритет, она, конечно, будет выполняться, и все задачи с более мелким приоритетом будут вышибаться с машины, если недостаточно ресурсов. Эта задача с более мелким приоритетом, соответственно, кидается куда-то, где она сможет выполниться, и на той машине тоже может не быть ресурсов, она вышибется в первую очередь в систему с еще меньшим приоритетом. Как оказалось, несколько лет назад у них была большая проблема, какая-то просадка производительности из-за того, что эти задачи из-за большого количества приоритетов друг друга очень сильно вышибали в большом количестве, им пришлось значительно сокращать количество приоритетов в их системе для того, чтобы эти вышибания не влияли на производительность. Я бы очень хотел почитать пейпер на эту тему, если кто видит или знает про этот пейпер, мне, пожалуйста, напишите, потому что здесь у них нет ссылки на пейпер. Кстати, в конце у них ссылка на большое количество связанных работ, и я прям даже заинтересовался парочкой, я обязательно их почитаю, еще, может быть, здесь тоже доложу. Значит, автоматические лимиты с помощью Автопилота делаются следующим образом. Во-первых, мы записываем всю информацию о предыдущих использованиях системы, о предыдущем использовании ресурсов за все время, использование ЦПУ и использование памяти. Они работают с этими двумя ресурсами только. Затем, помимо просто тайм-серии, они подготавливают более упрощенный набор 5-минутных значений, то есть для памяти они записывают просто максимальное значение за эти 5 минут на машине, а для данного ТАСКа, для ЦПУ, они записывают, у них там есть набор бакетов, который весь диапазон до 100% ЦПУ разбит на какие-то бакеты, по-моему, на полпроцента ЦПУ они бакеты имеют, то есть, соответственно, около 200 бакетов. И, соответственно, в каждые 5 минут максимальное значение в каждом бакете они туда записывают, единичку записывают, и получается в итоге какой-то счетчик, сколько там где было. Получается такая интересная система, что они с помощью вот этих упрощенных значений об использовании ресурсов могут дальше с ним делать какие-то математические выкладки, в частности, они приводят несколько разных способов, как подсчитывать, какие нужны лимиты на одной машине для одного ТАСКа. О, здесь я хочу заметить, что Автопилот работает в двух направлениях, он не только делает горизонтальное скалирование, о котором я немножко уже рассказывал, то есть мы увеличиваем количество инстансов данного типа для того, чтобы обрабатывать ТАСКи данного типа, ну как машин во Амазоне инстансов, и так далее, но он еще делает вертикальное скалирование. Он смотрит из-за того, что ТАСКи примерно одной задачи, примерно одинаково используют решурсы, он для ТАСКов одной задачи подбирает лимиты, в точке зрения вертикального масштабирования, то есть он может увеличить немножко количество памяти для того, чтобы реже вышибало по памяти. Или может увеличить количество CPU, чтобы меньше было троттлинга. И, соответственно, первая часть задачи – это вертикальное скалирование для каждого таска. Он увеличивает количество, устанавливает количество лимитов для каждого таска независимо. Соответственно, это делается с помощью… Так, так, так, так, так. Вот, он, значит, соответственно, рассматривает вот эту предыдущую статистику по использованию ресурсов, вот, и потом делает рекомендатор, запускает рекомендатор для каждой… для каждого типа задачи. При этом, что интересно, я вот этого, если честно, не очень понимаю, они запускают три автопилота, три инстанца автопилота в каждом кластере, и выбирают среди них мастера, то есть делают что-то вроде системы, я не знаю, ETCD, которые автоматически только мастер имеет право на запись, и, соответственно, сперва при установлении их работы происходит выбор мастера, и при поломке одной машины происходит перевыбор мастера на другую машину. Вот, не знаю, я не очень понимаю, почему это нужно, можно в принципе просто запускать по надобности одну машину, и чтобы она разбирала все завалы, но, не знаю, видимо, им так виднее, я бы хотел больше данных здесь, почему они такую архитектуру выбрали, они это не объясняют. Так, подожди, ты ускакал с того, как оно масштабирует самого интересного на перевыбор мастера. Да, я искал эту тему на тему, потому что 16 страниц очень тяжело, я что вспоминаю, то и рассказываю, давай вопрос. Ну, давай ты все-таки подробнее задержишься на скалировании, потому что ты как-то совсем быстро проскочил, я так понимаю, это самая важная тема, которая там есть, вот с этой рекомендательной системой, и вот как-то подробнее можно… Да, давай я сейчас расскажу. Я вернулся назад на архитектуру, потому что получается, что автопилот имеет внутри себя несколько инстанциев автопилота, и за каждым инстанцием закреплена каждая задача, то есть когда таск запускается в кластере на какой-нибудь машине, соответственно, Борг ее туда закидывает, Борг-лет, это аналог кублетов в терминах кубернетиса, он смотрит, что ему пришла задача, и он начинает ее запускать на локальной машине, и, соответственно, в момент запуска автопилот, который, соответственно, для этого таска был ассоциирован, он обрабатывает предыдущую информацию за 2 недели или сколько, и приходит к выводу, какие лимиты для этого таска здесь сейчас надо поставить. Соответственно, сперва он выбирает вертикальное масштабирование, то есть он выбирает лимит для одного таска, а потом он выбирает, сколько задач для данного, сколько контейнеров, давайте в таких терминах, виртуальных машин для данного таска нужно запустить в параллель для того, чтобы обработать все пришедшие задачи вот этого типа. Соответственно, вертикальное, он смотрит предыдущую статистику и выбирает, он может быть настроен, вы можете сами любой таск настроить, каким образом он будет вертикально масштабироваться. Вы можете выбрать максимальный пик за последнее время, ну там, я не знаю, скажем, за 2 недели вы можете выбрать максимальное значение используемой памяти, и на это значение он будет ориентироваться. Второе – это взвешенное среднее, по-русски, наверное, это будет так, когда он чем дальше по времени, тем меньше влияет пик использования ресурса на текущее значение ресурса. И третье – это percentile взвешенного использования, то есть это вы ищете максимальное, ну точно так же строите взвешенное значение, а потом берете какой-то percentile этого значения. И, соответственно, вот это вот рекомендованное значение – это рекомендация. И потом вы можете, насколько я понимаю, здесь вы можете написать функцию, которая покажет, как работать с этой рекомендацией. Я немножечко здесь запутался, как они это делают. Сейчас, подождите, посмотрю. Нет, нет, он не позволяет функцию, функция-то в другом месте. То есть фактически вы говорите, что надо использовать либо пиковое значение, либо взвешенное, либо percentile, и вы, соответственно, устанавливаете какие-то параметры. Ну, например, взвешенное – это там есть такой период, до какого надо смотреть, я не знаю, и период угасания. Нет, параметры угасания, как это называется по-русски, я не знаю. То есть последний, мы, скажем, используем, вес у него 40, предыдущий у него будет вес 20, потом 10, потом и так далее. И, соответственно, вот такие параметры вы настраиваете, и пик, рассматривать пиковое значение за последние две недели или за последнюю неделю. И он потом сам автоматически его устанавливает. В принципе, звучит интересно. У них здесь есть несколько рекомендаций на тему того, как правильно работать с памятью, какие показатели у них лучше всего сработали, как использовать CPU. Но я не хочу погружаться, лучше пойдем дальше. Потом они для горизонтального автоскалирования используют… Сейчас, секунду. А, да-да-да. Самое интересное, что… Да-да-да, вторая часть. Вторая часть вертикального автоскалирования – они применяют Machine Learning, когда они… Machine Learning ведет… У них есть вот этот специальный модуль, который вычисляет все вот эти же три возможных алгоритма для каждого таска, и по каждому из них он ведет какой-то… Здесь они не написали, каким образом, но он ведет некоторый ряд значений параметров. То есть, ну, к примеру, вот если для пикового мы возьмем два параметра, это за неделю или за две недели, для персентайла мы возьмем 10 параметров с угасанием за 2 дня, за 5 дней, за 10 дней, за 20 дней. Для персентиля мы возьмем те же самые значения угасания, но еще и возьмем разные персентиля – 95, 98, 90, к примеру. И вот весь этот набор разных возможных параметров и возможных алгоритмов расчета лимита для вертикального масштабирования Machine Learning у них каким-то образом считает. Причем, я так понимаю, у него есть какая-то вариативность, он может менять эти параметры в каких-то пределах. И у него в итоге получается на выходе не как пользователь устанавливает одно значение, а у него есть набор этих значений, там, 20, 50, я не знаю, сколько штук здесь не сказано. Среди этих 50 он постоянно смотрит, какой лучший алгоритм с какими параметрами лучше работает для данного конкретного алгоритма. И со временем он может переключаться с одного на другое, если у него статистика показывает, что таск поменял свое поведение и лучше переключиться на другой тип. Я правильно понимаю, что они Machine Learning не конкретную чиселку, а то, как выбирать чиселку? Слушай, здесь они говорят, что они имеют три алгоритма, и Machine Learning использует эти три алгоритма, но он Machine Learning именно чиселку. Чиселку параметра для конкретного алгоритма. Понимаешь, о чем я? Вообще запутался. У нас есть три алгоритма, пиковая, взвешенная и персентиль. Но у каждого алгоритма есть несколько параметров. И они с помощью своего Machine Learning Machine Learning эти параметры для этого алгоритма. Соответственно, у них есть три типа алгоритмов, и для каждого из них десяток параметров. И вот они их Machine Learning для того, чтобы посчитать оптимальное значение для данного таска. А какой алгоритм выбрать? Тоже Machine Learning? Я так понимаю, что они Machine Learning в каждом для каждого алгоритма, а потом, соответственно, смотрят, какой лучше бы сработал, и на него переключаются. Окей. Я ответил на твои вопросы? Наверное, да, но мне кажется, это как-то слишком заумно, но окей, им виднее. Да мне тяжело объяснять, наверное, из-за этого. Тут оно на самом деле очень интересно. Они в конце сказали, что на самом деле у нас была очень сложная задача, мы должны были объяснить наши рекомендации конечным пользователям. И поэтому мы ведем всю статистику по истории, мы показываем, каким образом мы считали какие алгоритмы, ну в смысле, каким образом мы подбирали параметры для каждого алгоритма, какие параметры использовались по истории, и потом показываем, почему мы переключились именно сюда, потому что здесь было лучше значение. Насколько я понимаю, это вопрос доверия данной системе, которая фактически управляет вашим продакшеном, и там в конце будет секция, посвященная тому, как происходила интеграция этого автопилота. Но это довольно веселая задача, то есть как бы в самой постановке задачи по автопилоту была формулировка, что они должны объяснять все решения автопилота. Это весело. Ну вообще-то это обычное требование к экспертной системе, за исключением тех, которые строятся на нейронных сетях, и это большой минус нейронных сетей, они не могут объяснить, почему они считают, как считают. Да, да. Поехали дальше. А, да, здесь они еще считают очень интересные показатели. Смотрите, когда у нас происходит вот этот trade-off, мы фактически имеем два показателя. Первый показатель – это… давайте так, у нас стоит лимит установлен в 10 гигабайт, а наш алгоритм использовал там 8 гигабайт. Значит, что 2 гигабайта места, они фактически не использовались, и мы просто платили за воздух. То есть теоретически мы могли какую-то еще задачу на 2 гигабайта сюда впихнуть, которая была бы на фоне, не знаю, почти не использовала ЦПУ, но ей надо было 2 гигабайта делать, и соответственно мы бы эти деньги могли бы как-то использовать, но мы их потеряли. С другой стороны, если бы мы поставили 8 гигабайт, а в какой-то момент времени пришел больший скачок, ну или скажем, поставили 7 гигабайт, и мы увеличили до 8, у нас в этот момент наша задача вышла бы по out of memory. Ну на самом деле они тут объясняют, что у них не всегда есть эти hard лимиты, у них есть иногда soft лимиты, но я не буду погружаться в дит, потому что это все заканчивается рано или поздно out of memory. И out of memory, он плох тем, что мы в этот момент, соответственно, что-то потеряли. Мы потеряли либо какие-то миллисекунды на перезапуск конкретной задачи, и, соответственно, пользователь увидел больше latency, чем он хотел, либо какая-то задача, которая была нам важна, или бетч, или еще что-то угодно, она позже выполнилась, и мы должны каким-то образом сравнивать вот эти две величины, и они это на самом деле делают. То есть они на самом деле сравнивают вот эти два показателя по каким-то коэффициентам. Я, если честно, не очень понял математику, я не вчитывался, здесь у них большие формулы, и мне непонятно, как они высчитывают вот этот overrun. Overrun – это как раз косты приложения из-за того, что оно остановилось, хотя могло бы выполниться дальше из-за того, что у нас лимиты занижены. Мне непонятно, как можно вообще посчитать вот этот вот… Это называется cost of opportunity. Мне непонятно, как с точки зрения бизнеса можно считать эту штуку. Может, Света могла бы что-нибудь добавить. Но они это делают как-то математически, и как результат работы алгоритма вот этого вертикального масштабирования, они показывают, что, ребята, если бы мы занизили столько-то, то у нас мы бы выиграли по цене миллион долларов за счет того, что мы больше бы задач выполнили на том же самом кластере, но с другой стороны мы потеряли бы на 2 миллиона долларов больше, потому что у нас задачи там часто бы перезапускались. Каким-то образом они это считают, мне непонятно как. Но и у них система как раз, у них оценочная система, которая стоит поверх работы этой логики вертикального масштабирования с ML. Это же только ML-версия, потому что все, что вручную человек сам устанавливает, его не контролирует никто. Вот эта система как раз снаружи мониторит и даёт фидбэк обратный, что типа ты слишком агрессивен, уважаемый автопилот, давай-ка будь менее агрессивным. Вот такая вот интересная система с вертикальным масштабированием. Пошли дальше на горизонтальное или есть вопросы? Я ответил с out of memory. Света, скажи мне, пожалуйста, как бы ты построила систему, которая бы считала бы косты of the lost opportunity на основе out of memory каких-нибудь выбросов задачи? Блин, это прекрасный вопрос на собеседовании, идеальный для менеджера. Кстати, спасибо за подсказку, я что-то даже об этом не подумал на самом деле. Буду задавать его. Ладно, пошли тогда дальше. Следующее – это горизонтальное масштабирование. Вообще, ответить на твой вопрос нужно или не нужно? Отвечай, если можешь, отвечай, потому что мне пока не понятно, как они это могут сделать. Кстати, Света, а когда у тебя испытательный срок заканчивается? Ты это кому задавал? Это нехорошая шутка, неважно. Продолжай. Все-все, я понял. Давай, Света, я хоть немножко отдохну пока. Вообще, я тоже не знаю, как правильно ответить на этот вопрос. Я бы предположила, что нужно прикинуть, допустим, у тебя есть какие-то клиенты, если бы они тебе платили, сколько бы ты зарабатывал на этом? Отсюда можно деньги взять. Здесь очень много нюансов есть, потому что есть же еще цена привлечения клиентов, например, еще такая штука как LTV, как long-term value. Я могу представить здесь какая-то формула, учитывающая все, но, знаешь, чтобы так сказать сходу, я бы начала копаться в первую очередь на то, что у нас клиенты, они приносят нам столько-то денег. Когда у нас машина стоит, мы эти деньги не зарабатываем. И это обычно то, что считается lost opportunity. Я бы так на это смотрела. Возможно, они что-то еще имеют в другое виду. У них сложность, ты знаешь, в чем здесь? Они бабки считают или потерянное время? Ну, они вообще... То есть это время можно конвергировать в деньги. Но обычно это считается, вот если бы тебе клиент платил за эту услугу, то вот такие деньги обычно ты зарабатываешь. Когда деньги эти не заплатываешь, то вот это lost opportunity. Ну, по-другому как считать? Не знаю. У них просто получается такая штука, что представь, что пришел запрос от клиента, и ты должен его выполнить на инфраструктуре Гугла. И этот запрос разбивается на 5 разных стадий, и каждая стадия разбивается на 100 маленьких тасков. И вот одна из тасков отвалилась из-за того, что слишком агрессивно работал автопилот. То есть мы можем посчитать, конечно, это во времени с точки зрения перезапуска, то есть этот таск должен был выполнить за 50 миллисекунд. Ну, там у них в десятках миллисекунд теоретически все таски должны закончиться. Там 95% или, по-моему, меньше сотни миллисекунд. Но отдельный таск перезапуск приведет к тому, что еще на 50 миллисекунд будет дольше. Они, может быть, считают именно так, что вот пользовательское время, задержка стоит вот столько баксов за каждую миллисекунду, и давайте рассчитывать. Мне кажется, я поняла, что ты другой вопрос мне задавал. Да? Не, ну ты, в принципе, тоже на него отвечала, то есть по крайней мере, с моей точки зрения, ты отвечала на мой вопрос. Мне кажется, я отвечала на другой вопрос. Давайте так, то, что после твоей ремарки, мне кажется, ты задавал другой вопрос. И, как это, correct me if I'm wrong. Собственно, вот lost opportunity – это тот кейс, когда у нас есть какая-то таск. То есть смотри, у нас есть задача, забивается на 5 маленьких задач. И одна из 5 маленьких задач убивается out of memory, и нам нужно её перезапустить. И вот как бы тот факт, что она у нас убилась, наш косяк, наши инфраструктуры, и это теперь должны как-то захэндлить, и нам нужно её перезапустить. Как бы это нас ложится, ответственность такая. Соответственно, как бы это дополнительные деньги, которые нам нужно потратить для того, чтобы эффективно по факту сделать 6 тасок, а не 5, которые они изначально планировали. Вот это ты имел в виду? Или я не правильно понимаю? Нет, они именно имеют в виду, что перезапуск тасков в инфраструктуре Борга – это решённая задача. То есть Борг видит все таски данной большой глобальной задачи, и он их запускает в нужную последовательность, там есть граф и так далее. Он отслеживает всё. Если какая-то таска упала по любой причине, не хватило памяти, упал инстанс, нетворк, всё что угодно, мы её перезапускаем на другой машине на свободных ресурсах. Поэтому когда одна таска падает из-за того, что ей не хватило места в одном месте, она будет перезапущена в другом, возможно автопилот уже по-другому сработает и так далее, мы рано или поздно её выполним, но мы потеряем вот это время между тем, как бы она выполнилась, если бы не было автопилота, или автопилот поставил бы менее агрессивное значение, и тем, как она выполнится из-за того, что он был слишком агрессивен. А вот это время часто очень бывает критично, особенно на каких-то задачах, на которых должна быть минимальная latency. И они как раз объясняют, что именно это приводит к тому, что нам нужно отслеживать разницу между костами по инфраструктуре, которые неиспользуемые ресурсы, кстати, косты неиспользуемых ресурсов против cost of the lost opportunity, потому что пользователи могли уйти из-за того, что слишком долго ждали, или эта задача потеряла актуальность, или мы должны там показать рекламу, а мы её не смогли показать, потому что мы вовремя не посчитали. Это то, что получается, нам нужно на каждый таз, который мы спускаем в нашей инфраструктуре, кроме всей его чудесной конфигурации, ещё навешивать, сколько он нам стоит, или как. Мне сложно представить, то есть я понимаю логику, мы в средство этого очень глубоко погрузились, но мне всё-таки практически интересно, что они на самом деле считают, потому что я сейчас сидел, думал, и наверное, единственное, на чего я могу додуматься, чтобы это как-то не приходилось конфигурировать на каждый таз, навешивать какое-то число баксов, которое разработчик скорее всего всё равно правильно не угадает, наверное, разумно просто посчитать количество повторно или в очередной раз пересожжённых ресурсов. То есть во сколько раз мы увеличили потребление ресурсов из-за того, что мы что-то прибили? Нет, они здесь говорят именно про то, что влияние на пользователя. Я думаю, ну то есть гипотеза. Гипотеза такая, что они, возможно, ввели какую-то единицу измерения, это вот мне похоже, похоже как story point в agile, и вот они скорее какими-то story point-ами меряют, вот каждая тазка, она столько-то story point-ов, либо там, например, у нас, давайте введём понятие такое, что когда у нас в инфраструктуре выполняется какая-то задача в течение n времени, n секунд, и которая требует n ресурсов, то это у нас будет одним попугаем. И они скорее всего меряют это всё в каких-то попугаях, и потом у них есть конвертация попугая в условные деньги кастомеров, потому что, как Валера сказал, добавлять туда деньги, ну это очень сложно, потому что если брать больших клиентов, прям очень больших клиентов, зачастую там идут специальные условия в плане договоров и в плане денег, и иногда компании предлагают огромные скидки, потому что это брендовая история, там как большие скидки каким-то большим крутым клиентам, потому что они хотят их иметь и рекламировать. Поэтому я не думаю, что там деньги в инфраструктуре, ну именно в инженерной части не завязаны, скорее всего там какой-то попугай введён и попугая меряют. Я однозначно согласен, что попугай введён для того, чтобы никак не завязываться на деньги, во-первых, во-вторых, потому что эти деньги в зависимости от попугая будут меняться со временем в любом случае, но я именно имею в виду, что как мне посчитать вот этот попугай к деньгам, это вообще очень сложная задача. Но они здесь вот целые несколько параграфов рассуждают на том, что пользователь может заметить деградацию в сервисах, на то, что некоторые задачи очень требовательны к лейтенансу и вот это всё. То есть это именно с точки зрения бизнесового влияния на работу автопилота. Именно поэтому они здесь ввели эту систему, которая помимо работы самого автопилота, есть система, которая следит за автопилотом, у которой есть набор каких-то параметров, насколько надо агрессивно, не агрессивно реагировать, и вот этот вот фидбэк на работу на автопилота, он контролируется уже людьми. То есть у них есть специальные графики, которые покажут, как много OOM, out of memory, произошло в системе за предыдущий день, какое количество по сравнению с две недели назад и с неделю назад в этот же день, и не нужно ли нам немножечко ослабить давление автопилота. Я думаю, они наверно ещё меряют количество клиентов, которые, или там запросов от клиентов, которые говорят, что «а-та-та, нам не нравится ваша инфраструктура, почему наши задачи занимают так много времени, чтобы выполниться». Или автоматические штуки типа «клиент ушёл после того, как не дождался», или там… Ну если там, знаешь, они даже такое меряют, что клиент ушёл, что не дождался, там же огромная цепочка, я даже представляю, огромная корпорация, там будет аккаунт-менеджер от гуглового сервиса привязан к какому-то конкретному клиенту, который там ходит на ужины с какими-то большими боссами и уговаривает использовать Google Cloud.",
    "result": {
      "error": "API request failed: Error code: 400 - {'error': 'Trying to keep the first 7848 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}",
      "stack_trace": "Traceback (most recent call last):\n  File \"/home/andrei/Projects/podcast-shownotes/scripts/build_search_eval_dataset.py\", line 157, in generate_search_query\n    response = await self.client.chat.completions.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 1927, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1767, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1461, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1562, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': 'Trying to keep the first 7848 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}\n"
    }
  }
]