[
  {
    "segment_id": "49f8c89d-7d6e-42bc-9aa0-10f4af126b53",
    "episode_id": "ba71f585-43bf-4010-a721-59d195e8c75c",
    "episode_number": 216,
    "segment_number": 7,
    "text": "Я, кстати, открыл статью, которая посвящена именно RAFT-алгоритму, и такого я не нашел. Дальше возникает еще интересный вопрос, на который я не нашел ответа. В RAFT-алгоритме подразумевают, что у нас как минимум три реплики. При этом у них два дата-центра. Возникает вопрос, как эти реплики, как эти хосты расположены между этими дата-центрами. В сайте прооркестратор написано, что количество хостов в каждом дата-центре может быть либо один, либо два. В принципе, если поместить два в одном дата-центре, один в другом, то у нас получатся три реплики. Три части, которые являются нодами и хостами RAFT-алгоритма. Это странное расположение, потому что Majority не должно находиться в любом дата-центре. Согласно мне, у них есть где-то еще один дата-центр, в котором как раз эти хосты расположены. Такую схему деплоймента я не смог найти в их статье. Но предположим, что есть еще какой-то дата-центр, в котором хостятся дополнительные реплики для оркестратора. Дальше оркестратор правильно сработал. Он увидел, что один дата-центр недоступен, переключился на другой дата-центр. И дальше начали работать primary сервера, то есть первичные реплики. То есть изменилась конфигурация MySQL за 40 секунд, в течение которого была недоступна сетевая связанность. И при этом продолжился traffic. Что тут стоит обратить внимание? Стоит обратить внимание на то, как сделана конференция... Не конференция, а как сделана репликация в MySQL. Я хотел бы сформулировать теорему, которую имеет смысл применять всегда, когда мы строим распределенную систему. Теорема формулируется следующим образом. Если мы делаем консистентная репликация в основе консенсуса, то мы делаем ее неправильной. Или по-другому можно сформулировать. Если мы делаем kissing не на основе консенсуса, то такая репликация не является Billy Builder как это faze staff. Собственно доказательство тривиально, а можно взять противного. Предположим что такая репликация консистентна. Что такое консистентная репликация? Это значит, что реплики договариваются о том, что у нас состояние будет одно и то же. То есть, по сути, они решают задачу консенсуса. Это и есть решение задачи консенсуса. Но так как они не решали задачу консенсуса, то мы приходим к противоречию. Таким образом, тривиальное следствие, что если мы не используем консенсус, наша репликация всегда будет неконсистентной. А можно сразу вопрос? Ты определяешь, как ты консенсус в данном случае? А что значит определяю я консенсус? Ну ты говоришь, что... Как ты сформулировал теорему на основе консенсуса? Вот здесь понятие консенсус довольно расплывчатое, нет? Или у тебя есть однозначное понятие консенсуса, которое ты здесь используешь? Есть определение однозначное понятие консенсуса. Есть классическая задача и классическая постановка, что такое консенсус. Я могу... Есть литература. Вкратце. Да, можно поразить. Идея в чем? Задача консенсуса, что у нас есть участники системы. Каждый участник системы говорит, что я хочу сделать то-то, я хочу сделать то-то. То есть они делают какой-то пропозал. Дальше они должны договориться, чей пропозал они будут принимать. И вот консенсус решает задачу, что все участники договариваются об одном и том же пропозале. Все или большинство? Вот это вкратце. Хороший вопрос, потому что некоторые участники могут быть недоступны, отрезаны или прочее. То есть если участник говорит, что он корректен, то у него либо предыдущее состояние, либо текущее. То есть либо, сейчас четко сформулирую, если он принял пропозал, то он принимает пропозал такой же, как и у других участников, которые тоже приняли пропозал. То есть если участник не принял пропозал, то мы его просто выкидываем из рассмотрения. Но если участник принял пропозал, вот если взять два участника, они приняли пропозал, то они приняли один и тот же. Любые два участника. Или участник, возможно, примет пропозал в ближайшем будущем, то есть у него сейчас устаревшая информация. Об этом задача консенсуса. Не говорить ничего, что позже, раньше. Она говорит, что если участник принял пропозал, такой же, как его принял кто-то другой. Кто не принял, у тебя есть еще возможность принять, но если ты после этого принял, то он такой же, как у других. Все, понял, спасибо. Продолжай, пожалуйста. При этом, если чего не приняли, то это не противоречит этому утверждению. Это, в принципе, консенсус не говорит о том, что у нас будет cleanliness, он говорит о том, что у нас будет safety. То есть в данном случае safety, что пропозал у всех будет одинаковый, если он будет. Соответственно, вот эта теорема на самом деле позволяет очень легко анализировать то, как устроена система. Но давайте посмотрим на репликацию москвы. Какие есть способы репликации москвы? Есть асинхронная репликация, есть синхронная репликация и есть так называемая полу... Не так называется. Полу-синхронная, это гибрид между синхронной и асинхронной. Но чем такое разделение мне не нравится? Дело в том, что есть консистентная репликация и неконсистентная репликация, давайте попробуем заматчить асинхронную и синхронную репликации с консистентной и неконсистентной репликацией. Если мы начнем это делать, то понятно, что асинхронная является неконсистентной. Но почему? Что такое асинхронная репликация? мы пишем лидера, и он асинхронно введет свою информацию до реплик. Если при этом у нас этот самый лидер падает, данные у него каким-то образом пропадают, например, что диски ненадежные, хасты ненадежные и прочее, то получается, что до реплик не дойдет та информация, которую только что пользователь закоммитил. Таким образом, мы теряем в консистентности, такая репликация не является консистентной. То есть это ненадежная репликация. Соответственно, а если мы возьмем синхронную, то, казалось бы, она должна быть надежной. Но это не так, потому что надежная репликация и консистентная репликация основаны на алгоритме консенсуса, как следует из теоремы. Консенсус является синхронной репликацией, но не всякая синхронная репликация является алгоритмом консенсуса. Поэтому, если мы просто делаем синхронную репликацию, она подпадает тоже... Что такое синхронная? Мы просто на реплики ждем, пока на все реплики мы записали. Такая репликация не является надежной, потому что... Ну, скажем так, она в каком-то смысле является надежной, но предположим ситуацию, что мы в одну реплику записали, а в другую реплику не записали. И дальше этот лидер упал. Что у нас происходит? У нас получается, что одни реплики содержат самые up-to-date состояния, а самые последние состояния, а какие-то реплики не очень. В принципе, можно выбрать, какие реплики наиболее свежие. Но теперь представим, что вот эта реплика, которая самая надежная, она тоже упала. Мы выбираем старую реплику, а потом вот эта реплика восстанавливается и начинает синхронизироваться. И возникает какая-то опять у нас упячка. То есть понятно, что этот сценарий очень маловероятен и в реальности вряд ли он будет. Ну, скажем так, в реальности это, наверное, не случается. Если у нас три реплики, то оно, скорее всего, в реальности не отличается от консенсуса. Если реплик сильно больше, чем три, то есть всего три, не знаю, мастер две реплики, то типа не должно случаться. Вот если у нас реплик сильно больше, то, наверное, может. В общем, там возможны всякие... В любом случае, если рассмотреть все возможные... Как у нас сеть может порваться, реплики доступные и прочее, то существуют варианты, когда у нас... Мы не можем четко сказать, что нужно нам взять для того, чтобы продолжить работу. Вот. И если мы начнем правильно хендлить ситуацию up-to-date реплики, правильно взять, то, соответственно, мы начнем решать задачу, на самом деле, консенсуса. По большому счету. Но так как мы об этом не думали, то, скорее всего, мы ее решим неправильно или не до конца. То есть для каких-то краевых ситуаций мы решим ее неправильно. То есть она не будет работать корректно. Немного просто в сторону отпрыгнул. Если мы вернемся к теме GitHub и репликации MySQL, то там ситуация все гораздо проще. Там, насколько я понял, в статье про оркестратор там было написано, что они использовали асинхронную репликацию. Я так понимаю, они переключились в полусинхронную. И для внутридата центровой они использовали 500 миллисекунд в качестве таймаута. Что такое полусинхронная репликация? Это такой хитрый гибрид. Значит, мы отправляем запрос на репликацию на реплике с лидера и ждем таймаут. Если в таймаут мы уложились отлично, значит, идем дальше. То есть мы находимся в синхронном режиме. Если же у нас таймаут случился, то есть прошло времени больше, чем мы ждали, мы переключаемся в асинхронный режим и теперь уже перестаем реплицировать данные на реплике. Соответственно, как только мы переключились в асинхронный режим, переключились в режим небезопасный, то есть тогда, когда мы можем терять данные. В случае Network Splitter, который произошел у GitHub между двумя дата-центрами, все стало еще хуже. То есть старые мастера, которые работали в дата-центре на северном побережье, которые были primary, они справились и думали, что они первичны и продолжили выполняться и продолжили выполнять транзакции, которые были локальны на этом дата-центре. Работали автономные джабы, которые делали какую-то работу. Какая конкретно работа? Соответственно, так как он был отрезан от мира, он не принимал информацию извне, которую пользователи постоянно постили. У них получилась ситуация, что у них произошло то, что происходит при обычном Network Splitter. То, что одна часть ушла в одну сторону, а другая часть ушла в другую сторону. Это закономерный результат такого способа реплики. Дальше, то, что у них происходило, там тоже есть ряд интересных моментов. То есть почему они решили... Следующий интересный момент, на который тоже стоит обратить внимание, это то, что переключение произошло штатно. Они удивились конфигурации, хотя ничего удивительного в конфигурации нет. Потому что, согласно логику оркестратора, именно такое должно было произойти. То есть все поведение системы, когда увеличиваешься latency, привело к тому, что трафик они не смогли удержать, тот, который у них был на данный момент. Именно поэтому им пришлось переключиться снова на первоначальный датансеттер, потому что, когда у вас увеличивается latency, у вас сразу проседает срок путь системы. Можно легко показать такую пропорциональность. Поэтому тот срок путь, который у них оказался в итоге, он стал несовместим с той доступностью, которую они гарантировали и обеспечивали. Я так понимаю, из того, что они написали. И, соответственно, они приняли решение переключиться, но переключиться они не смогли, потому что один из полушариев, все. Соответственно, дальше бэкапы. По счастливому обстоятельству, из бэкапов они становятся без каких-либо проблем. Единственная проблема, которая у них была, которую они не смогли догадать, то, что чтобы скачать бэкапы, требовалось значительное время. И вот все это время они как раз скачивали эти бэкапы. И восстановили свои сервисы. Но потом плавно переключили нагрузку. Не очень понятно, что произошло с теми данными, с двумя секундами и 30 минутами. Восстановили ли они, как-то реконсилили. Об этом в статье четкого указания нет. И на мой взгляд, технически, этот инцидент, анализ инцидента, ему не хватает некоторых важных технических деталей. Если мы проектируем распределенные системы, хотим понять, что же на самом деле произошло. В результат, они решили сделать несколько шагов по предотвращению таких событий в будущем. Но первое, что они хотели сделать, и это важный шаг, это сделать так, чтобы оркестратор не занимался такими вещами, как перенаправление всех параметров в другой дата-центр. Так как у них оказалось асинхронно, а дата-центры не эквивалентны оказались друг другу. Потому что в одном дата-центре, несмотря на то, что он мог содержать все данные, он содержал все реплики необходимые, но в другом дата-центре, в главном дата-центре, было больше реплик. Там работали необходимые джебы, поэтому невозможно просто так для всех сервисов в текущей конфигурации просто взять и подключиться на другой дата-центр. Поэтому они хотят изменить конфигурацию оркестратора, чтобы такого не происходило. Ну, репортинг-механизм у них там, что-то они говорили, были странности. И они, самый главный шаг, я считаю, что они хотят перейти от текущей архитектуры к архитектуре, когда у нас все дата-центры активны. То есть в текущей конфигурации вот этот первичный дата-центр, главный дата-центр, он является активным, а тот является пассивным. На него происходят автоматические переключения, но при этом он полезной работой и полезной нагрузкой не занимается. Они хотят от этого перейти к Active-Active дизайну, когда и там какая-то работа происходит, и там, и возможно еще в каком-то другом. И вообще хотят они идти к N плюс 1 редактинсе, то есть влияние падения как от дата-центра было минимальным.",
    "result": {
      "query": "RAFT deployment multi‑data center replication"
    }
  }
]