[
  {
    "segment_id": "5c4be467-fa58-4ad5-a9c8-62ce339850bb",
    "episode_id": "6f26d3e9-7a24-44a2-9a07-bf712df8418e",
    "episode_number": 374,
    "segment_number": 6,
    "text": "Сейчас я по этому перейду. Дело в том, что сейчас, где тут у меня пометочка, как раз она через пункт. Тут нужно понять, как Stardoc исполняет запросы. То есть, эта кардинальность важна для того, чтобы запросы исполнялись быстро. Но чтобы понять, почему, нужно понять, как он устроен. Когда ты делаешь селект по графовым данным, есть разные подходы, как это делать. Например, некоторые реализации берут какой-то узел в этом графе и начинают как-то из него делать обход. Вот Stardoc не так работает. Когда ты хранишь свои тройки в базе данных, по ним строится индекс. Соответственно, если ты посмотришь на то, как устроены запросы в Sparkle, это немножко напоминает pattern matching. Ты говоришь, что знак вопроса предекат какой-то объект, потом какой-то объект предекат то, что у тебя было первым знаком вопроса, ты использовал второй часть. И так далее. Ты перечисляешь всякие тройки с переменными. И потом говоришь, что все, что этому match, из этого достанет мне то, что ты в селекте через запятую указал. Важно, что в самом запросе в селекте у тебя фигурируют тройки. И по ним у тебя по каждой из этих тройок построен индекс. И фактически одна строка в твоем селекте, вот эта одна тройка, один pattern, который ты указал, она соответствует одному join. Одному сканированию по индексу и одному join. А в типичных запросах, которые к их системе идут, у тебя могут быть десятки или сотен строк в селекте. Соответственно очень много join. У тебя в SQL типично join, зависит от приложения, но ты можешь join три таблицы, пять, десять таблиц. Лично я не видел, что вы join-ли очень много таблиц в SQL базовом данном. Хотя я уверен, что такие случаи бывают. Но здесь в типичном запросе у тебя сразу сотни join. И для того, чтобы правильно оптимизировать эти запросы, чтобы они эффективно исполнялись, если верить этому докладчику, тебе нужно знать не просто статистику, а именно корреляцию между данными. Почему это знание помогает, нужно пояснить или это очевидно? Корреляцию между данными, которые запросил пользователь? Да. Что такое план запроса? План запроса – это дерево, в узлах которого ты говоришь, например, возьми эту таблицу и читай из нее данные. Все, что ты читаешь, пихай наверх. Или прочитал строку, пробили на какое-то условие, больше или меньше. И если выполняется, пихай наверх. В следующем узле дерева у тебя, например, сказано, а здесь мы делаем join. Join тоже бывает разным, бывает merge join, бывает hash join и другие join. То есть мы соединяем то, что было в левом под дерево, с тем, что было в правом под дерево, по какому-то условию. И опять же, пихаем это наверх. А потом у тебя может быть узел, который говорит, а здесь мы сортируем данные. Вот просто все, что к нам пришло, мы сортируем и пихаем наверх. И вот таким образом у тебя план запрос представляет собой дерево. Оптимизатор чем занимается? Он для данного запроса пользователя, который вот прям текстом или пропарошиной, он для данного запроса должен построить план, то есть дерево исполнения этого запроса. И вот это пространство поиска, то есть все возможные деревья, все возможные планы исполнения запроса, как его физически исполнить, это очень большое пространство. Особенно оно, я не помню, как там правильно это называется математически, но оно разрастается быстрее, чем факториально, в зависимости от того, сколько джойнов ты делаешь. А у них сотни джойнов. То есть ты представляешь, что такое факториал от 100, а это больше. Как тебе помогает статистика, вообще любая статистика, скорреляция или бескорреляция? Ты видишь, что у тебя, например, вот в этом узле дерева, ты говоришь, прочитай мне вот из этой таблицы все, где x равен 15. Ты смотришь на статистику и понимаешь, что ага, x равен 15 это всего пара значений. Это очень мало здесь будет данных, да? Или наоборот, ты смотришь на статистику и понимаешь, о, 15 это, короче, это пара миллионов значений. И на основании этого ты можешь понять, сколько операции чтения с диска или каких-то других операций будет делать данный узел в запросе. На основании этого ты можешь прикинуть общую стоимость запроса. Зная стоимость каждого из запросов, ты можешь в пространстве поиска из всех запросов выбрать оптимальный, то есть тот, который ты считаешь будет наиболее быстрым. Корреляция помогает тем, что позволяет тебе более точно оценить стоимость запросов и таким образом найти более оптимальный. То есть это оптимизация планирования, а корреляция помогает этой оптимизации? Все так. Вот, это было небольшое отступление. Дальше докладчик говорит. Погоди, это было отступление для кардинальности. Да, да. Давай, давай. Я просто хочу убедиться, я тебя не перебил, мы это обсудили, правильно? Нет, пока нет. Я пока не понял, как пользователь. То есть вот сейчас все больше баз данных говорит, что у нас высокая кардинальность. Можешь пояснить для пользователя высокая кардинальность, почему является плюшкой? Я не очень понял вот эту часть про у нас высокая кардинальность. Они просто говорят, что они умеют определять кардинальность данных. Да, я понял. Я сейчас многие вижу базы данных, которые говорят, что в качестве рекламы они говорят, что мы умеем работать с высококардинальными данными и мы умеем их очень быстро обрабатывать. Можешь это перевести на русский язык? Что значит база данных, которая имеет работать с высококардинальными данными? Я просто не очень понимаю, что значит в контексте баз данных. Что значит кардинальность, быстро работать и так далее. В данном случае у нас есть 100 джойнов, у нас есть какие-то данные в каждом куске, мы пытаемся найти что-то. Причем кардинальность? Что значит? Я просто потерялся, если честно. Я не могу перевести смартейсингового на нормальный, потому что будучи его ровным из контекста, оно может означать абсолютно что угодно. Я могу тебе объяснить, что конкретный докладчик имеет в виду под кардинальностью данных. Давай. Он имеет в виду, что у тебя твое гистограммо распределения значений, вот у тебя есть таблица, у нее есть значения. Ты можешь по ним построить статистику, как часто встречаются те или иные значения или те или иные интервалы значений. Он говорит о том, что эта статистика у них не одномерная, а двумерная. В смысле, она говорит не просто по этой таблице, а вот у меня есть столбец х и есть столбец у. И если у меня х равен 15 или х относится к какому-то интервалу, тогда по у у меня вот такая гистограмма. А если х относится к другому интервалу, тогда по у совершенно другая гистограмма. Понимаешь? Да. Теоретически мы неограничены двумя, мы можем также 3-4 измерности делать. Вроде как в этой системе им достаточно. Понял. Спасибо. Вот я надеюсь, что я ответил. Да, вот то, что я выше описывал про то, что в Sparkle ты фактически используешь pattern matching по графовой структуре. Это они называют Basic Graph Pattern Matching или BGP Basic Graph Pattern BGP Matching. И также отмечается, что Sparkle имеет строгую спецификацию. И для любого пользовательского запроса или любого входа ты по этой спецификации знаешь точно какой результат ожидается. То есть это не, знаете, как часто бывает у вас, когда вы делаете фото, вы делаете фото, вы делаете фото, вы делаете фото. Это не, знаете, как часто бывает у нас SQL Dialect, который мы сами придумали. Он без спецификации, он может возвращать все что угодно. Здесь все не так. У Sparkle на него есть нормальная спека. Про джойны я рассказал. Хэш джойн они умеют, мер джойн они умеют. Они умеют делать сорт. И да, вот это, тут еще у них есть интересная специфика, когда ты поддерживаешь разные джойны и в целом разные узлы в графе исполнения запросов, нужно понимать, что, простите, в плане исполнения запросов нужно понимать, что у тебя эта вариативность, она еще сильнее увеличивает процессом поиска. То есть ты, у тебя узлом графом, узлом плана может быть не просто там джойн, а он становится хэш джойн или мер джойн. А если у тебя 100 джойнов, тогда у тебя 2 флота и уже вариант принимается. Просто потому что 2 типа джойнов поддерживаешь. Дальше становится интереснее. Да, хорошо, мы выяснили, что пространство очень большое, мы выяснили, что найти в нем оптимальный план без кардинальности, без коррелированной статистики, почти невозможно. Интересный момент, первый заключается в штуке, штуке, которую они называют star-shaped graphs, или ssg. Оказывается, что вот представляете себе граф, и представьте себе, что у вас есть узел, а из него много-много стрелочек в другие узлы. И это похоже на звезду. Они это называют star-shaped graphs. И оказывается, что это штука, которую очень часто встречается в реальных данных, загруженных в граф УСУБД. И оказывается, что человекам им вот так привычнее моделировать свои данные, что... Фактически речь идет о том, что люди кладут в графовую базу данных, которые с кимлисом всякое такое, релизационные данные. И вот это релизационное отношение, как картридж в обычной релизационной базе данных, он представляется графом. То есть у тебя, если там первая колонка id равен 15, то ты говоришь, что тут у меня мой row, там стрелочка id 15, потом второй атрибут мой row, атрибут значения, атрибут значения. То есть у тебя имена атрибутов становятся дугой на графе, а значения становятся узлом, в который эта дуга указывает. А сам твой узел, из которого исходят дуги, он соответствует строке в графовой базе данных. Вот такие штуки люди загружают в графовые СУБД. Они умеют смотреть на данные и понимать, что вот здесь у меня Starshape Graphs, и он представляет собой релизационные данные, и они умеют из графовых данных преобразовывать их обратно в релизационные. И у них есть специальные кэшики, они выводят релизационную модель, то есть dirifle. Они понимают, что здесь у меня не звезда, а картридж, они его сохраняют как картридж и кладут в кэш картридж. И у них много оптимизации именно для таких структур графов, потому что это часто встречается. А оптимизация совершенно не сработает, если одна из лучей потом ведет еще куда-то? Ну, в смысле, чтобы появляться какой-то цикл в этом графе. А это следующая оптимизация, потому что то, что я писал, это фактически foreign keys, и foreign keys они тоже dirifle. Понял. Я очень сильно кекал на этом моменте, потому что люди вместо того, чтобы использовать релизационную модель, они сохраняют эту графовую модель, чтобы потом с OBD смотрел на эти данные и устанавливал релизационную модель. Мне кажется, это очень интересно. А я думаю, что это очень часто use case, наверное, часто такое случается. Продолжай. Во-первых, у нас очень много хайпа. То есть есть классный новый инструмент, пошли все быстро пользоваться этим инструментом, даже если он не подходит. А во-вторых, я много видел случаев, но видом. У нас уже есть одна база данных, зачем создавать еще одну? Давай туда и это тоже запихнем. Но она же не ложится туда, это же релизационная модель. Да ничего страшного за это не будет. Но зачем нам платить за парк технологий? Да-да-да. И получается, они вообще молодцы в этом смысле. Да, классная идея. Я до этого момента не знал, что такие штуки вообще бывают. То, что в графовую с OBD она притворяется графовой, но внутри все раскладывает обратно в релизационную модель. У меня просто был взрыв мозга. Дальше сообщается, что они активно в своей системе используют структуру под названием count mean sketches. Кто знаком с такой замечательной структурой? Первый раз слышу. Вот я тоже. Оказывается, это очень... Может быть я где-то краем глаза видел, но я вот напрочь забыл. А это очень прикольная структура. Идея очень похожа на фильтру Блума, но с отличием. Представь себе массив. Представь себе, что у тебя есть значение. И ты берешь значение х, считаешь от него хэш, и поэтому хэшу определяешь индекс в массиве. Ты идешь в этот индекс и делаешь там инкримент. Представил? Да. Теперь представь себе, что у тебя есть еще три массива и другие три хэш-функции. И ты делаешь то же самое. Соответственно, у тебя для того же значения ты попадешь в другие элементы массива и там сделаешь инкримент. Наверное, стоило сказать, какую задачу мы решаем. Наша задача это вероятностно пощадить. Вот у тебя есть поток данных, поток х. И тебе нужно примерно... И у тебя огромный поток. Он не влезает ни в Диск, ни в Память. Просто х и летят, летят, летят. А тебе нужно примерно вероятностно ответить на вопрос. А вот у меня поток, он летел, летел, летел. А сколько там было заданных значений? Вот. И как раз CMS, Count Me In Sketches, это структура данных, которая дает ответ на этот вопрос. И дает она таким образом, что когда я спрашиваю, сколько раз у меня х было в потоке, ты считаешь от этого х 4 х суммы, обращаешься к соответствующим элементам четырех массивов, у тебя там будут четыре значения, и ты от них берешь минимум. Почему ты берешь минимум? Потому что у тебя есть коллизия, и ты... Понятно. Мог с нескольких разных х попасть в один и тот же элемент. Да. И что, насколько точно? Интересное свойство этой структуры заключается в том, что в отличие от фильтра Бума, она саблиниар. То есть у тебя в зависимости от числа твоих коллизий... Ну да, если у тебя есть... Ты знаешь, что у тебя N уникальных значений, в принципе, то в фильтре Бума тебе нужна маска или массив черчками, смотря... Ну, просто фильтр Бума, они тоже разные, они могут считать сколько в хождении, а могут считать, просто было-небыло. То фильтр Бума, его размер памяти, который его нужен, он линейно зависит от того, сколько у тебя уникальных значений возможных. А CMS, Counting Minus Ketches, они саблиниар. То есть у них объем памяти растет менее быстро, чем линейно. От твоего N, где N это уникальное количество элементов. В этом главное преимущество этой структуры. Саша, а помнишь, мы давно в подкасте обсуждали структуру HyperLogLog? Мне интересно, если мы сравним CMS с HyperLogLog HLL, как они сравнимы? Это отличный вопрос. Честно, я напротив забыл, как устроен HyperLogLog, мне нужно это освежить. Или, может, ты напомнишь? Ох, я думаю, мне тоже нужно было бы это освежить. Мне кажется, HyperLogLog тоже саблиниар. Они будут более эффективны, чем фильтры Бума. Как я понимаю, твоя структура данных отвечает на вопросы... Мы можем говорить про количество элементов из нескольких множеств, и она может отвечать на этот вопрос. А HLL, насколько я помню, говорит про одно множество элементов из одного множества. Нет, позволь поправочкой. Count-MainSketch работает с элементами одного множества. Он внутри устроен как четыре массива, но это просто детальная реализация. Он работает с элементами одного множества. Интересно. Тогда это как-то I'm puzzled. Возможно, мне нужно детально посмотреть на CMS, потому что этой структуры данных я о ней не слышала. Но по первому приближению они, кажется, отвечают на один и тот же вопрос. И мне интересно, в каких ситуациях я должна использовать HLL, а в какой ситуации должна использовать CMS. Это очень интересно. Я бы почитала больше про CMS. Повторюсь, к сожалению, мне ошибла память. Я ничего не помню про HLL, поэтому мне нужно это освежить. К моему стыду. Да, мы сказали, что это sub-линия структуры. И важное свойство, то что она может overestimate количество значений. То есть переоценивать, что в стриме было больше значений, чем было на самом деле. Но она никогда не делает наоборот. Никогда не делает underestimate. Плюс к этому недостаток структуры, что она не может отличить редкие объекты от объектов, которые никогда не существовали в стриме. Потому что у тебя есть шум, опять же, из-за коллезий. В принципе, это тоже самое, что и предыдущее свойство. Ты не можешь отличить сценария, что этого элемента никогда не было. Вот сценарий этот элемент был, но один или два раза. Просто такая занимательная структура, которую они используют. И еще есть доклад, где я узнал, что есть отдельное направление исследований. Это поиск устойчивых планов запросов, рабаств планов. Идея заключается в том, что сейчас в существующих системах, если ты в связи с неактуальностью накопленной статистики или сферой, по какой-то другой причине построишь неправильный план запроса, то с большой вероятностью он никогда не завершится. Он будет настолько медленный. Устойчивые запросы интересны тем, что даже если твоя статистика достаточно сильно неверна, то эти запросы отработают медленно, но они отработают как бы окей. То есть по крайней мере пользователь дождется результата. Почему вообще у себя возникают неправильные планы? Причина в том, что у тебя, как мы отметили, деревья очень густые и пушистые, очень много узлов. И ошибка в статистике копится от листьев деревьев к корнях, потому что она везде суммируется. То есть ты на листьях такое говоришь, ну здесь у меня где-то там 100-150 элементов, потом выше говоришь. Ну вот здесь если нам из дочерних узлов придет по 100-150 элементов слева, по 200-250 справа, и мы их там по Джонни мастертируем, тогда у нас будет 300-300 элементов. И вот ты так вот по дереву поднимаешься, поднимаешься, у тебя проходишь пару сотен узлов, и у тебя в корне дерева накоплена очень большая ошибка. Возникает вопрос, а что если ты построил планы исполнения запросов, ты начал его исполнять, и вот ты его исполняешь, и вдруг понимаешь, что делаешь только фигню. В том смысле, что ты планировал, ты говорил, что вот в этом узле мы делаем хеджо, у меня будет 100 элементов, они легко поместятся в память. А ты реально исполняешь запросы, понимаешь, что у тебя ни фига не 100 элементов, у тебя и там 100 тысяч, и память уже почти закончилась, и что делать. Это тоже интересное направление для развития, потому что у тебя здесь много вариантов. Ты можешь вообще перестать исполнять запрос и заново его перепланировать с учетом новых вводных. Конкретно в этой системе они этого не делают, они говорят, что если мы в такой сценарий попадаем, то у них то ли memory mapping хедж таблиц, то ли какой-то свопинг, но они пытаются продолжить исполнять запрос, даже если узел, который предполагал, что вместо совпадения он не умещается в память, они его начинают свопать в диск. Другой вариант, ты в принципе можешь попытаться вот только это под дерево запроса перестроить, остальные не трогать, пока не выяснится, что они тоже неправильно были оценены и так далее. То есть это, как я понял со слов этого докладчика, тоже такая недоисследовательная область. Еще интересный момент, в какой момент можно не полагаться на статистику. Оказывается, что в некоторых случаях, а, ну в смысле, это тоже один из кейсов того, что я описал, что иногда тебе нужно плюнуть на статистику и пытаться как-то евреистически построить запрос, ну в общем использовать не костбейст-опрос. И вот возникает вопрос, где Валера скидывает в чат потрясающие фоточки. Привет, Валера. Ура, ура, ура, дети. Даже раньше следующего выпуска. Так, у меня поэтому все. То есть да, вопрос, как измерить, вот, что сейчас мы не должны полагаться на костбейст-модель, а полагаться на какую-то другую модель. Вот такой вот интересный доклад. Доклад мне в целом понравился. Возможно вам, ну если вам вот понравился пересказ, и у вас есть лишь час времени, вам может иметь смысл его посмотреть. Ну конечно, я что-то забыл поместить рассказать. У меня по этой теме все. Вопросы, возражения, комментарии. Можешь одним предложением просуммировать, когда надо использовать этот стардог? Когда у тебя какая задача? Тебе нужно использовать стардог, когда ты крупный интерпрайс, такой как eBay или Bosch или Cisco или NASA. Когда у тебя есть много денег, и ты хочешь себе DBAS, чтобы тебе его не отменить, куда ты вгрузишь кучу-кучу своих данных, а потом будешь по ним искать, ну используя графовый язык запросов Sparkle. Вот для таких задач он нужен. Спасибо. Еще он тебе нужен, когда ты разработчик баз данных, который ищет на себе копии работы. Ну потому что ребята нанимают. Я не смотрел, есть у них ремонт или нет, но если вам хочется подписать базы данных, то ознакомьтесь. Ребята нанимают. Давай может я следующую тему расскажу, ты немножко отдохнешь. Да, я был бы очень благодарен, спасибо. Следующая тема это из вопросов слушателей, принесли две темы про Гилу в питоне. И это две статьи, одна статья на Хабре, вторая это заметка на, я даже не знаю, что это за сайт, Lungo.pl. Мне конечно же понравилось больше на Хабре, она довольно детальная, очень много всякой информации дается. И вторая статья читается очень легко после статьи на Хабре, особенно тем, кто не очень в курсе про то, как питон вообще внутри устроен. Я лично с питоном всегда имел дело как небольшой обработчик мелкий, который не знаю что-нибудь запустит, как-нибудь строчки что-нибудь обработает и так далее. Потому что на шеле такие вещи писать удобно, пока они маленькие, как они становятся чуть больше и появляются чуть больше логики. Я предпочитаю переходить на какие-то более сложные языки, питон отлично подходит. Я никогда не упирался в этот Гил Global Interpreter Log, потому что это надо запускать его как многопоточное приложение с большим количеством входящих запросов и так далее. Я его так не использовал, поэтому мне было очень интересно почитать. И если вы тоже это плохо понимаете, что происходит, я вам очень советую и рекомендую. Для меня давайте я думаю, что во-первых многие знают, что это такое и наверное многие читали и разбирались. Давайте я затрону для простоты самые интересные вещи, которые лично мне были незнакомы. То есть я всегда знал, что Гил есть в питоне, что Гил это такой как лог, который позволяет работать многим приложениям одновременно, многим потоком одновременно. И этот Гил помогает синхронизировать какое-то глобальное состояние. То есть это было для меня понятно как общий концепт, мне было непонятно детали. И в качестве деталей начну с того, что он позволяет безопасно обрабатывать, безопасно сохранять. То есть если у вас есть много потоков, то Гил помогает в подсчете ссылок на объекты, для того чтобы потом делать по ним правильно гарбуч коллекшн. Во-вторых, работа с мутабельными структурами данных. В библиотеках бывают разные мутабельные структуры данных, чтобы это было потоко безопасно. В-третьих, какие-то глобальные данные и данные имеющие отношение к интерпретатору. И в-четвертых, это расширение на языке Си. Вот для этих четырех целей основных и существует Гил. А как вы можете внезапно упереться в Гил, это есть очень классный пример. В самом начале показывается проявление. Вы пишите программу, которая делает там в цикле while n больше нуля, n минус равняется единичка. И вот в эту функцию передается n и вы запускаете для какого-то числа n. И автор на... На самом деле это статья перевод, поэтому как бы это.. . Переводчик или автор, тут надо понимать. Автор запускает это для 100 миллионов в одном потоке, потом делает в двух потоках и в каждом потоке по 50 миллионов, потом в четырех потоках и 25 миллионов в каждом. И когда вы так делаете, есть такое предположение, что как бы это же просто обычная считалка. И соответственно количество потраченного времени должно быть... Должно уменьшаться с увеличением количества потоков, потому что у вас в любом маке несколько ядер, там еще гипертрайдинг есть, и у вас теоретически должна увеличиваться производительность, если вы все ядра используете, а не одно. И когда он это запускает, он показывает, что на одном потоке 100 миллионов, что на двух потоках по 50 миллионов на каждом, что на четырех потоках по 25 миллионов на каждом, что на восьми потоках 12,5 миллионов. Время исполнения всегда будет одно и то же. Ну плюс-минус там 6,52. 6 секунд 52 сотых. И это как раз из-за того, что мы имеем ГИЛ. ГИЛ работает следующим образом. Он выполняет какой-то блок операции. Сейчас я найду описание. Там есть несколько флагов. Несколько флагов. Там классное название. Я их не запоминаю. Брейкер. Да, Ивал Брейкер. Он работает следующим образом. Прежде чем захватить ГИЛ, каждый поток сначала проверяет какой-то другой поток. Захватил уже ГИЛ или не захватил. Если еще никто не захватил, то он его захватывает и начинает исполнение. Если кто-то другой захватил, он ждет до тех пор, пока не освободится захват. Он ждет какое-то время. По умолчанию есть это время 5 миллисекунд. Это интервал приключения. В течение фиксированного он ждет. А потом он устанавливает два флага. Это Ивал Брейкер и ГИЛ ДРОП РЕКВЕСТ. Эти два флага являются сигналом другому потоку, который сейчас находится в состоянии захватившем. Кому-то другому еще нужен ГИЛ. Второй поток смотрит, если эти флаги взведены, он доходит до какой-то точки и отдает управление. Точнее, он отдает управление без разницы к кому. Если есть несколько потоков, ожидающих этого ГИЛа, кто-то из них захватит. Если в этом функции кто-то будет читать, кто-то не будет. Все остальные будут ждать. Есть несколько исключений. Эти исключения связаны с операциями потока вывода, либо с передачей управления в сишной функции. Когда вы выполняете какую-то сишную функцию, в момент запуска этой функции отпускается этот ГИЛ. Если вы запускаете сишную функцию, можно считать, что ГИЛ на вас влиять не должен. Конечно же, есть куча исключений в виду того, что он должен временами переключаться для каких-то управлений, для каких-то дополнительных действий на этот поток. Есть несколько примеров, в котором считается Ша-256 от сообщения. Ша-256 на этом же самом ноутбуке в два потока работает быстрее, чем в один поток. Дальнейшее увеличение параллелизма не приводит к сильному ускорению. Чем дальше, тем больше вы увеличиваете количество потоков, тем хуже становится. И очень интересно это все посмотреть. Здесь есть еще одна любопытная вещь. Это про 5 мс. Где это написано? 5 мс. Идея 5 мс. это идея того, что когда вы начинаете выполнять какую-то задачу и поток хочет захватить ГИЛ, он ждет максимум 5 мс. Но это 5 мс. в разных контекстах может значать разное. В каком-то контексте, где эти 5 мс. вообще не показательны, это вы даже не заметите, что мы подождали. А в каком-то контексте это может быть очень заметно. Он написал кратенько веб-сервис, в котором он получал 1 байт и посылал 1 байт. Соответственно у него эта посылка байта занимала микро секунды. Но из-за того, что мы это делали в разных потоках, эти 5 мс. давали задержку. Так, сейчас. Где тут эта таблица? Я потерял. Не скажу. И так получилось, что у него около 30 000 запросов в секунду на однопоточной версии. А на версии, в которой одновременно с этим сервисом существует еще дополнительный поток, который просто делает какие-то вычисления. Поэтому он захватывает очень быстро вычисления и отдает только тогда, когда дырому потоку нужен ГИЛ. Получается, что другой поток, который работает с отдачей и получением данных, всегда будет просить отдать ГИЛ через эти два флага, которые я сказал до этого. Поэтому каждый раз получается, чтобы получить данные, он соответственно взводит эти флаги, 5 мс. приходит. Потому что, чтобы отдать данные, снова 5 мс. проходит. И получается, что вместо 30 000 запросов в секунду, когда мы запускаем еще одну рядом числа дробилку, которая использует CPU и захватывает управление, производительность упала в 300 раз. То есть было 30 000 запросов в секунду у веб-сервиса, а стало 100 запросов в секунду. То есть 300 раз это прям очень значительно, и он как раз показывает, что это произошло из-за того, что мы послали и получали 1 байт. Это происходило очень быстро из-за того, что ГИЛ никто не захватывал больше, один поток работал самостоятельно. Но когда происходит вот эта вот борьба двух потоков, вместо микросекунд для отправки или получения, он 10 мс. на каждый запрос ждал. И вот и получалось вот это вот падение производительности. Это очень интересно становится посмотреть, когда начинаешь варьировать этих мс. Оказывается есть специальные флаги, которые показывают с помощью этих флагов, вы можете установить в питоне, какая вот это вот интервал ожидания он будет. Можно установить там 5 мс. 1 мс. и так далее. Он устанавливал даже по-моему 1 микросекунда, если я не ошибаюсь. 0.0001, да. И соответственно он здесь приводит табличку, что интервал переключения, запуск этого сервиса без дополнительных цпу потоков, запуск сервиса с 1 цпу потоком, с 2 цпу потоком, с 4 цпу потоком. И как изменение вот этого вот интервала будет влиять на производительность вашей программы? Очень интересно посмотреть, что вот дефолтный там падает до 100 с 30 тысяч. Если один есть цпу поток, который съедает цпу, он соответственно падает до 100, как мы уже обсуждали. Если есть 2 цпу потока, то автоматически получается, что у нас есть 2 цпу потока, которые могут схватить себе гил для того, чтобы работать. И в этот момент, скажем, нам надо отправить какой-то пакет, мы устанавливаем этот флаг, насколько я понимаю, то поток отдает передачу, теоретически шедулер должен отдать после этого гил тому потоку, который работает с input-output. Но по факту мы видим, что производительность падает со 100 до 50, то есть с вероятностью 50 на 50. То есть он отдавал либо тому, либо этому потоку управления, и соответственно из-за этого падает производительность еще сильнее веб-сервисе. И соответственно можно посмотреть, что если, скажем, до 1 микросекунды уменьшить, да, вот получается, до 10 микросекунд это оптимальная величина интервала, когда производительность веб-сервиса падала с 30000 до 11000, если одновременно работал еще тпу поток. И очень интересно посмотреть, что если в дальнейшем еще уменьшать этот период, то у нас уже начинает не так сильно расти производительность, она хуже становится из-за того, что видимо уже вот этот вот интервал становится слишком маленьким, сравним с интервалом полезной нагрузки, и мы чаще переключаемся, чем делаем что-то полезное. Вот, статья в целом интересная, здесь в конце рассказывается, какие бывают, какие были идеи, как избавиться от ГИЛа, вот, рассказывается про разные подходы, которые в прошлом происходили, оказывается был не один подход, даже, вот, и оказывалось очень сложно из-за нескольких причин. Во-первых, получается, что как только вы избавляетесь от ГИЛа, вы должны очень внимательно думать о том, как правильно работать с глобальным состоянием, то есть все вот эти вот четыре цели, которые я перечислял в начале, они, их надо как-то решать, там, начиная счетчик ссылок и заканчивая си, уже написанными си программами, которые должны работать безопасно, вот, потока безопасно. И для, конечно, для всех четырех вот этих задач есть свои решения, вот, оказалось, что даже у глобальных переменных типа true, non, у них тоже ведется счетчик ссылок, для меня это была вообще интересная идея, я подумал бы, что надо бы вводить исключение для подобных констант, но для них тоже",
    "result": {
      "error": "API request failed: Error code: 400 - {'error': 'Trying to keep the first 8150 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}",
      "stack_trace": "Traceback (most recent call last):\n  File \"/home/andrei/Projects/podcast-shownotes/scripts/build_search_eval_dataset.py\", line 157, in generate_search_query\n    response = await self.client.chat.completions.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 1927, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1767, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1461, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1562, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': 'Trying to keep the first 8150 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}\n"
    }
  }
]