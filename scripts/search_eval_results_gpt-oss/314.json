[
  {
    "segment_id": "0f39a594-78fd-4dc1-ba93-f98ad4b298ae",
    "episode_id": "809d6413-0ae8-444f-9a0f-59f36e2f103a",
    "episode_number": 314,
    "segment_number": 7,
    "text": "Понятно. Следующий вопрос у меня про raft. То есть, понятное дело, когда делается консенсус, сделать его по-правильному достаточно сложно. Достаточно здесь в кавычках, потому что это очень сложно. И многие компании, которые сказали, что поддерживают консенсус, и у нас raft и все хорошо, на самом деле после тщательного тестирования оказывалось, что у них есть достаточно много проблем, и часть этих проблем решать очень тяжело. Одна из самых известных утилит, которая нужна для того, чтобы это тестировать, это Jepsen. Пробовали ли вы свои консенсус-алгоритмы тестировать Jepsen? Мы пробовали их тестировать до того, как я присоединился в компанию, была попытка использовать Jepsen, но это не... То есть, Jepsen достаточно сложный тул для эксплуатации. То есть, это не так, что какой-нибудь АБ взял, запустил, и он тебе дал ответ. Все проходит или нет. Нужно дизайнить систему fault injection по твою систему. Нужно учитывать топологию кластера. Например, Jepsen по умолчанию рассчитывает, что все ноды в системе гомогенные, а в некоторых системах это не так. Например, в кафке есть кафка и есть зукипер, поэтому нужно дизайнить fault injection под конкретную систему. И у Кайла, который занимается тестированием, у него обычно на новую систему уходит около трех месяцев, чтобы покрыть ее Jepsen. У меня была такая история с Microsoft и сейчас с Victorize, что люди начали использовать Jepsen, но не довели его до конца и забросили. И когда я присоединился в Cosmos DB, я начал использовать Jepsen, доводить это до конца, но в какой-то момент количество кода, которое необходимо было написать для поддержки Jepsen в Cosmos DB превысило код самого Jepsen и в какой-то момент я понял, что писать все однокложе непродуктивно, потому что в команде только я могу понимать этот код. И в Microsoft написал систему аналогичную Jepsen на C Sharp, а в Victorize это сейчас написано на питоне. Поддерживает Jepsen на питоне. Ну, с одной оговоркой, с чем я реально сталкивался, когда пишу достаточно большой сют, у питона есть такое неприятное свойство, что если ты опечатался в названии метода, например, или процедуры, взорвется пост, то есть у тебя полчаса скрипт работает, а потом ты вызываешь процедуру, которая не существует, я упал. Это был, пожалуй, самый неприятный опыт и, наверное, стоит выбирать либо C Sharp, либо Go, что-то построже. Да, я согласен. Но отчасти к этому есть контраргумент, что тестирование постоянно гоняется, и поэтому это код, который постоянно в горячем состоянии, если там что-то не так, то очень быстро мы натыкаемся на эту синтоксическую ошибку. И в плане тестирования консистентности у нас есть, собственно, тесты, которые тестируют разные сценарии, например, network isolation, убийство всяких нот и задержка данных в сети, задержка данных на диске. И сейчас это stand-alone tool, и мы интегрируем это в систему CI, то есть у нас будет тестирование консистентности, гоняться на каждый комит. На самом деле, мне кажется, Денису имеет смысл немножко пару слов сказать про то, как все-таки, в чем отличие вашего конкретного, как, не знаю, я забыл, как у нас называется, как-то... ГБКЛИ. Да, вот, потому что у вас там есть довольно интересное отличие с того, что я читал, и как бы можно, конечно, слушать и отправить, если тебе не хочется пересказывать, ты можешь слушать и отправить просто на твой блокпост, но в целом было бы даже также интересно послушать, в чем там, собственно, почему получается так, как получается. У Джебсона, когда его писали, система проверки консистентности, это сложная задача, то есть это NPEG, сложная задача, и поэтому, если, как бы, эксперимент тестирования консистентности длится достаточно долго, то сложность проверки просто зашкаливает, и мы физически не можем проверить долгие истории. И Джебсон написан вот с использованием этого NPEG-сложного провератора консистентности. Но в компьютерс саинс люди как бы интересовались, то есть как с прикладной задачей, так и стратической, думали, как это можно улучшить, как можно тестировать более длительные истории, и в распозеленных системах этот топик стал актуальным относительно недавно, но до этого он был актуален в многопроцессорном программировании, то есть у нас есть ячейки памяти, нам нужно обеспечить, что доступ к этим ячейкам памяти он линейризуемый, и люди занимались тестированием консистентности задолго до распределенных систем, и одна из статей, которая описывает, как это делать, называется Testing Shared Memories, и в этой статье, опять же, там упоминается, что общий случай тестирования консистентности, это NP сложный, но если мы внесем в систему дополнительные ограничения, например, каждое чтение сможет однозначно идентифицировать, кто записал это значение, по сути это значит, что когда мы пишем какое-то значение, мы всегда должны добавлять ID к этому значению, чтобы помогли идентифицировать писателя, это как бы одно ограничение, то есть если есть это ограничение, плюс если у нас все записи идут через Comparense, то есть прежде чем мы запишем новое значение, мы проверяем, что текущие значения, допустим, версии на единицу меньше, то тогда запись проходит, так вот, если есть эти два ограничения, то проверка консистентности становится линейной, и мы можем гонять тестирование консистентности просто неделями, например, и это не будет требовать дополнительных ресурсов, и я инклиментировал этот алгоритм из этого пейпера. Одна другая особенность, все вот эти вот тестирования консистентности, они нацелены на Key Value хранилище или хэштаб лиц, ну в общем что-то с Key Value интерфейсом, а Kafka это Append-on-List, поэтому пришлось подумать, как это объединить друг с другом, и в итоге я использую Красную панду как просто лог транзакции, и поверх этого делаю Key Value интерфейс, который уже тестирую с помощью Gebecle. Окей, спасибо, я наверное передам обратно вопрос Иване. Я на самом деле читал ваши документации немножечко, и вот вопрос возникает сразу от чтения документации. Мы не раз уже в подкасте обсуждали то, что по-правильному, когда работаешь с операционной системой, возникает очень много проблем на тех уровнях, на которых я никогда не подзревал, что могут быть проблемы, особенно когда ты пытаешься делать это своё приложение высокопроизводителем, особенно когда ты хочешь сделать своё приложение отказу устрачивым. В частности, мы постоянно говорим, что ненадёжно в компьютере всё, и одна из самых сложных систем, на которую мало кто обращает внимание, это ненадёжность файловой системы. Начиная с того, что она сама по себе ненадёжна, заканчивая того, что если тебе что-то вернулось, ты вообще может быть иногда даже не до конца уверен, что, ну в смысле вернулся ноль, это значит, что всё в порядке, но на всякий случай проверка, что ты записал, то что ты записал, и то, что оно там у тебя найдётся. Вы используете XFSC, XFS как там она называется, файловую систему, и рекомендуете её везде устанавливать, и у вас вы лично работали или может быть слышали про какие-то статистические статусики, статусики, статусики с точки зрения надёжности, файловая система, и вот это всё. Ты размьютился, я подумал, что ты уже начал отвечать, я согнулся. На самом деле, на самом деле, я всё же задал какую-нибудь информацию нам дайте, почему XFS, почему, как оно вообще работает внутри, как вы с нагрузками и файловой системой? Я в красной памяти больше фокусируюсь на уровне протокола и алгоритмов, поэтому точно не могу сказать о деталях работы файловой системы, но насколько я понимаю, она поддерживает спарс-запись, когда мы можем, и out of order-запись, и это позволяет добиться большей производительности. Евгений, ты что-нибудь про это знаешь? Нет, я практически не работал со storage, и кроме того, что мы на самом деле поддерживаем любые файловые системы, мы просто рекомендуем использовать ZFS. Репанда будет на любой современной файловой системе работать нормально. В основном мы просто пишем логи и всё. ZFS? Мне казалось, там был XFS, нет? Нет, это ZFS. Ваня что-то другое называл. Вот сейчас прямо смотрю на requirements, здесь написано XFS. Must be the file system for the data directory. Возможно, что. Мы нашли неконсистентность в документации. Ладно, нет ответов, нет ответов. На самом деле отсюда сразу можно понять, насколько становится уже большим проектом. Если пришли два человека в подкаст, и они занимаются очень сложными вещами в проекте, и они не знают, как работает конкретная часть, тоже достаточно важно. Понятно, что это количество строчек кода переваливает за N, которое больше чем M, и понятно, что проект очень сложный. И отсюда возникает вопрос. На самом деле у меня здесь более тонкая подводка. То, что фактически, я смотрю то, что вы написали, вы написали свою тонкую настройку сети. Причем там можно даже что-то немножечко конфигурировать, но не слишком всё. Плюс своё управление памятью, потому что по умолчанию вы съедаете всю сразу доступную память при запуске, и потом начинаете с меньшей задержкой работать с этой памятью. Плюс вы сделали своё управление процессами через Cgroups и приоритетом процессов в системе. То есть фактически вы реализовали там рланг-виртуальную машину, докер, высахторную работу, я не знаю, куча всего. И система становится такой большой, сложной на стероидах, сразу понятно. И я понимаю, как написать подобные решения на рланге, но с точки зрения функциональности. И я понимаю, что это решение чисто на рланге, без написания самой виртуальной машины будет огромным. А вы написали ещё и саму виртуальную машину, вы написали всю обвязку. И это огромная работа. Это понятно, что прям очень масштабный проект. Насколько вы уверены в нём? Как вы тестируете? Как вы... Для меня очевидно, что люди должны вам очень сильно доверять для того, чтобы отдавать самое ценное, то есть данные. И как вы можете доказать, что вам можно доверить? Это уже больше про продажи, наверное. У нас для тестирования используются разные тулуи. Например, вот эта вот система ГБКли, которую я упоминал, для тестирования консистентности. На самом деле она занимается тем, что делает кучу совершенно разных fault injections в систему, смотрит, что консистентность не нарушена. Плюс как сайд-эффект от выполнения всех этих экспериментов у нас есть много данных о availability. То есть когда система... Что-то идёт в системе не так, и система становится недоступна. То есть у нас измеряется, насколько она была недоступна. И так мы сейчас интегрируем эту систему с CI. У нас будут настроены аллерты, которые будут просто смотреть на поведение системы и говорить, когда она начинает вести себя не так, как мы ожидаем, мы будем идти смотреть и тянуть это. Кроме этого, у нас есть система, которая называется Punisher. Мы выполняем... То есть гоняем систему в тестирование несколько дней и тоже активно вводим fault injections и смотрим, чтобы данные не потерялись. Кроме этого, существует Open Messaging Framework для оценки latency for output. Мы используем его для тестирования. Да, как-то так. Понятно. Но это больше про внутреннюю техническую часть. Как вы продаёте это? Насколько я понимаю, мы даём систему попробовать, и люди могут сами это вылурить и посмотреть, как мы перформим. Мне кажется, что всё, что ты говоришь, это абсолютно верно, и к данным нужно относиться очень-очень серьёзно, и мы это делаем. Но мы сравниваем себя с существующим решением. Существуют случаи, когда кавка работает не очень хорошо и может потерять данные, поэтому нам нужно просто быть немного лучше. Здесь мне с тобой сложно согласиться. Я имею в виду два пункта. Первое. Мы даём пользователю попробовать эту систему и проверить, как она себя ведёт. У нас кавка работала в продакшене два года, пока не сломалась. И что? Мне два года проверять вашу систему и надеяться, что она не сломается через два года. А второе это... А второе я забыл. Давай первое обсудим, пока вспоминаю второе. У нас наша компания недавно пыталась перейти аналогично с логами с одного решения на другое, в котором говорили полная замена API, полная поддержка этого API. Вы меняете одну систему на другую и начинает работать. Я не хочу задаваться в детали, но мы реально попробовали и на самом деле оказалось, что мы сильно экономим по количеству машин и как следствие мы экономим по количеству денег. И мы перешли. Мы на самом деле попробовали, запускали всё одновременно нескольких. То есть два кластера у нас было и всё заработало. Как сделать то же самое с данными? Ну, не знаю. Мне надо запустить раз в десять это дольше, для того чтобы быть окончательно верным. Потому что логи потеряются чёрт с ними. Что-то сломается в данных, я никак это не восстановлю. Иван, а можно повернуть вопрос? Я хочу узнать, как это действительно делать и как убедить пользователь, чтобы тебя убедило попробовать красную панду по сравнению с сказкой? А ни что. У меня есть вопрос. Нет, у меня есть ответ. Не вопрос. Я, смотри, лучший вариант такой. Вы приходите ко мне, вы говорите, что мы готовы сделать инсталляцию на ваши же данные. То есть вы тут параллельную ветку, вы сами всё сделаете, вы сами разберётесь, как эти данные у нас схенлиться. Вы это сделаете, покажете, что оно действительно работает, так чтобы я не тратила своей команды на это. Если вы докажете, самостоятельно это такая офигенно. Переключаемся. Понимаете? Именно это происходит, когда вы хотите купить Enterprise лицензию. Вот для чего она. У нас есть solution. Как это лицензия работает? То есть он именно этим занимается. Он по сути делает то, что ты писала. Я не знаю деталей, потому что это не является частью работы, но вот мне кажется, мы как-то похожим образом с клиентами и работаем. Ну то есть вот в этот вариант самый, ну мне кажется, классный. Так, чтобы моя команда этим не занималась или там занимались какие-то небольшое количество людей, которые вам помогают с этим как-то разобраться, возможно, вам чем-то помочь. Но в идеале так, чтобы мне не пришлось покупать 100-500 машин, потому что это реально дорого, если речь идет о большом кластере, потому что именно тогда мы начинаем видеть то, как красная панда бьёт кавку. Вот если у вас есть какой-то большой доступ к большому количеству машин, так что вы показываете, вот смотрите, вот ваш кластер, развернутый на инфраструктуре, которую мы менеджим. Вот мы показываем, что он реально классно и быстро работает. Вот примерные касты, которые вас ожидают. Мы это всё подняли за менеджеры. Вот в этот параллельная ветка работает. Пожалуйста, купите нас. Я посмотрю, могу тогда оценить. Примерно, я вижу реальные цифры, сколько это реально стоит, потому что на практике всегда получается, какие-то дополнительные вещи вырезают, возможно, ещё что-то показывается, а там будут реальные не слова, а такие конкретные эзории. Вот. Ещё бы я хотел добавить, что если при этом вы покажете, что этот же кластер, который вы сейчас устанавливаете, по сравнению с кавкой, будет выдерживать. В смысле, это не то, что он работает на пределе, то, что он может выдерживать гораздо большую нагрузку, потому что вы добавляете ему запас прочности, первое. Второе, возможно, разобрать какие-то предыдущие проблемы с кавка кластером. У них отвалилась сеть, и кавка кластер повёл себя так. Вы можете показать, что ваш проект поведёт себя лучше, хотя я не знаю, в чём может быть лучность, но это как бы от клиента зависит. Какие-то разобрать возможные вопросы, как ведёт себя проект в случае каких-либо проблем, которые чаще всего возникают у клиента. Классно. Мы будем делиться этой информацией с будущим, и мы действительно работаем с клиентом примерно так, как Света сказал, у нас есть несколько клиентов, мы разбираемся с их кластером, сравниваем, смотрим, как он себя ведёт, и со временем, когда мы договоримся с этим клиентом о том, что их можно называть, мы как бы опубликуем эту информацию. Я знаю, что вот это solution архитекты, это обычно, я не знаю, как это у вас устроено конкретно, но я знаю, как это работает у других компаний. Там Amazon, например, Microsoft, они работают с моей текущей командой, и они разбираются там с моими текущими проблемами. То есть, знаешь, это значит, что моей команде нужно занбордить нового человека, помочь этому человеку разобраться, но по факту моя команда будет сама имплементировать новый подход с помощью данного узнаний этого архитектора, что на практике показывает, что моя команда не может делать фичи, и мы теперь застряливаем в этом долгом мигрантном проекте. Миграционные проекты — это очень такие тяжелые, долгосрочные истории, которые занимают кучу, ну, намного больше времени, чем изначально ожидались, и вот это, то есть они не очень интересны, в принципе, инженеры не то, что вы очень любят над этим работать, и вот если вы придумаете, как сделать так, чтобы это было не больно, чтобы это было быстро, приятно, по максимальному автоматизировано, это было бы офигенно, но вот миграционные проекты — они такие, знаешь, гадкие, геморройные. Если вы можете это сделать круто, приятно, быстро и так, чтобы моя команда продолжала делать фичи, а вы как бы параллельно работали над этим, потом как бы нам передали, было бы просто офигенно. Мне кажется, это классный фидуэк. Света, спасибо. Я думаю, мы обязательно им поделимся с коллегами. У меня вопрос по поводу тоже такой немножко продуктовой. Вот вы работаете с красной пандой, вы уже понимаете, в чем плюсы, в чем минусы. Скажите, кому нет смысла использовать красную панду? Вот кто действительно не получит никаких бенефитов? Света, что такое говоришь? Бенефит получит абсолютно все. Ну, давайте будем аэропилотичными, давайте будем смотреть на продукт, а как только показывать его с двух сторон — позитивный и негативный. Вот скажите, кому не нужно его использовать? Наверное, если вы используете, по крайней мере, на данный момент, если вы используете Kafka Streams и KSQL DB, то данная функциональность не поддерживается, поэтому можно, наверное, не пытаться переходить, потом думать, почему все не работает. Потом, если вам надежность данных не очень важна и вы пишете все с Axe equal zero, тогда, наверное, тоже. Хотя я не уверен. Сложно сказать. Если вы используете репликации между датацентрами, то у нас это тоже еще не готово и находится в начальном этапе разработки. Круто. Спасибо. Спасибо за то, что честно рассказали о том, чего не хватает. А еще хотел бы я парочку дать своих комментариев. Мне было очень приятно читать вашу документацию, когда я видел идеи, которые вы используете. Например, идея о том, что у вас есть готовые преднастройки для большинства известных типов инстанцев AWS. То есть, вы устанавливаете на такой-то инстанс, поставьте какую-то галочку, и оно автоматически заработает наиболее оптимальном режиме. Вам ничего не надо больше думать. Это же офигенно. Мало того, что вы можете перечислить все, вы же знаете, как они работают, как эти инстанции между собой, как CPU, memory и диск, как они настроены уже там. Вы можете, зная ваш продукт, сделать идеальную конфигурацию. Это очень четко. И то, что у вас есть замечательные волшебные команды RPK Tune All и IO Tune, это тоже классно. То есть, вы, зная, как работает ваша программа, вы запускаете эту программу. Настроим мне все очень хорошо на текущей системе. Она настраивается очень хорошо на текущей системе. Конечно, хотелось бы знать побольше документации, написать о том, что там такое делается, потому что без этого знания сложно доверять этой системе, потому что надо все-таки под администратором это все запускать в суду. Но в целом, вот это прямо видно, что 21 век все-таки наступил. Это был наш цель. Мы передадим эти слова человеку, который этим занимался. И мы в будущем будем еще больше делать подобных интеграций с существующими оплаками. А еще такой вопрос. У вас там есть какая-то посылка анонимных данных, анонимной статистики про то, какие пути в алгоритмах используются. Я не очень понял, что там. Но там я в целом понимаю, зачем это делается. Плюс анонимизированная статистика про использование продукта. Мне такой вопрос, а на самом деле кто-то посылает ее или нет? Ну, то есть, я бы отричал лучше все к чертовой бабушке. А некоторым, наверное, даже нельзя ничего посылать. Честно говоря, у меня нет данных. Я только сейчас узнал, что она есть. Да, я тоже не знаю подробностей. Но насколько я помню, наши сейлзлы, которые пришли из QQRotureDB, они были очень за эту фичу и говорили, что это используется. До этого много информации. Можете рассказать немножко деталей вообще про компанию? Потому что это, как я понимаю, стартап, и не так много людей слышали о нем. Расскажите подробнее, что представляет собой культура внутри, насколько большой инженерный отдел, и как это работает сейчас, когда COVID, и вы же наверняка работаете удаленно, если можно. Наша компания была удаленная с самого начала, и инженеры разбросаны по Европе, Америке, Латинской Америке, то есть по всему миру. Компания достаточно небольшая сейчас, около, мне кажется, человек 15. Извини, какие все вопросы были, я потерял идею. Расскажите немножко про культуру компании. Можете наводить какие вопросы? Да, конечно. Если это стартап, интересно знать, как вообще команды взаимодействуют между собой, насколько инженеры могут принимать решения касательно каких-то больших вещей, что использовать в проекте. У вас есть какая-то, например, идея по поводу фичей какой-то, и вы берете и импрементируете это, или вы говорите, о, надо сначала обсудить с продуктами, узнать мнение SEO и CTO, имеет ли это смысл, вот как такие штуки делается?",
    "result": {
      "error": "API request failed: Error code: 400 - {'error': 'Trying to keep the first 5680 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}",
      "stack_trace": "Traceback (most recent call last):\n  File \"/home/andrei/Projects/podcast-shownotes/scripts/build_search_eval_dataset.py\", line 157, in generate_search_query\n    response = await self.client.chat.completions.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 1927, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1767, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1461, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1562, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': 'Trying to keep the first 5680 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}\n"
    }
  }
]