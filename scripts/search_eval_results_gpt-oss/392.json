[
  {
    "segment_id": "298cdb84-48cf-45d4-9b73-d666f4893780",
    "episode_id": "b0096f0a-b75e-47a0-8b71-23d25ce74828",
    "episode_number": 392,
    "segment_number": 14,
    "text": "равно salary плюс 100, where salary меньше 900, например, ну короче update какой-то поле, где какое-то условие по полю, прикол в том что в наивной реализации ты допустим идешь все к скану по таблице для простоты, вот идешь и смотришь то что у меня x он меньше чего-то, ага меньше, хорошо, значит я делаю его update, прибавляю например 100 и записываю новый картеж в таблицу, у нас MVCC, поэтому мы его не меняем in place, мы дозаписываем новый, правильно, вот и прикол в том что пока ты обновляешь и дозаписываешь картежи и идешь по таблице, ты можешь идя по таблице наткнуться на те картежи, которые ты только что создал, но ты про это забыл, и ты смотришь на них, они снова удовлетворяют условия и ты их снова обновляешь и записываешь еще более новую версию, я понятно объяснил? да, я про это помню, а забыл что это называлось Halloween Problem, а почему я так называю? потому что Paper описывающая эту проблему то ли было опубликовано в Halloween, то ли ее нашли в Halloween и потом опубликовали Paper, но то есть только поэтому, я все понимаю, я все пытаюсь понять, то есть потрясающее название, которое совершенно невозможно связать с проблемой, обожаю. ну хорошо, предложил лучшее название, проблема двойного обдета, Tuple bleed, Tuple bleed, ну такое себе, вот, вот, но прикольная проблема тем, что она очень, ну, ее очень легко объяснить и про нее очень легко забыть, и получается что тебе вот при обдетах, тебе нужно прям запоминать какие кортежи ты трогал, чтобы их два раза не потрогать, вот, ну, достаточно неочевидные мысли. вот, в принципе это я в принципе рассказал 11 лекцию, вторая лекция, она мне понравилась меньше, она такая типа вот у нас есть процессы, вот у нас есть треды, процесс используется в депету Ораковой в Пазгрессе, в основном по историческим причинам, потому что на момент разработки этих систем не было более портабельного интерфейса, как например сейчас петредс, а вот треды используется и согласно этому докладчику, в депету Ораку, Microsoft Skill Server и MySQL, почему депету и Ораку попали в оба списка, я не совсем понял, то есть они попали в список и процессов и тредов, ну и ладно. Дальше дается... Я не знаю насчет депету, Орак для меня кажется там все очень хитро, типа там у тебя запускается отдельные сервисы на что-то, а внутри сервисы у тебя может быть много подключений, что там как-то заморочено. Дальше дается немножко запутанная терминология про виды параллельного исполнения, вот вопрос на засыпку, уважаемые эксперты по базам данных, вот у вас есть план исполнения запросов, вы хотите его исполнить параллельно, не подглядывая в шоу ноты, какой способ распараллелить запрос вам кажется наиболее естественным? Распилить, наверное, какие-то ноты, которые хорошо распиливаются по воркерам и типа скатер гатор короче. Правильно ли я тебя понял, ты предлагаешь взять дерево план исполнения запросов, сказать что вот у нас, ну например вот те три листа точно могут хорошо исполнится параллельно и запустить их на разных ядрах. Ну да, перед этим сделать что-то, нужно каждый при этом лист превратить, поставить перед листом скатер, после листа гатор, ну или типа если можно несколько таких секций объединить, то типа возможно скатер, потом параллельно-параллельно-параллельно, потом гатор, результаты. Но для меня это тоже более-менее такой естественный способ, но докладчик он предлагает аж три способа распараллелить. Его терминологию я не буду здесь пересказывать, потому что вот он говорит что есть интро-оператор параллелизма, есть интер, не путать, то есть первый был интро, а второй интер-оператор параллелизма. И притом интро-оператор, он предлагает два разных варианта, в общем я пытался понять разницу, я не преуспел, вы если хотите потом посмотрите слайды и разберитесь, мне кажется это не то что нужно знать. В одном из случаев предлагается ввести дополнительный эксчейнч-оператор и предлагается вот например у тебя есть запрос селекти звездочков, ромтейбо, вэр, там и условия. А ты можешь и допустим мы знаем что этот запрос он исполняется с чек сканом, ты можешь взять хип, взять хип, вот конкретно его кусок ответственный под эту таблицу и сказать что вот я делю его там например на три части и вот этот кусок хипа сканирует один цепу, вот этот кусок второй цепу, другой кусок третий цепу, а сверху у нас есть тот самый специальный эксчейнч-оператор, который эти данные соберет и пушнет наверх. Естественно это будет хорошо работать когда у тебя эти куски хипа они лежат, ну в идеале им стоит лежать на разных физических дисках благодаря рейду или table spaces, о которых тоже говорится в этом докладе, но отдельно. Дальше предлагается что вот то что ты Валера описывал про запустить по ядру на каждый узел в плане запросов, но при этом в одном случае вот этот тот где я запутался говорится что это интероператор параллелизм, да, а в другом случае что это интрооператор параллелизм, но при условии что у нас данные собираются в concurrent hash map. Ну собственно это вопрос того как делать scattergather, то есть я типа очень на пальцах это рассказал, но смысл в том что у тебя scattergather может быть или на каждый оператор, ты перед оператором берешь и все раскидываешь всю работу, после оператора берешь и собираешь, скан ты в этом случае в этом случае распараллелить тривиально, ну сложновато. Второй вариант ты берешь и как-то по умному плану анализируешь и смотришь к тебе что может делать параллельно, так это нужно здесь не вводить exchange-оператор, потому что ты прям план переписываешь в параллельный план, это собственно то как всякие Sparky работают совершенно точно, не знаю как Postgres в этом плане работают, Sparky совершенно точно, ну и другие параллельные базы данных, они прям когда если смотреть про них пейперы, я вот один разбирал полгода назад, там прям вот в формулках релиционной алгебры используется exchange-оператор, потому что тебе про эти планы потом хочется какие-то, не знаю, какие-то свойства доказать, например, что ты в своем потрясающем новом способе переписать план не ломаешь того, что ты не теряешь строк и не добавляешь строки, да, но в принципе с точки зрения имплементации все еще остается scatter, gather и какой-то, не знаю, оператор, который пересылает данные с одной на другую. Я тем временем нашел слайд, где объясняется почему Halloween Problem так называется, она была обнаружена ребятами из IBM, когда они разрабатывали C7R на Halloween 1976 года, вот такой занимательный факт. Вот, ну и в принципе я все пересказал, еще во второй части про исполнение запросов, до кучи было докинуто, что бывает партизирование, партизировать можно по строкам, можно по колонкам, ну и вроде как все. Вот, лекция мне пока нравится, смотрю с большим удовольствием, всем рекомендую вопросы, возражения, комментарии. И тогда у нас тема в закладке, статья называется используйте один большой сервер, я ее когда читал, выписал тут два экрана каких-то заметок, мыслей и вообще думал, что это будет большая статья на обсуждение, но дело было три недели назад, естественно все забыл. Вот, в памяти отложилось в основном то, что автор говорит, что вот вы в Amazon заходите и вы можете за какие-то разумные деньги, там порядка 1200 долларов в месяц, 1300 долларов в месяц, я точно не помню, купить сервер, у которого одной только оперативной памяти терабайт. И если вы там посмотрите на характеристики этого сервера, он вообще эквивалентен суперкомпьютеру 2010-2008 года, ну и дальше он идет по разным более дешевым предложениям, что в хессере можно памяти поменьше, но и подешевле и так далее и тому подобное. В суммам посыл такой, 1200 долларов в месяц это та сумма, которую наверняка любая нормальная компания, даже самый-самый начальный стартап может себе позволить, вы берете терабайт памяти и пихаете туда все свои данные и вы можете вот просто все свое приложение написать на Go, например, и просто держать state в памяти и время от времени его слушать на диск как вариант. Мне кажется, что у многих серьезных, я вот не знаю, честно говоря, вот за timescale данные клиентов, которые вот прям вот в базах данных вряд ли по месяцам терабайт, но вот мне и у меня нет точных цифр, но вот мне интуиция подсказывает, что если взять вот метод информацию, которые вот сама компания оперирует, то она достаточно легко в терабайт помесяц и у какого-нибудь, ну вот с магазином типа Azone.ru там посложнее, потому что у него есть фото-видеоконтент, его наверное все-таки частично присоединить на диске держать. Вот, но посыл такой, что в терабайт влезет очень много не самых маленьких бизнесов, которые вот даже на roundb, roundc и так далее, кто скажет мне ответственно такой вопрос? Вот я вот все расписываю, да, там положим, стоит в оперативной памяти и так далее и тому подобное. Внимание, вопрос, где я вас наебываю? То, что только ты стоит в оперативной памяти засунул, ты охренеешь отлаживаться. База данных это не только ценный мех, это еще и инструментарий. А я пишу, на Ирландии у меня интерсфекция. Неважно, я занимался такой же херней, которую ты сначала описываешь, иметь отдельную базу данных для хранения данных, то есть если ты не разрабатываешь базу данных, то лучше использовать, если это не основная твоя задача, разрабатывать основную базу данных, то лучше пользоваться базой данных, в которой, собственно, даже когда я разрабатывал базу данных на Ирланде, это было не с нуля, а в существующем базе данных с существующим инструментарием. Именно потому, что ты хочешь как бы иметь какие-то ручки, рычаги для, чтобы в случае чего проманипулировать данными или посмотреть, что вообще лежит и черт возьми, как это, спасибо, что есть SQL. Ты отчасти прав, но мне кажется, это не основная причина, почему я вас наебываю. Я бы еще добавил, что один сервер это всегда единая точка отказа всего. Нет, ну, естественно, ты понял вариант второй, сервисировался в него, но все как можно. Только ты поднял второй, это уже гораздо более сложная проблема. Я, кстати, с тобой, Саша, про это как раз спорил, что автор изрядно упрощает. Мне очень импонирует в целом посыл статьи, что может быть микросервис и кубернетис и прочее безумие это как бы тумач для большинства людей. Это я скорее всего согласен, но, короче, прямо уж совсем на baremetal и менеджить даже два сервака, это вообще нетривиальная задача, это full-time работа. Ну, может быть, два не full-time работа, но, короче, как минимум, кто-то должен демпиратически заниматься. А вот я с тобой в корне и согласен. Вопрос не только менеджить, вопрос сделать, чтобы твое приложение было готово к этому. То есть, если у тебя приложение готово к кубернетису, это одно, но если ты хочешь работать с памятью напрямую и у тебя, как бы, ты сам пишешь себе базу данных, а потом оказалось, что у тебя два сервера, я вот не представляю себе, как вот эти запросы распараллелить. Какой-то hot swap у тебя, вот если ты... Не-не-не, имеется в виду, что он просто вормбакап. Он он просто... Даже вормбакап тебе сделать это не так просто. Если у тебя все в памяти, у тебя нету никакого протокола синхронизации, как у тебя база данных, это... Ты, возможно, не использовал манезия. Ой, блять, я использовал. Я использовал, лучше бы не использовал. Я набрасываю, естественно. Нет, я совсем согласен, кроме того, что говорит Валера. Валера, я с тобой в корне и согласен, потому что то, что автор пишет, что якобы микросервис и кубернетис не нужен, это он фигню полную говорит, потому что у тебя даже в маленьких компаниях есть больше одной маленькой команда, и они хотят писать микросервисы, потому что хотят изолировать логику друг от друга, и у каждого микросервиса должна быть своя маленькая базюлька, то есть это явно не один сервер будет. Ты в какую-то сторону споришь? Нет, я со всех сторон спорю. По сути такой, что маленьким компаниям им намного важнее фичекатить раньше своих конкурентов, быстрее занять рынок, на который они ориентировались, и вообще наплевать на производительность намного важнее, чтобы оно не имело багов, не падало и так далее. А микросервисы? Да, микросервисы это огромная overhead, потому что у тебя любой запрос, это SQL, которую СОБД должна получить по сети, пропарсить, подумать, а как мы это будем исполнять, медленно сходить в диск. Микросервис — это еще и самый проклятый способ заставить команду просто работать в 10 раз медленнее. Это то, что ты сейчас пичал, это то, чему микросервисы мешают. Подожди, я просто пытаюсь к чему подвести, что база данных медленно прочитала с медленного диска, потому что у тебя же нужно много баз данных под каждый микросервис, это дорого, значит они работают на медленном EBS. Ты прочитал данные, серилизовал их, послал по сети, клиент распарсил и отправил опять, серилизовал и отправил по сети, ответ другому микросервису, понимаешь, а тот еще раз десерилизовал. Микросервисы это супер-супер медленно, это огромное прожигание ресурсов в пустую, и всем наплевать, потому что важно захватывать рынки, а не то чтобы у тебя твой один сервер супер быстро работал. Вот когда ты... Повторюсь, короче, микросервис, а то, что тебе мешает захватывать рынок, именно потому что как это колбасить код по-быстрому в монолите, короче, из говна и палок — это сильно быстрее, чем колбасить микросервисы. Просто потому что каждый очередной вызов между микросервисами — это прям вместо, не знаю, одного полреквеста — это минимум два полреквеста и, не знаю, недели согласования. С одной стороны... С одной стороны, микросервисы и отдельные команды — это вот прям как это... Не обязательно экспоненциональные, но, короче, это степенной взрыв сложности взаимодействия между программами и человеками. Я с одной стороны согласен, с другой — нет, потому что люди, которые пишут эти микросервисы, они тоже не глупые, и они... Вот этот монолит, который можно по-быстрому наглепать, они через три года не хотят поддерживать, они хотят остаться в этой компании и еще спокойно работать. Если мы спорим про стартап, которому нужно рынки захватывать, если писать микросервисы, можно не дожить до через три года. Мне кажется, реально столько колес разработки может быть меньше. Это мы с тобой просто оба работаем. Мы с тобой оба, и Денис тоже, в общем-то. Короче, так получается, что у нас много кто в подкасте, участников подкаста сейчас, мы работаем в компаниях, которые... Или в командах, внутри компаний, которые занимаются платформами или, так скажем, слоем, который можно, который лучше бы разрабатывать медленно. Но как бы мой опыт работы в продуктовых компаниях говорит мне, о том, что микросервис — это одна из худших просто решений, которые можно принять в плане скорости разработки. Если тебе важна именно скорость разработки. Поэтому я вообще этот аргумент не покупаю просто не на миллиграмм. Ну если у тебя большой продукт, как ты без микросервисов разделишь область и ответственность? Давай так, первые три года у тебя еще нет команды на 100-500 человек, которые нужно сильно делить. Первые три года, пока ты работаешь втроем, микросервисы не нужны. Так Саша же именно про это говорит. Нет, в моем представлении, даже в первый год у тебя команда уже далеко не три человека. Нет, естественно, если у вас три человека, то да, пилите свою монолит и все у вас будет нормально. Мне кажется, что меньше 20 человек вполне может работать в одном репозиторе над одним монолитом. Там же начинаются еще истории про время прохождения сиай, катить разные фичи асинхронные и всякое такое прочее. Ну, то есть там есть что обсудить, но не в этом выпуске, потому что мы так уже к трем часам приближаемся. У меня по этой теме все, статья понравилась, рекомендую почитать. Я скорее одобряю, но автор сильнее набрасывает на облака, чем мне бы хотелось видеть. Если вы такой важный middle ground, вы все еще можете менеджить, не запускать миллион маленьких серверов, а взять две большие машины, но оставить их management на платформу, купить их не как машины, а купить их как Kubernetes на этих машинах и затеплоиться в этот Kubernetes, например. Или еще лучше вместо Kubernetes взять какую-то менее абстракцию, как кажется Elastic Beanstalk называется у Amazon и App Engine у Google. Может быть в сразу здесь не подходящие критерии, но абстракция, которая позволяет вам приложение deploy, а не server managed. Мне в этой статье понравилась мысль, на которую она водит. Например, с одной стороны люди набрасывают, что вот у нас такие быстрые NVMe диски, которые переносят bottlenecks диск в кэш файловые системы, и поэтому наши будущие базы данных это чистые memory базы данных, всякие такие вот такого плана наброса. С другой стороны люди не дают себе отчета, насколько всем наплевать на производительность. Блин, люди пишут микросервис на Python или, не дай бог, на Java, которые там взаимодействуют с другими микросервисами, все время серилизуют туда и обратно. Надо держать в уме мысли о том, ну две мысли. Первая, насколько людям на самом деле наплевать на производительность, а вторая, насколько можно ускориться, если взять все ваши микросервисы, выкинуть их нафиг и реально положить все в память одного большого жирного сервиса, пусть даже на голову будет код, не носи, не носи плюс-плюс, но вы можете огромный при распроизводительности получить достаточно понятным рефакторингом. Я сейчас еще, короче, страшнее вещь скажу, можно все оставить на Python, просто логику не особо на Python писать, а побольше ее фикулизовывать и поставить одну большую жирную базу данных, которая написана всем, чем нужно, и вот просто по максимуму сложных каких-то манипуляций спихивать вот в нее, пусть она там шуршит на большой-большой-большой машине. Тебе сейчас бывалый гофер, знаешь, что на это ответит? Я просто знаю, потому что я реально там со знакомыми гоферами на эту тему общался, и мое первое предложение было сложить все в хранимке, знаешь какой ответ? Это ж медленное? Нет, нет, нет. А какое, это ж сложно, я не знаю SQL. Нет, нет, нет, все проще. Ты это хрен отладишь. В хранимке хрен отладешь, а вот если SQL запросы, то у тебя в любом случае есть SQL запросы. Ну я согласен, что программировать на хранимках это сложнее отлаживать. Ну, у меня, понимаешь, люди сразу скатываются в хранимке, а я не вижу, то есть я сам, конечно, адвокат хранимок, но я адвокат хранимок для какосмогательных функций, я не люблю в хранимке писать, пихать прям бизнес-логику, если мы не говорим про какие-то специфичные расширения.",
    "result": {
      "error": "API request failed: Error code: 400 - {'error': 'Trying to keep the first 5156 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}",
      "stack_trace": "Traceback (most recent call last):\n  File \"/home/andrei/Projects/podcast-shownotes/scripts/build_search_eval_dataset.py\", line 157, in generate_search_query\n    response = await self.client.chat.completions.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 1927, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1767, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1461, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1562, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': 'Trying to keep the first 5156 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}\n"
    }
  }
]