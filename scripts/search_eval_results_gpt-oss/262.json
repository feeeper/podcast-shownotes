[
  {
    "segment_id": "a5e36f28-b6e7-4bdb-9110-cbd13cfce8ac",
    "episode_id": "3d4aea2e-b2f3-428e-acae-35bf3c8c6128",
    "episode_number": 262,
    "segment_number": 4,
    "text": "Нет, нет, давай начнем с другой. Давай, ты рассказывай. Во-первых, очень хороший пример, который приводился в пейпере и в видео, заключается в том, что представьте, что у вас есть функциональность сервиса, который собирает какой-то ответ, а в тот момент, пока он собирает ответ, он крутит вам какое-нибудь видео на вашем сайте. И вот представьте, что если вот это кручение видео была бы отдельная функция, и вы внезапно после профилирования поняли, что эта функция выполняется дольше всего, она выполняется всё время, пока пользователь ждет ответ, давайте мы её будем оптимизировать. Если вы не знаете структуру программы, вы бы так, наверное, бы и сделали, глядя на профиль обычный. А по факту она будет крутиться только до тех пор, пока вы не соберете все ответы и не выдадите их пользователю. Профилирование этой функции вообще никак не поможет, вообще ноль. И идея как раз в том, что была у разработчиков, что если у вас есть сложный граф выполнения, то найти в этом графе такие участки, ручная оптимизация которых приведет к наибольшему, даст наибольший выхлоп. То есть если вы будете оптимизировать функцию, которая крутит вот эту вот радугу, это ноль. А если вы внезапно заоптимизируете какую-то соседнюю функцию, это может дать огромный выхлоп, хотя по факту вы потратили одно и то же время. То есть это идет поиск самых узловых и самых важных точек. И сделать они это предлагают нестандартным методом. То есть теоретически надо бы построить граф всей вашей программы. В каждой точке графа надо посмотреть, а давайте мы попробуем ускорить вот эту точку графа. И когда мы ускорили эту точку графа, посмотреть, а повысилась ли производительность вашей системы. Как измерять производительность системы – это вообще отдельный вопрос. Он тоже здесь покрывается немножко. Но допустим вы замерили, вы поняли, что вот эта точка на самом деле ускоряет. Если ее ускорить, то вся программа ускорится. Но ведь у вас нет возможности в реальной программе такое сделать. А вот этот профиль, казуальный профайлер, он именно работает с готовыми программами уже. И единственный способ это сделать – это не ускорить данный маленький кусочек, а замедлить все остальные кусочки. Я считаю, это вообще прорывная идея, которая и выстрелила в этом папере очень сильно. Работа заключается в следующем. Фактически, вы строите, не вы, а профайлер, который запускает вашу программу. То есть вы запускаете не саму программу, вы запускаете профайлер, которому говорите, как запускать остальную программу. Профайлер стартует thread, который сперва пытается разобраться в вашей программе, что там за программа, где есть дебажная информация, где исходники лежат, пытается что-то подстроить, какие-то графы, все что угодно может сделать. После того, как он с программой разобрался, он стартует саму программу, стартует все трэды программы и понимает, кто и где и как работает. Дальше он через какие-то равные промежутки по умолчанию 1 миллисекунды делает сэмплирование того, где находится каждый трэд исполнения. И потом он может сделать как раз вот эту магию, которая заключается в том, что мы попадаем на нужный участок, который мы хотим как бы ускорить. И мы говорим всем остальным трэдам, которые работают в данном случае под этим профайлером, мы говорим так, вот этот кусочек мы сейчас ускоряем, поэтому давайте-ка мы все остальные замедлим. И это будет иметь ровно тот же самый эффект, потому что если все остальное стало работать медленнее, это значит автоматически это стало работать быстро. И они там доказывают математически, почему это происходит, почему это правильно, даже с учетом того, что есть какие-то взаимосвязи, даже с учетом, когда у вас одна и та же функция выполняется в разных трэдах, и даже с пересечением и так далее. То есть как бы это все они делают. И суммарно получается, что они замедляют остальные трэды, и они смотрят на граф выполнения программы. Когда ты говоришь замедляют, ты имеешь в виду, что они слип 100 мс курс вставляют или что? На самом деле там у них сложнее логика, я до конца так и не разобрался, а в код и не посмотрел. Идея в том, что они не могут делать чистый слип, они добавляют батч из слипов. Почему именно батч из слипов, я так и не понял до конца, но идея примерно такая. То есть они фактически ставят слип на все остальные трэды, если данный трэд в момент этого сэмпла попал в тот участок, который мы хотим протестировать. Закономерный вопрос. Не проще было взять трэд, который мы хотим ускорить, и сделать ему финиш на одно ядро, чтобы работа других трэдов на него не влияла. А здесь идея не в том, а представь, что у него уже и так достаточно ресурсов для того, чтобы выполняться. То есть идея в том, что если ты предоставишь пользователю результат такой, вот у тебя есть функция А, есть функция Б. Если ты функцию Б улучшишь на 15%, у тебя вся функциональность всего твоего кода увеличится на 20%. А если ты увеличишь функцию А на 15% производительность, то у тебя весь код будет выполняться с той же скоростью, это никак не повлияет. Если ты будешь Affinity изменять, то ты просто дашь чуть больше ресурсов, чуть меньше ресурсов, возможно это вообще никак не повлияет. Им надо было иметь более точные результаты, чем возможное влияние Affinity. Это очень запутанная история. Вот ты сейчас звучал как точно не Саша. Я вот сейчас даже не уверен, что ты Саша. На моем ютубе есть мультик, называется «Девочка в джунглях». Это старый советский мультик, очень всем советую. И цитата оттуда. Там вот есть персонаж, который говорит, что это очень запутанная история. Понял. Значит, вопрос, как считать производительность, они покрывают следующими возможными вариантами. Во-первых, если вы можете исправить свою программу, то вы можете исправить свою программу, добавив туда отдельный макрос. Я так предполагаю, что вы на C++ программируете. Авторы тоже так предполагают. Соответственно, вы вставляете туда макрос и говорите, вот эта точка, это точка, которую надо мне померить. Фактически вы будете считать количество проходов через данную точку в данной функции. То есть вы будете тестировать, насколько увеличивается throughput. Вы можете также сказать, а вот здесь у меня есть кусок кода, не кусок кода, а кусок исполнения. Вот это начальный период времени, а вот это конечный период времени. Вот эти два куска. В смысле, вот эти две точки. И профайлер будет пытаться оптимизировать не throughput, а будет пытаться оптимизировать время прохождения от начальной до конечной точки. То есть фактически будет оптимизировать latency. Это если вы можете исправить свой код. Если вы не можете исправить свой код, то вы можете задать информацию в профайлер вида. Я вот хочу, чтобы ты мне проверял throughput, как выполняет на вот этой точке кода. Ссылаться либо через исходники, говорить ему вот такой C++ файл, вот такая строчка, либо через функцию. Из-за того, что дебажная информация в вашем коде есть и плюс у вас есть исходники, он может построить какую-то информацию. Но я не тестировал, сам не пробовал, не проверял, как он это делает. То есть как бы у утилиты, которая запускает, у самого профайлера есть параметры командной строки, чтобы вот эти источники задавать. Не источники, а фактически точки в программе. И получается с помощью вот этого, с помощью вот этих указаний точек вы можете сказать, что именно ему надо тестировать, что именно надо увеличивать. Либо производительность, пропускную способность throughput, либо latency какого-то участка кода. И дальше программа это делает случайным образом. Вот реально случайным образом. Она каждую, подождите, она каждую миллисекунду делает sample, она смотрит, где находятся трэды и случайным образом определяет, нет, нет, начинается не так, прошу прощения. То есть она сперва определяет, какой кусок кода она хочет протестировать, а потом она каждую миллисекунду проверяет и смотрит, попали мы в тот кусок кода или не попали. Если попали, мы начинаем все остальные трэды паузить, если один трэд попал. И так мы делаем, пока не наберем нужное количество случаев для того, чтобы можно было с какой-то более уверенностью сказать, что вот эта статистика верна. То есть, по-моему, по умолчанию они считают статистику более-менее правдивой, если больше пяти раз мы попали с сэмплом в этот участок. Если попали меньше пяти, они ничего не могут сказать и не говорят, соответственно, информацию не выдают. И, соответственно, чем больше программа ваша работает, тем больше получается точный результат, потому что, скажем, если вы сэмплируете каждую миллисекунду, то может так получиться, что вы в ваш маленький участок кода вообще не будете попадать. Или, например, когда вы туда попадаете, на сэмплы не пришлось, и когда вы уже вышли, пришел сэмпл, вы даже замерить не можете. В общем, получается, что профилирование вероятностное, и чем дольше вы набираете статистику, тем выше точность этого профилирования. В результате получились очень хорошие цифры по улучшению производительности на разных программах. Сейчас я скажу точно. Они сказали, что, руководствуясь вот этим профайлером, мы смогли оптимизировать производительность memcache на 9%, SQLite на 25%. Есть такой тест производительности parsec приложения, и они смогли там до 68% улучшить производительность. И чаще всего эти изменения, изменения исходников, которые понадобились для того, чтобы эту производительность улучшить, были меньше, чем 10 строчек кода изменения. По-моему, вообще офигенно. Я имею в виду, что я бы не ожидал, что SQLite, после того, как я знаю, сколько там строчек тестирования, сколько разработчики тратили время на профилирование и улучшение производительности, его можно так вот просто меньше, чем 10 строчек, потратив на 25% улучшить. Это прям офигенно, я считаю. Я не ожидал, то есть я предполагал, что можно хорошие результаты получить, но не такие. Есть в пейпере, прям на второй страничке, если вы откроете, можете посмотреть, есть пример отличный, который показывает, какие есть графики вывода после работы этого профайлера. В частности, приводится программа, которая, в принципе, ничего не должна, ее должно быть сложно улучшить производительность, и она приводится как пример для того, чтобы показать, что обычно профайлеры дают информацию недостаточную для того, чтобы принять решение, стоит оптимизировать эти функции или нет. А как раз вот этот casual профайлер, коз, как они его называли, он показывает, что даже если вы улучшите любые куски кода, насколько это угодно, то вы больше, чем 5-6% в пике вы не сможете выжить из-за оптимизации этого куска кода, что в принципе соответствует реальности. И это очень удобно, потому что это необходимая информация для того, чтобы понять, стоит ли вообще тратить усилия на оптимизацию приложения. В качестве вывода, я не знаю, как это сказать, результата всей этой работы, группа, это же не ученые, как это правильно сказать, или ученые, computer science, да, это тоже ведь наука. Исследователи. Группа исследователей, они дали вот этот paper, который очень интересно почитать, я прям советую. Они дали вот эти результаты работы, они прям очень четко описывают, что именно они поменяли, в какой программе, как они SQLite оптимизировали, как они профилировали, оптимизировали, вот эти все приведенные примеры. И они дали утилиту, которая называется Koz, она находится на GitHub, и вы можете прям сами ее у себя позапускать и посмотреть. Я считаю, что просто отпад, реально отпад. Что скажете? Я всецело поддерживаю. Мне стремно в прод тащить. Зачем тебе это в прод тащить? Потому что он должен в проде работать. Не обязательно, ты можешь это запустить на тест в твоей среде нагрузочной. Ну, тоже верно, но там же искусственная нагрузка. А, Саш, чтобы тебе немножечко добавить информацию, они сказали, что если вы это будете запускать на реальных нагрузках, у вас, конечно, будет более точная информация, но информация о профилировании и оптимизации возможной. Но что самое интересное, что не очень большую добавляет нагрузку сама программа, потому что, ну, я бы ожидал, что вот этот профайлер тяжело будет, и не стоит его запускать в продакшене. Они говорят, что минимальное увеличение производительности это было 0,1%, максимальное 65%, а мин – это 17%. То есть, я думаю, что если, скажем, у вас есть 20 нод, то на одной из них запустить такую штуку, ну, да, будет в полтора раза в пике медленнее, но это не так страшно. Они как раз очень хвастаются тем, что по сравнению с любыми другими профилировщиками, с GoPro, они в частности очень много сравнивают, то очень выигрышная получается, очень выигрышная цифра. А с Go работает? Хороший вопрос. Теоретически, не знаю, теоретически должна, но кто знает, надо проверять. Стремно тащить это в прод. Согласен, запускайте сначала на нагрузочном тестировании. Или можно оптимизировать по старинке. Чем мне не нравится этот профайлер? Тем, что он предполагает, что у вас все это либо один большой бинарь, либо бинарь с библиотеками, которые вы подключаете во время работы этого бинаря. Меня интересуют сейчас уже более сложные юзкейсы, вида. У меня есть пять микросервисов, которые каким-то образом работают, и мне нужно понять не то, какой из микросервисов мне надо оптимизировать. А, нет, наоборот, не то, что в отдельном взятом микросервисе, что оптимизировать, а вообще на какой из них сначала надо посмотреть внимательнее, потому что вот эти критические пути и так далее, все это надо смотреть уже на уровне более высоком. И это уже становится не так очевидно, особенно, когда их не пять, а 105. Я также, насколько я понял, оно профилирует по ЦПУ, внезапно тормозить может сеть, внезапно тормозить может диск, и, как правило, по крайней мере, на тех проектах, которыми я занимался, я очень давно… то есть я помню, что подгруз упирался в локе, например, я помню, что какие-то процессы упирались в память, какие-то микросервисы, а вот так, что вот прям конкретно ты тупишь в ЦПУ, я такого вообще не могу даже помнить. Саш, но то дело, что оно как раз не по ЦПУ, а в этом же и прикол, что там авторы делают как-то заявление, что большинство профайлеров профилируют по ЦПУ, и это бесполезно, давайте делать по-другому. Как раз в этом идея подхода, что ты делаешь, как бы вставляешь задержку во все, кроме какого-то кусочка, и смотришь, насколько у тебя… то есть ты отвечаешь на вопрос, насколько ответ сервиса ускорится, если вот этот кусочек составляния ответа будет быстрее. При этом учитывается, какие вещи, которые работают в параллель. У тебя лок моделируется тем, что что-то медленнее работает. Там даже в примерах есть как бы лок. Ты послушаешь, нет? В случае с лок контеншеном, никакие задержки не помогали. Нужно было шардировать лок и понимать, кто зачем выберет. В случае с сетью, никакие задержки не помогали. Нужно было выкидывать… на данный момент использовался СИБОР, и нужно было переходить на что-то, что не хранит ключи в твоих данных. Нужно было переходить на протобавл. В случае с диском, ну, ситуация аналогичная. Нужно было понимать, почему мы пишем лишние данные в диск. И никакие вот эти ухищрения с притормаживанием тредов не помогут. Почему? Ну, смотри, давай, у тебя есть какой-то кусок, который пытается писать в диск. Я правильно понимаю? В случае, про который я рассказываю, там была конкретная проблема, что объем данных был излишен. Можно было половину данных не писать. Нужно было сесть, посмотреть на код, внимательно подумать, почему мы пишем то, что не нужно, ну, и не писать это. Одно не исключает другого. Ты почему-то говоришь, что одно поможет, а друго нет. Я как раз хочу сказать, и Валера тоже самое пытается сказать. Ну, к примеру, вот с диском. Ты пишешь в диск, и у тебя запись в диск занимает очень много времени. Профилер этот бы тебе сказал, что в принципе ты можешь профилировать что угодно, но если ты вот этот диск ускоришь, то у тебя вся программа ускорится. То есть, ну, какое-то, понятно, очевидное знание, но оно не противоречит правде. Оно показывает, как на самом деле происходит у тебя. Ты, конечно, после этого идешь и смотришь, а что у тебя там записывается, почему так долго, и в принципе делаешь то же самое, что вы уже сделали. Аналогично с сетью, аналогично с локами. Оно добавляет тебе какую-то информацию к тому, что ты можешь узнать другими способами. Вот, да, именно узнать другими способами. То есть, я в целом понимаю пользу профилеров, но обычно ты узнаешь о узком месте каком-то, по метрикам на твоем Prode и том, как твои реальные пользователи, как реальные им пользуются, они как на стенде пришли к QA или как твой синтетический бетчмарк нагрузил твой Prode, который ты, конечно, пытаешься делать похожим на правду, но ты не знаешь, какие реальные пользователи придут и какую реальную нагрузку создадут. Поэтому, ну, по моей практике ты можешь, да, находить такие места, оптимизировать их, все такое, все здорово, но потом ты это раскатишь на Prode и выяснишь, что оптимизировал вообще не то, что нужно. Ну, смотри, на самом деле, смотри, скорее всего, как эта штука, ну, подполагается в использовании. То есть ты берешь, что-то выкатил на Prode, ты такой, ой, что-то оно медленнее отвечает, чем хотелось бы. У тебя там, не знаю, ты можешь, конечно, посмотреть на кучу метрик, там, 2 дня в них тупить, а можешь попробовать собрать, ну, воспользоваться этим профайлером и понять, какой компонент системы тебе наиболее выгодно оптимизировать. То есть это профайлер, который, по сути, берет, не знаю, все компоненты, которые там параллельно или последовательно участвуют в формировании ответа, в каких местах они синхронизируются и так далее, пытается вставлять задержки в разные места и скажет тебе, как бы, вот там, с учетом всяких вставлений задержек и попыток ускорять замедлять компоненты, скажет тебе, какой кусочек исполнения наиболее профитный для того, чтобы пытаться в нем что-то искать. То есть, как бы, оно не отменяет твоих слов. У меня, знаешь, в чём-то вопрос возникает. А как они строят этот граф последовательно-параллельного исполнения? То есть я считаю, что вообще вот эта задача построения этого графа, это вообще одна из самых сложных задач. Я не знаю, как они его строят. Я не уверен, что они его строят правильно для всех случаев. Я не уверен, что… Ладно, они не то чтобы его специально строят, они просто втыкают задержки в нужные места или в ненужные места. То есть независимо от того, какой граф, если у тебя эти компоненты… То есть если ты знаешь, что у тебя какой-то… Если ты знаешь, что если какой-то компонент тормозит, то тебе тормозит вся система, то ты в первую очередь пойдёшь в нем искать локи, проблемы с сетью. А, ну да-да-да, они же случайным образом определяют. Они не пытаются, глядя на граф, выбирать места. Они поэтому так написали, что они случайным образом… Да-да-да, тогда работает, согласен. То есть они вообще могут не понимать, что там за граф. Им пофигу, что за граф. Главное, что они вставляют задержки случайно во всю программу и смотрят, а что же получится. Да? Я не согласен, что… То есть для каких-то случаев работает, безусловно. Но в примере с тем же… Ну вот опять же из недавней практики у тебя есть rvlog. И ты видишь, что определенные запросы в определенных случаях обрабатываются дольше, чем хотелось бы. Методом вдумчивого курения кода и анализа логов ты понимаешь, что вот этот rvlog, его взяли очень много на read, и когда ты туда пришел на write, ты был вынужден ждать, пока все чтения закончатся. И как задержки помогут получить такой же вывод, что вот чувак, вот здесь у тебя rvlog, он вообще лишний на самом деле, он здесь даже не нужен. То есть я себе это с большим трудом представляю. Хороший вопрос. Наверное, такой use case тяжело вообще любым профайлером определять, потому что у тебя это не уровень зависимости по графу, не уровень зависимости по исполнению, а это фактически по данным, которые… Ну я согласен, здесь будет сложно. Я тут пытался… У меня, по ходу, микрофон выключился, нужно его, видимо, на зарядку поставить. Я что хотел сказать? Я хотел… Он у тебя опять не слышит. Опять выключился. А пока Валера не знает, что он хочет сказать, я хочу добавить… У меня батарейка садится, я знаю, что хочу сказать. Подожди секунду, я заряжу микрофон, поставлю его заряжаться и сейчас верну. Давай, давай, пока ты ставишь батарейку заряжаться, я хочу добавить к тему еще один paper, я его уже добавил сюда, он в шоу-ноутах появится. Это как раз тот paper, который я хочу, чтобы кто-нибудь реализовал, и пока никто не реализует, я в конце концов уже чувствую, сам реализую. Это называется mystery machine paper, его сделали в фейсбуке, но открывать исходники не будут, и open source, я чувствую, никто никогда не дождется. Мы уже рассказывали про него какое-то время назад, около года назад, не скажу точно, про то, как профилировать систему, состоящую из большого количества независимых компонентов. Вам не нужно анализировать, изменять какие-то отдельные бинарии, вам не нужно делать, изменять вашу программу или настройки какие-то, вам нужно только анализировать логи. То есть, каждая система должна выдавать, что к ней пришел запрос с айдишником таким-то, и когда есть сквозное трассирование, там есть айдишник оригинальный, который вызвал всю цепочку. Эти две вещи нужно в логах отображать, ну и там время и всякая дополнительная информация. А потом уже система, которая имеет доступ ко всем логам, будет на основе этих данных строить модель вашей программы, а точнее будет строить граф зависимости. И вот в этом пейпере, который я привожу, полностью рассказывается, как строить граф зависимости, каким образом нужно уменьшать, увеличивать точность этого графа, то есть точность модели, которую строит система. И потом с помощью логов, опять же вероятностное профилирование, я чувствую, только оно в будущем будет работать. Чем дольше она работает, система работает, чем больше логов имеется, тем больше информации будет о том, какие зависимости происходят между отдельно взятыми компонентами, и как производительность отдельного взятого компонента влияет на всю систему в целом. Это позволяет искать критические пути, это позволяет понять, какие части лучше всего оптимизировать, примерно точно так же, как и здесь. То есть фактически оба профайлера, и тот, который мы сейчас обсуждаем, и вот мистер машин, они оба пытаются найти критический путь и предлагать пользователям профилировать участки, только находящиеся на критическом пути. Вот найти критический путь в реальной большой системе очень сложно, по-настоящему сложно. И вот эти вероятностные профайлеры – это один из способов, как это легко и правильно сделать, при том, что вам не требуется чаще всего изменять саму систему, сами исполняемые файлы. Это очень удобно. И получается, что вот сейчас мы видим, что у нас уже есть два профайлера. Один, который работает на уровне отдельно взятого, бинария отдельно взятой программы, и второй профайлер, который работает на уровне всей системы, который смотрит, анализирует систему, состоящую из большого количества маленьких компонент-частей, которые обмениваются данными и запросами, и так далее. Валер, ты зарядил свой микрофон? Ну я… Плюс в том, что он просто остался беспроводного проводным, вот. И он собирает какие-то наводки, я слышу песк. Чёрт возьми. Все не так плохо, говори. Нет, оно в постобработке будет чудовищно. Да, Вань, тебе слово. Отлично. А раз Валера ничего не хочет сказать, ха-ха-ха, как мы с ним здорово спорим, то следующая новость – это конференция DUMB в Казани, которая придёт 8 ноября. Мы давно с ними пытаемся связаться, они попросили нас в том числе рассказать, что конференция будет, что есть программа, много интересных докладчиков, мы попросили у них скидку, и они наконец сказали, что если вы введёте промокод DEVZEN, у вас будет скидка в 10% на билеты. Ооо, 10%! Отлично. Сколько? Целых 10%. Ничего себе. 10% DEVZEN. И раз уж мы коснулись конференции, я хотел сказать, что я буду выступать на DWOOBS конференции. Напомнить, я уже говорил, по-моему, это будет через 2 недели в Питере. Буду рассказывать про Kubernetes и как надо писать автоскейлер для Kubernetes, и какие проблемы в автоскейлерах, и вот это всё. Так что если кто будет в Питере, давайте пересечёмся. Я обещаю взять с собой наклейчик, может быть, пораздаю наклейки DEVZEN. Откуда у вас столько бесконечных наклейчиков? Я уже своё 100 лет раздал. А, ты знаешь, я просто когда последний раз печатал, я сказал, напечатай мне 500 штук, пожалуйста. Как тебе? А, я говорил за мьючный микрофон, да? Нет, нет, 500 штук, я понял. Да, да, да. Печатаешь 500 штук, и они не заканчиваются. Как я теперь слышно? Очень интересно. У нас вообще Саша не Саша, Валера не Валера. Давай, Валер, говори. Ну, то есть я теперь хотя бы ни одного не ловлю, да? Да, да. А, ну это, по сути, просто калитура наушников. Что я хотел сказать, что представь, у тебя есть функции, которые сыграют ответ на твой запрос в базу. Если ты будешь, у тебя есть какой-то профиль, который будет замедлять все функции, кроме одной, и смотреть, как изменилось общее время выполнения. И вот что он тебе поможет сказать, он тебе поможет найти такие функции на пути формирования запроса, тормоза в которых, или локи в которых, или что угодно в которых, просто замедление воплощения которых, наиболее сильно затрагивает общее время ответа. То есть он тебе покажет функции, на которые в первую очередь нужно посмотреть во время вдумчивого вкуривания в код. Вот что он тебе поможет сделать. И в том-то и дело, что он тебе поможет не по циклам ЦПУ это посмотреть, а по тому, что вот именно, просто что, если здесь медленнее. Неважно по какой причине. Валер, ты обсуждаешь тему, которая была две темы назад. Сколько можно? Да! Потому что вы не дали обсуждения. На самом деле, то, о чем вы говорите, это показывает любой нормальный профайлер. Вот, например, Гошный, точно так же строит себе графы зависимости и показывает, что вот он даже не в абсолютных значениях показывает, а просто в процентах. Вот у тебя твой условный запрос, вот он 30% времени проводит здесь, а эти 30% распадаются вот так-то. Вот, но ты смотришь на него и видишь, что да, вот я это оптимизирую, получу 50% выигрыша. То есть там и задержки вставлять не нужно. Мне кажется, мы уже пошли по второму кругу. Давайте дальше. Саш, расскажи нам про… Да, более скоро мы затронули железячные проблемы с пищащими микрофонами. То немного железячных тем. Вот, во-первых, минутка безумия. Компания Stem Microelectronics, ST Microelectronics, написала пресс-релиз про то, что они запихнули 32-битный микропроцессор в 8-пиновый корпус. И это просто довольно безумно, потому что это достаточно крутой процессор, типа с двумя аппаратными UART-ами, с тремя аппаратными SPI-ами, с странными протоколами типа I2S, про которые немногие слышали и так далее. Короче, супер-супер навороченная железка в 8-пиновом корпусе. Это просто достаточно забавно. Ну и заинтересованные разработчики могут поискать кит для разработчиков. Называется Stem32G0316-Disco. Его официальная цена составляет менее 10 долларов, но я поискал. Его достаточно непросто купить в Россию. То есть на всякие eBay, Amazon пока не завезли. На сайтах официальных партнерах Stem Microelectronics я не нашел доставки в Россию. То есть там, например, можно заказать в Уотсвета, может в Великобритании заказать, например. А российские магазины их тоже имеют, но продают партиями, по-моему, то ли от 10, то ли от 11 штук. Что тоже не очень удобно. То есть тебе нужно сразу 100 долларов отдавать. Всего-то. Ну, понимаешь, это такая штука, она тебе не то чтобы безумно нужна, но как бы ей прикольно просто поиграться. Объясни мне, пожалуйста, плюсы 8-пиновых процессоров. Я имею в виду, это же значит автоматически, что ты в течение более долгого времени должен передавать туда данные или еще что-то. А какой смысл-то? Почему не сделать их 16? Ну, смотри, нужно смотреть по задаче. То есть, например, у тебя очень небольшое устройство, в котором не важна скорость передачи, но, например, важен факт наличия в твоем микроконтроллере часов реального времени. В этом микроконтроллере такие есть. Или там, не дай бог, какой-то встроенной поддержки криптографии, который туда тоже сфигачили. А у тебя из ввода-вывода, не знаю, экранчик с I2C интерфейсом и две кнопки. Ну, в принципе, его хватит для этой задачи. Бывают случаи, когда тебе больше не нужно. Ну и зачем тебе делать сложную разводку платы, когда вот такую штуку вкорячено все. Чем славится STMicroelectronics? То, что они на базе армовых ядер делают микроконтроллеры примерно под любые задачи. То есть, у них десятки, может быть, сотни моделей. И вот на каждый чих, на любой случай жизни, в таких корпусах, в сетях, с таким количеством пинов, с другим. Вот эти с низким энергопотреблением, вот эти наоборот для тяжелых задач, где тебе, по сути, процессор нужен такой не безумно мощный, но почти как настоящий процессор. Ну и так далее. И они сделали еще один микроконтроллер для непонятных странных задач. Все понятно. Фактически только для упрощения разводки. То есть, 8 лапочек прикрепить легче, чем 16. Заметьте, две из них для питания. То есть, на самом деле, лапочек 6. Отлично. Поехали дальше. А дальше у нас проект под названием Glasgow. Изначально его разработал широчайший известный в очень узких кругах программист, хакер с ником White Quark. Он ведет достаточно активно Twitter, в свое время писал статьи на Хаббр. И вот это его проект, он open-source, лежит на GitHub. Там открытое железо, открытая прошивка, все открытое. Что собой представляет Glasgow? Думайте об этом как о гидробазе. То есть, такая небольшая плата, которая может общаться по разным протоколам с какими-то железками. Опять же, A2C, опять же, SPI. Плюс там разные протоколы, которые мне не очень знакомы, которые в каких-то определенных чипах памяти используются. Вот это вот все. В отличие от гидробаза, который работает на микроконтроллере STM32F405, если память не подводит, здесь используется FPGA IceFoty семейство, что еще делает эту плату так же интересной, потому что ее можно, наверное, я не проверял, использовать как просто отладочную FPGA плату. Про IceFoty я рассказывал в предыдущих DevZen'ах некоторое время назад. Эти FPGA интересны тем, что под них существует открытый стекл разработки, называется... Как бешен называется? Ice... Сейчас, секундочку, я должен помнить. Что-то совсем вылетело из головы. Я некоторое время не трогал FPGA, поэтому сорянчик. Господи, Yosis и Arachne PNR. Вот, такой вот открытый стекл. И, собственно, в чем заключается новость? Проект на GitHub давно был, а новость заключается в том, что на CrowdSupply запустили, ну, точнее, скоро запустят crowdfunding компанию по выпуску этих плат в готовом виде, некоторым тиражом. Поэтому, если вы себе хотите такую плату, то подпишитесь на страничку на CrowdSupply и следите за обновлениями. Такая вот новость. Вопросы? Пожалуй, нет. Мне, наверное, не очень нужен Glasgow. Я серьезно подозреваю, что это будет очередная плата, которую я куплю, и у меня будет валяться без дела. Для всяких SPF, I2C я успешно использую HydroBus, потому что... Ну, это просто удобно, когда у тебя какое-то новое устройство, и ты не хочешь проектировать целый проект под него, то цепляешь к HydroBus и ждешь отдельной командочки и смотришь, как оно отвечает. Ну, для сладки удобно. Согласен. Продолжая электронно-радиейную тему, видео на YouTube, тоже широчайшее известного в очень узких кругах радиолюбителя Алексея Ягонина, позывной у него RA3TLB, по-моему, ОМ3, если я ничего не перепутал, радиона 3 Тамара Леонид Борисовна. И у него такое нехарактерное видео про то, как... Про оборудование, которое используется в широковечательном FM-станциях, и там такие, знаете, огромные фильтры в корпусе, знаете, как Full Tower Computer, примерно такого масштаба, даже больше. Ну, потому что через все это проходят десятки киловатт, там как бы маленькими коробочками не обойдешься. Опять же, поскольку это десятки киловатт, там не используются обычные коаксельные кабели, там используются буквально, не знаю, из чего сделаны, алюминиевые или медные трубы, и как бы, то есть железный фидер твердый, который не гнется, да, то есть вот такая труба, внутри которой еще проводник, и вот это используется для",
    "result": {
      "error": "API request failed: Error code: 400 - {'error': 'Trying to keep the first 8700 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}",
      "stack_trace": "Traceback (most recent call last):\n  File \"/home/andrei/Projects/podcast-shownotes/scripts/build_search_eval_dataset.py\", line 157, in generate_search_query\n    response = await self.client.chat.completions.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 1927, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1767, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1461, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1562, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': 'Trying to keep the first 8700 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}\n"
    }
  }
]