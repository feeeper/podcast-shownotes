[
  {
    "segment_id": "1d7150db-5871-4668-a461-626b85b60a8c",
    "episode_id": "41d2fea9-2f32-4c1b-8444-afbcd216ad66",
    "episode_number": 238,
    "segment_number": 7,
    "text": "Пожалуйста, присылайте интересные ссылки на эти темы, я с удовольствием посмотрю. Посмотрел всякие вот эти на Netflix, как они работают с хаосом, посмотрел на Slack, посмотрел на Spotify, посмотрел на ещё, там куча всяких интересных есть видео, если будет интересно, потом как-нибудь расскажу. Вот, но сейчас я наконец-то прочитал пейпер, который был добавлен в тему наших слушателей какое-то время назад, какое-то время это больше месяца, поэтому я даже сейчас не смогу найти номер, где его нам подкинули, но тема очень интересная, правда. Тема о том, как правильно работать с перегруженностью отдельно взятых сервисов внутри вашей микросервисной архитектуры. Тема, на самом деле, сложная, то есть с наскок вот так вот довольно сложно придумать, как это правильно сделать, и многие компании делают какие-то ad-hoc решения, которые подходят лично им, но не делают какого-то более генерального решения. WeChat попытался подойти с более генеральным решением, потому что, во-первых, оказалось, что у них очень большие нагрузки, довольно много микросервисов, но то, что это большие нагрузки, неудивительно, а то, что много микросервисов, я вот не знал, что у них несколько тысяч микросервисов, у них были циферки, циферки, циферки, сейчас я найду их, 3000 микросервисов и 20 тысяч машинок, на которых эти микросервисы крутятся, вот, и поэтому какие-то минимальные ad-hoc решения подойти не могли, они хотели сделать это как генеральное решение, вот, и они сделали генеральное решение, которое они назвали Dagger, не знаю, почему именно так, но вот так. Идея в чем? В чем сложность? Я сейчас буду много такого, монолог подобного стиля, вы поэтому давайте перебивайте, будут вопросы, потому что скучновато, я думаю, будет тем, кто нас слушает. В чем сложность? Сложность в том, что, когда у тебя есть один сервис, и ты хочешь его сделать, как это правильно сказать, сделать защиту от перенагрузки, ну, ты просто берешь и какое-то количество внешних запросов клиента отбрасываешь на основе каких-то характеристик, ну, например, там, CPU у тебя на 100% загружена, LA там больше 10 при двух процессорах, ну, значит, что-то идет не так, давайте лишнее отбрасывать. И ты прям пришел запрос, а ты его сразу хоп выкинул, хоп выкинул, вдавнул его, спускаешь, и постепенно у тебя восстанавливается работоспособность сервиса, постепенно все становится хорошим, ты можешь использовать. Сразу вопрос. Да. А вызывающая сторона, получается, по тайм-ауту отваливается? Ну, как она понимает, что запрос... Нет, нет, ты его не вдавнул, это не с той точки зрения, что ты оставишь коннект висящим и так далее, нет, ты просто говоришь ему, у меня там 500-ая ошибка, я не хочу обрабатывать твои запросы, у меня тут проблема. Вот. Но в том плане, что ты даже не будешь начинать его разбирать, то есть ты, может быть, даже не будешь читать, что там внутри написано, ты скажешь, все, ребята, у меня тут вообще плохо-плохо, я не хочу и ничего с этим не иметь дело. То есть fail fast, такой Erlang style. Вот. И эта вещь работает на каких-то маленьких системах, в которых у вас есть основная система и есть один микросервис. В основной системе вы проверяете, у меня тут 500-ая ошибка, что-то пошло не так, ну давай я попозже тебя спрошу, что-то мне надо и все. Вот. Проблема становится гораздо хуже, когда у вас многоуровневая архитектура, когда у вас куча сервисов, которые зависят между собой. Во-первых, потому что вы должны вручную каким-то образом настраивать мониторинг всех сервисов, проверять везде все ваши критические какие-то KPI. Они могут быть разные на разных системах. И если в одном месте у вас внезапно не настроен мониторинг или не настроен вот этот вот load shedding, как он называется по-английски, то получается, что может упереться именно в этот сервис и вся работа всего вашего проекта встанет, потому что именно в этом сервисе вы не настроили мониторинг. Вот. А как вы понимаете, бывает очень часто такие системы, в которых даже один микросервис, если внезапно перестает работать или отвечать на запросы, может стать реально все. Вот. Этого, конечно же, надо избегать как можно больше, и поэтому получается, что надо все покрывать, все везде сделать. Это довольно сложно сделать. Во-вторых, очень сложно реализовать реально правильно работающий вот этот load shedding из-за большого количества сервисов. Из-за большого их... Ну, в смысле, что это надо вообще все везде сделать. Это, во-первых. Во-вторых, правильно, концептуально это тяжело сделать. Представьте, что у вас есть запрос клиента, и этот запрос клиента уходит, там, есть внешний уровень сервиса, который принимает запрос клиента, и он там раскидывает, там, скажем, есть 20 подзапросов, которые нужно ответить для того, чтобы клиенту ответить правильно двухсотым кодом. И с какой-то вероятностью каждый из этих запросов может не отвечать. Ну, к примеру, 50%... У нас вся система загружена, и мы выбрали такую статистику, что 50% запросов мы отбрасываем для того, чтобы система хоть как-то работала. Тогда получается, что у нас вероятность того, что мы ответим клиенту, будет равна 0,5 в 20-й степени. То есть фактически 0. Это значит, что на самом деле будет система обрабатывать большую часть всех запросов, но при этом клиенту мы будем отдавать 0 правильных ответов. Ну, я утрирую, не 0, там, какой-то чиселка будет, но все равно это будет очень маленькая величина. И при этом это не поможет системе. То есть правильная система в микросервисной архитектуре должна четко понимать, что вот этот входящий запрос от клиента мы всегда будем отбрасывать в любых его подсистемах, чтобы не перезагружать систему, а вот этот вот запрос от клиента мы должны всегда во всех системах обработать правильно. И получается, должна быть сквозная система, которая понимает приоритет этого запроса. Эта сквозная система должна четко принимать решения на всех уровнях, отбрасывать, не отбрасывать. Получается такая довольно сложная архитектура. И как раз именно этим этот даггер и занимается у WeChat. Идея на пальцах, если объяснить, то идея такая, что каждый запрос, приходящий от клиента, получает приоритет, так называемый бизнес-приоритет и юзер-приоритет. Бизнес-приоритет – это какая-то чиселка, там, не знаю, от 1 до 100. И юзер-приоритет – это какая-то чиселка от 1 до 100, я чуть пониже про них расскажу. В дальнейшем каждый микросервис, который существует в системе, верхнего уровня, среднего, нижнего, без разницы, он сам строит собственную статистику по его работоспособности и сам строит, сам воспринимает, настраивает, высчитывает уровень своей работоспособности. Соответственно, если внезапно оказывается, что он плохо работает, то он говорит, так, все запросы с приоритетом ниже, чем 42, я делать не буду. Все, что выше, делать буду. Вот этот уровень 42 он выбирает самостоятельно. И во время перезагрузки, перенагрузки сервиса, соответственно, все, что приходит с приоритетом низшим, оно будет отбрасываться, и отбрасываться оно будет не только на уровне данного сервиса, но отбрасываться оно будет на уровне того сервиса, который посылает запросы в данный сервис. Секундочку, у меня тут все зависло. Меня вообще слышно? Да-да-да, слышно, но видимо лошадник у тебя не работал. Черт, меня лошадник... Кажется, Ваня потерялся. Да, Валер, а у вас возникает на работе вот какая-то проблема с backpressure или что-то такого? Ну, она возникает, но не в таком контексте, у нас нет такого количества микросервисов, нам не нужен такой сложный менеджмент, поэтому у нас немножко по-другому решается. Как? Я не уверен, что могу об этом говорить прямо так открыто, ну и во-первых, потому что я не совсем компетентен об этом говорить, потому что лично на моем сервисе там как бы... Если проблема возникает до него, она не дойдет. Ну, то есть у меня есть штука, отдающая данные, там все довольно банально, там написан просто в самом сервисе, написан как это... Штука, которая считает количество активных соединений на клиента, и как бы если их слишком много, он перестает давать этому клиенту создавать новые, и он просто типа будет... Что там, 424 или 425, какие-то ошибки, типа too many connections, вот, или too many requests. Входящие данные у меня другого сервиса, и входящие данные у меня из Kafka, соответственно, типа ну, мы просто мониторим, что мы успеваем Kafka разгребать, если не успеваем, ну нам нужно машинами добить, мы там особо backpressure сделать не сможем, потому что как бы Kafka является таким буферным местом, а те, кто пишут в Kafka, это очень большая сложная история, которая немножко не мой scope, и я не могу про нее рассказывать, но она сильно проще в том смысле, что... Она сложнее в том смысле, что нагрузка очень большая, проще в том смысле, что там это один сервис, который просто сложно менеджиться, а не лапша из микросервисов. У нас еще сложнее, если бы ты прочитал paper, ты бы об этом знал. Дело в том, что у нас, когда вот крутится твой daemon in solrd, то он в разные моменты времени исполняет разные роли, потому что система византийская, и роли назначаются случайным образом, и там происходит такое, что ты вот был одним сервисом, был, был, был, а через 10 секунд уже другой сервис. И возникают разные непонятные истории, что, например, что делать, если ты обрабатывал чей-то запрос, и вот пока ты его обрабатывал, у тебя роль переключилась. Тебе как бы нужно продолжить его обрабатывать, или может быть передать информацию тому, кто теперь стал вот этим сервисом, чтобы он мог быстрее эту информацию, ну как бы этот запрос обработать, а если он примет от тебя эту информацию, то как бы на каком основании он ей доверяет, например. А, еще более важный вопрос. Дело в том, что на вызывающей стороне роль тоже изменилась, ну или могла измениться, и может быть запрос, который ты обрабатываешь, он ей уже не нужен, ну или он нужен кому-то другому, понимаешь, да? И, к сожалению, каких-то общих подходов к этой проблеме мне лично вообще неизвестно. Может, тебе что-то такое попадалось для византийских систем? Ну, если честно, я не очень понял с того описания, то есть, например, роли, то у вас роли не в смысле консенсуса, а в смысле какой-то там, видимо, вашей бизнес-логики, и готовых решений, скорее всего, нет. Для конкретного примера, допустим, есть только две роли. Ты либо исполнитель какого-то контракта, либо ты валидатор исполнения. И ты, ну вот, ты как бы в этот интервал времени что-то исполнял, но ты не знаешь, кто будет за тобой проверять. Потом случается следующий интервал времени, и ты уже либо ты исполнитель других совершенно контрактов, либо ты чей-то валидатор, но ты уже не исполнитель того, что ты исполнил. И ты как бы заранее не знал, кто тебя будет валидировать, то есть там как-то случайно назначились, как это происходит, как бы можем отдельно обсудить, когда ты все-таки дочитаешь наш пейпер. В общем, твоя роль сменилась, и происходит вот это вот все, что, может быть, вызывающей стороне уже твой ответ не нужен, потому что у нее тоже роль сменилась, или ты должен как-то передать данные. То есть для простоты, допустим, ролей две – это либо исполнитель, либо валидатор. Мне кажется, как исполнителю проще всегда доделать, то есть как бы не следует потом выкинуть, ну и пусть выкинут, как валидатор. Так тебе сыплются другие запросы в это время, которые ты должен в этот интервал времени исполнить. Слушай, мне тяжело тогда рассуждать в целом про всю вашу систему, потому что в том же большинство около других вещей, они обычно как бы здесь блок, тебе ничего другого. Ты, к сожалению, стал пропадать, Валера. Лучше или там исполняешь, зато я игрался. Да, Вань, давай ты продолжишь с того места, где остановился. У меня случилось уникальное явление. Мах повис. Да-да, техник Apple, как известно, не ломается. Apple не повисает. Но иногда висит. Ладно, перезагрузились, можно дальше обсуждать. Итак, я остановился на том, как работает этот Dagger. Они попытались сделать генеральное решение для того, чтобы решить все проблемы с перегрузкой. И самое главное, что они подчеркивают, что тот сервис, который перегружен, он всегда сообщает вышестоящему сервису, от которого приходят к нему запросы, о своем уровне отбрасывания, какие запросы с каким приоритетом он будет отбрасывать. Соответственно, сервисы, которые посылают запросы, они не будут посылать запросы с приоритетом ниже того сервиса, который отбрасывается, ниже уровня нижестоящего сервиса. Я, наверное, запутанно сказал, но примерно понятно. Я не понял, то есть вызывающая сторона как-то узнает, на каком уровне. Да-да, вы смотрите, сервис А вызывает сервис Б постоянно, он его использует. И он говорит, вот сервис Б, получи новый запрос, у него приоритет 1.1. Он говорит, отлично все, я его обработал. Но, кстати, все, что... Ой, там 100 100, к примеру. Он скажет, отлично, я его обработал, но все запросы с приоритетом ниже, чем 42, я обрабатывать не буду, ты мне их не посылай. И сервис А все записывает в какой-то кэшик, что вот ниже 42 не посылать сервису Б. А как долго эта информация остается актуальной? Это целая логика, про нее здесь отдельно есть секция, я сейчас про нее расскажу немножко. Я погружаться в детали не буду, все заинтересованные могут посмотреть, но в целом логика выглядит следующим образом. Во-первых, они разобрали, какие вообще бывают системы перегрузки сервисов. То есть есть обычная перегрузка сервиса, когда у вас там сервис А вызывает сервис Б, вот. А есть перегрузка с последовательными вызовами. К примеру, сервис А два раза вызывает сервис Б для того, чтобы сделать один запрос пользователя. И получается, что мы должны либо оба запроса выполнить, либо оба запроса не выполнять, потому что иное не имеет смысла. Аналогичный сервис А может выполнить запросы к сервисам Б, С и Д, и аналогичному точно также должны либо все выполнить, либо все не выполнять, потому что иначе смысла нет. Понятно, почему смысла нет? Логично все? Да, да, да. Вот. И, соответственно, вот как раз принятие решения о том, кто кому что посылать, оно поднимается от уровня того сервиса, на котором проблема, на уровень выше, именно для того, чтобы вот этот уровень выше он тоже понимал, что он не сможет обработать запрос, потому что ниже стоящий сервис этого приоритета не примет. Вот. И дальше как по поводу выбора приоритета. Так, так, так, так. Сейчас я найду у меня тут заметочки. А, да, по поводу выбора сначала нагрузки. Каким образом выбрать правильно загружен сервис или не загружен? Они посмотрели несколько разных методик, они их сравнивают по характеристикам, и там даже прямо вот графики есть. Вот мы использовали там CPU, вот мы использовали ResponseTime и так далее. Но в конце концов они пришли к времени нахождения запросов в очереди каждого сервиса. То есть это время между тем, когда запрос пришел от кого-то и попал в очередь, до того, как сервис начал обрабатывать этот запрос. И опытным путем 5 лет назад они пришли к тому, что 20 миллисекунд, это значит, если меньше 20 миллисекунд, то сервис не нагружен. Больше 20 миллисекунд, значит сервис слишком сильно нагружен. Вот. Они этот уровень используют в течение 5 лет, они очень им довольны. Надо указать, что при этом у них общая настройка на все сервисы в их системе, что таймауты больше, если запрос длится больше, чем 500 миллисекунд. Вот. И он говорит, оно у нас отлично работает, мы всем советуем, прям все классно-классно. Вот. И, ну то есть 20 миллисекунд это прикольно. И почему это полезно? Это бывает полезно, потому что ResponseTime он невалиден с точки зрения, что если у вас, в смысле мерить по ResponseTime, потому что запросы бывают совершенно разные, запросы бывают синхронные. Когда вы вызываете сервис, он там должен еще тысячи сервисов внутри вызвать. И там, скажем, 400 миллисекунд может быть вполне ок, нормальное время. И оно ни о чем не говорит. То есть время ожидания в очереди, при этом у него там будет полмиллисекунда, он сразу начинает обрабатывать и все правильно делает. Поэтому ResponseTime не показатель. CPU тоже не показатель, потому что, как бы, если вы на CPU грузите не на 100%, но при этом быстро разбираете входящий очередь сообщений, значит у вас все в порядке. Вот. И именно время ожидания в очереди, входящей очереди, это хороший показатель, который они используют. Вот. А дальше самое интересное. Дальше интересно по поводу вот этих вот уровней приоритетов. И они сделали несколько уровней приоритетов. Самое главное это бизнес приоритет, который выбирается на основе того, какого типа приходит запрос. А это все диктуется бизнес-задачами. То есть запрос на логин это самый важный, потому что пользователь не сможет сделать никакой другой запрос, не будучи залогиненным. Поэтому логин самый важный. Следующий это платежи. То есть платежи и по статистике у них, заявлений на проблемы и по логике самого приложения. Платежи это важно. Люди всегда заботятся о Ниндах. А дальше всякие там посылка сообщения, посылка какого-то фотографии, там бла-бла-бла. И там куча всего. И получается, что вот ранжирование такое достаточно четкое. И потом они как бы пришли к выводу, что, ну и может быть я раньше объяснял, я уж не помню, что если отрубать на уровне... нельзя отрубать. Нет, давайте так. Можно отрубать только целыми готовыми блоками бизнес-приоритетов. То есть ты, например, отрубил все платежи, но не отрубил логины. Это ок. Но если ты начинаешь отрубать там половину платежей, это не ок, потому что у тебя, возможно, будет пересечение где-то внутри по запросам. И как вот я уже обсуждал, то есть как бы если сервиса вызывает сначала B, а потом C, у тебя B может выполниться, C может не выполниться, и ты ничего не сможешь сделать в итоге суммарно. Поэтому надо только целиком вот эти бизнес-куски выключать или включать. Но потом они поняли, что у них бывают такие ситуации, когда, к примеру, какой-то сервис перегружен, и если включить целиком посылку образов, фотографий, то система будет перегружена. А если выключить ее целиком, то система будет недонаружена. И непонятно, как действовать в этой ситуации. И они пришли к такому выводу, что надо ввести еще один уровень приоритетов, это пользовательские приоритеты. То есть внутри одного уровня бизнес-приоритетов может быть много уровней пользовательских приоритетов. И они это делают следующим образом. Они на один час хешируют там ID пользователя. И в дальнейшем по вот этому хэшу убирают, к примеру, хэш 128 символов они хэш используют. Соответственно, первые 64, они имеют приоритет достаточно высокий, следующие 64 приоритет достаточно низкий. И получается градация уже в 128 внутри одной бизнес-единицы. Вот. И вот это вот хэширование на час, оно сделано для того, чтобы ввести понятие честности в систему, чтобы у одного пользователя не всегда тормозило, а только не больше, чем в течение часа. Вот. Меньше, чем часа они боятся делать, потому что тогда консистентности какой-то не будет, и будет слишком часто переключаться между режимами, что тоже плохо для системы. Вот. Ну а дальше все логично. То есть как бы система начинает тормозить, ты начинаешь сначала выбирать бизнес-уровень, на котором надо отрубать, а потом внутри него ты выбираешь пользовательский уровень, на котором надо отрубать. Вот. И все, что имеет приоритет ниже, оно будет по остаточному принципу или отрубаться, или не отрубаться. Ну, то есть как бы как система себя покажет. Вот. Примерно понятно пока? Сложно. Вот. Но в итоге получается система... Меня очень эта система удивила. Удивила тем, что вот эта сквозная приоритизация, она должна вообще поддерживаться всеми кусками системы. Вот. И это должно быть на уровне поддержки библиотек всех языков, на которых вы работаете, это должно быть прям сквозным таким элементом, который проходит везде. Вот. Но когда ты это на самом деле сделаешь, и они это сделали 5 лет назад по их заявлению, все настолько прозрачно и легко становится, что пользователи, которые... В смысле, пользователи, разработчики, которые пишут сервисы, они подключают вот эти блоки каким-то легким универсальным жестом, способом. Вот. И даже не знают, как оно работает. То есть как бы они дальше работают, как обычный сервис пишут, а вся эта система, я все время забываю, как называется, даддеры, она уже работает самостоятельно, что вот на вот этот сервис мы вот для пользователей вот этого уровня мы уже будем сразу отрубать автоматически. Вот. И прям звучит как магия-магия, но выглядит логично и вроде как даже работает. Там было у них несколько проблем, связанных с тем, что они сначала пытались сделать по пользовательской сессии. Вот это тоже интересно. Это уже больше теории игр задействованы. Смотрите. Они сделали сперва не юзер приоритет с хэшами, они сделали сессион приоритет. То есть пользователь регистрируется, у него есть какая-то сессия, и они эту сессию хэшируют и в дальнейшем смотрят, вот как бы нам надо эту сессию отрубить, и они ее отрубают. То есть все запросы с этой сессии не работают. Пользователь начинает что-то работать и понимает, что у него логин, логаут работает, платежи работают, а все остальное ничего не работает. Ну и первым действием, которое он хочет попробовать, совершенно логично выходит, дай-ка я выйду, зайду. Либо логаут, логин, либо закрою приложение, снова зайду и там автоматически логиниться ты будешь. И в конце концов, просекли фишку, что если ты начинаешь логаут, логин, логаут, логин, логаут, логин, в какой-то момент ты получаешь сессию, которая будет приоритет чуть выше, чем у всех остальных, ну случайным образом получается такое. И у тебя все будет работать. И у них оказалось, что по статистике, при нагрузке большой на систему, когда у них система сама выбирает, что вот на этом бизнес приоритете мы вот этих пользователей начинаем отрубать, у них происходит ловинообразный логаут-логин, для того, чтобы получить сессию, которую не отрубают. И в целом, данному конкретному пользователю становится хорошо, и он поэтому так и действует. А система в целом лучше не становится, она становится только хуже, потому что вдобавок к предыдущей нагрузке еще количество логаутов-логинов выросло. И они в конце концов стали отказываться от сессион-приоритетов и переходить к тому, что просто каждому пользователю назначается юзер-приоритет, исходя из вот этого числового хэша. Вот. Они там привели кучу графиков к тому, почему они именно это выбрали. Сессии они показали графики, они показали графики с временем ожидания, в зависимости там, или использование CPU, как метрика загруженности, или респонс тайм, там так далее. Но все эти графики мне не показались интересными. Мне вот сама идея показалась интересной. И в принципе я ей все рассказал. Суммарно можно сказать, что, пожалуй, я не помню других систем, которые вот так генерально решают этот вопрос. И мало того, что это концептуальная идея, но это идеи, которые на практике уже реализованы и уже работают в течение 5-ти плюс лет. Если у кого-то есть информация о подобных системах, которые тоже работают, пожалуйста, присылайте мне на скидку, я такого не помню. Такое интересно было бы сделать, конечно же. Валер, ты вернулся? Ну, я не знаю, меня слышно, нет. Алло, раз-раз. Тебя и было слышно, ты просто прерывался. Вот. А одно у них открыто, закрыто? Вообще ничего нет. То есть, никаких исходников, ничего. Но ты понимаешь, что это все сильно очень зависит от приложения, от языков и так далее. То есть, я боюсь, что... Одно из основных у них решений, как это, концептуальных, получается, решений в данной системе было то, что они поняли, что нельзя делать какую-то централизованную систему для решения о том, где кому давать отлуп или не давать отлуп. Это кусочек системы по работе с нагрузкой, она должна работать по маленькому кусочку на каждой отдельно взятой машине. То есть, фактически, если ты работаешь в Kubernetes, то у тебя в каждом поде должна работать кусочек этой системы. Вот. И, ну, я на Kubernetes сейчас перекладываю, мы же все в Kubernetes работаем, да, Валер? Вот. И получается, что это все становится очень, из-за этого это все становится очень сильно специфично для проекта. Ты не можешь зарелизить, там, в open source выложить централизованную систему, которая все это будет делать. У тебя это должно быть интегрировано в каждый кусок твоего проекта. Поэтому я не думаю, что они вообще что-нибудь откроют, к сожалению. А что еще на данный момент остается закрытым, это информация о том, когда выйдет следующая версия отладочной платы Black Ice. Это, я начинаю первую новость из пачки новостей одной строкой. Есть такой человек с ником FalconLogic, который является создателем отладочной платы на базе FPGA серии IceForty. И также на ней есть микроконтроллер STM32. И вот он пишет в твиттере о том, что готовится следующая версия, называется Black Ice MX. Я не видел спецификации, но вот судя по фотографиям пустых плат, которые он приводит, тут, похоже, делается переход на USB Type-C. Если я ничего не путаю, мне кажется, я вижу HDMI разъем. И еще здесь на обратной стороне какая-то ESP. То есть, либо ESP32, либо 2866, или какая она там. Да, то есть, если вы чем-то таким интересуетесь, вот подпишитесь на FalconLogic. Также анонсировано расширение для SDR под названием Lime SDR. И это расширение называется Lime RFE. Это плата для, ну она вроде как позиционируется для радиолюбителей. То есть, обещают, что в ней будет, ну во-первых, какое-то небольшое усиление, правда всего лишь до 3 Вт, но все-таки будут фильтры на радиолюбительские частоты, чтобы отрезать все бесполезные сигналы из прочих частот. Вот, обещают IP, UI и интеграцию с Lime Suite. В целом пока не очень понятно. А, ну да, измерение коэффициента светочной волны и мощности. Не очень понятно мне эта плата. То есть, она наверняка будет стоить как чугунный мост, и при этом быть не самой лучшей SDR для любительского радио. Вот, то есть, у меня какие-то смешные чувства, но тем не менее, перейдите по ссылкам и ознакомьтесь, если вы чем-то таким интересуетесь. Также анонсировано более специфичное, как-то специальное SDR именно для радиолюбителей, называется Alt 512. Пока нету ценника или чего-то, ну, то есть, мало информации по этому устройству. Известна мощность 10 Вт, известно, что у него будет встроена звуковая карта, то есть, не нужно будет, не... для работы в цифровых видах связи понадобится меньше проводов, короче. Вот, это от той же компании, которая делала Sky SDR, широчайше известный в очень узких кругах. Опять же, если вы чем-то таким интересуетесь, ознакомьтесь, выглядит как очень интересный SDR для любительского радио и для работы в полевых условиях или чего-то такого. Я ненадолго прерву твой монолог. Оказывается, тут в чате задавали вопросы к пейперу, который я рассказывал. Я чат только открою, поэтому простите. Во-первых, вопрос от Андрея Апанасика. В Dev.Null сам сервис отправляет или нет? Я надеюсь, я уже ответил, что да, именно каждый сервис, работающий на каждой отдельной взятой машине, ответственный за то, чтобы понимать свою собственную перезагруженность и давать лупы на входящие запросы, которых приоритет ниже, чем тот threshold, который он поставил. И это принципиальное решение. Конечно, при этом, как я уже говорил, он вышестоящему сервису дает понять, какой threshold он сам себе поставил, и поэтому в дальнейшем сервис вышестоящий не будет посылать эти запросы. То есть это облегчит жизнь этому перезагруженному сервису. И он как-то дает комментарий, что странно в сам сервис пихать проверку нагруженности, CPU и так далее. Дело как раз в том, что получается, что он CPU-то и не проверяет. Они пришли к выводу, что нет смысла проверять, это CPU. И вот следующий комментарий тоже, что средняя загрузка процессора от GMAT и AOAID, я так понимаю, это хорошая характеристика для понятия загруженности или нет. Дело в том, что они пытались найти генерально хорошую характеристику. Это не генерально хорошая характеристика. То есть вы можете, я могу придумать несколько примеров, когда средняя процессорка загрузки, это от GMAT и AOAID будет высокая или там низкая, и при этом все равно сервис будет перезагружен. Понимаете, о чем я. А им хотелось придумать одну простую, но эффективную характеристику, которая будет работать всегда. И вот время ожидания запроса в очереди запросов – это очень хорошее время. Оно будет работать независимо от того, там, вас по CPU перезагружено, по IAO или как угодно. То есть вы просто будете дольше разбирать запросы, и у вас очередь будет копиться. Но и плюс, это же то, что работать для них не обязательно будет работать для всех, для вас, там, и так далее. У них своеобразные характеристики, у них фактически мессенджер. И поэтому у них большое количество быстрых запросов, которые должны быть мгновенно отработаны, и нет долгих сессий, и нету длинных, не знаю, там, пересылки видео, там, и так далее. Ничего такого нет. Поэтому, может быть, оно не будет работать для вашего сервиса. Все, на этом я затыкаюсь. — На этом я разотыкаюсь. Есть небольшое продолжение истории про, как это, двойное списание налогов с индивидуальных предпринимателей, про которые мы рассказывали. Пришло заказное письмо с постановлением суда о том, что вот эта вот заморозка средств на моем ИП, она легитимная, все правильно. То есть, ну как бы, да, вы зашли на Налог.ру, загрузили там какие-то платежные документы, подтверждающие, что вы там что-то как-то переводили, вроде как все налоги заплатили, но мы вот по нашей базе посмотрели, и вот что-то не сошлось.",
    "result": {
      "error": "API request failed: Error code: 400 - {'error': 'Trying to keep the first 7870 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}",
      "stack_trace": "Traceback (most recent call last):\n  File \"/home/andrei/Projects/podcast-shownotes/scripts/build_search_eval_dataset.py\", line 157, in generate_search_query\n    response = await self.client.chat.completions.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 1927, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1767, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1461, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1562, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': 'Trying to keep the first 7870 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}\n"
    }
  }
]