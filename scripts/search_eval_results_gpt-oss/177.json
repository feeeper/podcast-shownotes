[
  {
    "segment_id": "40b511bf-cce4-4212-8d6f-a55898d9e9ae",
    "episode_id": "7862d7a2-70aa-478c-867d-3f10039628b8",
    "episode_number": 177,
    "segment_number": 6,
    "text": "Потому что в противном случае можно взять что угодно, запровизженить вместо двух нот, которые теоретически могли бы быть нужны для этого кейсы на пяти, и все будет все равно очень быстро, и никто не будет об этом париться. А если кто-то действительно думает об ИК-дате, то есть о большом количестве нот, и думает о том, чтобы это занимало мало места, эффективно работало, мало денег платить за счета в Клауде и так далее. Выбирать систему надо на основе того, какую из этих трех систем компания, которая собирается ее внедрить, сможет развивать и сможет допиливать в том направлении, которое ей будет надо. То есть в частности, мне кажется, критически важно, что если компания, это скажем так, C++ шок, то есть сам C++ используется, то надо безусловно идти в Кликхаус, потому что Кликхаус написан на C++, но не в Друид и Пинот, потому что это Java-мир. А если Java, то наоборот, даже если там кажется, что Кликхаус лучше, но там в компании вообще никто там никогда не писал на C++ и не хочет этим заниматься, то Кликхаус лучше не брать, и лучше хоть там, возможно, в конкретном кейсе там чуть-чуть будет менее эффективный Друид или Пинот, надо их все равно брать, потому что они написаны на Java, и компания сможет для конкретного есть кейса добиться там с небольшим количеством там инженерных усилий, гораздо большего, чем если они будут просто сидеть и ждать, пока разработчики Кликхауса сделают то, что им надо, если они вообще когда-то это сделают, потому что сами люди в этой компании не смогут это сделать, потому что они на C++ не пишут. Вот. Это, собственно, основная мысль блог-поста. Я чувствую, у Вани есть возражения или вопрос, или комментарий. У Вани всегда есть возражения или вопрос, или комментарий. Мне очень понравилась статья, я её тоже читал, и я даже не знаю, как бы мне так похвалить, чтобы не слишком сильно хвалить. Но вопрос у меня о следующем. На самом деле вопросов несколько, но самое главное для меня это то, что там интересная есть статистика, что для того, чтобы написать полноценный дженерик, для общих целей Big Data All Up систему, необходимо затратить, там есть такая оценка, 100 человека лет. Какая-то странная, на мой взгляд, оценка, учитывая то, что все компании тратят несколько, по десятку хотя бы человека лет каждый год. Я считаю, что они данным ракет слишком много усилий. Для меня, на мой взгляд, команда, скажем, из 20 человек это уже неплохо. В смысле, это не слишком много, не слишком мало, но неплохо. Если компании очень-очень нужна хорошая система, то она может больше усилий на данную систему. 100 человека лет, на мой взгляд, это какая-то сложная оценка. Ты можешь что-нибудь прокомментировать здесь? Ну, я как раз имел в виду, что это общее. То есть, если, грубо говоря, в компании сидит 10 человек, которые тратят 10 человек лет в год, то примерно через 10 лет система будет похожа на что-то нормальное. Если 20 человек лет, то, наверное, через 5. Ну, это примерно так. Я считаю, что больше гораздо. Я слабо себе представляю, как компания Google наработала свою... Ну, почти generic, она всё равно специфичная для них, но сколько они потратили? Я не знаю, сколько это миллионов человек лет. Какую систему? Big Query? Да, да. Я не думаю, что систему... Или ты не считаешь её general purpose? Нет, я как раз на основе Big Query делал свою оценку. Но мне кажется, если там пейпер по Big Query вышел, по-моему, в 2009 или 2010 году, и они начали... Там, по-моему, даже в этом пейпере говорилось, что система начала разрабатываться в 2005-2006, но вначале её разрабатывало, очевидно, мало людей. То есть, примерно 10-12 лет она уже разрабатывается. И я не думаю, что там команды сильно больше 10 человек. Может быть, окей, может быть больше, может быть 20. Но тогда получается, что в Big Query в данный момент вложено 200 человека лет. Она уже хорошая. Наверное, она была хорошая, когда на отметке 100 человека лет она уже была хорошая. Я думаю, примерно так. Саша, ты можешь сделать оценку, сколько в Postgres было вложено? Очень много. Ну, то есть 20 лет умножаем на какое-нибудь там усредненное число разработчиков. Ну, там, не знаю, пусть будет... Ну, в последний релиз, я помню, там 130 с чем-то контрибьюторов. Очевидно, они не все тратят полный день на это. Очевидно, это не полный список людей, потому что я не уверен, вошли ли туда именно ревьюверы патчей, вошли ли туда тестировщики, вошли ли туда какие-то локальники. Ну, то есть, не знаю, берём, скажем, 50 человек, это такое среднее за весь период существования. Умножаем на 20, понимаем, сколько там лет вложено. Кстати, я чуть не забыл, хотел ещё прокомментировать поинт, который Валера успел сказать, но он тут же потонул в обсуждении open-source, о том, что, в принципе, пресса могла бы конкурировать с тем же друидом, лепенотом. Да, это верное замечание, потому что эти сферы, то бишь такой более общий дата-процессинг, будь он замаплен на SQL или не обязательно SQL, и вот такая колоночная обработка, они не являются взаимоисключающими. То есть, собственно, BigQuery — это пример проекта, который непосредственно конкурирует, и это касается не только пресс, это касается и Impala, и Drill, и Spark, и даже Flink. То есть, это всё системы, в принципе, одной природы, скажем так. Ну, может быть, Flink — это сложный вопрос, возможно, Flink — это немножко другое. То есть, BigQuery — это пример проекта, который непосредственно конкурирует и со Spark, и с пресса, и с друидом. То есть, это такая общая система, которая и те, и те юзкейсы охватывает, и они не мешают друг другу, они даже дополняют друг друга. То есть, если бы пресса или Spark захотели бы идти в этом направлении, да, они могли бы в нём идти. Но я подозреваю, что в данный момент это не является фокусом разработчиков, поэтому, так как ресурсы в open-source очень сильно ограничены, тут, видимо, пока что проще разрабатывать что-то более локальное, чем пытаться сделать такую убер-систему, как BigQuery в open-source. Я подозреваю, что тема начала себя постепенно исчерпывать, поэтому, если нету дополнений и вопросов, я бы перешёл к чему-нибудь следующему. Валер, скажи, о чём ты хочешь больше поговорить, о бекапах или андулоге? Ну, давай зайдём, наверное, с андулога. Давай зайдём. Поскольку я эту статью не читал, я надеюсь, что ты хотя бы расскажешь, о чём она, а я поддержу. В общем, есть такой небезызвестный в узких кругах разработчиков Postgres человек Роберт Хаус, и он тут на днях, да, на общем-то на днях, это было 30 января, публиковал в своём блоге статью о том, как Postgres планируют делать вещи, которые, не знаю, многие могут даже закричать, что они еретические с точки зрения Postgres. Как вы знаете, Postgres, или можете знать, Postgres используют для версионирования данных внутри себя, он использует, собственно говоря, MVCC механизм, то есть у каждой строчки есть, как бы прицеплена версия и просто новые, в время транзакции новые данные просто пишутся сразу в кучу, а update – это удаление старой по",
    "result": {
      "query": "Выбор СУБД C++ vs Java"
    }
  }
]