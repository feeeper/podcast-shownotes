[
  {
    "segment_id": "6a13f20e-2e50-4907-ad57-a2f84f2be5a7",
    "episode_id": "9f1d48d6-af4d-45a5-84d9-78087f836b4b",
    "episode_number": 154,
    "segment_number": 5,
    "text": "Они используют, это называется ОК кодировка, там реально прям единички, нолики, это не слишком ракетная наука. Да, а про машины, это то, что на этой же частоте, соответственно, открываются и закрываются машины с ключа, вот, и в последнее время появилась популярная тенденция красть из машин вещи с помощью того, что когда на заправках, на парковках, на недолгих люди уходят из машины, они ключом закрывают, но не проверяют, машина закрыта или нет. И если в этот момент пустить большой поток сигнала на этой же частоте, то машина может не схватить сигнал от брелка, вот, это называется джаммер, я даже не знал, что такое название, есть уже специальное. Ну, глушилка по-русски. Ну вот, да, глушилка, но я этот джаммер уже даже в русской видел, то есть, как бы, это уже к нам перебежало в русский язык, вот, и этот джаммер, соответственно, нужно как-то либо детектировать, либо каждый раз проверять, что машина закрылась, вот, и один из способов решения это сделать себе маленькую схемку, которая будет диодиком светить, как только проходит сигнал на этой частоте, и, соответственно, если где-то рядышком работает джаммер, то можно будет увидеть, что этот диодик горит все время. Это первое, значит, решение, которое, вот, первая ссылка, вторая ссылка это то, что когда работает джаммер, можно по силе сигнала, в общем, схема, приведена схема и показано, как работает устройство, которое по силе сигнала делает разную высоту звука в наушнике, и, соответственно, когда ты ездишь на машине, и у тебя есть это устройство, ты можешь примерно найти, где этот джаммер находится, его источник сигнала. Вот, такая вот охота на лис, как в свое время была в Советском Союзе. Естественно, автор не рекомендует на практике искать людей с джаммерами. Да, пожалуй, это не очень безопасно. Меня в этих статьях сильно удивило то, что я всегда думал, как машину угоняют, если именно про перехват брелка, что там наверняка есть какой-то протокол, который там типа с асимметричной криптографией, что брелок как-то авторизуется, что-то там посылает в мобиль, а тот посылает обратно. На основании этого сигнализация ставится, снимается, и чтобы это сломать, нужно как-то посередине вклениться, обойти асимметрическую криптографию. Ну, короче, я себе это очень сложно представлял. Оказалось, нужно банальную человеческую невнимательность. То есть, ты глушишь несущего, человек думает, что он поставил на сигнализацию, не замечает, что машина не помигала фарами, и машина остается открытой. То есть, вот эти модули, про которые тут и речь, которые передатчик и приемник на 433 МГц, это прям любая книжка по Ардуине, там типа сотая страница. То есть, это прям кто угодно может такое спаять легко. Еще один хак мне сказали в дилерском центре, что надо закрыть свой ВИН-номер, который на окошке, обычно на лобовом стекле, потому что можно заказать с завода по номеру ключ, который будет подходить. А разве сигнализацию ставят не люди, которые не имеют никакого отношения к производству кузова? Если у тебя в заводском есть ключ, который закрывает лок, делает на машину, то сигнализация к нему привязывается. Соответственно, если у тебя будет такой же лок, ты можешь сигнализацию отключать. Сложно. Не, в смысле, это все сигнализации, если ты делаешь на ключ, на котором уже есть беспроводной лок, то они связываются вместе в один флакон, и ты можешь с ключа отключать сигнализацию, не с брелка. У меня небольшой вопрос на базовые знания. А разве машина не вибикает, когда ты там? То есть не мигает фарами, не трещит, фидбэк никакого не дает, когда ты блокируешь блок? Саша поэтому и сказал, что это на невнимательность. То есть не все на это обращают внимание. Окей. Я предлагаю уже все закрывать тему. А следующая тема это ПГ-хак. Что это такое, Саша? Несколько тем на самом деле про предстоящие конференции, метапы и так далее. Я на самом деле сам не знаю, что за ПГ-хак, у меня оно как-то случайно прилетело. Это 2 сентября организуют Авито, если я не ошибаюсь, мероприятие для гуру Postgres, которые придут и будут что-то хакать для Postgres. Я понять не имею, что они там понимают под хаком. Судя по описанию на таймпэде, там постановка такая, что есть Postgres, в котором что-то сломано. В смысле там база бита или как-то так. И задача в том, чтобы это починить. С призами. Приз это квадрокоптер и билет на ПГД Russia 2018. Ну и там какие-то еще гаджеты. Такое вот мероприятие, я так понимаю, первый раз проводится. Знакомьтесь, если вас что-то такое интересует. И второе мероприятие под названием High Load Cup организует Mail.ru. Это проходит уже прямо сейчас и до 31 августа. Задача, насколько я понимаю, написать какой-то супер отказоустойчивый масштабируемый и прочий базборд сервис. Давай я поясню, я прочитал там условия. Они не говорят конечных целей, но цель первого этапа написать сервис, который будет через докер включаться. То есть ты загружаешь им докер образ, они его включают. После включения докер образ он должен считать начальные данные, которые будут у него на диске лежать в определенном файле. Он их считывает себе в состоянии, а дальше его бомбардирует запросами на записи чтений. Соответственно, если все совпадает, все данные на записи, на чтение там совпадают. Ну то есть ответы совпадают с тем, что должен сервис отвечать, имея эти же самые начальные данные. То как бы это вам плюс очки. Плюс дополнительные очки дают за скорость отдачи ответов. И за отказ устойчивости, что ничего не сломалось, он не перезагружался, постоянно отвечал. Вот это все тоже дополнительные очки. Но это цель первого тура. То есть я так предполагаю, что будет и второй, но я пока не нашел, есть ли там отдельные цели. Такие вот интересные хайринговенты. Вообще я считаю, что этот хайлот кап, это прям один из главных должен быть в нашем подкасте. Я имею ввиду тематика, очень совпадает. То есть все, кто слушает наш подкаст, бегом регистрироваться и участвовать. И вы получите Apple iPad Air 2 с 16 гигабайтами, ребята. Или там еще что-то, или футболки. Я несколько слушаю, и мне кажется, довольно офигенная тема. Я все давно хотел попробовать Tokyo RS, но мне как-то что-то большое писать на нем сразу странно. Ничего маленького, простого, чтобы при этом не было нелений, в голову не приходит. А тут если хочется пробовать какую-то штуку, какой-то клубочек новый, задача достаточно правильного размера с коупом. И не бессмысленно, потому что там за этом можно iPad дать, например. Да. Или футболку. То есть по сути это эхо сервера нужно сделать? Нет, там полноценный рест. То есть там информация пользователя, там несколько типов ресурсов. Пользователь, достопримечательность и путешествие, по-моему. То есть есть пользователь Вася, Петя, Саша. Есть достопримечательность Лондон, Питер, еще что-нибудь. Есть путешествие Вася в Лондон в таком-то году и так далее. То есть фактически это просто рест. И, соответственно, может быть запись вида «я добавляю нового пользователя» или «я добавляю новое путешествие вот этого пользователя» и обмениваться надо Джейсоном. Ага, ну это уже интересно. Да, я должен был рассказать, что я это торможу. Мне кажется было бы интереснее, если бы они это проводили в оффлайне. Я понимаю, что это плохо с точки зрения хайринг-ивента и дороже для компании, но это было бы прикольное такое оффлайн-мероприятие, тусовочка. А что значит офлайн-онлайн в данном случае? Ну как в офисе собраться или в хакспейсе в каком-нибудь. А, ну понятно, да. Ну может они, как его, на второй тур всех вызовут в Москву? Технически они же сейчас-то говорят, участвуйте отовсюду, откуда угодно. Вот в принципе в тему системного программирования Александр как раз увлекается всякими там футурами, прочими вещами на плюсах. Вот почему бы не зайти и не попытаться. .. Не болеть нам тоже. А что? А, извиняюсь. Саша уже начал писать, просто ему нельзя и поэтому ты зря это рассказываешь. Саша еще не бенчмаркал свои самопальные футуры и он не убежден, что STDM-Utex это достаточно дешевый примитив, поэтому там возможно еще работа на полгода. Ну вот и хороший способ проверить и выиграть iPad Air 2. И футболку. Блин, мне так нужна футболка, вот я прям без футболки сижу сейчас и у меня нет футболки. Мне так понравилось. Там, так как вы запускаете в докере, вы можете туда в этот докер-контейнер добавить вообще все что угодно, то есть ты можешь пасгри туда установить. Слушатель с ником CIIOL говорит, что организаторы этого Highload Cup специально сделали простое задание, чтобы понять стоит ли его проводить. Но учитывая, что узнал я про него как-то случайно по сарафанному радио, там от бывших сотрудников мейла, которые теперь работают не в мейле. Проблема у них не с заданием, а с тем, чтобы об этом кто-то узнал, ну как мне кажется. Ну сейчас-то они все узнают, весь мир сейчас узнает. Вот, что бы они делали без нашего подкаста. Точно. Кстати, мы ищем больших крупных постоянных патронов. Так, ну а раз тему с соревнованиями закрываем. Нет, не так, кстати о патронах. О, кстати о патронах, точно, тут шла моя тема. Какое-то время назад к нам пришел патрон и обратился с вопросом пояснить, как бы так сейчас правильно выразить. В общем, какое-то время назад я же работал в компании, которая тоже работала с кубернетосом, и мы выпустили статью настройки сети в нашем кубернетосе. И патрон пришел с вопросом, а можешь поподробнее рассказать, что они используют сейчас, перешли ли они на какие-то другие настройки, подкручивали ли какие-нибудь дополнительные network policy. То есть, если есть желание ограничивать исходящие запросы в разные сервисы, как это можно делать, какой network-продукт и какой backend использовался. Ну и все, в принципе. Я, соответственно, поздавал вопросы, ради такого случая я подготовился. Значит, сперва вопрос про предыдущего работодателя. Они сейчас в основном используются CNI для того, чтобы докер своими силами создавал интерфейсы. И поэтому все немножко упростилось по сравнению с тем, что было раньше. Сейчас они начали использовать калеку и, соответственно, пользуются всеми преимуществами калеки. Подожди, что такое калека? Калека – это как раз продукт, который настраивает сеть в Kubernetes. То есть, Kubernetes позволяет использовать плагины для настройки сетей. И, соответственно, появилась сразу куча всего. Мы в это время сидели на Flannell, был VIF, и только-только появлялся калека, но в тот момент она была не очень стабильной. Сейчас калека вроде как более стабильная. И на предыдущей работе и мы сейчас везде используется калека. Я небольшой сейчас знаток, я прям вот вам не смогу сильно рассказать про калеку, чем она лучше остальных, уж простите. Если хотите, могу потом подготовиться, домашнее задание сделать. Но как факт, примите, что она сейчас используется. Продолжай. Следующий ответ был про нашу текущую компанию. Мы используем калеку, соответственно, мы используем KOPS, это Kubernetes Operations. Соответственно, мы не решаем проблему разделения и позволения трафика на выход на уровне подов, как спрашивал наш патрон. И я не уверен, что это вообще где-то может быть решено из коробки. Соответственно, в качестве CI мы используем дрон, я про это уже немножко рассказывал. Мы сейчас прямо прикручиваем дрон к хелмчартам, чтобы это было удобно делать из коробки для CI CD. Я запишу, что все, наверное. Прям на скидку, я так не помню, что еще можно сказать. Да, да, да. Мы сейчас еще рассматриваем сервис-мэш, использовать или нет сервис-мэш для того, чтобы рулить разделением сети на уровне приложений в Kubernetes. Посмотрите на Ist.io, который вышел недавно, и есть еще парочка, я правда сейчас забыл, как они называются. Мы сейчас это в разработке, ну в смысле мы пытаемся понять, хотим мы это или не хотим. У меня такой вопрос. Я Kubernetes вообще в жизни не пользовался. Я вижу две хорошие книжки, у Manning Kubernetes in action и у Array Kubernetes up and running. Обе с пометкой неап и early release соответственно, обе примерно 17-го года. Но правда подкупают, что у Array 200 страницу, а против 500 у Manning, то есть я подозреваю, что Manning сделал 100-500 воды. Ты что посоветуешь, чтобы изучить Kubernetes? Слушай, по-моему Kubernetes up and running, он был, подожди, там кто авторы? Ты можешь его рассказать? Авторы Brandon Burns. И Келсли Хайтауэр. То есть я очень-очень доверяю Хайтауэру, я хотел поглядеть на эту книжку, но я пока еще не глядел. Все его статьи про Kubernetes, они прям очень хороши, я их с удовольствием читаю. Но я ни одну книжку еще не читал, поэтому посоветовать ничего не смогу. А ты сам доки курил или как? Во-первых, я курю доки, во-вторых, я прям небольшой-большой спец, то есть я так сбоку стою. Интересно, что я-то на самом деле довольно скептический, возможно ты знаешь, ко всему этому хозяйству отношусь. Мне даже странно, что ты читать эту книжку собрался, 8 лет еще не исполнилось. Ну это нужно и нашим клиентам, и для каких-нибудь конференций, потому что спрашивают про Postgres на Kubernetes, вот это вот все мне как-то и неудобно. Кстати, вы не пробовали Postgres на Kubernetes? Мы используем RDS или Postgres прямо на инстанциях. Кстати, а вы уже пользовались, или просто RDS, Postgres? Прямо сейчас мы RDS пользуемся, на Aurora мы не переходили, но сейчас ведем переговоры, соответственно, пытаемся понять, хотим или нет. Просто я как бы исследовал, как там Aurora устроена, она очень клевая, я как-то не хотел даже на возможном подкасте поговорить. Очень похоже на то, что прямо сейчас Amazon делает ставку на Aurora, а не на RDS. Да, именно потому как она сделана на Aurora, то я рекомендую почитать пейпер, мы его с тобой обсудим. И Саша, я тоже рекомендую почитать Aurora, потому что это довольно короткий пейпер, там довольно интересная идея о том, как оторвать storage от базы. Ты ссылку-то кинь, чтобы я мог это почитать. Или я могу загуглить там Aurora. Мы в тему добавим, просто в нашей почитаем. Я к чему это говорю? Я говорю, что мы начали рассматривать переход с RDS на Aurora или полностью на Instance, именно из-за того, что Amazon, похоже, не рассматривает сейчас RDS как свой основной продукт, и они будут его либо постепенно гасить, либо будут все меньше ему внимания уделять. Насколько я знаю, у меня косвенное сильно сведение, но там проблема в том, что они строят RDS на форках, ну там, соответственно, Postgres и других баз данных. Ну, Aurora это еще большее форки, на самом деле. Так нет, подожди, смысл в том, что представь, у тебя там какой-нибудь Postgres, не знаю, dv3, форкнутый, который ты допилил под себя, а теперь у тебя проблема, тебе нужно накатить на него 10-й, например. И при этом у тебя ничего не сломать. Другой вариант, когда ты, ну, может быть, ты что-то форкнул, но ты это будешь поддерживать сам, ну, как самостоятельную базу. Ты бы сказал, что это Aurora, все. Ну я не вижу в этом смысле разницы между RDS и Aurora. Нет, одно дело, что ты говоришь, что у тебя, ну это, возможно, не техническая фишка, это такая немножко маркетинговая, что один вариант, если ты говоришь, что у нас масштабируемый там софт фейловером и так далее Postgres со всеми его фичами, другой вариант, что ты говоришь, что у нас своя масштабируемая база, и мы ее делаем, как мы хотим. Ну, там, с точки зрения протокола, обратно совместим его. Там не заявлены, насколько я понимаю, там не заявлена поддержка всех фичей того же Postgres. Ну, Postgres у них отстает, да, и он явно с патчами, потому что... Нет, то, что он с патчами, это ни для кого не секрет. По-моему, они прям это открытым текстом всем говорят. Мы из-за этих их патчей тоже иногда страдаем. Прям сказывается? Например? У нас есть одна база, сильно нагруженная, проблема у нее с даттюплами, проблема у нее с переиндексированием и вакуумингом, и в какие-то моменты мы обнаруживали, что их RDS-овские процессы, которые мониторят базу и смотрят, не пора ли там, не знаю, сделать что-то. То есть у них есть дополнительные свои внутренние в Postgres работающие процессы, которые они там обзывают RDS, Amazon, бла-бла-бла, которые начинают забивать нам нашу производительность, ну в смысле занимают место CPU. Я неясно сказал, да? Сейчас попробую переформулировать. Нет, ты сказал ясно, но я удивлен, что они это где-то палят в каком-то мониторинге. Да, причем они это палят в каком-то неявном мониторинге, то есть это просто так не увидеть, но у нас ребята как-то заметили и научились это смотреть. То есть когда у нас сейчас происходит проблема с базой, они бегут в это место поглядеть, не Amazon ли это нам всю малину портит. Кстати, на самом деле надо самому посмотреть, я не помню где это они смотрят. Интересная тема. Скажи, ты озвучил все, что патроны спрашивали? Да, если остаются вопросы, пожалуйста, спрашивайте, я потому что сейчас пробежался по верхам, но этот вопрос был именно обзорный. Если нужны какие-то детали, то я готов ответить. Я предлагаю перейти к вопросам патронов к Валерии. Валера обещал как-то поверхностно чешись ответить. Ну, в общем, да, что тут говорить-то. Честно говоря, я посмотрел на вопрос, он немножко не очень понятен, что от меня хотел слышать.",
    "result": {
      "error": "API request failed: Error code: 400 - {'error': 'Trying to keep the first 4891 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}",
      "stack_trace": "Traceback (most recent call last):\n  File \"/home/andrei/Projects/podcast-shownotes/scripts/build_search_eval_dataset.py\", line 157, in generate_search_query\n    response = await self.client.chat.completions.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 1927, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1767, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1461, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1562, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': 'Trying to keep the first 4891 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}\n"
    }
  }
]