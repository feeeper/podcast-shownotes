[
  {
    "segment_id": "4e0b8b50-1b55-4337-a83b-eaead4561a96",
    "episode_id": "abb33af3-be76-4bc8-a589-66d082ea1895",
    "episode_number": 102,
    "segment_number": 15,
    "text": "Но в последнее время появилось буквально за неделю, за полторы появилась сразу и большая статья про Граал и Трафал. И до этого было несколько видео с разных конференций про него, и я решил про него рассказать, что это такое. Граал и Трафал это две части одной большой истории, Граал это компилятор, а Трафал это такой, скажем так, language framework. То есть это набор API, который можно использовать для того, чтобы описывать AST и работать с этим AST, то есть писать какие-то интерпретаторы и всякое такое. То есть идея в чем? У вас есть язык, вы пишете для него грамматику, вы пишете на этом Трафле, как вы обрабатываете этот AST, как вы его эволюционируете, получаете интерпретатор, расставляете какие-то аннотации и после этого вот этот Грааль, вундервафля, берет и компилирует это все вам и делает вам либо ahead of time компиляцию, либо JIT компиляция. В случае, если это JIT, то значит, что он генерирует вам JIT компилятор. По своей системе это похоже на то, как работает PyPy, то есть PyPy тоже самое, есть у них язык, который называется restrictive Python, и вы на restrictive Python пишете интерпретатор, и после этого у вас получается программа для PyPy, которую вы используете как компилятор или JIT компилятор для вашего языка. В этом случае что происходит? Запускается PyPy, компилирует код вашего интерпретатора, у вас получается ваш собственный customized JIT, заряженный конкретно под ваш язык, под ваши спецификации, под ваш AST, и ваш код в результате начинает быстро работать. То есть написав этот truffle один раз и написав для него кучу-кучу разных интерпретаторов, можно относительно дешево делать быстрые реализации разных языков программирования. Это как бы promise, идея большая. То есть Oracle перезабрели LLVM? Не совсем. Дело в том, что LLVM, ты пишешь компилятор для каждого языка, frontend для него, то есть это CLang, допустим, для C, RastC для Rasta, я не знаю, какие еще языки есть для него и всякое такое. Здесь получается, у тебя есть конструктор для того, чтобы быстро на коленке сделать свои CLang и RastC. То есть у тебя, грубо говоря, написан уже какой-то каркас, какой-то framework существует, в который ты подключаешься и получаешь сравнимую с CLang по качеству реализацию компилятора для своего языка программирования. Значит так, как будто они переизобрели не только LLVM, но еще и Flex за Bizon, вот эти все вместе. Да, скорее всего так. Самая близкая интерпретация – это PyPy, потому что PyPy именно так и работает. На самом деле много научных работ, которые вот про эту штуку, про такой подход. Это называется метатрейсинг, то есть это когда ты делаешь tracing JIT для языка путем трясения интерпретатова этого языка. То есть у тебя есть какой-то интерпретат, ты сбоку сидишь, поэтому интерпретатова собираешь трейсы, и там есть интересная тема, как бы JIT-компилятор работает же каким образом из серии. У тебя есть серия операций, и ты для этой серии операций генерируешь машинный код, и в результате в следующий раз, когда ты оказываешься в той же точке выполнения программы, ты знаешь, что у тебя уже есть трейс, и при выполнении инвариантов из серии количество операндов совпадает, грубо говоря, ты можешь просто этот трейс прогнать. Но штука какая, что вот эта часть, куда вставляется трейс, она называется inline cache. То есть в разных местах вашей программы у вас есть разные вызовы, возможно одного и того же метода, но этот метод в разных случаях может сгенерировать разные трейсы. Например, в одном случае вы, допустим, складываете два целочисленных, целых числа, а в другом случае, например, вы складываете два числа с плавающей точкой, и для этого можно сгенерировать два трейса, один из них в результате будет скомпилирован так, что он выполняется на основных, допустим, регистрах, а второй на регистрах для чисел с плавающей точкой, допустим. Так вот, если в одной и той же точке у вас приходят значения разных типов, то можно просто иметь один трейс, а можно иметь несколько трейсов и в этом месте просто поставить такой, грубо говоря, переключатель, который смотрит на типы и в зависимости от этого вызывает разные трейсы. Это называется polymorphic inline cache. И большинство динамических языков программирования, для того чтобы они работали быстро, вам нужно иметь возможность джитить вот эти polymorphic inline caches. Так вот, в случае с метатрейсами, простите за мотоциклистов, в случае с метатрейсами все осложняется тем, что самая горячая часть вашей программы – это как раз интерпретатор. Интерпретатор внутри выглядит примерно так из серии. Мы взяли следующий, грубо говоря, байткод, следующую операцию, у нас длинный кейс-стейтмент, в зависимости от того, куда мы попадем, мы вызываем уже разные куски. И получается, что у вас все это в цикле, один байткод, одна операция, следующая операция, следующая операция и так далее. И получается, что у вас появляется один огромный inline cache, который очень-очень большой тормозной и всякое такое. И на самом деле в этой точке джит нужно отключать и получать уже трейсы для отдельных операций, находящихся в отдельных зонах, и потом эти… получать трейсы только с самих операндов, но не с интерпретатора, а уже потом эти трейсы каким-то умным образом склеивать, не через вот этот длинный кейс, а именно по ситуации. То есть как у вас идет код, вы получили трейс, допустим, для первых пяти строчек, потом для следующих пяти строчек, вот вы и должны соединить. Поэтому мета-трейсинг это такая большая тема в плане того, что джиту нужно выставлять какие-то подсказки, еще там что-то. И вот PyPy много-много лет назад был первой реализацией, вот этот truffle и gral это реализация для JVM. JVM интересна тем, что как бы эмиттер кода, то есть именно вот тот джит, который в Java есть, он очень-очень эффективный. Garbage collector в Java один из самых лучших и в принципе большой интерес в индустрии есть тем, чтобы на JVM гонять много-много разных языков. Доклад, который я положил, доклад проходил на ивенте, который называется VM Summer School, его проводит по-моему университет то ли Кента, то ли еще кого-то. В Великобритании раз в год, по-моему, второй раз или первый раз проходит такой ивент, небольшая конференция. На самом деле все доклады с этой конференции достаточно интересны, но вот этот интересный тем, что с одной стороны рассказывали про вот эту систему, а с другой стороны показывали как все эти, как и gral и truffle работают именно на примере конкретного языка. В данном случае это был Ruby. Ruby интересен тем, что он очень-очень динамичный, очень такой развесистый по фичам язык, и они показывают как даже самые навороченные куски языке, допустим, когда мы динамически посылаем сообщение объекту, и объект вначале проверяет, а есть ли у него такой метод, или там динамически генерирует этот метод, если этого метода нет и всякое такое. И как вот такие совсем-совсем динамические штуки компилируются в результате в очень хороший код. Один из примеров, который у них был, это из серии функция min, получить из двух значений минимальное значение. Реализация внутри выглядит так, что вначале мы из этих двух значений делаем массив, и затем этого массива мы вызываем метод sort, и затем мы берем первый элемент из отсортированного массива. Что этот гралл в результате может сделать? Он может взять код для сортировки массива, вставить его туда, несколько раз зайнлайнить, в результате окажется, что метод вечь компилируется в просто сравнение двух элементов между собой, и затем вся эта получившаяся функция min в разных местах еще и нлайнится по месту вызова. То есть производительность на самом деле сравнима со статическими языками, с нлайнингом совсем таким. Только делается это в рентайме, вот эти оптимизации? Эти оптимизации можно сделать и в рентайме, и в compile-тайме. Потому что гралл может тебе сгенерировать то, что называется ahead-of-time компайл. На самом деле этот ahead-of-time компайл компилирует не в машинный код, а в байт-код. То есть либо ты интерпретируешь, а да, этот джит джитит все в байт-коды Java, JVM, и затем уже включается второй джит, который внутри JVM, для того чтобы из этого всего сделать уже машинный код. Либо ты включаешь ahead-of-time компиляцию, компилируешь это все в джарники, грубо говоря, и затем уже эти джарники просто выполняются на Java. Что они делают с этим трафлом в результате? У них есть несколько каких-то таких маленьких реализаций каких-то язычков. Есть более-менее большая реализация для Ruby. И еще они делают то, что у них называется managed C или truffle C. Идея в том, что они компилируют C, C++ код в байт-код LLVM, вот то, что у них называется bitcode внутри LLVM. И затем этот биткод они могут компилировать уже в байт-код Java и выполнять C-шник код в результате на Java. Это им интересно для того, чтобы вот эти вот нативные экстеншены, которые есть в том же Ruby или в JavaScript или еще где-то, чтобы их можно было выполнять на JVM. И как они утверждают, в принципе на крейсерской скорости, когда все джиты отработали, когда все кэши прогреты и все хорошо, оно по скорости сравнимо с плюсами. В общем-то ничего необычного в этом нет. Но первое, с учетом того, что у нас есть один джит, потом второй джит, а еще есть интерпретатор и всякое такое, время прогрева очень большое. И второе, вся эта штука, все эти дополнительные вещи, которые у них есть, данные для профилирования, какие-то дополнительные структуры, это все живет много-много-много памяти. На самом деле у PyPy точно такая же проблема. Пайпай всегда дает выигрыш по производительности обычного CPython. Часто бывает, что этот выигрыш может быть многоразовый, чуть ли не десятки раз. Но в продакшене его на тех же серверах используют редко, именно потому что PyPy сразу же требует гораздо большего потребления памяти. В случае с Graal и Truffle, ребята утверждают, что у них есть какие-то идеи, как это нужно минимизировать, в плане того, что после того, как джит отработает как следует и, грубо говоря, достигнут какой-то уровень и уже оптимизации не включаются, можно начинать освобождать часть вот этих собранных данных для того, чтобы в результате освободить часть памяти потом. Вот такая вот интересная вещь, над которой сейчас в Oracle работают. Часть вещей, в частности Graal и Truffle, они доступны. Сейчас это отдельные билды, но часть из них в результате попадет в Java 9. Причем попадет в Java 9 тот минимум, который нужен для того, чтобы это все работало эффективно. И после этого вот этот кусок останется внутри JV, а вся остальная реализация, то есть весь Graal, по-моему, почти весь Truffle, это будут внешние библиотеки, которые можно будет просто включать, выключать и всякое такое. С релизом Java 9 Truffle можно будет использовать всегда и везде. В Open Source пока нет вот того магического компилятора, который сирсный код превращает, объектный код превращает в Java bytecode. И, конечно, на это было бы интересно посмотреть, потому что есть интересные какие-то идеи в плане того, что, грубо говоря, у тебя есть нативный код, который при этом запрятан внутрь JV, и тут есть тоже какие-то свои вопросы, может быть, с той же надежностью, еще с чем-то. Может быть интересно посмотреть, как это работает. Вот такая штука. Честно говоря, у меня шевелятся волосы в разных местах от идеи того, что мы берем src-bytecode, джитим его в JV, потом джитит обратно в объектный код, но обратно в машинный код.",
    "result": {
      "query": "Graal Truffle JIT compiler"
    }
  }
]