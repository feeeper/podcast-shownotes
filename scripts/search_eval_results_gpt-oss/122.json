[
  {
    "segment_id": "362cfe02-1500-4a2d-98ff-90d78cbd6371",
    "episode_id": "c465d7ae-adec-4bc7-b98a-9e629907dc1a",
    "episode_number": 122,
    "segment_number": 3,
    "text": "Я прямо могу даже пример сказать, как вы у себя дома, пойдя на какой-нибудь клауд-провайдер и запустив машинку с NVIDIA, TESLA и на 12 гигов видеопамяти, за 6 баксов где-то сможете натренировать рекуррентную нейронку на перевод языков. Или на чант-бот. Да, вот просто из-за того, что вам вечером нечего делать, вы там провели час, запустили, потренировали, а с утра начали с ней разговаривать. Я, к слову, как раз планирую записать видео, как это сделать. На самом деле, ты прямо удивляешься. Тебе это доступно за 5-6 строчек кода и вечером за машинкой. Ну, еще датасет, само собой, но датасет есть. Ну окей, это, собственно, причина, почему сейчас нейронки выстрелили, почему они не выстреливали раньше. А теперь вот к теме, к самой главной теме. Что такое TensorFlow и с чем его едят. TensorFlow, как любое определение, его лучше объяснять, начиная с этимологии слова. TensorFlow, тут два слова. TensorFlow. TensorFlow – это, на самом деле, обобщение многомерного массива. Нет вообще ничего сложного. Если вы создаете переменную А, одну, это скаляр. Если вы создаете массив, это вектор. Если вы создаете многомерный массив, это уже тензор. И тензор, на самом деле, определяет все эти три подможества. Просто, чтобы не было много терминов для каждого, есть тензор. Он говорит, это N-мерная сущность. Вот и все. Ничего там заумного нет. А flow – это флоу, это поток. Соответственно, как нетрудно догадаться, TensorFlow предоставляет собой возможность создания вычислительного графа. Если, может, вы работали когда-нибудь с этим… Господи, какая же популярная Java для Big Data. Сейчас, сейчас, сейчас. Hadoop. Hadoop немножко тоже, но у Spark есть абстракция Graph, где ты можешь создавать вычислительный граф на базе Spark. Вот это где-то в ту сторону. У нас есть вычислительный граф. И по факту TensorFlow с точки зрения машин-лендинга – это очень низкоуровневая платформа, которая позволяет описывать операции над тензорами. И несмотря на то, что он используется для тренировки нейронных сетей, он может использоваться для чего угодно, что можно описать в виде вычислительного графа. Прямо чего угодно, любую задачу, которую вы можете описать вычислительным графом, вы описываете. При этом он вам в комплекте дает совсем низкоуровневый API, плюсовый, который работает на разных платформах. А сверху он вам дает красивый Python. В некотором плане можно думать о TensorFlow как виртуал-машине Java, где вы пишете высокоуровневый код на Java, в данном случае на бетоне, а он его потом маппит на низкоуровневую платформу. И на самом деле я бы здесь очень рекомендовал пойти посмотреть на White Paper. Сейчас я даже название специально записал. TensorFlow Large-Scale Machine Learning on a Heterogeneous Distributed System. Потому что он описывает основную проблему TensorFlow. Вам этот граф нужно распараллелить. То есть вам нужно этот граф, неимоверно большой, раскидать или на n число GPU на одной машинке, или на разные машинки. И здесь прям проблем огромнейшая куча, они в процессе решения. Например, как понять, как грамотно разбить граф на два подграфа и кинуть на 20 машинок один подграф и на 2000, другой требует 2000. А что, если у вас еще машинки гидрогенные, на одном Tesla с 20 гигабайтами, на другом Tesla с 8 гигабайтами? А на третьем Windows? И что в таком случае делать? И это все машинки, которые у вас есть, вам надо быстро раскинуть на это хозяйство. И он должен это делать автоматически. На самом деле ровно то же самое, что вот, как можно подумать о concurrency, GVM, которая за вас решает кучу вещей, вам даже знать не надо, но если вы полезете туда в код, там проблем неимоверно. Вторая задача, как граф оптимизировать. Например, в графе могут быть случаи, когда вы берете корень из квадрата. Понятно делать, что корень из квадрата нужно каким-то образом быстро оптимизировать, чтобы если там берется квадрат, корень не вычислялся из этого хозяйства, а каким-то образом сразу был результат. Но и у вычислитого графа есть такие возможности. Он знает о всех нодах, он знает, что происходит, он может проводить высокоуровневые оптимизации. Вторая проблема, которой тоже защищаться, не перезащищаться на кучу темп. И самое последнее, наверное, преимущество графа, ты на нем можешь делать градиентный поиск прямо по графу. То есть ты можешь описать какую-то операцию и сразу же делать обратную операцию, градиент, что очень важно для машин леарнинга. И я хочу, чтобы основная мысль, которая запомнилась, это то, что это на самом деле очень высокоуровневая прослойка. На ней можно писать то. Если вы работаете… Извините, ты сказал градиентный поиск. Я знаю, что такое градиентный спуск. А градиентный поиск, что это такое? Спуск, конечно, это как это я просто называю… Ну, потому что градиентный спуск ты используешь для того, чтобы найти какой-нибудь global maximum. Хорошо, просто я думала, что это какой-то специфический поиск для графа, и мне стало очень интересно. Если градиентный спуск, то окей, вопрос снят. Да, у тебя же на самом деле вычислительный граф, но вычислительный граф, он же каким образом? У тебя, например, есть матрица переменных, есть матрица ответов. То есть ты складываешь эти две матрицы, получаешь результат, но на самом деле это же представляется в виде графа, где ноды графа – это операции, а ребра графа – это тензоры, которые передаются между операциями. То есть вот TensorFlow, он описывает как раз вот эту вещь. Ты можешь сказать, вот у меня два тензора, они вливаются в операцию перемножить, полученный тензор вливается в операцию, я не знаю, обратить эту матрицу, и так далее. Но поскольку это очень низкоуровневая прослойка, было очень много проблем, чтобы эту штуку вязать, и на TensorFlow сделали много настроек. Но перед тем, как я еще чуть-чуть расскажу о настройках, я хочу ответить на еще один вопрос, который мне чаще задают, а почему, собственно, Google вызывал консорсов? Вот, вот, а зачем? И эта тема, на самом деле, очень интересная, о ней говорили много гугловых сорсеров, прямо вот есть, если вы не цураетесь, английского, хороший подкаст Change.log, там приходил свой персонал от Google, он, по-моему, хорошо педалировал эту тему, но на английском, там, 2019-й. Но если кратко, то посмотрите, что происходит у Google. Выходит какой-нибудь whitepaper, ну, давайте, например, вспомним по Bigtable, да, он вышел где-то там в 5-6 году. Что происходит дальше? Рынок смотрит на этот whitepaper, ух ты, крутая штука, давайте мы, референсов, имплементаций нет, давайте мы сделаем нашу, получается, HBase. Получается, HBase, и потом, когда HBase появился спустя, я не знаю, наверное, лет 10, ну, ладно, не 10 лет, это вру, потому что это не 10 лет, это в этом году, но так, существенно позже, вдруг Bigtable вынужден начинать поддерживать API HBase, который не совсем совместим с Bigtable. А почему? Ну, потому что, вот, open-source хотел этот whitepaper реализовать, там есть куча умных людей, и они посмотрели, это классно. То же самое MapReduce. Все это знают, теперь продвигают везде, то есть приходит человек, знающий HBase, и ты не можешь не использовать его знания и навыки. Он уже пишет фактически на другом языке. Конечно. Вот-вот. То же самое MapReduce. Если возьмете whitepaper, MapReduce, что-то там про упрощение дата процессинга, называется whitepaper, из него вырос Hadoop, он тоже появился раньше. Я уже молчу про мой любимый Borg, то есть если вы есть такой, я тоже рекомендую почитать, whitepaper, сейчас я название утроил, Large-scale cluster management Google with Borg, где описано менеджмент кластеров, который побит на селы, у каждого села там среднем, медианный размер, это 11 тысяч машинок, и как вот Borg менеджит все это хозяйство в виде контейнеров. А мы, кстати, обсуждали эту тему в каком-то выпуске подкаста, мы это paper проходили. Да. О, ну вот, а потом появился Docker, да, и как бы, то есть посмотрели люди на это хозяйство, и по-моему об этом говорил Jeff Dean, о том, что вот, мол, open-source все создает, то, что мы экспозим через whitepaper, все, больше такого не будет. Вот у нас есть TensorFlow, а вот у нас референсная имплементация. И судя по тому, что происходит, это действительно сейчас самая популярная платформа для обучения нейронок, и она медленно становится стандартом де-факто в индустрии. Не просто whitepaper и ждать, пока кто-то для его сделает, а потом нам внутреннюю инфраструктуру придется делать вот в соответствии с стандартами на рынке. Теперь вот нет. Само собой, стратегия выбрана другая. Но опять, я напомню, я не official spokesperson, это вот логика, которую можно выстроить на базе открытых источников. Я вот даже указал на подкаст, где official person на английском доносит светлое и прекрасное. Так, давайте чуть допомню по поводу различных библиотек. Но надо сказать, что TensorFlow на самом деле не единственный фреймворк для нейронных сетей, который популярный. Есть еще достаточно популярные такие, как Tiana, Torch, дальше Keras, что еще приходит на ум? Coffee, который мы обсуждали неоднократно в подкасте. И у каждого там есть свои плюсы, свои минусы, но TensorFlow на самом деле считается таким объединяющим все другие в плане функциональных возможностей. И еще хочется тебе добавить, дополнить то, что ты сказал, Google его анонсировал, заопенсорсил, но в 2012 году у Google был другой проект внутренний, назывался Disbelief, да. И они его не опенсорсили. И продукт этот использовался для внутренних сервисов Google, таких как поиск, дальше карты и YouTube. Вот. И он был чисто для нейронных сетей, насколько я знаю. А TensorFlow, он дает больше возможностей. Я бы хотел еще добавить. Если все тебя перебивают, извини. Это вообще в целом получается тенденция. То есть у Google все последние инициативы, они очень часто уходят в open source. То есть мне сразу на память всплывает Kubernetes, который они по аналогии точно так же решили открыть народу. И сейчас очень большой комьюнити здесь развивается. Но я фанат этого направления, поэтому тоже хотел добавить. Да, это изменение курса, которое вызвано теми проблемами. Кстати, Disbelief, опять же, если вы из TensorFlow, пойдите почтите пейпер по Disbelief, наверное, в начале, чтобы понять проблемы, которые решает TensorFlow, которые были у Disbelief. Они его опубликовали в 2012 году. Он еще по факту развивался в той схеме, когда он in-house, но у нас есть whitepaper. И, кстати, к списку нейронных сетей Света не сказала, MXNet, который педалирует активно AWS. Да, совершенно верно. MXNet тоже довольно известный. Да, я думаю, они сейчас обещали в него хорошенько вложиться. Ну вот, это, собственно, понимание того, что TensorFlow дает очень низкоуровневую платформу. А теперь же надо какие-то API делать. То есть на самом деле разработчик TensorFlow обычно искать, я хочу рекуррентную сетку, давай-ка я сейчас ее налобаю. Нет, так не работает, там есть тензоры, он смотрит на эти тензоры, а куда тут к ней запрыгать? Ничего не работает. Тебе надо знать математику, тебе надо, кстати, пользуясь случаем, почитать вот есть Deep Learning Goodfellow. Прекраснейшая книга вышла в 2016 году, буквально недавно. Я вообще рекомендую прочесть, если вы любите это дело. Она причем с самых начала, потому что… Вот, да, дам ссылочку. Так вот, что же делать обычный дем, которые хотят вот в четыре строчки, как я сказал, натренировать рекуррентную нейронку? Для этого, само собой, создаются высокоуровневые API. Одно из направлений – это TF Learn. Это высокоуровневый API, который напоминает Skylearn, и он на самом деле в части API даже с ним совместим, когда на высоком уровне просто описываешь я хочу нейронку натренировать, и я ничего не хочу больше знать. Вот мои данные, вот мой тип нейронки. При этом, само собой, на первом этапе ты будешь пытаться совместить несовместимое, оно даже будет пытаться учиться, но бог с ним. Вот TF Learn – это как раз высокоуровневый подход. Дальше есть Wide and Deep. Wide and Deep – это когда у тебя есть данные, но ты их не можешь нормально структурировать, но ты хочешь найти там закономерность. Он пытается это все сделать. И, наконец, вот это все завернуто в такие как Data Lab, где ты можешь, это по факту Jupyter Notebook, который плотно интегрирован в Google Cloud, где ты можешь прямо в браузере запускать ноутбукчик такой, который тебе делает графики, где ты можешь вести это хозяйство. Все это ранится пока что на базе контейнера, который легко развернуть на Kubernetes. Если у вас нет кластера на Kubernetes, есть Cloud ML, где ты можешь написанный код на TensorFlow запускать для обучения прямо там. Ну и, наконец, для невысокоронивой вещи – это есть DL Repository, Models. Туда сейчас часть вещей переезжает. Мне больше всего нравится эта моделька для транслейтера. Если вы пойдете на, он до сих пор лежит в основе ветки на GitHub TensorFlow, дальше папочка, сейчас по памяти скажу Models, под папкой RNN, под папкой Translate – это полностью готовый код, который качает и обучает код на тему. При этом я сделал такой забавный эксперимент. Я скачал базу данных диалогов фильма, разбил диалоги на то, что сказал герой А, то, что сказал герой Б, и вычленил диалоги, которые в одно предложение, чтобы не было, что А говорит много, потом Б. Дальше я сделал два файла, где с одной стороны, допустим, строчки 5 – то, что сказал герой А, с другой стороны строчки 5 – то, что сказал герой Б. И вот буквально за одну ночь получился шикарный чат-бот. И чтобы это сделать, вам надо зайти по тому примеру, который я сказал, вместо английского и французского подсунуть ей вот эти два дампа чатов. Я, наверное, выложил в open-source поделку, которая генерируется из дампа обычного, вот такой специальный, но там, понятное дело, нечего писать. И пойти взять машинку с какой-нибудь Теслой, и вот за ночь вы можете ее натренировать. Если вы, например, найдете дампы разговоров «Властелина колец», вообще будет жесть и классно. Вы натренируете чат-бот, который говорит в стиле Гендальфа, и будет прям ништяк. Слушай, но ты так говоришь, что будто бы это на самом деле очень легко и просто. Это что, на самом деле так легко и просто уже? Ну, смотри, рекуррентную сеть уже сделали. Это знаешь, чем можно сравнить? Как натренировать линейный регрессор на Skype.py. То есть да, ты должен знать, что такое линейный регрессор. Но тебе надо рекуррентную нейронную сеть, а не другую. Но тебе надо понимать на high level, понимать, что обычная нейронная сеть с прямым распространением принимает фиксированный input. Если тебе надо изменяемое число input, как в предложении, она может иметь изменяемое число, но тебе надо рекуррентную. Но тебе не надо писать в саму сеть. Ты можешь пойти, где этот RNN тренируется, у него есть метод fit, дать данные, натренировать. И пока что действительно тебе надо машинка с мощной GPU или действительно кластер. И я же не зря этот пример привел, потому что я привел достаточно информации, чтобы любой слушатель пошел натренировался и чат бота. Слушай, ну это реально такое ощущение, что время нейронных сетей на ассемблере прошло и пришло время, когда все пишут уже на C или даже на Python. Но я имею в виду, концептуально все становится проще, и у тебя уже есть великие инструменты, которые тебе за вечер позволяют гэндальфом говорить. Да, поэтому я еще и сравнил. Это такая себе новая версия виртуальная машина для Machine Learning. Ты пишешь на тоне, а она это все мапит на уровень плюсов, на уровне машинного кода даже разных платформ, и пытается оптимизировать вот этот, давай так скажем, кусочек логики, который написан сверху. Вот ровно как работает JVM. Мы уже созрели на то, что все готовы платить overhead по вот этой низковыровневой платформе за все вот эти garbage-collectors и так далее. Здесь в этом случае мы говорим о том, что мы созрели платить за то, что некоторые куски графа, которые ты бы мог оптимизировать, переписав это сам на плюсах, не будут соптимизированы пока что, потому что оптимизатор графа еще их не умеет оптимизировать. Можно вопрос быстренько? Да, конечно. Во-первых, слушатели притащили, Олег Агваров притащил ссылку на GitHub HF Trader Deep Learning Book. Это оно? Я, вот я, сейчас, ладно, я открою одну вкладочку, посмотрю. HF Trader, не уверен, сейчас я смотрю. А, да, да, да, да, да, да, да, это те же авторы, да, Goodfellow, конечно. И у них есть еще сайт, наверное, Deep Learning Book, наверное, dot.com, вот, и на сайте у него просто шикарное объяснение. Да, Deep Learning, да. А это, наверное, GitHub с отцами на этой книжке, то есть да, это точно оно. Ну, это не книга, но это в ту сторону. Слушатель Илья Кичек спрашивает, не является ли, не кажется ли гостю, что в тензор слов в последнее время слишком много всего притащили? Например, из последнего поддержку Винды и OpenCL, не замедлит ли это добавление новых фич? Так тут важно понимать, что, опять же, думайте об этом как о платформе и о библиотеках. Если вы с ML связаны, вы наверняка работали с такими вещами, как NumPy, Pandas, Skykit-Log, TfLearn, и они работают на разном уровне абстракциях. В TensorFlow вот эти вещи, которые я перечислил, это разные проекты. То есть да, они, ну, например, TfLearn, он называется TfLearn, но у него своя подрепа, своя часть, своя команда. Соответственно, основная часть, которая занимается платформой, они растут, у них там свои задачи, и вот это ощущение насчет того, что там очень много, потому что все эти высокоуровневые библиотеки пытаются объединиться одним названием, и они действительно вокруг TensorFlow построены, и можно логически об этом думать, как вот Pandas, NumPy и SkyLearn, которые работают на разном уровне абстракции, и от того, что появляются новые библиотеки, это не сильно импактит разработку, например, NumPy. Ну, наверное, за тем исключением, что если выйдет суперкрутая новая либа, и корк-атрибьюторы NumPy захотят туда уйти. А я увидел, что туда притащили еще и Go. Он-то тут зачем? Ты можешь прокомментировать? А, во, это хороший вопрос. Дело в том, что поскольку есть разделение высокоуровневого и низкоуровневого API, то сам API на самом деле плюсовый. И у вас есть SWIG, если я не ошибаюсь, вещь, которая позволяет делать в wrapper на высокоуровневом API. Она для того, чтобы вы могли присобачить любой высокоуровневый API. Там вот есть на GitHub issues по поводу работы с Java, которые открыто педалируются, и я не знаю, честно говоря, в каком он состоянии. И это на самом деле очень важно, потому что TensorFlow, окей, вы модельку обучили, дальше что? Вам же надо эту модель запустить, то есть задеплоить на production, запустить реальный код, который будет через нее просеивать данные. На Prode у вас может крутиться что угодно. Соответственно, вам действительно надо в высокоуровневый API того языка, который у вас крутится на Prode, чтобы вы могли это деплоить и связывать с вашим кодом. Поэтому это проблема, и туда надо прикручивать разные высокоуровневые врапперы, по факту, через SWIG, чтобы они могли общаться с низкоуровневым TensorFlow и работать с моделями, которые вы тренируете. Это очень интересный вопрос, как затащить TensorFlow в production, и надо ли пользоваться какими-то ThreadPath решениями, какими-то SAS решениями, или если проект крутится на AWS, можно ли каким-то образом внедрить его в эту AWS структуру? Как здесь нужно мыслить? Ну, это хороший вопрос. Я хочу, кстати, сказать, что у AWS, как минимум для изучения есть, вот если вы решите учиться на AWS, у них есть Deep Learning Optimized AMI, они так называются Deep Learning from Amazon, где у вас предустановлены практически все популярные Deep Learning Frameworks. Не просто предустановлены, там даже папочка Source есть, где они уже и склонены, и предполагается, что как минимум для исследователей с этим AMI будет работать проще, но в целом, да, в мире Machine Learning отсутствуют такие вещи, как, например, пока что Continuous Integration, Continuous Delivery, нет такого-нибудь маркета, куда бы ты мог сгрузить свою модельку обученную, а кто-нибудь за деньги мог бы ее взять себе, просто тупо склонить и начать использовать, потому что у вас, например, появились данные, какой-то датасет, вы обучили нейронку, вы хотите теперь эту модель продать. Вот нет такого решения, которое бы позволило очень легко и просто взять и продать это хозяйство на маркете. Ну, в основном, я не знаю, может уже и появляются. У тебя есть впечатление, ощущение, как скоро это появится? То есть это дело месяцев, лет? Я без понятия, не знаком, насколько… То есть технически вам что надо? Вам надо какую-нибудь вещь бинарников, типа, как это, артефактория, которому запилить специальным образом плагин, который умеет грузить, качать, и еще какую-нибудь админку для продажи. То есть в теории звучит вроде просто, но я много отсюда компонентов не знаю, насколько это просто реализовать. Мне кажется, что вот должно скоро появиться. Но в принципе, да, я не знаю. Мне кажется, это поскольку в воздухе летает, кто-то это сделает и очень скоро, и будет отлично. Но в принципе много machine learning и science, они просто не знают, что это такое. Они не знают, что они этого хотят. У них есть ноутбук, в котором они работают, и они не сильно парятся в этом, а они просто говорят, что это не нормально, вот тебе с Артемом GitHub, скелируй, давай запусти, а там трава не растет. Да, нужно быть в обоих мирах, для того чтобы понять, что их можно соединить между собой. Да, да, да. А вот, кстати, подсказывают чатики, что есть уже всякие маркеты. Я единственное, что вообще пока не видел турнирно-инстанциальную поддержку с двух сторон. Вот думайте об этом, например, о Mavani, или о Amazon Маркете, о Ami. Здесь должен появиться мощный игрок, у вас будет поддержка со стороны платформы, вы в XML-е написали, я хочу модельку, которая распознает фоточки, вот отсюда, я за нее заплатил или не заплатил, она тебе тупо донлойдит и все делает. В некотором плане чуть-чуть похожая вещь, вот если вы воспользуетесь моим советом, и пойдете учить транслейтер на рекуррентной сетке, то вы заметите, что автоматом качает датасет. Если ему не кормишь готовый, он тупо идет и качает датасет. Вот такой себе пример. Но вот таких средств, которые бы автоматизировали вокруг даже готовых решений, которые качают бинарники, ты просто зависимость прописываешь и все, я не встречал. Я, конечно, еще могу и углубляться в разные архитектуры нейронных сетей, чем они отличаются и зачем, но, не знаю, наверное, я так уже много наградил по времени. Кстати, Слава, а вот ты не считаешь, что TensorFlow ставит разработчика довольно в жесткие рамки? То есть он загоняет тебя в конкретную модель работы, когда у тебя есть отдельная задача по моделированию данных, и у тебя есть отдельная задача по работе твоей модели. То есть она всегда заключена в сессию, и вот эта жесткая модель, тебе не кажется, что она слишком такая жесткая и трудно как-то ее поменять? Ну, да, и тут я не буду спекулировать, потому что я не знаю, почему случилось так, но у любой нейронной сети есть такие проблемы при обучении, что, например, у него есть такие проблемы при обучении, одна из которых, например, это из-за того, что она ресурсосотратная, тебе приходится делать ее жестко, очень сильно имитабельной. Причем тебе надо эту имитабельность навязать юзеру, чтобы ты мог в нужный момент сделать dump, и если у тебя что-то произошло на машинке, ты мог с нужного чекпоинта за все заресторить и продолжить обучение нейронной сети. То есть вот когда я обучаю сети, я их в инфинит режиме запускаю на обучение, потому что они все равно не доучатся на моем Тесле несчастной, потом я просто келяю, когда я считаю, что уже все, у меня денег нету дальше учить, вот я их кельнул и начинаю юзать, и юзаю с последнего чекпоинта. Чтобы сделать эту схему чисто технически, когда у вас есть постоянно рабочие дампы по мере обучения, вам приходится навязывать определенную схему и определенную философию работы с этим графом, и очень трудно иметь одновременно и то, и другое, когда вы даете DT флексибилити, а с другой стороны юзер может чего угодно делать, но тебе это чего угодно надо задампить, чтобы потом зареюзить, и тебе надо middle ground какой-то. Сейчас это чисто теория, я на самом деле не знаю, какая была более высокоуровневая философия, но подозреваю, решалась вот какая-то из этих проблем. Вот. Вот, вот, вот. Ну, девушка, такой вопрос. Давайте я… Вопрос такой. Давай вопрос. Говори, говори. Вопрос. Вопрос такой. Мы все знаем классическую проблему о том, как затащить новую технологию, новую платформу, новый язык программирования в существующий production проект. Как продать его менеджменту, заказчикам, коллегам, есть разные советы по этому поводу. А вот как затащить TensorFlow и ML в какой-то типичный бизнес проект? Есть какие-то советы по этому поводу? Ну, конечно, ML на самом деле, она такая вещь, где ты можешь очень наглядно показать выигрыш от ML. Если у вас есть размеченные данные, количество юзеров, которые приходят на ваш сайт, количество юзеров, которые регистрируются с вашей платформы, с вашей лендинг странички. Это, кстати, идея, которую я разрабатывал еще до того, как присоединился к компании Добра, где ты можешь оценивать то, какого качества у тебя лендинг, насколько много он приводит юзеров. Соответственно, потратив пару вечеров и взяв даже wide and deep, о которым я говорил, и оскорбляя его неструктурированные данные, получив какую-то зависимость, ты можешь прийти в бизнес и сказать, вы знаете, вот я сделал такой первичный анализ и есть зависимость нелинейная между, например, цветом кнопочек, размером баннеров и сколько людей к нам регистрируются. И это означает, что меняя вот это, я могу вам привести больше клиентов. Более того, я даже могу сказать, насколько больше я их приведу, по мере того, как это будет меняться. И вот эту вещь очень легко продать. На самом деле, сейчас… Ну, надо сказать, что сейчас есть уже такие сервисы. Сервис называется Instapage, который может делать A-B-T-S-T для твоего лендинга. Вот. Ну, просто хочется пойти дальше, чтобы у тебя была не просто система, в которую ты задаешь, вот у меня страница 1, страница 2, давайте проведем A-B-T-S-T. А такая же система, чтобы автоматически меняла цвет кнопки, а не так, что вот я думаю, синий либо зеленый, а на самом деле пользователям больше нравится желтый и больше приносит денег. Вот. То есть хочется, чтобы она была такая настолько умная, чтобы могла сама придумывать для людей сайта, так и для, наверное, различных слоганов на вашем сайте. Ну, да, это будущее A-B-T-S-T. Сейчас то, что называется A-B-T-S-T, это прошлый век. Ты должен eyeball-ить результат, ты должен пойти посмотреть результат, ты должен понять, что лучше, ты должен поменять. И что самое интересное, ровно на следующий день, поскольку ты поменял, эти данные уже устарели. Потому что кто его знает? Смотр, это прошло, но толку, у тебя уже закончился твой A-B-T-S-T-инг, ты уже не можешь проводить эксперименты, и ни одна компания в мире не начнет проводить тот же самый A-B-T-S-T, на который надо труд человека часы, заново еще раз через неделю, через две. Соответственно, они уже не знают, что поменялось в мире. Тебе надо вот какая-то динамическая, постоянно дообучающая МЛ-моделька, которая будет постоянно дообучаться на продакшн данных, при этом не обязательно это делать стримом, и раз в месяц сделать, который будет ее переобучать, потом заново апплайить это на продакшн. То есть МЛ позволяет пойти на шаг выше. Знаешь, давай я тоже добавлю, просто касается МЛ, вот ты, мы говорили про разные цвета кнопки, но ведь на самом деле страница лендинговая только один шаг. Но ты эту страницу каким-то образом показываешь или, как это сказать, распределяешь по различным источникам потребления информации. И то есть получается довольно интересная картина, когда ты оптимизируешь только последний шаг, непосредственно твою страницу, но на самом деле тебе нужно оптимизировать целый граф. А граф начинается, когда ты страницу показываешь, а показываешь ты ее в какой-нибудь, например, в социальной сети, и с помощью какого-нибудь рекламной площадки, будто какой-нибудь Facebook Ads либо Google Ads. И это все можно, в принципе, автоматизировать. И мы получаем довольно интересную картину, когда мы можем с помощью машинного обучения создать такой граф, который приносит больше всего пользы вам. То есть вы хотите оптимизировать по количеству сайнапов либо по количеству еще чего-то. И это не только последняя часть, а на самом деле весь граф, который касается вашего места, где вы заказываете рекламу, дальше социальной площадки, где вы рекламируетесь, будь то там, не знаю, в какой-нибудь сети, Facebook, Google, Twitter, LinkedIn для кого-то. И дальше, конечный этап – это ваша страница. Я вот хочу сказать, что совместная проблема, которая же бежит вместе, но она про email, но она про то, как найти фичи для входа вашего email. И это действительно проблема и все это все не понимает. Сейчас хотя бы донести бизнесу о лендинге и подобных вещах – это уже big deal на самом деле. Куча людей вот это все не понимает. Вот это все, что сейчас описала Света, это такое далекое будущее, которое мы однозначно придем, когда бизнес все это поймет. Даже на самом деле тот небольшой бизнес, который уже начал это понимать, они создают весь этот хайп, который происходит вокруг email сейчас и начинает его юзать. Вы думаете, что для таких сложных вещей, как построить граф и потом найти зависимость, вам нужно сложный email? Не. Вам даже TensorFlow может понадобиться, вернее понадобится, но, например, может быть TF Learn, где есть линейный регрессор. И окажется, что все эти данные, которые у вас есть в фиче, они прям линейно зависят и влияют на одну вещь. Вам даже не надо большой машинки с NVIDIA, вы просто реально это учите прям совсем на локальной машинке. Эта проблема грамотно сформулировать фичи, она есть, она несложно решаема, но да, это надо объяснить текущему бизнесу, чтобы все начали массово двигаться и всем TensorFlow. Вот, вот, вот. Ну да, конечно, вам еще понадобится человек, который будет знать, какой тип email находить, но это такое, это тоже. А вот у меня к тебе вопрос. Ты с TensorFlow довольно, скажу, много работаешь уже, наверное, или как-то сталкивался. Ты можешь привести примеры задач, для которых TensorFlow вот вообще никак не подходит и вот нельзя его использовать, нужно что-то другое брать, какую-то другую библиотеку. Хороший вопрос. Хороший, хороший вопрос. Я просто из-за того, что, опять же, это, напомню, это вычленный граф, на нем можно решать любые email-проблемы, и по нему нет негативных регрессоров. Он кажется, ну то есть из того, что написано по TensorFlow, он кажется, что вообще для всего подходит. Но если tool написан, для всего, то наверняка, это, знаешь, какая-то проблема, когда мы пытаемся решить весь класс задач, и у нас есть какой-то tool более специализированный, то специализированный tool, как правило, лучше решает какие-то конкретные небольшие задачи в какой-то конкретной области. Так это опять немножко пеннисов в параметрах. Он не решает все проблемы, он дает общий tool. Это, знаешь, как сравнивать с NumPy. NumPy тебе предоставит операции над матрицами. А теперь вопрос, есть ли какие-то задачи, которые трудно решать NumPy? Ну почти все, что матрицами можно решить NumPy. В данном случае ты, имея TensorFlow, можешь выбрать высокоуровневую настройку. И вот настройку у тебя надо выбирать не общую, а конкретно заточенную под задачу, которую ты решаешь, которая на низком уровне будет бежать над TensorFlow. Или опять же, поскольку я люблю продолжать сравнивать с виртуальной машиной, это вот то, что будет бежать сверху на VM. В данном случае, само собой, есть задачи. Я видел от моего имени на Stack Overflow написано пару вопросов, которые меня конфьюзят, когда, например, регрессор построен не совсем так хорошо, линейный регрессор построен не совсем так хорошо, как я бы хотел. Его можно построить получше, с учетом скалерна, но возможно это уже пофикшено в последней версии. То есть просто из-за того, что некоторые высокоуровневые библиотеки, они новые. Там бывают интересные и странные баги, которые, я уверен, или уже закрыты, если я его дергал. Но в целом, если ты можешь использовать матрицы, то TensorFlow тебе подходит. Но тебе надо найти высокоуровневую абстракцию, которая хорошо решает твою задачу. Вот. Это хороший вопрос, как найти такую высокоуровневую абстракцию. Как узнать то, чего ты не знаешь. Да, то есть вот это вопрос тоже. Вот есть много нейронных сетей, которые решают какие-то поставленные задачи. И, наверное, люди, которые придумывают архитектуру этих нейронных сетей, экспериментируют, они пробуют различные варианты. И хорошо, вот я разработчик, инженер, например, я не data scientist, я посмотрел какие-то пейперы, ага, вижу, вот, это вот такая нейронная сеть, она решает такую задачу. Хорошо, взяла, запрограммировала, с использованием того же TensorFlow, либо ещё чего-то, и оно работает у меня. Но вся соль же в том, чтобы понять, как эту архитектуру сделать, как создать такую нейронную сеть, которая будет решать эту задачу. А дальше уже сделать её работающей at scale, это уже инженерная задача. Мне кажется, это не настолько трудный, такой вот именно наукоёмкий процесс, как создание моделей, создание архитектуры непосредственно. Само собой, да, мы вообще эту тему обошли стороной. Действительно сейчас есть как бы две направления деятельности. Это science, где разрабатывают новые типы архитектурных сетей. Есть чисто прикладное, такой краткий обзор. Если у тебя есть на вход какие-то данные изменяемой длины, очень хороший пример, это как раз предложение, то нужно использовать правила хорошего тона. Люди часто используют рекуррентные сети, потому что рекуррентные сети, они на вход потребляют один элемент за другим, запоминая прошлый элемент, и в какой-то момент ты можешь сказать, всё, элементы закончились, давай мне результат. Соответственно, если у тебя похожая задача, ты берешь RNN. Если у тебя есть более скима данных, когда у тебя есть скима чёткая, у тебя есть набор полей, ты знаешь, как разбить, ты используешь, как правило, классические сети прямого распространения, когда у тебя фиксированный вход, фиксированный выход, точно знаешь, что будет на вход. Есть ещё некоторые гибриды, которые, например, используются, если у тебя очень большой вход, например, в мультибоксу, которые используют так называемые свёрточные сети. Свёрточная сеть – это такой интересный тип, который говорит, окей, картинка очень большая, просто нереально выучить такую огромную сеть, которая бы прохавала всю эту картинку, давайте мы выучим маленькую сеть, которая, например, хавает 10 на 10 пикселей и строит абстракцию на базе этих 10 на 10 пикселей, что она там увидела. Например, представьте, у неё запяток, час девятки или ещё что-то. Дальше вы этой сеткой прибегаете по всей картинке, и по факту у вас вся картинка разбита по квадратикам, каждая из которых 10 на 10, но это одна и та же сеть. После этого результат работы этой сети передаётся более высокоуровневой, высокоуровневой и так далее. Вы как бы сворачиваете картинку до тех пор, пока более высокоуровневый слой, который выслит абстракциями, не выдаст какой-нибудь ответ. Раз за разом ты повторяешь одно и то же, только вопрос у тебя есть как-то обратная связь от предыдущего уровня, что-то типа такого. Да, конечно. У тебя по факту, если ты представишь свёрточной сети, это будет pipeline, и ты на каждой новом этапе на вход, например, на втором слое ты на вход подаёшь на n результатов работы одинаковой нейронной сети, которая свернула эту картинку. То в рекуррентном ты на тот же самый вход и дальше данные, у него просто есть память. При этом свёрточная сеть, она всё-таки будет оптимизирована в конечном итоге под какой-то размер входящей картинки. В неё нельзя запихнуть совсем какие-то, не того размера, который что-то запихиваешь. И, кстати, это создаёт проблему, потому что эти сети тренируются обычно на квадратный вход, а у тебя же снимок не квадратный. И, например, если у тебя есть сеть, которая смотрит по всему снимку и решает снимок, чтобы ты, наконец-то, нашёл такой угол и такой сайз, в котором апельсинка этой сети таки распозналась. Да, тут, кстати, есть последний довольно-таки серьёзный прорыв в этой области, который был сделан. Это сети, которые сначала смотрят на снимок и определяют, а где же здесь область и объекты, которые бы выделил человек. Есть интересный whitepaper, который как раз рассказывает. И потом вот эти области, они как раз расположены в один квадратный номер, и ты знаешь, где они, какой они имеют scale, ты можешь скормить второй сети, которая уже распознает, что на этом. При этом, кстати, распознавание, что на этой картинке, оно тоже на самом деле, если вы думаете, что есть одна сеть, которую вы даёте на выходе, это апельсин или конь, нет, у вас есть динь. Одна умеет искать апельсин, другая умеет искать конь. И вам надо скормить обоим этим сеткам, и в теории, наверное, можно создать одну большую, которая прямо на выходе будет говорить, что это, это конь, это апельсин или так далее, но на практике те whitepaper, которые мне попадались, они вот довольно-таки консервативны в этом плане. И вот всё, что я сейчас перечислил, это 4-5 типов. И просто поигравшись этими 4-5 типами, вы сможете знать, какой выбирать, если вам надо прагматично выбирать тип сети. И только если вам не подходит, тогда начинайте уже копать что-то своё в плане ресёрча, в плане джавы. Поставьте синхронайст, не парьтесь, будет медленно, и только если это bottleneck, окей, тогда открывайте concurrency, учите happens before и копайте в ту сторону или переписывайте. Но пока оно не bottleneck, то делайте просто читабельно красиво. Вот здесь ровно то же самое. Берите готовый однострочный RNN, учите, и вот только если он не подошёл, окей, тогда ладно, тогда что-то делайте. Но на самом деле мало случаев, когда реально готовая сеть вам не подходит. И взяли, решили с нуля рульцами написать, написали то, что косо-криво, но работает. Вы даже не подозреваете, что рядом есть готовый RNN, который бы работал лучше. Слушай, а может есть какой-нибудь, не знаю, блок-схема, либо какой-то блок-пост, который говорит следующее. Вот у вас есть такая-то задача, которая решается с помощью методов машинного обучения, либо искусственного интеллекта. И ты понимаешь свою задачу, и там будет описано, что есть какой-то структуры, либо вам нейронная сеть не подходит, вам нужно использовать алгоритм машинного обучения. Есть ли что-нибудь такое? Вот, к сожалению, по нейронкам нет, я даже не знаю, дойти сделать, а вот по Skykit Learn есть пару классных датаграмм, к сожалению, их под рукой нет. Я постараюсь найти, где вот очень красивый граф, а сколько у вас данных? До такого количества больше. Они размечены, не размечены? И вы можете сделать то-то-то, и он так вот медленно идя по графу вводит вас на тот пакет Skykit Learn, который вам подойдет для решения конкретно вот этой задачи с такими простыми вопросами. Вот действительно было бы круто что-то похожее по нейронкам, когда ты отрянешь такие простые вопросы, приходишь к тому, что да, нейронки подходят, вот тип нейронки, который тебе стоит изучать или нет. Сорри, нейронки не подходят, у тебя мало данных, ну типа такого. Я не видел, я не знаю, может это есть, я не видел, и прям руки чешутся сделать. Но я думаю, что это будет очень интересный и очень интересный видео, где я таки покажу как две строчки своего бота на RNN написать. Скажи, а у тебя не появляется задача, где нужно основу придумывать архитектуры для нейронной сети, либо ты больше используешь уже существующие? Я помогаю, я работаю с проектом в том плане, что я работаю над системой самой, то есть нет, я не разрабатываю новую архитектуру, работаю с людьми, которые разрабатывают, может когда-нибудь начну, но нет. И опять же, я, к сожалению, не могу варить само собой за рабочими задачами в теме хобби, где я копаюсь и решаю, там у меня пока что не было задач, где бы текущая сеть не справилась. Может быть, новый тип сети решил бы задачу, ну это пальцем не в... так всегда можно сказать, может быть, новый тип языка, алгоритм или еще что-нибудь решил бы это быстрее, ну может быть. Просто очень интересно понять, вот как они вообще создаются, вот эти новые архитектуры. То есть сидит ученый, думает, а давайте-ка я подумаю и сделаю чуть-чуть вот иначе. Либо есть какая-то методология, которая говорит, что вот нужно использовать здесь вот лучше это, а там вот то. Ну смотри, вот Multibox, если посмотреть в whitepaper, я вот попытался его найти, но не хочу сейчас активно гуглить, но к шоу-ноткам постараюсь добавить. Он же по факту вот Multibox или как-то так называется, это вот тот концепт, что я писал, где одна сеть говорит, какие элементы на фотографии важны, потом другая сеть определяет, что на этих элементах. Так вот, если ты посмотришь, это же просто соединение двух пайпов. Взяли один пайп, соединили с другим пайпом. И на это сложно смотреть, когда ты этим не занимаешься. Но точно так же ML-щик смотрит и говорит, а как вот здесь эти люди решили, что сюда надо вставить Kubernetes, там же столько огромных вариантов, а они каким-то образом поставили только Kubernetes. Хотя на самом деле, это же не только Bigtable, еще что-нибудь. Короче говоря, сегодняшняя работа инженера, особенно архитектора, это соединять трубы. И вот если посмотришь на современные пайпы, там очень редко такие суперконцептуально новые вещи. Там, как правило, происходят вот такие вот вещи, как я писал, соединение двух разных труб. Недавно вот, например, Питер Варен, если я не ошибаюсь, на своем блоге опубликовал просто интереснейшую вещь о том, как портировались с TensorFlow на Android, на мобильный устройство. Там взяли и сделали очень простую штуку. Для того, чтобы хранить модель, тебе надо огромное число цифр с запятой, да, и часто эти цифры могут занимать 32 бита. На модельке у тебя может весить 16, но на мобильнике это не работает. В результате они сделали такую штуку, как Quantized Tensor, когда ты научил модель, после этого на каждый элемент модели выделяешь для хранения только 8 бит. А где-нибудь сбоку приписываешь максимум и минимум. Например, максимум – это 12, минимум – минус 12. А дальше 8 бит представляет 256 позиций от минимума до максимума. Любое, ну, то есть дробное число, там минимум, максимум может быть, на самом деле, 0,1 и 0,5. И каждый 8 бит репрезентует вот какой-то диапазон. Казалось, что 8 бит хватает. Ты теряешь точность дико, но если модель уже обученная, то после уже обученной модели ты можешь вместо 32 порезать каждый из них будет по 8 бит, будет где-то к 7, и это не сильно теряет в полете. И ты эту модельку потом махонькую можешь деплоить на мобильник. У него есть хороший пост, но я к тому, что это со стороны действительно кажется вау, но если ты с этим постоянно работаешь, у тебя появляются вот такие вот идеи, и, повторюсь, точно так же, как инженеры Мэля смотрят на вас, как на людей, работающих с Big Data, и думают, вау, как они додумались соединить эти два пайпа и сделать вот это, это ж так круто, я бы даже сказал, что можно уменьшить точность значения, мы получаем уменьшение модели. Есть Piper, я, может быть, тоже найду ссылку и добавлю, который говорит о том, как улучшить производительность тренировки моделей. И просто получается, что когда мы модель тренируемся, в конце тренировки у нас получается много значений, которые являются близкими к нулю. И, в принципе, либо там близкие к единице, но там чуть-чуть различаются в каких-то долях, в долях в 10, например, знаке, либо в 5 знаке. И к человеку пришла идея такая, что, а что будет, если мы вот эти значения, близкие к нулю или к единице, заменим на те самые нули или единицы? Таким образом, у нас получается вычисление намного проще при умножении и сложении, что обычно происходит с матрицами, и за счёт этого мы улучшим производительность. И есть компания, по-моему, это китайская компания, которая занимается оптимизацией нейронных сетей по GPU, которая была основана тем человеком, который сделал этот paper. Он когда-то учился в университете, сделал PhD, потом либо не закончил, либо закончил, но ушёл сделать свой стартап, который делает именно оптимизацию нейронных сетей. И это как раз просто мне напомнило и мне кажется очень интересная идея в этом плане, что про оптимизацию путём отрезания лишней информации. Вот то, что ты сказала, кстати, очень сложная задача. Я сейчас объясню, почему. В нейросетях очень любят логарифмическую вероятность, потому что logarithman up умножить, когда ты, прошу прощения, умножаешь их, ты на самом деле можешь так или иначе всё замамить на саму, потому что если тебе просто надо вероятности перемножить, ты перемножаешь, в случае логарифма ты можешь сложить. Сложение работает всегда лучше, чем умножение исторически на куче платформах. Вот такой подвох, подводный камень. Если ты всё перевёл на логарифмическую вероятность, то очень маленькая величина, близкая к нулю, вычисляема. А логарифм от нуля, он очень плохой. И вот это на самом деле, то, что я говорю, это то, что Фелло описал в нескольких статьях, и действительно, ты можешь это сделать, но это трудно, потому что у тебя с одной стороны у тебя матаппарат, который любит всё в логарифмах делать, чтобы можно было складывать вместо умножения, с другой стороны, если поставишь ноль, то эти логарифмы на куче языков не вычисляемы. Они могут превратиться в NAN, в Infinity, из театр платформы, и тебе надо построить вот эту какую-нибудь штуку, которая будет понимать, окей, здесь ноль, но давайте мы вернём логарифм какого-то очень маленького величины тут, ну и так далее, и поехало. Это на самом деле достаточно трудоёмкая работа, да, целая компания пытается вот это обойти и сделать оптимизацию.",
    "result": {
      "error": "API request failed: Error code: 400 - {'error': 'Trying to keep the first 12276 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}",
      "stack_trace": "Traceback (most recent call last):\n  File \"/home/andrei/Projects/podcast-shownotes/scripts/build_search_eval_dataset.py\", line 157, in generate_search_query\n    response = await self.client.chat.completions.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 1927, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1767, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1461, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1562, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': 'Trying to keep the first 12276 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}\n"
    }
  }
]