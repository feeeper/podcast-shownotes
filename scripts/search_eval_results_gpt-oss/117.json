[
  {
    "segment_id": "f6abcf68-2aaa-4949-ba4c-639bbb58ffde",
    "episode_id": "8e26e14d-4fa4-4c6c-bb06-aed305dc5647",
    "episode_number": 117,
    "segment_number": 6,
    "text": "Какая-то там очень мощная. Обычные железо используете, либо GPU? Стараемся. Для таких тасков не имеет смысла использовать GPU, потому что они очень простые, и ты можешь арендовать на самом деле CPU машину на Амазоне или посчитать у нас кластер, и у нас есть кластер в фонде. Но вот когда мы занимались ворд-эмбеддингсами весной, у нас там был проект, мы пытались обучить более хорошие ворд-эмбеддингс с большим количеством background knowledge, то есть мы пытались DB-педию загнать ворд-эмбеддингс. И, собственно, там обучение на GPU инстансе, самом мощном Амазоне, занимало 12 часов. И это было уже... Нам приходилось потеть, чтобы улучшить производительность. Но в целом можно считать на CPU, никакой проблем в этом нет, телефон нормально скалируется на CPU. Иногда даже люди говорят, что Tiana лучше скалируется на CPU. Так считается, что TensorFlow быстрее всего работает на GPU, а Tiana быстрее всего работает на CPU, но TensorFlow на самом деле тоже неплохо работает на CPU. В общем, у нас... На самом деле в Лесёче у нас пока не было больших проблем с производительностью. Во всяком случае, тех, которые просто докупка интесов на Амазоне не могла решить. А вы для профайлинга используете в основном то, что человек пишет и то, что он постит у себя? Да, у нас есть скроллер, который бегает по Твиттеру, Фейсбуку, Линкдыму. Для Фейсбука, Твиттера мы используем опишки публичные для Линкдыма. Мы просто сканируем страницу, и мы сохраняем всё про пользователя, то есть его посты. А это разве в Линкдыме разрешено? Насколько я помню, они такое не разрешают делать. Да, они такое не разрешают делать, но мы очень-очень-очень мало собираем. Мы просто не наглеем, и у нас всё хорошо. Но Линкдым нам нужен, то есть Линкдым – это очень хороший социальный сет. А как вы решаете проблему, когда вы получаете по одному человеку, хотите посмотреть его Твиттер, Фейсбук, Линкдым в лучшем случае, правильно? А что вы делаете, когда, например, у меня... Вот пример моей ситуации. У меня имя есть на белорусском языке и на русском языке, соответственно, фамилия на белорусском и на русском. На английском оно звучит по-разному. Вот как вы объединяете меня в одного человека и понимаете, что вот это я и там, и там, и там? Ну, то есть, например, если в Твиттере по одному написано, а в Фейсбуке по-другому? Да, примерно так. Ну, я имею в виду на больших масштабах. И фамилии могут быть заменены, и вот как вы определяете... А ещё могут быть тёзки. Имя же это не только, это не единственный атрибут. То есть, если мы делаем фичи инженерик, вот у нас в статье, которая последняя, там очень простой способ определять людей. Там мы делали фичи инженерик, и там одна из фичей просто Левенштайн Дистанс и Жаро Винклер Дистанс от имени, который у нас есть в базе к имени, которое мы нашли у кандидата. Ну, а вообще, как это люди обычно делают, вот если им надо салайнить два профиля в разных соцсетях, они просто берут все атрибуты, запихивают их в вектор, и, соответственно, у второго профиля тоже запихивают все атрибуты в вектор, в один большой. Дальше они либо уменьшают размерность этого вектора, и учат имбединг этого вектора, и сравнивают симилярики. То есть, либо Дистанс считают вклидовый, либо кассинусные симилярики. Либо вот прямо на изначальном большом векторе. На изначальном большом векторе это плохо работает, то есть надо уменьшить размерность. Либо при помощи СВД, либо нейросеть обучить, чтобы она уменьшала размерность. Какая размерность вектора считается приемлемой для этой задачи, в частности? Ну, ты можешь выбрать любую размерность вектора. Например, для word embeddings, когда векторное представление слов обучают, обычно используют что-то в районе 300. То есть, если мы используем для слов, то мы используем 300. Потому что 50, например, не хватает, 100 это еще где-то средненькое. А вот где-то на 300 у нас такая золотая середина, потому что иначе мы уже трейдофим производительность. Извини, я все-таки прервал. Вы идете по базе и делаете полный join всех пользователей со всеми пользователями, и сравниваете вот эти вектора из трех стафлоутов, или есть более хитрый алгоритм для этого? Мы стараемся сделать список кандидатов. То есть понятно, что мы не матчим всех со всеми. Обычно есть имя или какой-нибудь атрибут, который более-менее... конечно, не совсем primary key, а какой-то атрибут, который... или юзернейм, или имя. И мы пытаемся просто сделать сетч, используя либо сетч API самой сети, либо мы эмулируем просто сетч API сети в нашей базе. И мы пытаемся сделать граничный просто список кандидатов и по нему поматчить. Потому что, естественно, делать всех со всеми это очень накладно, у нас так не получается. Я вот вспомнила, что, насколько я знаю, есть сервис, открытый в открытом доступе от университета, то ли Манчестера, то ли Карнаги Мелана, там точно была буква М, где ты можешь просто посмотреть свой профайл и ты просто соединяешь свою социальную сеть с этим сервисом, и он тебе показывает весь твой профиль. Я постараюсь найти ссылку, довольно интересно и очень правдиво то, что мне показали. Да, есть даже не один такой сервис. Я помню, что это именно от университета, и он считается straight of the art для этой задачи. Ну, это они делают примерно так же. Но вообще смысл в том, что если, например, сайт, где ты просто указываешь все свои соцсети, и потом люди, которые эти сайты управляют, они пишут интересные статьи, ой, смотрите, у нас есть датасет в этой соцсети, в этой соцсети, они слинкованы, и ты можешь сделать машинное обучение и научиться линковать автоматически пользователей в разных соцсетях. Вот просто сделали сайт, люди, которые потом использовали в ресерче целях. Я пытаюсь вспомнить, как же сайт назывался. Ты тоже сошел с ним? Нет, нет, есть какой-то about me или типа того, где ты вот просто указываешь все свои соцсеточки. Такая страничка типа «Ищите меня вот-вот здесь». Я просто пытаюсь название вспомнить. А, я знаю, да, по-моему, about me или что-то в этом роде. Хорошо, тогда мне следующий вопрос у гостя. Вот расскажи, а вы используете ли вы картинки для построения профиля? Потому что вот людям, например, в возрасте помладше, им наверняка свойственно постить картинки всякого трешового содержания, а людям в возрасте постарше они такого не делают. Что используете? Картинки, которые человек постит в социальных сетях, именно содержание картинки? Мы конкретно не используем, потому что нам бы пришлось строить целый пайплайн имидж-процессинга для этого. То есть мы можем сравнивать аватарки, просто они одинаковые или нет, при помощи библиотечных методов в weapons-ных библиотек, но мы не встраиваем это в наш пайплайн. То есть мы знаем, как это сделать, но это бы очень сильно усложнило наш пайплайн и очень сильно мы бы просили по производительности, по той самой, которой мы сейчас не имеем проблем особо. А может быть это бы улучшило ваши качества работы? Это точно бы улучшило наши качества работы, но во-первых, мы можем использовать методату сами картинки, а не сами картинки. Во-вторых, мы изначально везде себя позиционируем как людей, которые специалисты в НЛП, и мы стараемся применить что-то из НЛП, а сказать, что если вам нужен имидж-процессинг, то вы просто допилите до себя имидж-процессинг, вот и всё. Или найдите людей, которые вам допилят имидж-процессинг, вот это и в наши вещи. Вот, кстати, по поводу, вернусь к предыдущему вопросу, когда ты говорил про подсчёт эвкалиптовой разницы, например. Есть ли какие-то готовые библиотеки для этих целей, потому что задача довольно распространённая, и что ты можешь посоветовать? Ну, как бы эвкалиптовую дистанцию посчитать очень просто. Я говорю именно про большие вектора, и когда у нас речь идёт не про примитивный пример, для более общего случая. Что значит большой вектор? Обычно, если у тебя большой вектор, если у тебя размерность миллиона или миллиарда, то у тебя он спарс, то есть ты просто бегаешь по чиселкам и перемножаешь их руками. То есть не как с вектором работаешь, а как с таким множеством значений, которые в определённых местах вектора стоят. Иногда, ну, в несёчи вообще как делают? Иногда пишут своё, если у них просаживается по производительности, то они используют какие-то библиотеки, которые считают это при помощи каких-то нативных имплементаций на процессоре, используя... ну, вообще, оптимизируют при помощи нативных бэкэндов. И это очень просто сделать, потому что если вы пишете на Питоне или на Джаваскрипте, всегда найдётся такая библиотека. То есть для Джаваскрипта это... Вот у Deep Learning 4J есть библиотека, которую написали те же самые люди, которые пишут в Deep Learning 4J. Там есть библиотека, которая заточена на работу с векторами. То есть ты просто туда свои чиселки загоняешь, у тебя получается вектор, и ты можешь с ним делать векторные операции очень быстро. Там есть нативный бэкэнд на GPU, нативный бэкэнд на CPU. То же самое в TensorFlow. То же самое в целом в Питоне. Есть NumPy, у которого тоже, если у вас есть NumPy-евский массив, и вы перемножаете с другим NumPy-евским массивом, то он перемножится на бэкэнде очень эффективно. То есть не питонскими средствами. Кстати, а питон вы используете обычный ванильный, не PyPy, ничего такого? Да. Причём версия 2.7, потому что с 3.5 у нас бывают проблемы с библиотеками, которые мы используем. Я предлагаю передать слово ведущему. Давай последний вопрос тогда, и передадим слово. Вы так классно разговариваете, зачем вообще я? Вам так интересно. Мне нужно будет переслушать выпуск два раза, чтобы понять, о чём вы, ну да ладно. Хорошо. Что касается профилей, есть тоже распространённая задача, которая называется Cruise Device Mapping. Вот у нас есть... Как понять, что у нас один и тот же человек пользуется этим телефоном, вот этим компьютером, этим планшетом, у нас есть логи от этих девайсов, и мы хотим понять, что вот это один и тот же человек. Как бы ты решал такую проблему, что ты можешь рассказать про такую задачу? Потому что это тоже про профиль человека. Ничего не могу сказать про эту задачу, решалась бы она точно так же. Только, наверняка, логи были бы очень большие, то есть если у нас, например, есть тысяча твитов, мы там можем не напрягаться особо с производительностью. Если у нас логи по 5 гигабайт, для одного человека, наверное, мы бы что-нибудь интересное придумали. Но я точно знаю, что этим занимается очень много кто, даже в нашем университете есть специальная лаборатория Telecom Италии, которая просто берёт все свои логи, она же мобильный оператор, берёт свои логи, и они занимаются как раз этим. Они выясняют, что вот этот девайс с этой симкой – это всё один и тот же человек. Потому что, в принципе, это могло бы быть даже использовано. И вот у тебя, например, нет никакого идентификатора вроде имени для разных социальных сетей, и ты можешь сказать, что это один и тот же человек в Твиттере и в Фейсбуке просто по идентификатору, по каким-то косвенным признакам? Ну да, так и происходит. То есть очень часто у людей, даже не просто у людей, у компаний даже бывает, имя совсем не совпадает. Кстати, один из примеров, мы когда мапим, ну, собственно, вот последняя работа, вот моя, когда я маплю энтити из Википедии в Твиттер, в Википедии, например, Российское космическое агентство, как Российское космическое агентство, особенно в старых изданиях, а в Твиттере, если ты поищешь при помощи твиттеровского дуранского поиска Российское космическое агентство, ты не найдешь ничего, только упоминание Российского космического агентства. А все дело в том, что на самом деле называется Роскосмос. И для того, чтобы найти этот Роскосмос, тебе надо по косвенным признакам, собственно, посмотреть. То есть посмотреть, с кем этот Роскосмос дружит, мапить сначала его друзей, потом замапить сам Роскосмос. То есть задача становится сложнее, но решаемая. Я предлагаю передать слово ведущему тогда. Окей. Да-да-да, пора бы. В общем, ну, я думаю, что Machine Learning мы обсудили так, что нам хватит на ближайшие три выпуска или типа того. Если, конечно, кто-нибудь еще не приведет. Подожди-ка, я буду ведущей, поэтому ожидайте того, что я приведу гости из этой же сферы. То есть вот у нас давеча, люди жаловали, что слишком много постгресса. Вот интересно, будут вовищи, что слишком много, как этого, Machine Learning'а? Ну да хотя бы 3-4 выпуска. Да-да, видимо. А нужно еще, знаете, что я хочу сделать? Надо евангелиста в парочку привести, я думаю. Нужно еще сделать выпуск, короче, чтобы у нас Machine Learning был на постгрессе. А, и чтобы все это было как-нибудь привязано к GameZen. Просто будет такой выпуск полной ненависти. В смысле полной ненависти. На самом деле я могу рассказывать очень долго о особенностях имплементации алгоритмов машинного обучения в Deep Learning 4j и TensorFlow, и все умрут. Мне было бы интересно это послушать, честно говоря. Я не знаю, как другим. Ну мне кажется, это... Это будет с матами или без? Это будет без мата, я аж... такой парень. Понятно, да. Не будем показывать пальцем, я не... В отличие от некоторых не будем показывать пальцем, слова не материться. На меня не будем показывать пальцем. В общем-то, раз мы не хотим больше про ML, то не странно то, что Света хочет. И вообще всем было бы интересно, но прошло уже 40 минут, давайте о чем-нибудь другом. Вот. Тут мы... Извини, слушатели Dbf256 пишут верните постгресс, я больше не могу. Это неправильные цитаты. Верните постгресс уже. Ты, Саша, добавляешь, потому что хочешь больше постгресса. Ну ты понимаешь, не соврешь красиво, не расскажешь. Это ты больше не можешь. Ты уже себя выдал. Я, если ты не заметила, я с большим интересом поддерживал про TensorFlow и все остальное. На самом деле, идея впилить ML в постгресс не такая уж и бредовая, как кажется. И можно сделать. Вот, да. А то тут у некоторых... Мы дальше обсудим по тексту. У некоторых тут и мегабайт ответов в запросе, это уже бигдата. Ну да ладно. Вот. На самом деле, я вот тут в Берлине, и я думал, что ко мне может быть Ваня присоединится, а он не присоединится. Правда, Ваня? Да. Это вопрос, да, я так понимаю? Надо рассказывать что-то. Ну видимо. Там секунду назад в документе была ссылка, сейчас она пропала. Она упала чуть ниже в разделу, может одной строкой, но не важно. Ну я написал вкратце, что я нашел себе другую работу и не поеду в Берлин. Кому интересно, добро пожаловать в ЖЖ, Twitter или Facebook, там все связано. То есть у нас не будет X-Podcast, ну ладно. Да, к сожалению. Извините. Но надо как-то разбавлять, а то представляешь, все бы собрались в Берлине, и что было бы? Скукотища. Ну почему же, я не собираюсь в Берлин. Это ты так думаешь пока. Не, на самом деле я... Я понимаю, к чему, кстати, это. Говорят, Brexit-а не будет, значит, наоборот, может быть я наоборот в Лондон поеду, раз Brexit-а не будет, все стартапы обратно в Лондон намануться, или же иначе. Зато я буду рассказывать про удовлетворенную работу, проблемы, недостатки и все такое. Расскажешь. Проблему, запятая недостатки. То есть позитивного в этом ничего нет, сори. Все остальное позитивно. Расскажи, насколько ты потолстеешь после года работы удаленно. Не дождетесь. Или не потолстеешь, мне интересно. Индекс вес как это, as a service. Ваня же плавает, ему это не грозит. Ваня утром сякал, я за сервис. Это было бы сложно. Теперь вы понимаете, почему эта тема была где-то внизу. Окей, ну похаливолили, ладно. На самом деле, раз мы достаточно знали о том, что Ваня не едет никуда, я предлагаю поговорить о том, что в гоу, наконец, сделали нормальный garbage collector. Я очень люблю пенать гоу за то, что у него плохой garbage collector. Тут, судя по списку рассылки голландьев, туда наконец завезли что-то такое, что характерно, ресерчи, которые это позволяют делать опять-таки времен царя Гороха. Но поскольку у гоу в целом довольно простой garbage collector, там достаточно простые техники удалось применить, и внезапно оно работает, судя по всему, офигенно. Говорят, что там чуть ли не... В общем случае, пауза меньше миллисекунды. Извини, я тебя прервал. Вы обратили внимание, насколько Горох был примечательным царем? При нем было сделано много замечательных вещей. Простите. Да, это просто... Все управители были такие, чтобы при них столько хороших пейперов выходило. Человек явно вкладывался очень сильно в науку. Я всячески поддерживаю управление царя Гороха. Но вернемся к сборке мусора. Я сейчас, возможно, скажу какую-нибудь фигню, если кто-то пишет ногой и понимает то, о чем я буду говорить. Потом придите в комментарии, поправьте. Я постараюсь не врать, но тем не менее. В ГО коллектор... В принципе, любой коллектор, мы не так давно и орланговый обсуждали, он начинается с каких-то мест, которые точно никуда не могут делаться. Например, стек. Это называется root set. Обычно это дальше по этому root set идет какой-то traverse куда-то. В итоге помечаются все объекты, которые недостижимы из root set, а все недостижимы, значит, можно выбросить. В ГО используется какой-то, если я правильно помню, гикстер стайл... Ладно, не буду пытаться привязать к конкретной статье. Гошный коллектор это конкурентный коллектор, который был описан в гикстере в 78-м. Каждый объект помечается или белый, или серый, или черный, и серый нужно перепомечать. В общем, там проблема в том, что если у нас больше одного потока выполнения, то мы начали с какого-то стека, мы по нему шли-шли, мы из него расползались, что-то пометили, и проблема в том, что... Сейчас. Я уже не могу это из головы писать, статью сейчас перечитывать некогда. В общем, белые объекты это объекты, которые можно выбросить, черные объекты это... Сейчас. Да. Белые объекты то, которые можно отбросить, черные которые точно отбросить нельзя. И вариант в том, что ни один черный объект не может указывать на ... не может указывать ни на один белый. Ну и, понятно, если у нас конкурентная память, там могут быть ситуации, когда у нас некоторые объекты серые, я не буду сейчас даваться как это получается, потому что я их помню. Боюсь врать. Но суть в том, что у нас могут быть ситуации, когда у нас памяти серые. И когда мы все отметили, сделали первый проход, первый проход делается полностью конкурентно. Потом нам нужно сказать, что у нас Stop the World, пометить то, что мы точно не знаем, о чем мы не смогли определиться, то, что было серым. И потом уже отпаузиться и пойти выбрасывать белое. Почему нужно вставать на паузу? Потому что если мы не встанем на паузу, у нас снова может какая-то неопределенность случиться, и так до бесконечности. И, соответственно, эта пауза, это была одна из тех вещей, которая тормозила Garbage Collection в Go. И они придумали при текущей пометке, при старой пометке, помечался только... Объекты помечались при записи. То есть если мы делаем right pointer, и правильно у нас бежит Garbage Collection, то мы то, куда мы... В общем, да, по-моему, то, куда мы pointer записали, мы его тоже помечали в зависимости от цвета, по-моему, самого pointerа, не помню уже. Но, в общем, это было очень сложно. Вот. Теперь суть в том, что они поменяли этот барьер на запись, и теперь он помечает как-то по-другому. По-моему, он помечает вроде как... Ну, опять-таки, я боюсь соврать, что он куда помечает, но... Суть в том, что он... Вот, как бы, right barrier изменился, он раньше был... назывался просто right barriers, а теперь он называется hybrid right barrier. Я... не хочу соврать, но суть в том, что это изменение делает запись немножко дороже, потому что запись случается редко, по крайней мере, сильно реже, чем чтение. И... оно в итоге за счет того, что записи во время garbage collection становятся чуть-чуть подороже, они за счет этого сильно при этом уменьшают время самого коллекшена, за счет того, что убирают stop the world, притом, судя по некоторым заметкам, сильно упрощают, в принципе, реализацию всего garbage collector. Там есть некоторые сложности с стартующими грутинами и с... записью в чужой стек, которая случается в некоторых случаях. Я, честно говоря, не заменил, вообще не помню, чтобы, у нас как-нибудь разруливают, но судя по всему, разруливают как-то. Вот. В общем, к чему и это все? К тому, что в Go garbage collector, возможно, в следующем релизе станет настолько быстрым, что можно будет даже использовать его в каких-то приложениях, тем, которым чувствительны. Вот мне до сих пор интересно, как у них будут дела с приложениями, которым просто по долгу службы нужно много данных просасывать из сокета в сокет и что-то правильно с ними делать, потому что это вот такой был как раз типичный плохой случай для существования вашего горшного коллектора. То есть нужно им будет все-таки делать какие-то особые оптимизации для молодых хипов, для молодых поколений по отдельным грутинам, или с новым тем подходом им вообще ничего больше никогда доделывать не придется, но всегда будет работать хорошо. Мне интересно это посмотреть. Вот. Там в той же рассылочке кто-то говорит, что у них действительно очень сильно упали паузы, приводят скриншоты просто потрясающей совершенно цифры. Правда, все-таки не SAT milliseконд, то есть у людей оно все еще, я так понимаю, иногда немножко выскакивает за даже миллисекунду, но все равно сильно быстрее. При этом в данный момент у некоторых, по-моему, все тех же людей оно, несмотря на то, что оно стало сильно быстрее, оно при этом начало жрать CPU, как не в себя. Будем надеяться, что это пофиксит. Ну, собственно, и... Я только хотел сказать, что мне кажется, для любого GC можно подобрать такую нагрузку, что он либо будет немножко утекать, либо тупить, либо жрать CPU, но то есть как-то мне слабо верится, я понимаю, что это хреновый аргумент, но вот что придумает GC, который при всех нагрузках прям очень крутый SAT milliseконд, с чего-то мне мало верится. Нет, ну, почему нет, там мне кажется, все-таки в случае с Go там же вполне... то есть Go, чем хорош, у него очень много объектов вообще на стэке. То есть чем координат отличается от той же Явы. Ява тоже умеет на стэке. Да, но там гораздо... ну, с того, что вы мне рассказывали, с того, что я там гуглил, читал, все-таки у Go гораздо больше всего происходит на стэке, чем у Явы. Да, она может на стэке, но в реальности очень много происходит не на стэке. Я немножко перефразирую. Мне кажется, трудно это сделать так, чтобы она работала вот всегда хорошо, не уронив, как сказать, не открыв пользователю немножко... как сказать, нет, не так. Ну вот взять абстракцию, да, и немножко ее вот открыть для пользователя, чтобы он знал, что там внутри. То есть, например, хорош в этом плане Erlang, то, что пользователь хочет, он создает локальную кучку. Ну, хочет, создает много процессов с маленькими кучками, если видит, что там происходит накладные расходы на копирование, объединяет процессики в один со своей одной кучкой. Вот. То есть, не открыв пользователю немножко кишочков, я сильно сомневаюсь. Ну, я потому и говорю, что мне интересно посмотреть, как это будет, будет ли вот этих оптимизаций достаточно, или они им все-таки придется сделать вот эти раздельные молодые хипы, что если мусор, который намусорили недавно, обслуживая, например, там один отдельно взятый запрос, что вообще было, чтобы его, ну, зачастую этот мусор можно вообще не трогать, выбросить сразу, как только у нас городкина умирает, и ничего с ним больше не делать. Вот. Мне интересно, в итоге они это сделают или нет, потому что разговоры об этом слышал. Да, еще что забыл упомянуть. В этой статье, она о чем примечательна, там в конце есть куча доказательств теорем и лемм. Это довольно интересно. Вот. И... Да, к чему это... Да, в общем-то все. Я все, что хотел сказать, я сказал. Есть еще какие-то вопросы, комментарии, дополнения? Насчет теорем недавно был какой-то пример у нас на работе, я боюсь не вспомнить деталь, но какой-то идиотский пример, что то, что и так вот, и как бы очевидно, что если есть два множества, и там какая-то битовая маска, я точно уже не воспроизведу, но наличие теорем или лемм не обязательно знать, это не признак. Может быть они формализовали то, что было и так всем понятно. Ну окей. А следующую тему это я добавил. Я тут пытался осилить... По последней теме все, да? Я полагаю. Сильно только не обгайся. Постараюсь, да. Хотя это трудно. Я тут пытался осилить по галлоджику. Напомню, да, это в POS грея SQL, все наши слушатели радуются, хлопают в ладоши. Есть такое решение, second quadrant, для логической репликации. Короткое содержание такое, что я это настроить не смог. Я с этим возился несколько дней. У них очень непонятная документация. То есть ты вот... Будет ссылка на stackoverflow, где я пытаюсь со всем этим разобраться. И в issue tracker у них я тоже спрашивал, они там мне ответили. То есть ты читаешь доку, там сказано, выполните такую-то команду, да, и вот у них, например, заменен... Ну, написано там, типа, master host, да, то есть хост, которого мы хотим реплицировать. И непонятно, где это выполнить. На мастере, на реплике. Вот этот хост, он должен быть в внешнем IP, внутреннем IP, ну, там, типа, 127.0.0.1. Какие порты должны стоять, торчать наружу, ничего непонятно. Я методом тыка там попробовал 3-4 конфигурации, оно не работает ни при какой конфигурации. Ну и в итоге я плюнул. Плюс к этому у них на самом деле есть очень серьезные ограничения, что если ты добавил таблицу, тебе там нужно выполнить определенные запросы, чтобы она тоже реплицировалась. DDL не реплицируется, и, в общем, все довольно производит печальное впечатление. То есть такой поделки, что у них явно была какая-то проблема, да, такое чувство, что они свой отдельный кейс решили, а на все остальные забили. То есть, наверно, у них был такой кейс, что есть одна табличка с какими-то логами в JSON, которую нужно было к какому-то клиенту реплицировать. Имеется в виду клиент, который занес SecondQuadrant, вот они эту задачу решили, и все. Вот, то есть, я это все к тому, что мы раньше озвучивали такую идею, что PGLogical пришел и всех победил, слони больше не нужны и так далее. Вот, к сожалению, это не так. Светлое будущее не наступило. То есть, насколько я осведомлен, это сейчас не работает. И я вам это не рекомендую. Вот. У меня все. А у них есть какие-то планы еще, насколько живой этот проект прямо сейчас? Ну, они его развивают, что-то пилят. Никто не знает. Понятно. Надежда еще не умерла до конца, но пользоваться пока нельзя. Не пользуйтесь. Я знаю, что некоторые люди смогли это настроить. От коллег я слышал отзывы, что там есть серьезные баги, что оно, например, неправильно партит каталог Postgres, то есть оно работает в предположении, что там в каком-то поле стоит true при определенных условиях, но оно там стоит на самом деле при других, и эта штука, она ломается в таком случае. Я слышал отзывы, что оно ломается в продакшене, там у одной известной компании DBA со мной делились и так далее. Ну, то есть этим реально нельзя пользоваться. То есть я себя не считаю тупым, но если я за два дня не смог это настроить, наверное, с этим что-то не так. Я правильно понимаю, что это как бы PageLogical абсолютно то же самое, что логическая репликация или логическая репликация, или PageLogical просто одна из имплементаций? Одна из имплементаций, да. То есть что такое? Вот есть потоковая репликация, это когда ты реплицируешь все базы в инстансе и так далее. Зачем нужна логическая? Ты, например, хочешь реплицировать только одну базу, или ты хочешь реплицировать две таблицы, или фильтровать строки из этих таблиц, или ты хочешь сагрегировать два мастера в одну реплику, чтобы потом Алаб на ней погонять, например. То есть получается у тебя два мастера пишут в одну реплику. Может быть так, что ты на ней еще и что-то сам пишешь, например, забил временные таблицы. То есть это вот такие типичные задачи, которые пытаются решать логической репликацией. И PageLogical, да, одна из имплементаций, есть еще слони и вторая забок, как называется. Если ты думаешь, что мы вспомним, ты напрасно надеешься. Чуваки из Skype опилили, как-то на L начинается, но я просто не помню. Это печально, что там еще был логичный парсинг, или что-то такое там еще было? Это что? Я не знаю, о чем речь. Логичный декодинг, или что-то такое там. Это фича самого Postgres, то есть ты можешь создавать эти репликации слот и подписываться на вал. Тебе будут приходить апдейты в JSON. Поверх этого можно что-то сделать, наверное. Но это будут твои, собственно, к стилю подпорки, не погаллогика. Погаллогика это просто extension. И ты ему говоришь, типа, окей, и среплицировать табличку такую-то, точнее не так, добавить ее в replication slot. Потом ты на реплике говоришь, окей, сходи на dotmaster и подпишись на такой-то replication slot. Понятно. Короче, печально, что как-то реплиментация, подававшая надежды, похоже, не оно. И что смешно, у них звездочек на гитхабе меньше, чем у моего засона. Ха-ха-ха, простите. Ну, да. А что еще? Здесь не получится плавного перехода, потому что на самом деле... Что еще имеет звездочек? Что еще имеет звездочек? На самом деле я хотел перейти от этого, когда ссылку рядом ставил, я хотел перейти плавно от go. Ну да ладно. Тут вышло с клада 12. Нам на самом деле еще и в темы слушателей принесли, но мы сами про это знаем. Но все равно спасибо, что вы делитесь с нами темами. Извини, пожалуйста, слушатель Flash Gordon правильно напоминает, что называется ландист. Ты имеешь в виду еще одно приметное слово? Да, слон и ландист. Но они все на триггерах, и поэтому медленно и назад-то работают. Вот. Ну, в общем-то, к чему я? Вышло с клада 12. Там у нее довольно длинный пресс-релиз, или как это называется. Но все сводится к тому, что скала до 12, такая скала, которая вам позволит, которая вас обязывает выбросить на мороз все, что ниже 8 джавы, но при этом на 8 джаве оно активно использует фичи рантайма 8 джавы, и по-хорошему, по идее, это должно повести нас в счастливый, добрый мир, где скала работает быстрее, чем обычно, потому что в скале очень много функциональности, а в джаве 8 виртуальная машина стала более в курсе того, что бывает не только императивный код, но еще и функциональный. Вот. То есть теперь в виртуальной машине будет проще догадываться, что происходит, ну и потенциально у нас производительность сбрасывают. Вот. Какие дела? А я так понимаю, у нас уже никто особо не занимается скалой, да? Нет, ну как же, как минимум Света же, да? Нет, почему? Я занимаюсь по-прежнему. Почему мы не слышим тогда радостных возгласов? Ура! Два дня назад вышла скала.2.12 Разрадуемся. Срочно в продакшн. Вот, как раз у меня салют за комп, это в честь скал. Можно и так сказать. Да, наконец-то. А можно в темку тут, я не помню, мы пиорили или нет, но есть подкаст про скалы, вот, Скалолаз. На самом деле довольно неплохой. То есть я послушал, у них довольно приятный чистый звук, без компрессии, но в остальном я прям не нашел к чему придраться, и я скорее одобряю этот подкаст, чем осуждаю. Правильно, что ведущий был у нас в гостях? Разумеется. Совершенно верно, да, был. Алексей Фомкин, да? Да, один из ведущих. Ну, давай, расскажу про скалы, раз никто больше не хочет рассказывать, что нового. Расскажи. Трудно рассказывать, учитывая, что я больше года за ней особо не слежу. Ну, я думаю, я постараюсь это упростить, чтобы было понятно и тем, кто пишет. Чтобы даже я не понял. Чтобы даже я не понял. Давай для меня, я вообще ее не знаю. Кто пишет постгрозу, было бы понятно. Окей, из больших обновлений это то, что теперь есть полная поддержка Java 8, в частности, LAMPED. Потому что в 8 Java это сделано нативно, в скале это эмулировалось так, что у нас создавался интерфейс, в котором есть метод, и потом... Ну, давай так, начнем. Чуть раньше. В скале есть возможность создавать трейты, в трейтах можно делать имплементацию функций каких-то. Вот. Так раньше трейт невозможно было смапить один к одному в javas interface. Но в 8 Java появилась такая возможность сделать дефолтную имплементацию в интерфейсе. И теперь в скале появилась эта поддержка нативная. А до этого маппинг был следующий, создавался интерфейс с каким-то методом, и создавался уже класс, реализующий этот интерфейс с непосредственной реализацией методом. Но, тем не менее, это не всегда работает. На текущий момент это и сейчас не является маппингом один к одному, потому что есть возможность в скале сделать не константы в трейте, а в java, если у вас в интерфейсе есть какие-то поля, то эти поля должны быть константами. Вот. Это из такого основного. Дальше произошло очень много изменений во фьючах. Есть 8 постов кого же? Одного из по-моему создателей этого всего. Я забыла, как его зазывать-то. Ладно, потом добавлю эту ссылку. 8 постов про то, какие изменения происходят во фьючах. Появились вы методы, про то, как они работают. Это всё описано. Я не буду здесь углубляться. Они приведены более к такому правильному с точки зрения функционального программирования подходу. Дальше произошло много изменений во сколодоках, потому что, если у вас есть код, в котором есть и java.yscala файлы Вот. Виктор Кланг, спасибо большое. Это тот самый. Так, сколодок. Если у вас есть в вашем проекте как java.yscala классы, файлы, соответственно, были некоторые сложности с документацией. Теперь добавилось возможность это делать безболезненно. И это одно изменение. Дальше есть изменения. Появился, наконец-то mutableTreeMap, потому что раньше его не было. У меня как-то возникла такая необходимость в скале иметь mutableTreeMap. Я с удивлением обнаружила, что его, оказывается, не существует. Вот. Теперь он появился. Дальше из-за такого... Были изменения в классе either. То есть, что теперь... Можно было отдельно мапить right and left, если у вас есть в объекте типа either. Но теперь по умолчанию всё будет мапиться для правой части. Но у вас появилась новая функция, называется slope. Или как она вроде так и называется. Так. Что ещё интересного есть такого, чтобы вам вспомнить? Можно пока вопрос задать? С бит-тим. Да, давай. Вот как главный эксперт по скале в нашем подкасте, скажи мне, пожалуйста, имеет ли смысл всё ещё писать на скале, когда вышла Java 8 с лямбдами, с трейдами и так далее? То есть, что всё ещё есть в скале, чего нет в Java 8? Давай так. В одном из собеседований, когда меня собеседовали на позицию скала-девелопера, мне задали вопрос. А вот назови, пожалуйста, две фичи языка, которые тебе нравятся в скале и которых нет в Java, и без которых ты считаешь очень сложно и неприятно. И там тоже был вопрос касательно 8 Java, ведь там же тоже есть лямбда. Так вот, я ответила, я сказала две вещи. Первая вещь — это был pattern matching. В Java этого нету. Ты, конечно, можешь сделать всё с помощью свитчей, кейс, или на просто ифах, но это совершенно не то получается в итоге. Другой опыт использования. И второе — это то, что есть кейсклассы. Наличие кейсклассов — это большое-большое дело. Вот эти две фичи языка, которые нету в Java, они есть в скале, и которые сильно упрощают жизнь. Не то, что упрощают, они делают жизнь лучше, приятнее. И таким языком не знаю, мне не очень хочется возвращаться в Java, хотя я спокойно в последнее время отношусь к тому, что если мне нужно писать код на Java, ну ничего страшного. Нет такого отвращения или того, что нет, я не буду это использовать. А такой вопрос. А в Java 8 уже можно объявлять несколько классов в одном файле? Можно было и раньше. У тебя должен быть один паблик классового на файл, но просто не паблик классов, ты можешь использовать несколько. Обычно это не принято. В классическом, как это, с точки зрения классических подходов к оформлению кода просят писать в один класс, один файл, и это легче искать, это легче смотреть, когда ты ориентируешься в проекте, особенно новом. Слушатели nlinker яростно плясают все, что ты говоришь. Про typeclass и так далее. Наконец-то хоть кто-то одобряет. Дальше из изменений, которые произошли, были изменения в SBT. Теперь, кстати, Scala собирается с помощью SBT. Раньше Scala сама собиралась с помощью Ant. Если вы хотели что-то попробовать у себя, либо сделать какую-то свою сборку, вам пришлось бы вставить Ant, устанавливать множество переменных среды. Теперь это делать не нужно. Все делает один файл SBT. Но в IDEA вы это сделать не сможете, потому что Scala, она циклически компилируется, IDEA это не может поддерживать, поэтому для IDEA есть специальный файл. То есть они тоже это предусмотрели. Дальше, есть некоторые изменения в REPL. Теперь он теперь его можно делать цветным, или такое раньше было сделать, но есть какие-то улучшения в этом. Хотя не скажу, что очень часто REPL пользуюсь именно для разработки на Scala, скорее для Spark, да, но не для Scala самой. Дальше, есть такой класс, который отвечает за конвертацию ALON. Да, ты как младший. Он отвечает за конвертацию джавовских коллекций в скаловские коллекции. И наоборот, теперь его задепликейтили, потому что там куча implicit преобразований, и теперь просит использовать класс JavaConverters. Раньше был JavaConversions. В принципе, из такого основного... А, кстати, еще довольно большое изменение, это то, что выбросили свою реализацию fork. join в библиотеке, и заодно теперь Scala больше не использует SunMiskOnSave, поэтому инженеры Oracle радуются и бросают чепчики в воздух. Ну, в принципе, это все, что хотелось рассказать. Отлично. Кстати, а, Performance, еще момент. Были улучшения по перформансу, но после того, как они добавили лям... скажем, после того, как лямды стали использовать джавовские нотации на уровне байт-кода, то теперь появилась проблема с перформансом при старте приложения. Но обещаю, что это только во время инициализации происходит, но во время работы таких проблем не замечаются. Если такие у кого-то появятся, они просят сабмитить, будут разбираться. Вот теперь у меня все. Распределенные системы, Paxos и Rayek. Это я пытаюсь Валеру разбудить. Рафт, что? Рафт, рафт, рафт, рафт. Рафт, как слышно. Обсудили Scala, прекрасно. Я поспал. Говорят, услышал краем уха, что Scala стала меньшей болью. Мне кажется, так не бывает. Да ладно. Каждый раз, когда делают меньшую боль, кто-нибудь прибегает и делает еще. А ты знаешь, что тут насчет меньшей боли, давай я тоже расскажу историю. Я на этой неделе была на митапе. Кстати, очень приличный наконец-то митап, не просто маркетинговые разговоры здесь в Лондоне, что довольно редко. Выступал SEO Flink'а. И у меня получилось с ним пообщаться, порасспрашивать у него про различные решения, которые были приняты во Flink'е. И один из вопросов, который меня прям очень сильно интересовал, это почему все-таки для Flink'а использовалась Java, а не Scala? На что он ответил следующее, что Flink изначально, это был проект Research'а. И ответ самый простой. Мы хотим брать студентов, которые не знают, как это на Scala, но которые готовы вкладывать в Research, поэтому на Java. Больше нет никаких, скажем, других оснований, чтобы не использовать Scala. Я считаю, это прагматизм. Тем временем Ярослав хотел рассказать про неживый кот, который восстает и съедает мозги. Undead кот. Ярослав уснул. Какими фразами его надо было Он размлютился. У него, похоже, какие-то технические неполадки. Я видел, как он был влюблён. Раз, раз, раз. Ура, с нами снова. Снимай из мёртвый кот. Неживой кот за ним пришёл. Не, ну неживой кот, окей. А вот про Scala-то ещё, что я не согласен, что Scala чем-то сложнее Java, если ты хочешь его дать студентам. То есть, в принципе, у нас же весь стек на Java, вообще, в нашем институте, у нас приходят самые разные люди, с разными разными бэкграундами. Приходят вообще лингвисты, которые только вчера сели, и это нормально. И им надо как-то тоже интегрироваться в наш пайплайн. И как-то не возникает иногда у людей проблем использовать Scala. Она не сильно сложнее им оказывается. Хотя, вроде как, это неочевидно. И я немножко ещё хотел похалеварить на тему трейтов, которые пришли в виде дефолтных имплементаций в интерфейсы. Вот я недавно спорил с человеком, что лучше PHP или Python, и за это меня, конечно, надо расстрелять, за то, что я начал от HollyWare. Но в PHP трейты появились раньше, чем в Java. И это всё, что я хотел сказать про Java. А в Java нету трейтов? Ну, я имею в виду, что вот этот вот воркраунд, который ты можешь делать с интерфейсами. С дефолтным методом. Да. Вот. А насчёт мёртвого кода? Мёртвый код, да. Знаешь, позволь мне прокомментировать по поводу одного момента, когда ты сказал про... Я себя слышу у кого-то, замьютесь, пожалуйста, другие. Ты сказал про то, что студенты быстро начинают на скале что-то делать. Я думаю, разница в том, что специфика Flink и специфика того, что у вас происходит, это разница. Flink, у него большая задача — это перформанс, и это движок для других приложений, которые будут базироваться на нём. А у вас задача просто ресёрча. Если у вас там пара, не знаю, секунд больше, Джо будет выполняться, ничего страшного не произойдёт. Я думаю, вот в этом есть тоже одна из причин, почему они так и сделали. Может быть, да. Это правда, что у нас не сильно перформанс важен. Ну, во всяком случае, вот где-то внутри в деблях наших билдинг-блоках, которые мы используем везде в pipeline, он важен, но в целом он не сильно важен. Ну так вот, мёртвый код. Проблема очень простая. Как вообще ресёрч делается? Человек приходит к идее, он читает State of the Art, он понимает, что ему нужно что-то находить, какую-то имплементацию. Например, он там решил придумать новую нейросети, какую-то пологи для нейросети. Он делает какую-то имплементацию. Эта имплементация в лучшем случае попадает на GitHub. Окей, не в лучшем случае, в апер... Ближе к хорошему случае он попадает на GitHub, совсем как попадает в какую-то библиотеку. В худшем случае он вообще не публикуется. Вот если вы посмотрите статьи на мейджор-конференциях, там вы не найдёте ссылки на какие-то ссорцы, и ссорцы вам никто не отдаст. Потому что это одна из проблем вообще ресёрча, что reproducibility... что ты берёшь статью, и тебе надо с нуля её реализовывать. И у тебя не факт, что с первого раза получится циферки, которые получились в статье. Допустим, что он опубликовал этот код в какую-то библиотеку, он запаблишил статью, статья опубликовалась, и он про этот код забыл. То есть существует десятки, я не утривлюсь, десятки библиотек для обработки естественного языка, и там есть какие-то имплементации, каких-то алгоритмов, которые часто сделаны, кстати, авторами пейперов, и они часто никак не поддерживаются. То есть один раз написаны, баги в них не фиксятся, и когда алгоритм морально устаревает через 15 лет, это никого не волнует, он всё ещё живёт в библиотеке. Собственно, человек, статья и один год, человек говорит, это как-то не дело, давайте мы всё-таки будем с этим библиотеком, который вышли из ресерча, и который люди обычно внедряют у себя в продакшене, давайте мы будем относиться как ко всем остальным библиотекам, мы будем мёртвый код туда удалять, следить за релевантность алгоритмов, которые там реализованы, обновлять их, поддерживать старые алгоритмы, если они по каким-то причинам кому-то нужны. И поскольку его послали с этой идеей в NTLK, это библиотека на бетоне для обработки естественного языка, он сказал, у нас 9 стандартов, давайте я создам стандарт, который всё объединит, и у нас теперь 10 стандартов. Вот так же он создал ещё одну библиотеку, вот она вышла в релиз, собственно, на прошлой неделе, вернее, две недели назад, ещё неделю назад они опубликовали уже версию 1.1, и, честно говоря, я сам её не пробовал, но автор клеймит, что все алгоритмы, которые попадают в библиотеку, они будут курируемы человеком, который, собственно, курирует библиотеку, и там будут фиксы сабаги, она будет поддерживаться. Но как на самом деле Google делает с TensorFlow? То есть TensorFlow, если вы засобите туда ишью, к вам тут же прибудет человек специально обученный, который с вас всё расспросит, как вам надо, что вам надо, и кого-то туда засайнят, и даже это быстро, скорее всего, вопрос решится, чего не наблюдается во многих ресёрч-библиотеках, потому что обычно это делается университетами, а студентам не платится, потому что они пишут, аспирантам я имею в виду, ресёрчерам, им платят за публикации, поэтому у них нет никакой мотивации поддерживать свои старые алгоритмы. И это, собственно, проблема в NLP, что нам нужно как-то перейти от отношения, что мы один раз написали и забыли, к отношению, что это продукт, который им пользуются люди, в том числе и в продакшене. Собственно, про эту статью. Алло. Ну, наверное, один способ сделать, перестать платить за статью и начать платить за код. Да, в частности. Но это работает лишь только в компаниях, которым платят за код. Если ты скажешь, окей, я хорошо пишу код, они скажут, ну, молодец, но если ты не публикуешь статьи, то до свидания. Если ты не делаешь ресёрч, если ты не делаешь проекты, если ты не выбиваешь финансирование с Евросоюза, например. Мы же бюджетники, кстати говоря. Наш институт, он бюджетная организация. Мы боремся за деньги Евросоюза, боремся за деньги провинции, боремся за деньги Италии. За деньги налогоплательщиков. И они нам платят только за ресёрч, нам не платят за код. Ты считаешь, может быть, имело смысл использовать, например, два типа ресёрчеров? Один тип будет писать статьи, у них это хорошо будет получаться, а второй будет больше писать код и подниматься поддержкой библиотек? Ваня, открой чатик. Ну, это работает до определенной степени. Все равно человеку нужно... В общем, это непонятно. У каждого института, у каждого университета есть обычно scientific board, который эвалюирует каждого сотрудника, и у них обычно им начисляется чиселка в зависимости от того, где они публиковали статью. И, в общем, ты им скажешь... Окей, они спрашивают, что ты сделал? Я говорю, я поддерживаю библиотеку, на которой у нас всё построено. Ну, и они говорят, ну, окей, как нам тебя эвалюировать? У нас нет такого пайплайна, где бы мы тебя эвалюировали за написанный код. В общем, это, наверное, можно было бы сделать, но это такое глобальное преобразование для Лесёча, что наверное, это сработает только в каких-нибудь Лесёч-подразделениях, которыми управляют компании. Например, DeepMind или OpenAI и всё такое, где я всё-таки ближе к прикладной части Лесёча идёт. Распределённые системы... Ты же не знаешь, чем ещё до меня затёрся. И наконец, Ansible. Кубернетис. Ну, между прочим, Ansible, да.",
    "result": {
      "error": "API request failed: Error code: 400 - {'error': 'Trying to keep the first 12859 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}",
      "stack_trace": "Traceback (most recent call last):\n  File \"/home/andrei/Projects/podcast-shownotes/scripts/build_search_eval_dataset.py\", line 157, in generate_search_query\n    response = await self.client.chat.completions.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 1927, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1767, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1461, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrei/Projects/podcast-shownotes/envs/lib/python3.12/site-packages/openai/_base_client.py\", line 1562, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': 'Trying to keep the first 12859 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}\n"
    }
  }
]