[
  {
    "segment_id": "39a664fa-d818-42be-8ad3-7989886362c8",
    "episode_id": "c2f5e451-71ab-443b-8ec1-32b0a198f120",
    "episode_number": 203,
    "segment_number": 2,
    "text": "То есть, вы правильно все поняли, можно попросить пылесос, чтобы кофеварка приготовила кофе. Такой ликвидизм? Да, именно так. Это часть того, что называется SNIPS AIR, то есть предполагается, что будет в квартире растыкана по углам разная мелкая MCU или Raspberry, которые отвечают за свои непосредственно маленькие задачи, и где-то там есть один координатор, который принимает запросы, как-то пытается обработать и понять, что же им надо сделать, и, возможно, он входит в интернет, чтобы, не знаю, ответить на вопрос, какая сейчас погода. А вот на сайте в разделе Platform Features тут все понятно на девайсе, там прайвеси, работает офлайн, а вот часть, которая называется MultiRoom, она вот к этому относилась или к чему-то другому? Возможно, мне надо посмотреть, что на сайте написано. Написано, что есть такая фича MultiRoom. Весьма вероятно, да. Ну, само собой, то, что я сейчас говорю, это мое личное понимание того, как здесь все устроено, возможно, я где-то в чем-то не прав. Расскажи, пожалуйста, чем конкретно ты занимаешься? Непосредственно я занимаюсь тем, что пытаюсь упростить жизнь различным командам машин обучения. В компании их три штуки, одна на wakeboard, одна на расслабление голоса, одна на попытки понять смысл того, что происходит в этом сообщении от юзера. Они все достаточно давно работают, у них всех есть какие-то наработки, и все они, соответственно, разные. И наступил такой момент в компании, когда становится немного невыгодно держать три разные огромные ветки кода, плюс они используют совершенно разные библиотеки, тузовины, и было бы очень здорово иметь какую-то общую базу, на которую могут все команды использовать. Например, из самых таких забанных вещей, которые были, когда я пришел, это то, что для обучения сеток для wakeboard команды использовался Jenkins, на котором пара скриптов на грубе была сделана в подобие к Automaproduce framework. То есть непосредственно одна из тех задач, которые я сейчас занимаюсь, это сделать написание какой-то внутренней библиотеки для того, чтобы можно было удобно задавать эксперименты, их трекать и так дальше. А скажи, какие инструменты ты для этого используешь? Смотрел ли в сторону Airflow, например, чтобы делать много экспериментов, трекать прогресс, такой пайплайнинг? Да, конечно. Airflow – это то, что планируется использовать под капотом, потому что очень не хочется переписывать, скажем, заново перезабытать велосипед. Но есть нюанс в том, что критично важно оставить текущий сформировавшийся в Airflow команд таким, как он есть. То есть не пытаться изменить привычки людей, которые занимаются машинным обучением. Я сильно подозреваю, что это происходит в каждой крупной компании, и это именно та причина, почему сейчас инструментов типа Airflow где-то минимум в десяток. Это только те, которые я рассматриваю. В планах написать свою какой-то метод, будет, скорее всего, внутренняя Тулза, которая для разных задач под капотом использует наиболее подходящие библиотеки типа Airflow или Ray или что-то такое. А скажи, что за Ray? Я никогда про него не слышала. Ray — это достаточно малоизвестная библиотека. Она изобретена теми же людьми, которые делают Spark, то есть из Berkeley. Сейчас версия 0.5. Она позволяет делать три вещи. Это Reinforcement Learning, который в компании не то чтобы очень сильно нужен, а вообще не используется. Позволяет делать распределенное обучение на нескольких машинах на нескольких GPU. И позволяет как-то более оптимизированно искать подходящие гиперпараметры для моделей. То есть правильно ли я понимаю, она делает Grid Search или что-то более умное? Grid Search — это только самая лобовая концепция, как искать гиперпараметры. Их на самом деле очень-очень много. Грид Сеарч, например, пытается строить байсовскую модель того, как-то пытается построить распределение того, какие параметры могут влиять на модель. И из-за этого что-то очень интересное. В общем, это активность. Это все поддерживается в этом фреймворке? Да. То есть это как конфиг? А можешь рассказать, кто этим фреймворком сейчас используется? Потому что звучит довольно интересно. Но такое ощущение, что он совсем-совсем новый. Из крупных я даже не знаю, кто используется. Кто использует... По-моему, какая-то китайская компания, Alibaba, что ли. Они там были упомянуты в папере, если не ошибаюсь. Но так особо ничего нет. Стоит заметить, что эти сдачи, которые я писал, есть очень много разных библиотек для того, чтобы с ними как-то бороться. Ray — это просто та, которую я непосредственно сейчас изучаю и пытаюсь оценить, стоит ли ее встраивать. Например, тот же TensorFlow, он и строит. Делает распределенное обучение. Но есть нюанс. Для того, чтобы делать распределенное обучение TensorFlow, последний предполагает то, что код модели написан в определенном стиле. Если быть более точным, есть такая штука как TensorFlow estimators. И если модель написана с его использованием, тогда можно относительно безболезненно использовать низкоробочное решение TensorFlow. Но в случае с NIPS это не так. Это не используется. И переписывать весь код машинного обучения, конечно, когда-то придется, но не сейчас. Поэтому идет определенный компромисс между, что можно использовать уже сейчас и что даст наибольший профит в будущем. Вот сейчас интересный момент. Код машинного обучения, как это вообще выглядит в компании? Скажем так, когда смотришь на всякие TensorFlow со стороны, то есть я человек, который вообще TensorFlow в продакшене не видел никогда, это просто есть готовые сетки, есть какие-то небольшие примерчики. С другой стороны, есть в принципе какой-то опыт в индустрии, опыт в продакшене каких-то других вещей в ML какого-то на кластерах. Но опять же, с того, что я видел, модели, которые вот, ну, то есть обычно, скажем, не сложные модели, проблема просто типа их запустить на большом объеме данных, чтобы они как-то посчитались. В принципе, вот сколько у вас объема кода моделей, сколько у вас времени и силы уходит на поддержку, с какими людьми это поддерживается. Потому что у меня, например, нет опыта, не знаю, я не видел прям больших отделов, которые послали бы прям большое количество моделей, каких-то слишком сложных кастомных. И вообще, продолжая вопрос, насколько у вас там используются какие-то стандартные версии из того же TensorFlow, и насколько приходится писать что-то кастомное? Окей, очень хороший вопрос, как раз собирался про это заговорить. Как происходит в принципе создание модели? Изначально это просто запуск разных экспериментов на заранее приготовленных данных. Из этого получается определенная модель в портобафе. В принципе, TensorFlow из коробки позволяет это дело задеплоить на различные устройства, это называется TensorFlow stack или deployment, как-то так. В общем, у него это есть из коробки, но есть пара нюансов. Нюанс первый, это было анонсировано не так давно. Нюанс второй, это работать не так высокопроизводительно, как могло бы быть. Поэтому в компании написано, опять же на Rust, написана библиотека, которая интерпретирует протобаф-файл модели и занимается тем, что уже приготовленную модель, как там, evaluated, пытается, она ее в рантами прогоняет. То есть у любой machine learning модели есть два этапа. Первый — это непосредственно обучение, второй — это inference. Инференс — это когда у тоже на девайсе надо модель как-то заиспользовать. И вот inference происходит на девайсе с кастомно имплементированными частично TensorFlow API, которые у нашей компании как-то открыты. Сделано это было достаточно давно, это основная причина, почему, наверное, основная причина, почему сейчас развивается свое решение. Ну и плюс оно чуть более производительное за счет того, что не нужно поддерживать все фичи TensorFlow, а лишь какой-то используемый субсет. То есть у вас типа кастомная имплементация TensorFlow на девайсе вежет? В каком-то смысле, да. А вот мне сейчас на самом деле вопрос еще был в том, как бы, как вот вы, не знаю, сколько людей пишут эксперименты, как много кода этих экспериментов, как он вообще пасется, не знаю, закамитили или забыли, или он как-то развивается, модели переобучаются, деплоятся на девайсе, и как вот это уже все живет? В компании примерно человек 40 инженеров, из них где-то 15-20 занимаются машинообучением. Машинообучение происходит именно в экспериментах. А остальные? Rust, Scala, чуть-чуть, JavaScript, вот это все. Нет-нет, а чем, ты сказал, 15 человек занимаются машинообучением, а чем занимаются остальные? Rust, Scala. Ага, все понятно, окей. Ну а что пишут на них, окей, допустим, у Rust кастомная имплементация, девайсов какие-то. JavaScript, вот это все. Смотри, вот не очень понятно, что на Rust и Scala пишут, то есть, допустим, про Rust упомянул, что есть кастомная имплементация TensorFlow для девайса, плюс еще какие-то вот прошивки для девайса. Алло? Алло, меня слышно? Я боюсь, пропустил последние секунды, я секунд пять последний упустил. Я говорю, что с Rust, допустим. Можешь уже воспользоваться вопросом? Да, я вроде пытаюсь, у меня такое ощущение, что задержка большая. Ну, и я тебя нормально слышу. Окей. Я тоже хорошо слышу. Вот, короче, Rust, Scala, но что на них делают? С Rust, допустим, понятно, что есть у вас, ты уже упомянул, что кастомная имплементация TensorFlow, еще какие-то куски прошивки, устройств. А что у вас на Scala? На Scala пишется бэкэнд, который связан с генерацией данных, плюс все связанное вообще с каким-то там админкой, бэкэндом и так дальше, и на ней же пишется все связанное с блокчейном. Вот. На эту тему я, боюсь, дальше не готов развивать, потому что я не пошел именно в Scala, именно в этой компании, именно по причине того, что они занимаются блокчейном на Scala. Это, конечно, интересно, но не очень мне лично. Так, а разве получается самоцель такая, ну, то есть на Rust пишется Rust? Нет, на Rust пишется все, что так или иначе происходит в устройствах, то есть абсолютно весь код, который со звуком, с инференсами и так дальше, это на Rust. Rust, человек, который пишет на Rust, примерно тоже 15, наверное, где-то так. То есть это код, который потом у клиентов на пылесосах будет работать? Да, именно. Но это не машинное обучение, это именно взаимодействие с микрофонами, ну, что-то такого плана? Нет, почему же? Допустим, библиотека, допустим, я упомянул компонент, который natural language understanding, занимается попыткой вычтенить смысл из текста. Он реимплементирован, модельки делаются на Python, но при этом инференс происходит на Rust, опять же, целью производительности. То есть примерно та же самая история, что с TensorFlow. То есть правильно я понимаю, что специальные эксперты, data scientist и вот эти вот, ну, как-то так они обучают модели на Python, а потом реальная имплементация переписывается на Rust? Нет-нет-нет, сама модель, она учится на Python, но для того, чтобы эту модель использовать, это совершенно отдельный процесс, не связанный с обучением. То есть первый этап – это тренировка, второй – это инференс. Как перевести инференс на русский, я вот сейчас сходу не придумаю правильно. И весь инференс происходит на девайсе, и там тоже необходимо иметь некоторое познание о машинном обучении. Так слушай, в такой связи с этим, как происходит вообще процесс? Вот у вас есть команда data science, есть люди, которые, так сказать, productionize эти модели для девайсов? Да, иногда эти люди даже совпадают. Как ни странно, люди, которые пишут в Python машинное обучение, им понравилось писать на Rust, и они поддерживают и ту, и другую библиотеку. Я загуглил возможные переводы инференс, и предлагается логический вывод – это допустимый вариант? Возможно. Ну, концепция такая, что сама модель, когда уже натренирована, допустим, она использует 20 серий, разочарковых и все такое. Но это непосредственно для тренировки. Для того, чтобы ее использовать, этот большой бинарник, нужно что-то другое. И это совсем не обязательно иметь GPU. Вот этот второй этап, который пытается эту модель использовать, называется инференс.",
    "result": {
      "query": "инференс модели на Rust устройства"
    }
  }
]