[
  {
    "segment_id": "e2e1f593-54a1-4a13-a686-249af038f8e5",
    "episode_id": "6aebb4c4-0bb6-4e8f-ae68-bef7198fff72",
    "episode_number": 75,
    "segment_number": 6,
    "text": "Нам нужна система подготовки кадров, система творческого роста для всех, который каждый человек мог бы достичь своего максимума развития. У меня вопрос неожиданно к Валере, потому что этот вопрос будет связан со следующей темой. Дело в том, что Роберт Хасс, известный деятель в сообществе PostgreSQL, опубликовал у себя в блоге пост про PostgreSQL и про прошлое, настоящее будущее. И выделил там три пункта, по которым, по его мнению, сейчас наиболее интересно развивать Postgres, где у него самые большие недостатки. Он отмечает это горизонтальное масштабирование, ну, мы тут все более-менее понимаем, сделать pluggable storage, потому что это сейчас просто как-то неконсистентно. У нас есть pluggable индексы, но storage один. Почему бы его не сделать pluggable? А почему бы не делать? Нет причин не делать. А вот зачем это тратить время, если он хороший? Ты понимаешь, что все зависит от характера твоей нагрузки. Да, понимаю, я специально спрашиваю вопрос. Ну, понятно. Где-то выгодно у тебя, например, сжимать данные, где-то у тебя LSM3 хорошо работает, а где-то не очень хорошо, и так далее. А вот третий пункт, я не уверен, что имеет в виду, встроенный connection pooling. У тебя есть идеи, что это? У меня? Да. У меня на шелке пятый лап. А что тут дебать? Просто сейчас, чтобы использовать их в нормальном серьезном проекте, тебе нужно всегда поставить по Gbounce, ну, какой-то pooler. Это какая-то лишняя... Можно я немножко макнусь глубже. Дело в том, что по Zgresovaya модель, когда на каждый connect есть один процесс, ну, в свое время такая... у самого популярного сервера в свое время, веб-сервера Apache, была известная проблема в том, что там 50-100 коннектов в секунду, и все. То есть, если ваше приложение не умеет пулить коннекты к Postgres, то вы лимитированы в производительности вот этой циферкой. Значит, либо правьте свое приложение, там кэшируйте коннекты, сделайте собственный пулинг, либо ставьте по Gbouncer, который там будет, собственно, уменьшать эту проблему. Ну, по Gbouncer там вводят свои неприятности. Например, если у вас требуется какая-нибудь тонкая настройка каждого коннекта, то вы должны это делать каждый раз, а не только при коннекте, потому что по Gbouncer можете вам вернуть не инициализированные соединения, а может вернуть соединения, инициализированные как-то по-другому. Значит, у вас тут выходов не так много, опять же, вы можете переложить это на по Gbouncer, ваше монолитное, скажем, приложение, вы должны из него выдернуть кусочек, отдать по Gbouncer. Ну, в общем-то, немногие соглашаются. То есть, встроенный пулер в PuzzleGrace – это, прежде всего, удобство разработки приложения. Оно мало влияет на производительность, если вы грамотно и с учетом особенностей PuzzleGrace написали приложение, но если вы мигренируете с другой базы, скажем, с MySQL, у которого коннект дешевый, то это вам здорово поможет. Все, я понял. Вопрос, на самом деле, к Валере был немножко другой. С точки зрения пользователя, что ты хотел бы видеть в PuzzleGrace, и мне интересно, как наши гости, что они на это скажут. Ну, чтобы его можно было шародировать вторая раз, во-вторых, чтобы был автоматический фейловер. Значит, вот эти вот мечты, я их вполне себе понимаю. Но мечта какая, что у вас есть PuzzleGrace, какой-то инстанс PuzzleGrace, и у вас там подходит к концу быстродействие, ну, то есть вы начинаете упираться в быстродействие, вы ставите рядом вторую тачку, каким-то образом, заклинаниями объясняете PuzzleGrace, что он теперь не одинок, у него есть братик, у вас волшебным образом все начинает работать. Мы над этим работаем, пытаемся эту вещь улучшить, ну, как не улучшить, а вообще создать. Есть несколько подходов, не только мы этим занимаемся, но, во-первых, тут есть принципиальные сложности с каптеоремой, там выберете любые два из трех. Не, ну мы на самом деле ведь знаем, что на самом деле выбор сети вполне себе работает, ну, то есть сейчас уже прошло оцепенение от осознания этого факта, и многим вполне нормально понятно, что если выбрать ЦП и выбрать какой-то нормальный логарифм репликации, то в принципе, ну да, оно не будет доступно, если мажорити упало, но если упал кто-то один, ну и фиг с ним. Ну, тут вообще колдовать, понимаете, требования приложений разные, универсальное решение, в общем-то, сделать, ну я не буду говорить, что близко к невозможному, но весьма и весьма тяжело. Мы пока вот собираемся вывести на орбиту классический мультимастер, который, собственно, больше о х, о дадежности и так далее, но не о производительности, по крайней мере на запись. Потом мы к нему надеемся приделать, ну не то чтобы сторонние, а научиться считаться другими решениями, которые сейчас существуют на рынке, которые жертвуют транзакционностью, то есть вы можете видеть не консистент состояние базы, вы можете на разных нодах видеть разные состояния, они этим жертвуют сознательно, с тем, чтобы получить производительность. Собственно, такое решение хорошо подходит, скажем так, веб-проектам типа социальных сетей. В общем-то, я социальными сетями проработал в своей жизни, и в общем вполне себе представляю, что потерять процент данных, ну ладно, в течение недели излогов восстановим. Для них гораздо хуже то, что пользователь тыкается в сеть, ну в браузер, а ему отвечают, что ведут с работы или у нас, упс, что-то случилось. Эта ситуация, как ни странно, намного хуже с точки зрения пользователя, потому что он тут же пойдет в конкурирующую социальную сеть. То, что там он какую-то реплику не показывает, но практика говорит о том, что далеко не все пользователи замечают, что им что-то не показывают. То есть представьте себе, вы мне написали сообщение, да, в социальной сети что-то упало, сообщение не потерялось, но мне не показывается, покажется оно мне через час. Значит, если это не было что-то срочное, то есть контрагент мне что-то написал, и ты требуешь срочного ответа, там 15 минут проходит, ответа от меня нет, потому что я его просто не вижу. Он мне начинает телефоном звонить и спрашивать, ты что, не видел мое сообщение? И я как-то начинаю на это реагировать. Вот это вот реальная ситуация, в которой я заметил пропажу, недоставку сообщения. А с другой стороны, как бы большинство людей эти пропажи не замечают. То есть, ну, пришло на час позже, да ну и что? Для банковских приложений такое лучше не надо. Не дай бог это вообще что-нибудь связано там, пусть не традиционный пример типа атомной энергетики, а управление поездами или самолетами. Там проматывание сообщений, что самолет занял эшелон там 9700, это гораздо хуже. Так что, безусловно, вещь очень интересная, вот это вот шардирование, вообще распараллельное, но она же интересна в силу своей крайней сложности и неординарности. Я думаю, это... Я могу поменять пару проектов. Извини, Саша. У меня просто маленькая мысль, что вот решение, которое я как пользователь хотел бы видеть, оно, наверное, должно быть по дефолту очень простое, то есть, какой-то простой вариант предлагать, но у него должна быть вкладка с кучей разных настроек, чтобы я мог все настроить, вот прям зная, что я делаю. Я могу поменять пару решений. Более-менее один проект уже известный довольно-таки, а второй набирает потихонечку известность, но он пока совсем закрытый. Первый проект, ну они оба никак не связаны с пользовательством, первый проект называется CockroachDB, как таракан. Это ребята, выходцы из Google, они пилят изначально распределенную SQL-базу данных, они ориентируются исключительно на OLTP, то есть, я не знаю, скорее всего, в Solap оно будет работать никак. Но при этом у них есть подходы к тому, чтобы... У них, в принципе, всегда не Serializable, а Snapshot Isolation, во-вторых, даже для транзакций, которые между шардами случаются, во-вторых, у них предполагается более-менее нормальная обработка тех джоинов, которые случаются в типичных OLTP-приложениях, и они выбирают... ну, у них компромисс сделан в сторону Consistency Partitioning, то есть, у нас, допустим, есть три реплики, если одна лежит, пользователь этого не замечает, если две лежат, то пользователь, возможно, замечает, но при этом у них три реплики, понятное дело, не на всю базу, а три реплики на шард. Второй проект, это Apollo от Facebook, это пока совсем закрытый проект, мы про него, по-моему, в прошлом подкасте говорили, они очень интересно делают, они решили, что мы будем поддерживать вообще все от Serializable Snapshot Isolation и вплоть до так называемых RAM-транзакций, которые как раз в AP-системах бывают, и у них там вот просто полный спектр того, что вообще может быть. Ну, понимаешь, они тогда, скорее всего, платят производительностью за это. А вот именно что нет, то есть, как бы они... которые из... Которые из людей просто тормоз. Ну, я не пробовал, я не буду так говорить, но... Каптеорему никто не отменял. То есть, то, что она на единственном запросе аккуратненько делает это в течение двух минусекунд, это да. Я вот в свое время столкнулся с Кассандрой, ее пытались применить на практическом проекте, не выжила. Не выжила она, потому что она написана на Java, и, собственно говоря, на Java... Я Java лично не очень люблю, ну, как бы признаю за ней право на существование, но практический эффект этой Кассандры был, что еще до того, как нам не стало хватать памяти, процессы Кассандры, одно за другой, уходили в Garbage Collection. То есть, проводили там больше 99% времени. Значит, это... несмотря на то, что она там явно жертвовала consistency, у нее модель eventually consistent, у нее не очень хорошо на тот момент, это было лет пять назад, сейчас не знаю, не буду врать, у них не очень получалось с большой нагрузкой работать. Если бы нагрузку уменьшить раза в два-три, то она вполне себе, наверное, работала и даже, наверное, масштабировалась. Но как бы фондроподаватель на тот момент решил, что 16 машин по 32 гигабайта памяти – это уже чересчур. В результате были брошены средства на самописную, то есть специализированную базу данных, на которой там трудилось несколько человек, и в течение пары месяцев выкатили решения, которые, в общем, стала справляться с нагрузкой, хотя и имела определенные неприятности при решарде. То есть решард вызывал... То есть когда мы попытались это сделать первый раз, прибежали системные административные, своплили, вы нам пачку свечей положили, просто передача данных туда-сюда, и об этом тоже не надо забывать. Такие вещи... То есть мы продумали замечательный кластер, мы все сделали замечательно, даже подумали о решарде, но совершенно упустили момент, что при решарде возникает такая нагрузка на сеть, что кластеры встают колом. Кстати, через эту проблему, по-моему, прошли почти все пункциональные распределенные базы данных, я почти у всех... У них, то, что я трогал, по-моему, у Кассандры была эта проблема, у Иряка эта проблема была, и все, конечно, в итоге починили, но очень многие через это прошли. Федор сейчас рассказал мой опыт работы с NoSQL в последние годы, то есть я тоже трогал Кассандру, и у нее были с ней точно те же ощущения, что вроде как все работает, но он нагрузку держит смешную абсолютно. Тоже проблемы с фейловером, правда, другого характера, и в CouchBase. То есть оно все красиво из коробочки, но одна проблема не работает. Нет, но с другой стороны, у нас, например, очень понятный use-case, то есть я могу совершенно точно сказать, что одна нода React, она, грубо говоря, сильно медленнее одной ноды Postgres, вот прям сильно медленнее. Но с другой стороны, мы имеем некоторое нетривиальное количество пользователей, и нам гораздо приятнее быть уверенным в том, что когда у нас бизнес хочет завести, еще пользователи покупают рекламу, мы можем в кластер поставить в два раза больше узлов и привести в два раза больше пользователей, и мы это спокойно прожуем. Если у нас одна машина с Postgres, то это сделать тяжело. Другое дело, что Postgres можно, конечно, шаргировать руками, но это уже нужно как бы какие-то усилия прилагать. То есть меня бы как пользователя вполне устроило даже автоматическое шаргирование Postgres с фейловером, в плоти того, что шарды друг с другом разговаривать вообще не умеют. Окей, меня устроило бы. Ну, знаете, то есть как бы еще очень много зависит от приложения. То есть в одном из проектов я столкнулся с замечательным требованием. Ответ должен формироваться за 100 миллисекунд. Это не real time, то есть это не жесткий real time, то есть реактор не взорвется, самолет не упадет, если мы ответим за 102 миллисекунды. Но как бы среднее время ответа должно быть 100 миллисекунд, то есть с соображением бизнеса. И тогда получилась интересная вещь, что вот мы сделали систему, которая специализированная база данных, несколько шаргов, и ты обращаешься на любой шарг с любым вопросом, если что, тебя там он проксирует на нужный шарг и выдаст тебе ответ. Так вот оказалось, что этого делать нельзя, потому что лишний вот этой вот сетевой хоб может причинять большие неприятности. То есть знание о том, как распределены данные в шарге, пришлось все равно держать в приложении, чтобы оно все равно сразу ходило на правильный шарг. Ну, подводных камней здесь много. И Олег ввел замечательный термин у нас в компании, может быть, он и снаружи где-то есть, но я его не встречал, это диагональное масштабирование. Потому что сейчас стали, как вот, понятно, что такое горизонтальное масштабирование, все представляют. Что такое вертикальное, но оно выглядит как-то, термин стрёмноватый, но перпендикулярно горизонтальному. То есть ставим машину больше и должны получить производительность больше. Мы нацелились на то, чтобы добиться диагонального масштабирования, чтобы подвес можно было масштабировать, как и ставя на более мощную машину, имея в виду то, что сейчас даже X86, много десятков, доступны машины с многим десятком процессоров, и даже первые сотни появились. Там Power от IBM уже появились с процессорами, которые с числом ядер уже не первые сотни, то есть там 512, по-моему, ядер. Это не в процессоре, Федь, это на сервере виртуальных ядер столько. Реальных процессоров ядер даже. Там можно спорить насчет разницы реализации аналога гипертрейдинга, но на Power оно работает хорошо. В 9.5, который недавно вышел, уже проведена значительная работа в этом направлении. Мы смогли догнавиться с несколькими компаниями, которые нам предоставляли очень крупные ящики, с которыми можно было поиграться. Мы нащупали еще узкие места, и часть из них уже решили, патчи выставлены. Кстати, ими занимался, собственно, здесь присутствующий Саша. Часть из них еще в процессе работы. Опять же, совершенно не ожидал, что опять приходится спускаться на уровень ассемблера в таких местах. То есть если на Power есть примитив компилятора атомарная операция, внутри она, когда мы посмотрели в ассемблер, она реализована как цикл. А мы вокруг этой операции тоже делаем цикл. Мы получили, что у нас два вложенных цикла. Кстати, этот кусочек для Power пришлось переписать на ассемблере. И, собственно, такие задачи вызывают повышенный интерес. Внезапно все у нас так абстрактненько, все у нас язык высокого уровня, ну сравнительно. И вдруг, как Джоэл Спойский говорил, это дырявая абстракция. Вдруг надо спускаться на уровень ассемблера и получать от этого неожиданно заметные профиты, разов два. Кстати, тут Валера, по-моему, говорил про React, да? Да. А вы знаете, что React не поддерживает транзакции? Знаем, нам оно и не очень нужно. Я поэтому говорю, что сравнивать React, который не поддерживает ACID, с Puzzle.js, который полностью консистент, не очень правильно. Я говорил в том ключе, какие гарантии нужны. Я сказал, что меня бы вполне устроило, что мне не нужен глобальный для всей базы ACID. Мне бы хватило ACID в рамках одного ключа. Даже не ACID, просто автоматность получается. Ну то есть тебя устроит 10 реплика-сетов или 10 просто мастеров? Да, да. Мне не обязательно, чтобы они могли между собой общаться. Для моего инстинкта. Я буквально, раз меня все равно Федор спалил, как раз недавно изучал этот вопрос, и у меня заняло... Вот знаешь, я пришел на работу в 10, и к обеду я знал, как это очень легко настроить. То есть вот это нежелание людей открыть документацию, прочитать 5 страниц на английском, оно меня очень сильно печалит. То есть это реально делается легко. Очень. Просто всем лень. Я предлагаю обсудить пару заметок... Подожди, Саша, извини, я не понял, что именно? Вот то, что ты хочешь. Оно настраивается за один рабочий день. Очень легко. А решардинг? Ну а что с ним? Ну вот смотри, ты сделал... Ты, например, заранее решил, что к нам приходил гость из Альпари, по-моему, и рассказывал, как у них устроено, ну правда, на мускуле, но тем не менее, что у них есть заранее заготовленное число VNOD, и есть словарь, который указывает на эти VNOD, если я правильно помню. Соответственно, ты говоришь, что твоя VNOD — это база данных. Ты их заводишь, распределяешь по серверам. И потом у тебя остается только вопрос, как реализировать базу данных с одного сервера на другой. Я правильно пока все говорю? Да, не отключая пользователей, желательно при этом не теряя данных. Угу. Ну, я тебе так скажу, что я вот пока не готов тебе ответить, потому что я даже не думав про решардинг, у меня был более простой кейс. Давай я... То есть мне даже без решардинга просто перетащить шарт? Просто перетащить шарт. Ну, условную VNOD. Ну, я вот сейчас не готов тебе ответить, как это сделать. Ты меня подловил, можешь считать. Но я, тем не менее, считаю, что это тоже, знаешь, часа четыре покурил долг и тоже разобрался. Тут, кстати, в общем, Саша начал это говорить про пост Хаса. Тут надо отметить такую вещь, что Хас указал, что есть задача, к которой можно заняться. То есть этот встроенный пулинг — это хорошая задача для того, чтобы начать работать с подрестом. То есть это говорит о том, что Хас будет поддерживать этого человека. А это очень важно, потому что если Хас обозлится или не будет поддерживать, то жизни этому человеку будет довольно тяжело, пробить что-нибудь. А тут можно сразу сесть и сказать, что, ребята, вот у меня есть такой пропозал, я хочу заниматься встроенным, конечно, пулингом. И посылаться на Хаса и... Да, нет, даже не надо ссылаться, просто Хас сразу скажет, о, так я же это написал, и начнет там, так далее, ну, как бы поддержит. Я только немножко попугать хотел. Это не та задачка, к которой можно справиться за неделю, посвящая этому два часа вечера. В общем-то, ну, средняя квалификация на оси уже нужна. Так, я предлагаю обсудить несколько статей в блоге DPSh.",
    "result": {
      "query": "Postgres горизонтальное масштабирование pluggable storage"
    }
  }
]