[
  {
    "segment_id": "e3eb35e2-d387-421d-b5ca-30050754ba84",
    "episode_id": "e16f910d-dc7b-40f6-998d-563764d382cb",
    "episode_number": 368,
    "segment_number": 8,
    "text": "Потому что обычно, когда что-то такое происходит, очень трудно просто взять и пойти поспать. Потому что ты понимаешь, что у всех горит жопа, и даже если у тебя есть возможность техническая пойти поспать, очень сложно взять, выдохнуть и успокоиться и уснуть. И не просыпаться посреди ночи, проверяя слайк, вдруг что-нибудь очень важное случилось. Собственно, такое если прям совсем по верхам, то было две проблемы. То есть они используют в своем стеке, то есть для управления этим зоопарком они использовали хашестек так называемый. То есть это был консул Nomed и Vault. Мы эти программные продукты несколько раз обсуждали в подкасте. Я думаю, еще немножко вернусь к этому через пару минут. И проблема была в хашестеке в двух местах. Это были проблемы, которые провзаимодействовали. Второй момент, то что у них были странные архитектурные решения в духе того, что например, давайте мы будем весь конфиг и даже некоторые кейвы или данные ссовать просто в консул. Это один из этих хаше продуктов. Хотя больше того этот консул кластер был всего лишь один на все. То есть скажем, Quorum из пяти машин, из которых там по-моему три в Quorum, еще три, не из пяти из шести, еще три это типа реприки на чтение. Это прям очень мало машин, чтобы поддерживать очень много всех остальных машин. Следующая проблема, которая у них была, это то, что диагностировать два временно issues, которые произошли, которые были в принципе друг с другом. Они были в разных местах. Одно к другому не имело никакого отношения, но они взаимодействовали у худших ситуацию. И поскольку это был внешний продукт, это был не самой компании, это было довольно сложно продиагностировать. Еще ситуация сложилась тем, что мониторинг, то есть метрики, логи были тоже в свою очередь завязаны на консул и на Nomet. То есть у вас что-то пошло настолько не так, что ничего не работает, все тормозит, потому что сам консул и сам Nomet начали прилегать. Вы пытаетесь на это посмотреть, ваша система метрики ломится в систему, которая прилегла, но вы поняли. Ну и наконец, когда они уже более-менее разобрали, чем было дело, им потом пришлось очень медленно поднимать это обратно, чтобы оно не легло снова. Что такое Nomet и консул? Надо вспомнить. Nomet это примерно Kubernetes только от компании HashiCorp. Я в какой-то момент делал сравнение его с Kubernetes и с MESOS, это было в середине 2019 года. И Nomet по сравнению с Kubernetes гораздо более легковесный. То есть настроить Nomet в минимальном сетапе или в максимальном сетапе, или в каком-нибудь сетапе, это гораздо более простая задача, чем настроить Kubernetes. Потому что в первую очередь Nomet гораздо меньше требований на то, как вся инфраструктура связана. Учитывая, что ребята селф-хостятся, я в принципе понимаю этот выбор, потому что на селф-хостатой системе развернуть Nomet гораздо проще, именно потому что требования к сети и требования к тому, что на самих машинах крутятся, гораздо более расслабленные. Console это такая система управления конфигурацией с RAVT внутри. Оно чем-то похоже на TCD, если вы с этим работали, но оно еще умеет DNS отдавать, прямо вот само. То есть даже если вы ничего кроме Console не настроили, вы можете через него сделать сервис Discovery, это будет работать довольно прозрачно, потому что Console еще сам по себе без каких-то дополнительных инструментов умеет быть DNS-сервером. И Vault наконец-то такой кусок софта, который управляет секретами. Немножко я про их стап-кластер уже рассказал, и вот собственно с чего все началось. Они проапгрейдили Console с одной версией на другую и включили фичу, которая по-хорошему должна была улучшить перфоманс. И они ее раскатывали, и раскатывали на одну подсистему раскатили, на другую подсистему раскатили, даже не помню линию подсистемы, они там на машины раскатывали. В общем, если честно, я не очень уже помню, давно читал блок. Но они делали какой-то такой, они не сразу для всех включили, и оно все везде работало, работало-работало, пока в какой-то момент не перестало. И это проявлялось так, что люди просто не могли залогиниться. То есть вечером 28-го почему-то Vault, собственно управлялка секретами, перестала отдавать видимые секретики, и на одном из консул-серверов поднялся спул-лод. Инженеры начали в этом комплекте, в этом консульсале, и они поняли, что это было глупо. И они пытались закидывать довольно долго железом, перезагружая и так далее, и это делалось все только хуже. И они не могли понять, почему. То есть мы ставим более мощное железо, а ситуация становится хуже. В какой-то момент они увидели, что, по-моему, Perf они использовали что-то похожее, и увидели, что contention возрос. То есть они не могли понять, почему. Они увидели, что, по-моему, Perf они использовали что-то похожее, и увидели, что contention возрос. То есть ситуация становится хуже, потому что возрастает contention. И они откатились, поставили... То есть они в какой-то момент запровижили 128-ядерные сервера, вернулись к 64-ядерным серверам, чтобы contention вернуть как было. Но ситуация прям сильно лучше не остановилась. В какой-то момент они уже не помню, вспомнили или как-то докопались. Сейчас точно уточню. Я сейчас уже не вижу так сходу, и я уже не помню, как именно они докопались. Возможно, просто анализируя, что они делали, какие изменения в систему вносились, и по-моему так. Они поняли, что последние крупные изменения, которые они делали, собственно, включили это фичу. Они стали консультироваться с HashiCorp и с, ну, с, как бы, в принципе, с MDIF-код. И выяснили, что имплементация, которая новая была этой фичи, она использовала, как говорится, Go channel в таком виде, что в некоторых ситуациях, особенно на NUMO, архитектурах типа вот с 10-ядерных процессор... с 12-ядерных машин с двумя сокетами, возникал очень сильный contention там, где... потом только в определенных условиях. Но, однако, не стоя на том, что они нашли проблему, они не могли даже выключить фичу, встанить кластер в нормальное состояние, потому что, если я правильно помню, пока у них вот это происходило все, аутатж начало его происходило, поскольку они меняли железо, поскольку они включали-включали ноды, пытались менять лидеров, у них очень много накопилась трафика в RAFT-логах. И для RAFT-логов консул использовал BallDB. В BallDB была не то что прям бага, но упрощение. Для контекста BallDB это такой бошный дисковый бэкэнд K-value с логом и транзакциями, который последние кода 2 или 3 находятся на гитхайве в состоянии archived. То есть человек его написал, он его написал как клон LMDB, такой сишный B3 дисковый бэкэнд, и просто как бы после того, как они там в каком-то месте, где он работает, в тот момент взяли его в продакшн, вроде поиспользовали, вроде ничего страшного не было, потом он оттуда уходился, перестал поддерживать за архивирил проект, проект продолжили использовать многие другие open source решения в экосистеме Go. Сейчас в принципе есть Fork, который поддерживается, но консул сидел до сих пор на BallDB. И в принципе надо понимать, что консул это система, которая работала на масштабах, типа вот как здесь, 18 тысяч машин, сотни тысяч контейнеров, и она в принципе долгое время без проблем работала, то есть это в принципе не стреляло. Что собственно такое это, в чем там упрощение? В том, что B3 в том стиле, которое вот в LMDB и в BallDB, после того, как какое-то место на диске было лоцировано под дисковый бэкэнд, оно больше никогда не отдаётся операционной системе, оно просто, если вы что-то удалили, что там лежало, то эта страница помечается как доступная для переиспользования. И вот этот вот список страниц, доступных для переиспользования в LMDB он хранится, как я понимаю, в каком-то там дополнительной структуре, или может быть в каких-то специальных страницах, которые внутри той же структуры, не помню деталей. Смысл в том, что в BallDB он хранится просто, короче, таким прям списком, и он коммитится каждый раз, когда происходит дисковый коммит, и он вырос до, по-моему, каких-то сотен, не до десятков мегабайт, то есть, грубо говоря, на любое минимальное изменение на диске, BallDB в итоге писал постоянно весь фрилист на диск, и это создало очень адекватную, очень адекватный дисковый паттерн, и вот, получается, такой в рейнтопу... Да, я уже разучился говорить, в рейтамплификейшн на ровном месте. Подожди, я не очень понял, из-за чего случилась эта мплификейшн. Смотри, они много-много-много раз перевыбирали лидеров, меняли всякое разное в кластере, из-за того, что они меняли машины, меняли лидеров, постоянно включались-выключались сервисы, и консул все это, короче, каждый раз согласовал через RAFT, и у RAFT было много трафика, это понятный момент? Да, это понятно. Вот, соответственно, когда много трафика, в какой-то момент у нас страницы в дисковой структуре освобождаются, это понятный момент? Страницы в дисковой структуре в консуле? Ну да, в консул хранит, то есть в BallDB хранится, если я правильно помню, RAFT log. Соответственно, какие-то вещи, когда мы из RAFT log'а уже все согласовали, видимо, всех машин договорились, мы это оттуда убираем. Ну то есть, мы какой-то... Да, логично, и чем больше трафика, тем больше мы убираем. Да. Если фрилист хранится не оптимально, и мы не успеваем как бы, типа, переиспользоваться страницей так же быстро, как мы их освобождаем, то мы вот эти просто... фрилист, который постоянно коммитится целиком вместо того, чтобы как-то эффективно хранится, он начинает коммититься просто с каждым изменением, постоянно создавая неадекватную нагрузку по записи. И когда вот эта вот нагрузка по записи вот такая неадекватная, это увеличивает write latency, и то есть как бы начинает такой снежно коммнаращиваться, то есть типа лидер медленно коммитит, в итоге все считают, что лидер там в общем впал в маразм, давайте его перевыберем, начинает перевыбирать в лидера, перевыбираем, добавляем запись в RAFT log, но на новой машине случается то же самое, и в общем оно вот так вот по кругу ходит, пытаясь стабилизироваться, но каждый раз впадая в ситуацию, когда у лидера большие дисковые latency, потому что слишком большой фрилист у BOLDB. Если я правильно помню, они... я вот сейчас уже давно пост читал, если я правильно помню, они вручную почистили содержимое, BOLDB как-то руками укомпактизировали, после этого начали перезапускать все, и отдельной проблемой было перезапустить каши, потому что чтобы восстанавливать консул, поскольку то, что было причиной Outage, это были консул и NOMAD, то есть то, что собственно крутит инфраструктуру, им пришлось чтобы снизить нагрузку и найти проблему, и даже до того, как они нашли root cause, пока они просто пытались снижением нагрузки или повышением, увеличением мощности машин, решить проблему, они частично выключали какие-то подсистемы, и в какой-то момент они просто ушли в полный downtime и выключили почти все, что было снаружи, и больше того, даже если что-то было в клавиатуре, даже если что-то было включено в какой-то момент, из-за всех этих манипуляций было сложно понять, будут ли в константном состоянии каши и будут ли в константном состоянии сервиса, или например, там даже была какая-то проблема, что",
    "result": {
      "query": "hashicorp consul nomad vault outage"
    }
  }
]