[
  {
    "segment_id": "f23d5059-838c-4bd7-92e2-f50bacc1aa00",
    "episode_id": "04bfcb80-d402-4394-8888-8a1fa226374f",
    "episode_number": 159,
    "segment_number": 3,
    "text": "Так вот, waifu и Nier Reborn Enhance объединяет две вещи. Это довольно старые проекты. И там много более от плейд кода. Код, который загружает картинки, который трансформирует картинки в тензоры. Это как бы... Если ты хочешь играться с нейронными сетями, хотелось бы играться именно с нейронными сетями. А не с тем, как загрузить картинку в нейронную сеть. Как пройтись по этим пикселям, вычислить там, нормализировать их, трансформировать. Да. Но опять, эти проекты довольно старые. Я посмотрел, что сейчас доступно из нового фреймворка для нейронных сетей. Там с этим уже попроще. Есть уже готовые методы для работы с этими вещами. Ну вот оно реально, Сережа. Реально большая часть времени тратится на то, чтобы подготовить данные. Реально на обучение. Да, подготовить данные и еще собрать данные. Потому что я пытался нагуглить пачки картинок для нейронных сетей. Есть наборы по лицам, есть наборы по смайликам. Может быть, какие-то наборы где-то еще есть, но я их не нашел. Ты в итоге все правильно, на самом деле, сделал. Пользоваться можно на чем угодно, в том числе и с картинками из сериалов. В статьях, из того, что я видел, они пользуются ImageNet. ImageNet — это такой очень большой датасет. Очень популярный. И он подходит для разных всяких тасков, потому что у картинок есть дескрипшины, есть выделенные объекты с квадратиками, в которых эти объекты заключены. На самом деле, тренировать Upscaling можно на любых картинках. И они это в статьях пишут. Что они просто сделали? Они брали любые картинки, какие нашли, в частности, из ImageNet, применяли фильтры, даунскейлили, и это у них получился тренировочный набор. То есть они берут даунскейленную картинку и пытаются сделать из нее обратную. И вот, собственно, и все. То есть они не ищут каких-то датасетов, которые конкретно для того, чтобы обучать нейросеть. Просто на обычных картинках делают все. Да, я так и делал. Еще прикол в том, что картинки должны быть желательны в PNG-формате или в JPEG с низкой компрессией. Потому что один из режимов Upscaling, там есть так называемый noise reduction. И если обучать его на плохом JPEG, где много шакалов, глаз не видит, но сеть это чувствует, и результат на выходе не такой хороший. Еще момент. Если кто-то хочет поиграться, рекомендую обратить на Neural Enhance, потому что это круто. Я игрался с ним на Macbook на CPU. Он Upscaled картинку где-то в районе минуты двух в Full HD картинку. Точнее, HD картинку в Full HD картинку. И результат впечатляет. А вот я вижу у Вайфу есть вроде как предтрейд модели, которые можно прям загрузить и ничего тренировать не надо. Ты пробовал? Да, я в результате ими и пользовался, этими моделями, которые дает. Причем анимешные картинки, на которых обучали это, он Upscaled лучше всех. У меня были маленькие картинки 1214768, я их в 4K Upscaled. И Upscaled выглядит как новая, как рисоватая в оригинале. Идеально. Еще момент такой был. Была довольно паршивая картинка, я ее заUpscaled последовательно 8 раз. Результат был так себе. Но Upscaled, потом Downscaled в 2 раза и картинка тоже снова выглядела хорошо. Тоже трюк так называемый. Ну вообще да, я вижу эти репозитории достаточно старые и они все из при-TensorFlow эры, где надо было писать очень много boilerplate кода, где надо было руками сидеть и писать эту нейросеть. Сейчас, наверное, надо все-таки поискать что-то на TensorFlow или хотя бы на PyTorch, на новую версию от Facebook. Еще могу, например, сравнить сети, которые вот эти сети. И я в университете в 2005 году занимался нейросетями. И то, что мы делали тогда, это кровь и слезы. Потому что максимум, что мы распознавали, это были циферки. Циферки на одном ядре, самописные сетью. Да, ты знаешь, как она там внутри досконально работает. Но что она могла? Распознавать циферки. Сейчас можно распознавать изображения. Кстати, hackathon, который у нас в компании проводился, еще две команды делали проекты на нейросетях. Одна команда распознавала смайлики, типа эмоции на смайликах. Они, кстати, вошли в топ-3. И вторая команда занималась распознаванием образов на видео. Ну, и это выглядело так. На видео был лев, который прыгал на стенку. И программа нейросети удавала, что на видео лев прыгает на стену. Я думаю, они где-то там стрюковали, схитрили. Потому что я не думаю, что настолько нейросети могут распознавать, что происходит на видео. Или там надо конкретно на это видео, или похоже, обучать. И он будет распознавать львов, прыгающих на стенку, и не прыгающих на стенку. Например, тот же трюк, как был в фильме Силикон Валли, где был хот-дог и не хот-дог. Я прав? Не, ну понятно, что-то похожее должно быть в тренинг-сете. То есть хотя бы оно должно понимать, что есть на картинке животное, что есть на картинке стена, и что происходит какое-то действие. Вот мне интересно, как оно будет прыгание, от залезания, или от каких-нибудь других похожих действий распознавать. Да, вот это вот... Наверное, там они где-то что-то схитрили. И в принципе, вообще они выиграли. Они выиграли. Но в чём прикол? Сейчас на Амазоне есть сервис, где ты можешь загрузить картинку, и она тебе её классифицирует. Довольно прикольно классифицирует. Классифицирует, в смысле, несколько тагов. Да-да-да, скажешь, что на ней нарисовано, и это дёшево. Так что свою писать, если только что-то очень специфичное надо. Плюс ещё на Амазоне этот же сервис позволяет загружать фотографии, распознавать на них лица. Ты можешь тегать лица, и он будет на других фотографиях находить эти лица. И также эту базу хранить на Амазоне. Опять же, Амазон за это берёт деньги, за этот сет. И как бы делать свой фотоальбом, если не хочется использовать АйФото. Или нет возможности, к примеру. Я бы сказал, что если вы компания, которая нужна вот этому функционалу, то использовать Амазон можно только если не хочется писать кучу всего этого boilerplate кода, и не хочется писать хранение и обработку всех этих изображений. Если есть возможность сделать что-то in-house, то очень много есть туториалов, и очень много есть пейперов, и очень много есть реализаций, я уверен, на TensorFlow всего этого. А вот как тут железо, потому что когда... Смотри, очень плохая связь пошла. Я не знаю, с чем это связано. Связь у меня плохая пошла. Да. А сейчас? Ничего лучше не стало. У тебя что-то где-то отходит. Поковыряй там железки, в которые ты говоришь. Я тогда может быть немножко расскажу про математику, которая там используется. Прям очень кратенько. В общем, вайфу они цитируют статью Image Super Resolution Using Deep Convolutional Networks от людей из Университета Гонконга и Microsoft Research. Они на самом деле... Это, по-моему, статья от 2014 года или даже 2013. Нет, 2014 все-таки. И они там просто поверх за даунскейленной картинки применяют обычную сверточную нейросеть. Сверху у них потом еще... Сверху из сверточной нейросети у них еще один есть плеер, который понимает, как нужно обратно деструктурировать картинку заапскейленной версией. И получается заапскейленная картинка. То есть очень простой подход. Скорее всего, реализация этой штуки где-то есть. Потому что обычно такие простые вещи доступны. Особенно эта статья от 2014 года. У нее огромное количество списирований. 700 других статей я ее процитировал. Так что уверен, что где-то она есть, реализованная. И в самой статье они берут ImageNet, как я и говорил, и используют его для обучения. Но вместо ImageNet можно было бы взять любой другой датасет. На самом деле там это не принципиально. И сравнивают они с обычной бикубической интерполяцией. И говорят, что у нас лучше, чем бикубическая интерполяция, и мы молодцы. И все, и они закруглили. А вот Neural Enhance, они цитируют уже статьи посвежее 2016 года. Там используются новомодные Generative Adversarial Networks, где у тебя получается две нейросети. Одна генерирует завскейленное изображение, а другая пытается отличить изображение завскейленной нейросетью от настоящего изображения. И у них такая игра получается. Там Winmax, где они попарно тренируются. И нейросеть, которая отличает одну настоящую картинку от фейковой, она попеременно тренируется с нейросетью, которая генерирует картинку. И потом ты просто берешь нейросеть, которая генерирует картинку, она уже обучена обманывать другие нейросети. И считается, что она обучает гораздо лучше, чем просто нейросеть, которая обучена попиксельно сравнивать завскейленную картинку с настоящей. Так что с точки зрения математики Neural Enhance гораздо круче. Но, собственно, все эти статьи очень популярны, поэтому я уверен, что где-то есть еще альтернативные реализации. Как меня сейчас слышно? Норм. Отлично. Еще вот проблема, я ее упоминал уже, кто как решает. А железо под это дело? Своё собираете или арендуете мощности где-то? Изначально я арендовал мощности на Амазоне, потому что там... Амазон — это очень хорошая на самом деле облако, несмотря на то, что вроде как большая корпорация, и все на него подсели, и должны быть цены какие-то не очень копейки, но на самом деле там очень хорошие цены, учитывая, что у них есть спотовые инстансы, которые можно арендовать с очень большой скидкой, если не принципиально, что инстанс может в любой момент упасть, но обычно не принципиально, потому что ну окей, у тебя инстанс упал, потом в какой-то момент поднялся и начал с чекпоинта дальше считать, ничего страшного. После того, как я выиграл грант на Microsoft Azure, я пересел на Microsoft Azure, там все очень дорого, по сравнению с Амазоном, но бесплатные... бесплатные вычислительные мощности, поэтому я это кушаю. Но вообще Microsoft Azure — это ужасное облако, мне очень нравится. Не с точки зрения юзер-экспириенса, меня как ресерчера, так и с точки зрения цены за инстансы. А можно сказать, что не нравится по UX в Azure? Такое ощущение, что... Просто попробуйте собрать инстанс на Амазоне и собрать инстанс на Азуре. На Азуре для того, чтобы запуститься, надо пройти миллиард действий в их UI, и она создаст миллион каких-то ресурсов, типа для IP-сника отдельный ресурс, для DNS-а отдельный ресурс, еще для чего-то отдельный ресурс. И все это выглядит как-то очень... Как будто бы мы в 90-х, и у нас есть большой список вещей, которые нам заарендовали, а нам всего-то нужно один инстанс было поднять, и нам не важно, что это все лацует. Как-то вот Microsoft, по-моему, не понимает, что надо некоторые вещи прятать, несмотря на то, что это дает, может быть, какую-то гибкость, но можно было бы это и спрятать, а там где-нибудь в расширенных настройках это обратно включить. У Амазона как-то все просто. Ты просто поднимаешь инстанс, окей, тебе там не хочется поднимать инстанс за подценнику, ты можешь поднять спотовый инстанс. Раз, тут же тебе выплюнуло на цену на спотовые инстансы сейчас для каждого региона. Бери, пожалуйста, сравнивай. И все это как-то делается так. Раз, два клика, три клика, очень просто. Поэтому вот такое вот у меня ощущение от Azure vs Amazon. Наверное, если поднимать это все дело через консоль, через какие-то более автоматизированные вещи, наверное, без разницы, но с точки зрения ресерчера, который делает все через UI и руками все это поднимает, это не очень удобно. А у тебя есть опыт использования консюмерского железа, например, GeForce 1080T, 1080, насколько это медленнее в сравнении с Теслами? Это сложный вопрос. У нас есть в Юните несколько воркстейшенов, где у нас есть 1080 и 1080Ti. Один воркстейшен с двумя 1080Ti, один воркстейшен с 1080 один. Это все очень удобно для ресерчера, потому что ты можешь собрать себе кастомную машину под свой таск. То есть, если тебе надо терабайт памяти и при этом еще GPU, ты такое на Амазоне не найдешь, на Microsoft Azure тоже не найдешь, потому что там либо memory optimized instances, либо GPU instances, у которых определенное количество памяти. Чтобы RAID туда добавить на 7 терабайт, например, в качестве storage, тебе надо там что-то сделать. Тут ты можешь собрать себе машину, какую хочешь. С точки зрения медленно-немедленной, конечно, медленнее, чем Тесла. Консюмерское железо медленнее, чем Тесла. Но, во-первых, если мы говорим о... По-моему, там половинная точность. Там нет никакого penalty. Надо посмотреть сейчас на Википедии. Но там, по-моему, с двойной точностью очень медленно консумерское железо вычисляют, а Тесла, наоборот, хорошо справляются. А с обычной точностью консумерское железо нормально скейлится. То есть, чуть-чуть послабше, но не сильно слабше, чем Тесла. Основное отличие для нейросетей между консумерским железом и Теслами — это объем памяти, видеопамяти. И это очень большая проблема. Бывает, когда реально нужно запихнуть в GPU большую модель, но она не помещается. Придется ее don't-scale. Но если ты don't-scale модель, то получится у тебя чуть-чуть хуже результат. Потому что, скорее всего, в большинстве нейросетей, которые вы будете обучать, нужно запихнуть всю модель на GPU. Дальше ты можешь сделать 10 GPU, и в каждой будет копия модели, они будут синхронизироваться, когда будут обучаться. Но на каждом GPU должна влезать модель. И это самое большое ограничение. А вот радионы взять... Я смотрел сайт AMD, они представили новые радионы, пишут, что они хороши для машинного обучения. Но вопрос, как, Карл? Там же CUDA нет, а все заточено под NVIDIA, под CUDA. В этом и соль. В этом и соль, да. Мы сидели очень долго на CUDA, и CUDA был монополист. Сейчас есть TensorFlow-версия для OpenCL, но, честно говоря, я ей ни разу не пользовался, потому что у всех уже закуплена CUDA, у всех уже закуплены GeForce и Tesla. И как-то я таких серьезных сравнений Radeon vs NVIDIA для машинного обучения я не встречал. Хотя очень странно, потому что если есть OpenCL-версия TensorFlow, кто-то же должен был это проверить. Поэтому просьба к тем, кто может проверить, если у кого есть время, пожалуйста, скачайте вайфу и попробуйте ее обучить. Там же она на CUDA написана, да? Там тот торч. Вайфу нужна CUDA. Я пробовал Neural Enhance. Точнее, что я пробовал? У меня Radeon 290 как бы поддерживает OpenCL. И я попытался поставить драйвера под это дело, чтобы работало под Linux. Я сломал Linux, когда поставил это дело. У меня графическая оболочка больше не грузилась. Ну, может, это считается старым Radeon, я не знаю. Но вообще, вычисление на OpenCL было одним из сейлз-поинтов вот как раз таких последних Radeon. И очень странно, если у них драйвера не поддерживают OpenCL, или там падают, или там не компилятся. Особенно под Linux, потому что все же будут арендовать инстансы с Radeon и считать на них. Я там еще сейчас посмотрел. А AMD, они сделали свою версию CUDA. И по синтаксису, в принципе, это один в один CUDA. И можно конвертировать CUDA-код в AMD-шный код, который будет запускаться на Radeon-овской карте. Вопрос, кто все это сделает и перепишет. Ну, очевидно, разработчики TensorFlow. Но вопрос, им лучше развивать OpenCL-версию, которая вроде как вендор-агностик, или им нужно затачиваться под очередное проприетарное что-то, что там клеймит, что оно похоже на CUDA. Но это не гарантия, что все взлетит просто через эту лузу, которая тебе магическим образом конвертит CUDA в AMD-шную вещь. А что насчет... А, извини. Нет, давай, продолжай. А что насчет TPU от Google? Это Tensor Processing Unit. Специальные карты для ускорения нейронных сетей. Пробовал? Нет, не пробовал. По-моему, это было доступно только... Мы это уже обсуждали, но это было доступно только в бете, и только на Google Compute Cloud. И я этого не видел, чтобы кто-то пользовался. В любом случае, из ресерчеров этим никто не пользуется, потому что проще индавать instance с GPU и сделать все на GPU. Но если они это реально будут как-то промоутить, то почему нет? Они наверняка TensorFlow, у них это работает замечательно на этих TPU. А вот как нейронные сети работают на телефонах? Для них же нужны огромные мощности. Но в тот же iPhone засунули нейронную сеть. Или это в железе они делают, что думаете? Непонятно, куда они это засунули, я, если честно, не сильно в этом разбираюсь, но мне это видится, что они просто берут и модельки downscaled, и модельки работают только на inference, то есть на распознавание.",
    "result": {
      "query": "тренинг нейросетей для upscaling картинок"
    }
  }
]