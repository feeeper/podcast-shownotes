[
  {
    "segment_id": "0a73693d-59ab-4fc7-8e56-e0df6f451b57",
    "episode_id": "2e59e9bd-276e-43d3-94dc-30d51c8dff71",
    "episode_number": 505,
    "segment_number": 7,
    "text": "Я вообще немножко озадачен, что в Алапе возникают такие запросы. Полнотекст? Полнотекстовый, да. Ну слушай, в смысле у тебя Поясни, почему тебя это удивляет? По моим представлениям дилетантским, я не то чтобы вот прям аналитик, а Lab это у нас построение отчётов, где обычно дофига разных цифр, где обычно дофига разных джоинов, а полнотекстовый поиск это для интернет-магазина, в нем нужно поискать товары вот для пользователя, который ищет товары. Я допускаю, что в аналитике могут быть полнотекстные запросы, мне просто сложно представить конкретно юзкейс, когда у Дмитрия была бы такая задача. Я тебе объясню. У тебя сейчас моден такой способ мышления о том, какие у тебя данные есть, вот то, что называется типа бронзовые данные, типа, знаешь, это Data Lake, потом из него у нас есть бронзовые данные, серебряные данные, золотые данные, хуе-мое. Но то, что ты говоришь это золотые данные, когда у тебя уже есть какой-то известный юзкейс, ты под него уже сделал свой чудесный развесистый Datapipeline, который уже там из твоих, не знаю, логов все раскрасил, куда нужны циферки положил. В реальности у тебя зачастую есть какие-то сырые логи или там какие-то json или еще какая-то непонятная херня, которая тебе прилетела из непонятно какого источника. Возможно даже из нескольких источников, они еще и разного формата. Вот, и у тебя есть какой-то вот во-первых у тебя есть процесс проведения всего к одному чему-то, но там тебе может быть не то чтобы очень нужен именно полный текст, там скорее всего тебе нужно просто будет какие-то возможности парсинга. Но вот когда ты на данных пытаешься построить какой-то новый отчет и в принципе разобраться, а можно ли по этим данным ответить на какой-то вопрос. Ты можешь хотеть из какой-то сырой строки выцепить какой-то токен. Я уже понял, да. У меня куча сырых данных, я не хз что там, но типа достань мне все документы, где есть вот такая подстрока, я посмотрел на нее более внимательно. Я понял. Ну да, хорошо. Еще ты plithouse, в принципе, абстрагирусь вообще эта lap, это просто система, которая кроме прочего популярна для всяких зеробилити ситуаций, потому что опять же она довольно быстро умеет молотить, а для тебя обычные хорошие Греб не мне логи? Ну типа да. Логи, трассы, тут все. То есть мы вспоминаем собственно та же самая Виктория Медокс, если я правильно помню построена поверх ЛиКауза, Ворфор на кликауза. А ты уверен, что такая хорошая мысль и первое, и второе класть в колоночную СУБД? Что такое первое, и второе? Что первое, и что второе? В первом случае у тебя какие-то сырые данные непропаршеные, то есть я так читаю длинная большая строка. И во втором случае у тебя логин, ну допустим я готов поверить, что там скажем одна строка до килобайта, ты почему-то сложил это в колонки, ну допустим я готов поверить в цифры. Ну у тебя вот строки там. Ну сырые данные имеется ввиду как раз таки, то есть даже по сырыми данными предполагает, что это типичная ситуация, есть скажем число, какие-то общие метки типа айпишник, какой кластер, не знаю, блин, сейчас тяжело так сходу все придумать, но у тебя есть десяток колонок, которые везде встречаются, ну или более-менее везде встречаются, особенно строчечные, и собственно блок сырых данных который никак дальше не отработан и из него пока ничего другое не выцепили потому что типа это входящие данные и более специфичные вещи будут выцепляться следующими стейджими в твоем ITL pipeline. Я в целом понял интересный кейс, просто я не то чтобы о нем когда-либо задумывался особо. На самом деле Мы даже то же самое делали без всякого лапа, как разгресить, когда мы в таймскейле делали промскейл, там была очень похожая задача. У тебя вылетающие данные это в общем-то просто какие-то OTL строчки, смотрел на телеметре. И мы собственно очень похожим образом хотели делать. Мы не успели это сделать, прежде чем прамскейл кончился, но у нас была очень похожая идея использовать блум фильтры и steaming для того, чтобы проиндексировать те вещи, которые мы еще не как-то не индексируем систему, то есть грубо говоря у нас был индекс по верхнеуровневым ключам и значениям, причем значениям до какого-то размера. Мы делали там отдельным образом специальное дерево, по которому мы могли делать и по дереву мы приведем поиски. У нас там даже мы могли джин индекс переиспользовать. Я помню делал для этого рерайт плана, не плана, а support planger support function, неважно. Но если у тебя есть какой-то value, который ну никак больше не подразбит, ну или ты просто может даже json, но он просто еще более глубоко структурирован, Ты хочешь по какому-то вот сильно-сильно низко стоящему значению это сделать. Это очень типично собственно в дачах трассировки. Ты не можешь предусмотреть все возможные индексирования на всевозможные глубокие значения. Вот опять же насчет конкретно с JSON, тот же самый паркет, ну и прочие другие колоночные форматы, они более-менее еще сделали так, что они могут JSON нарезать на колонки и соответственно это может быть не не совсем уж сырые строки, не совсем уж типа килобайт json, а скажем он будет нарезан на, экс не знает делает ли это кликхаус, но он будет типа нарезан на как бы, у тебя колонка это как бы путь в json, имя колонки, а значение это собственно значение по этому пути. У тебя все равно есть какая-то глубина, глубже которой ты это не делаешь, это раз, во-вторых, у тебя все равно будут какие-то колонки, которые просто строка. Типа да, она может быть чуть эффективнее храниться, чем если просто килобайт json за килобайтом json, даже не чуть, будет сильно эффективнее храниться, но все еще у тебя там будут какие-то строковые штуки, по которым ты можешь хотеть делать строкой поиск. И здорово этот строковый поиск делать на основе метаданных, а не на основе того, что ты все данные бросишь.",
    "result": {
      "query": "полнотекстовый поиск в аналитике данных"
    }
  }
]