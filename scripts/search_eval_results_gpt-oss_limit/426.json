[
  {
    "segment_id": "3d6e5653-4737-4f85-83ea-03ebfa9266e2",
    "episode_id": "255f87c5-8776-45c9-b925-6982dfd2cf74",
    "episode_number": 426,
    "segment_number": 4,
    "text": "Это потому что Apple не мог сделать нормально, обновить свою линейку Mac Mini давным-давно, лет, наверное, 4-5 лет назад. Так что рабочая история Proxmox не только ради Кубернетеса можно использовать, его можно использовать для каких-то домашних историй. Даже как рабочие станции. Так ладно, вернемся к Кубернетесу. В общем, поднимал я эти ноды, отпускал эти ноды, у меня там был Postgres, свои личные данные, еще что-то, и понял, значит, как мне бы их сохранить, при этом не терять при удалении виртуалок и заново запускания. И начал искать какой-нибудь сетевой драйвер, точнее, CSI драйвер для Proxmox или вообще что-нибудь вокруг этого всего, чтобы можно было использовать нативно в Кубернетесе. Создаешь ресурс, у тебя появляется диск. Удаляешь ресурс, соответственно, и диск освободился, пространство на диске освободилось. Вариантов очень много, но большинство из них это связано с каким-нибудь кластером. Вам нужно много серверов, поднимать сеть, 10 гигабитные линки иметь, чтобы диски не тормозили. И, в общем, понял, что этот вариант мне не подходит, потому что у меня всего лишь один серверок, и нет ничего другого. Никаких файловых хранилищ нету, NPS, iSCSI мне тоже не подходит. Может быть, каждый администратор в своей жизни пробовал стримить вау-логи с Postgres на NPS раздел и больше так не пробует. А у вас были такие варианты? Такие кто? С NPS и Postgres. Нет. Я бы купил домой, если бы я поставил себе дома Kubernetes, я бы, конечно же, подключил к NAS, и поэтому на твоем месте я бы тоже купил какой-нибудь NAS и баловался бы с ним. Мне кажется, это была бы прям знатная парочка. NAS чисто для сториджа и маленький сервер для Kubernetes. Звучит прям, как вот мне уже захотелось самому сделать на своей малинке. Я пока на нем поднял ноль полезных контейнеров. Ну, то есть я пару раз на нем пытался всякую ерунду поднимать, но так пока ничего не прижалось. Ты верно. Да, я еще сверяю там свою памятку по Praxmox, имею сказать, что было сказано, что он работает с виртуалками, но также он умеет работать с контейнерами сам по себе. И не с докером, а с LXE, которые выглядят как виртуалки, но контейнеры. Я тоже думаю, что на NAS ничего не прижилось, я имею в виду именно докер-контейнеры, потому что просто Synology имеет кучу встроенных сервисов, типа того же DNS-сервиса, и они просто нормально работают. У меня нету острого желания поднимать сторонних вещей именно в контейнерах, именно в Kubernetes. Вот, кстати, кто-то нас недавно слушает, и у вас стороной обошла история про LXE, это прям очень классная штука. То есть это выглядит, как будто у вас виртуалка, но с накладными расходами контейнера. Сохраняется, стоит, можно управлять с консоли, там, типа, запусти виртуалку, опусти виртуалку, у них нормальная сеть, там, без конфликта портов и так далее. То есть, ну, попробуйте, если вы еще не пробовали LXE. Очень клевая штука. Единственное, что в PSAX вы не видите процессов ядра, но, как бы, почти как докер, только без этой истории, что один процесс на контейнер. Там прям настоящий инит, и все крутится до докера, я очень много использовал LXE. Ну, в общем, с Synology и подобными файлами-всторожами, как раз проблема в том, что там опций не очень много. Чаще всего iSCSI, опции чего? Чтобы использование сетевых дисков. В домашних, там, типа, у меня SMB, и SMB в моей сети, ну, гигабит, в принципе, выдает. Я больше, у меня просто большая сеть не может, у меня в сети гигабитный провод. Ну, в смысле, у меня свитч тоже гигабитный, я, наверное, могу в теории 2 гигабитный свитч поставить, будет 2 гигабита, может быть, но я не уверен, что мои диски столько вытянут. Да-да-да, получается NFS, Samba, SMB и iSCSI. Вариантов больше нет. Все равно, если делать как-то более автоматизированную историю с созданием ресурсов в Kubernetes, и у вас там что-то появилось, то какие-то надо драйвера ставить и на сторону верталки в Kubernetes ноде, чтобы NFS, либо iSCSI, либо SMB монтировался прям в под Kubernetes. А мне, как раз, этим не хотел сталкиваться, и я понял, что все основные варианты, которые сейчас есть в интернете и на GitHub, мне не подходят. Я хотел что-то нативное, как в нормальном облаке, чтобы диски монтировались прям в виртуалку, без всяких дополнительных костылей. Просто если ты запускаешь все свои контейнеры к Kubernetes и все такое прямо на самом NAS, и тебе никаких костылей не нужно, у тебя же локально внутри NAS просто его файловая система. Неправильно, Валера. NAS — это NAS, это хранилище, не надо запускать всю нагрузку. Это должно быть на отдельном сервере. Ну, вот, кстати, Synology, CSI, да, клевышка, не спорю. Вот как раз у меня этого нету, зачем мне два устройства? У меня есть только один сервер с локальным диском, и я хотел достаточно стандартный, предсказуемый, моего перформанса, его пропускной способности и никак не связываться с сетью. Потому что сеть бывает разная. Может быть, у вас вообще на Wi-Fi все крутится. Кто-нибудь включил микроволновочку, и скорость ваша упала. Wi-Fi. Вот, в общем, гуглил я, гуглил, и ничего не нашел, и начал искать документацию, как же, вообще, насколько сложно сделать свой CSI драйвер. Есть официальный док, я кинул сейчас, чат. На самом деле под Kubernetes уже много чего написано. Практически вся сложная задача уже давно решена. Есть сайткары, или middleware, между вашим кодом, плодином, и самим кубернельцем. Они следят за ресурсами, и просто вызывают ваши функции, процедуры в вашем коде, и вам нужно только всего лишь реализовать несколько функций. Чаще всего CSI, драйвер бьют на две части. Контроллер, и то, что оно не запускается. Контроллер, это, можно сказать, проксиа между кубернельцем и вашим клаудом. В моем случае это был Proxmox. Его задача входит создание, удаление блочных устройств, потом оттачить и тачить этого блочного устройства прям в виртуалку, и ресайзинг. И в некоторых случаях, еще есть capacity функция, это когда вы хотите узнать, сколько свободного места есть у клауда, чтобы вы могли его аллоцировать для себя. Например, у вас есть какой-нибудь аккаунт лимитированный в каком-нибудь облаке, вам доступно 1 терабайт данных, и больше терабайта вы не сможете купить. Вот как раз это про этот случай. Нодовая история, она, в принципе, ищет новый диск, который присоединился к виртуалке, форматирует его и монтирует уже непосредственно в директорию пода. Там же есть еще и та же самая функция ресайзинга, потому что файлосистема ничего не знает про блочное устройство, пока ей не скажут, что у тебя, кажется, в хвосте что-то появилось, и посмотри на свободное место, может быть, ты можешь себя увеличить в емах. С этим, ребят, понятно? Ну, то есть, состоит из двух частей, контроллер работает... Контроллер работает где-то, ей не нужно... Да, это может быть на control plane, либо, чаще всего, в облаках это у вас прям на подах, ой, на нодах запущен один или два демона, для... Которые следят за всем этим. А, соответственно, в Kubernetes есть еще вторая часть этого решения, которая с ним, с контроллером удаленным общается, но умеет работать с локальной файловой системой или с операционной системой, или как там оно должно представлять на уровне Kubernetes? Все верно. Вот эта вот нодовая часть, она запускается на каждой ноде в daemon-сете, имеет рутовые права, привилегированные для того, чтобы монтировать диски, форматировать, если что, фиксировать файл в систему, ну, в смысле, Eurora фиксировать, фиксить, вот. И, соответственно, они могут делать все что угодно на вашей ноде. И это у меня заняло примерно расследование и написание такого драйвера примерно 2-3 дня. Больше всего времени я потратил на написание тестов. Тесты — это ужас. Вам придется мокать все API-вызовы в ваш клауд и в Kubernetes, если вы общаетесь с Kubernetes. Погоди, давай паузу, паузу. Ты сказал только что, что состоит из двух частей, а дальше сразу перешел к драйверу. Ты имеешь в виду вот эту часть, которая в Kubernetes работает? Ты ее написал? Или что ты написал? Я что-то потерялся. Я написал две части. И контроллер, чтобы создавать диски в Proxmox и нодовую часть, которая форматирует бочные устройства и монтирует в поды. Контроллер у тебя тоже работает здесь же, на Proxmox? Он работает, но он не работает. Он работает в Kubernetes сам. Это просто отдельный deployment, который ловит event с Kubernetes и уже отправляет запросы в сам Proxmox, чтобы создать диск и примонтировать, ну, не примонтировать, приоттачивать его к виртуалке. Понятно. То есть как бы обе части работают в одном месте фактически и позволяют работать с файловой системой. Да, просто контроллер нужна одна копия, а нодовая часть нужна на каждой ноде. И запустить. После того, как, собственно, контроллер создал диск и примонтировал к виртуалке, нодовой части надо найти в операционной системе, потому что, ну, Linux он просто дает, как правило, следующую букву диска SDA, SDB и так далее. И вот нодовая часть и ее задача как раз найти именно то устройство, которое ты хочешь примонтировать к своему поду. Вот, соответственно, найти его, отформатировать и примонтировать. Это первый раз делается форматирование, следующий раз он просто проверяет, есть ли файл система и просто монтирует. Но иногда, в некоторых случаях, даже пытается фейс-чеком как-то пофиксить файл структуру, если что-то поломается. А написал ты на чем? Ну, вся экосистема Kubernetes на Go и я написал на Go. Самое важное, что Самое важное, что на расте, на расте. Самое важное, что уже есть даже шаблонизаторы. Вы можете найти в интернете просто так утилиты, которые вам создадут скелет этого плагина. Все уже процедуры будут там определены. Вам нужно только имплементировать их. Единственное, что надо знать, что самое важное, я хотел бы как раз поделиться, что Kubernetes и вот этот middleware proxy, они не гарантируют, они могут несколько раз вызвать вашу процедуру с одними и теми же параметрами. Например, создание диска с ID таким-то. И если вы как-то у себя не будете это контролировать, то можно понаделать дров в Proxmox в моем случае. Вот, надо как-то на вашей стороне давать индексы или уиды для вашего устройства, чтобы чтобы в следующий раз не создать то же самое, копии не оплатить. И вторая история не очень хорошая. Proxmox не умеет транзакций. Если вам вот так вот прилетело много-много запросов создать несколько устройств, вам надо решать, каким-то образом делать шар-мемори, не шар-мемори, mutex реализовывать, чтобы несколько раз не прийти и не поправить свою виртуалку, конфигурацию виртуальной и не перезаписать какие-нибудь данные. Вся эта история лежит уже на ваших руках. Но если вы... Погоди, я не понял, в каком смысле это Proxmox не умеет транзакций? Ты же программируешь целиком все, что ты хочешь. Смотри, какая там история. Ты сначала приходишь в Proxmox и говоришь, отдай-ка мне конфигурацию этой виртуалки. Потому что тебе нужно узнать, сколько там жестких дисков уже примонтировано. Значит, скачиваешь ты этот конфиг, и думаешь, ага, у меня есть свободный слот для нового диска. И опуляешь новый уже конфиг в Proxmox, в этот слот уже забиваешь новое бочное устройство. И Proxmox его применяет. А теперь представь такую ситуацию, что одновременно несколько таких запросов прилетело. Они забрали конфигурацию, заняли один и тот же слот и отправили в Proxmox один и тот же, ну почти один и тот же запрос. В итоге Proxmox может создать несколько бочных устройств и потом их как-то заменить, либо удалить, либо непонятно, что произойдет. Да, я понимаю, но я не знаком с подобной системой, как это, скажем, на каком-нибудь обобщенной Ubuntu на лэптопе бы, как это заработало. Как это решается вообще в целом в современном Linux? Я смотрел код OpenStack, и ты просто, когда создаешь бочное устройство, ты даешь ему ID-шник. Точнее, тебе OpenStack дает ID-шник. И ты его запоминаешь. А потом говоришь, что хочу приоттачить к этой верталке жесткий диск с таким-то ID-шником. И все. И уже сам OpenStack должен все это менеджировать. А в Proxmox этого нет. Там есть конфиг, и все, что туда пульнешь, то он пытается выполнить. Ну понятно. Но, как ты пришел и сказал, что вот сейчас я буду менять конфигурацию, никто другой не может его менять. Ну, как это? В WriteLog. У тебя нет такого понятия в Proxmox. Это, можно сказать, основные проблемы, которые вы можете встретить именно в Proxmox. Но, я думаю, если вы возьмете какой-нибудь другой клауд, я думаю, там не будет никаких таких проблем с этим диском. В общем, реализация недолгая. Пару-тройку дней можно потратить на написание этой всей истории. С юнит-тестами подольше придется повозиться. Но я хотел бы затронуть очень важную в этой вещи, очень важный вопрос безопасности в этой всей истории. Я отвечу на вопрос в Telegram. Очень плохо. Для Proxmox есть два Terraform вагина. Погоди, погоди, давай. Давай для остальных тоже озвучим. Как с блоковыми устройствами работать Terraform в Proxmox? Вот, отвечаю на вопрос. Есть всего два плагина для Terraform, точнее, которые я находил. И оба, точнее, тот, который самый популярный, работает очень ужасно. Если вы пульнете, точнее, создадите виртуалку с диском, не знаю, с одним диском, и решите попасть в Terraform, любой параметр этой виртуалки, этот плагин создаст второй жесткий диск такого же размера. И есть тикеты на эту тему, и они пытаются переписать эту часть, связанную с дисковой подсистемой. Но пока у них все это в планах. То есть, фактически, если вы хотите пользоваться Terraform и Proxmox, вам нужно делать life cycle и повторить всякие различные изменения в виртуалке, чтобы неожиданно не получить вот такую историю с несколькими дубликациями жестких дисков. Но вообще, на самом деле, это кривоход, можно было проще это все делать, и не было такой проблемы. Но в результате, да, чтобы чтобы что-то добавить, вам надо каким-то образом сказать Proxmox, что вы сейчас будете менять конфигурацию. А такой функции в IP нет. Вот. В общем, я хотел затронуть самое важнейшее, мне кажется, это монолога, это история про безопасность, потому что про нее все забывают. Вот нодовая часть, которая запускается на ноде, она имеет максимальные права доступа. Привилегированный доступ и рутовые права. Значит, этот контейнер, все, что там внутри, имеет доступ ко всем файлам, которые у вас есть на ноде. В том числе, секретам, environment'ам и тому подобное. Я, когда это осознал, точнее, я и раньше это знал, а тут я решил все-таки поискать, а что же люди делают? Вот большие клауды с этой проблемой. И понял, что ничего не делают. Ну, то есть, есть Google, то, что я в Google нашел, да, они значит, сделали очень маленький контейнер, положили туда самый минимум файлов, которые нужны, без Shell, без Bash и тому подобного, все вычистили. И контейнер очень маленький и плюс-минус безопасный. А большинство других клаудов используют Debian как базовый контейнер. В Debian уже есть и Bash, и Perl, когда вы ставите утилиты для монтирования, форматирования, еще и Python прилетает. Вот, и в общем получается, что контейнер очень большой, и любой, кто может сделать Shell в этот контейнер, может забрать все, что у вас есть. на вашей ноде. И все, что я... я был очень сильно удивлен, что я увидел, что у OpenStack такая же интересная история. Но благо, на прошлой неделе, после недолгого общения с ребятами, мы сжали этот контейнер, и новая версия в OpenStack будет уже более-менее безопасной. Если у кого-то возник вопрос по поводу, как можно стерить из контейнера, из любых контейнеров, environment и секреты, спросите. Где спросить? Ну, типа у меня спросите, я могу это... Хорошо, хорошо. Если вам интересно, я расскажу. Давай, давай, расскажи. Вот этот под, он имеет ко всей файлосистеме доступ. Соответственно, если у вас секреты примонтированы в виде файлов на диске, то это memoryfs, по которому вы можете просто пойти. Вам нужно узнать директорию, где это лежит. На самом деле, обычным поиском, либо как-то предугадать, можно найти и директорию, где это будет лежать, и будет вам доступно для чтения. По поводу environment'ов, чуть-чуть посложнее. Но если у вас есть ротовые права, вы можете просто у PID'а забрать обычный, ну, там, через slashprog, slash, там, номер PID'а и тому подобное, забрать все environment'ы, у любого процесса. Соответственно, все секреты, все файловые системы, все файлы на диске можно забрать, либо сделать вообще rm-rf, и никто вам это не запретит. Ну, безопасность в подобном окружении, конечно, это очень такая тяжелая головная боль, потому что мало кто задумывается, на самом деле, векторов атаки очень много. На самом деле, есть еще один контейнер, это сетевой драйвер, который тоже имеет максимальные права доступа, ему надо сеть настраивать, ему нужны права, и фактически на любой ноде есть два таких опасных пода, которые могут забрать все, ну, поломать все тоже. Я, наверное, в Telegram сброшу тикет, который я создал в Kubernetes сообществе по поводу давайте уменьшим, как сделать контейнер, либо сделаем документацию для всего мира, как делать. Давай-давай. А я это положу в шоу-ноуты. Ты в шоу-ноуты-то не складывал? Я складывал. А второй уже сам плагин для Proxmox, если кому-то интересно. Можно посмотреть, как я сжал этот контейнер для нодовой истории, для нодового плагина. Ага, отлично. Добавим в шоу-ноуты, будет видно в шоу-ноутах. Спасибо. Пошли дальше по темам? Да, давайте. Я хочу чуть-чуть отдохнуть. Отлично. Кстати, вот смотрите, уважаемые слушатели, как можно запросто прийти и рассказать хотя бы, чему научились за неделю. Вот замечательная инициатива, замечательно пообсудили. Следующая по темам моя, потому что это тоже как бы чему научились за неделю, и это обсуждение, которое мы с бывшими коллегами ведем в чате. У нас тоже есть чат бывших коллег, да, это у всех есть, я знаю. Банально, но тем не менее интересные темы иногда всплывают.",
    "result": {
      "query": "CSI драйвер Proxmox один сервер NAS"
    }
  }
]