[
  {
    "segment_id": "1a6f57b0-2073-4e99-bb7d-2a68c4ef29b8",
    "episode_id": "9844537d-87bd-4c3b-b540-0cc100237188",
    "episode_number": 209,
    "segment_number": 9,
    "text": "А вот, собственно, это... я думаю, что обсуждение этой темы в это и вырвется, потому что обсуждать архитектуру я особо смысла не вижу. Ну, она, во-первых, не очень хорошо документирована еще. Вот, в общем, что, собственно, о чем новость? Ребята из Фейсбука опенсорснули нечто под названием LogDevice. Я уже слышал некоторые ремарки, чтобы понять примерно, что это. Если вы в курсе, что такое Apache BookKeeper, вот это очень похоже, тут немножко другие тройдовы инженерные. Но в целом идея очень похожа. В чем отличие? Наверняка все слышали уже про кавку. Что такое кавка? Это натурально просто шародированный как бы файл с байтиками. И вот можно по офсету читать. В новых версиях можно вот в разные типа топики и разные там, ну, скажем так, продолжая аналогию с файлами, можно в разные файлы и в разные там типа партиции файла писать транзакционно-атомарно. Вот. Короче, то есть кавка – это такой распределенный лог с байтиками, с очень хорошими гарантиями доставки и очень хорошими гарантиями надежности. Но тем не менее, то есть там хорошая абстракция в плане работы с датами. То есть он хорошо абстрагирует от вас распределенную природу, то есть если вы все правильно сделали, вы можете из кавки выложить exactly once. Но при этом вы сами себе отвечаете, как вот работать с тем, что у вас внутри этого лога. Это первая такая задача с кавкой. То есть на кавку хорошо ложатся только на самом деле логи репликации или там, ну, всякого рода pipeline передачи данных. Вторая проблема на самом деле с кавкой в том, что она связывает операции записи логов, ну, то есть CPU и network-aware intensive вещи, связывает их, ну, и в принципе координацию с дисковым сторажем. То есть у вас все брокер-машины, все, кто участвует в репликации, это все те же самые машины, которые принимают данные и пересылают их куда-то. То есть кавку, пока именно в текущий момент, ее довольно тяжело скейрить за пределы вот тех машин, которые вас что-то обслуживают. Поэтому если вам нужно просто, если у вас, например, не очень большой объем данных, но очень интенсивные писатели, вам тяжело отдельно поскейрить CPU. Если у вас, наоборот, там очень много данных хочется хранить, больше, чем вы можете дисков напихать физически в машины с кавкой, то у вас там другая проблема возникает. Вам, типа, нужно добавлять машины в кластер, хотя вам на самом деле нужно просто дисков хочется в кластер добавить, а не машины. Хотя там есть в кавке поползновение для того, чтобы, типа, исторические данные загружать на какое-нибудь вообще внешнее хранилище. Условно-медленное или даже не локально приатаченное. Из плюсов у кавки есть то, что у нее при чтении она, вот, то, что называется Mechanical Sympathy. То есть она очень клево работает, если вы, как бы, читаете только вот то, что недавно записали. Потому что оно там всегда, вот на машину прилетели байтики, они оказались в кэша файловой системы, если я их сразу же читающий прочитал, они из этого же кэша файловой системы отдались и там супер просто топорные структуры на дисках, ну, топорные не в смысле, что они тупые, а в смысле, что они не требуют каких-то сложных манипуляций с ними, как правило. И, соответственно, там overhead довольно маленький. Вот. У Facebook, я подозреваю, задача как раз, ну, у них масштаб такой, что им, видимо, с чем-то вроде кавки возиться очень тяжело, это раз. А во-вторых, у них более широкий круг задач, у них не только логи, репликации, у них хочется еще и task queues на этом реализовывать, им хочется на этом какие-то такие, ну, более сложные стрим-процессинги делать. И я уверен, что им не хочется в каждой клиентской системе еще реализовывать вот как бы вот это деление на отдельные сообщения, как вот что в кавке, как я уже сказал, там уровень абстракции это байтики. Ну, то есть там большинство клиентов, конечно, реализуют вот сообщения, но именно протокол кавки там байтики. Вот. Ну, то есть так или иначе появилась вот эта идея сделать вместо кавки буккипер пансорсный, а сейчас еще лог-девайс пансорсный. То есть самое главное отличие в том, что у вас есть отдельные ноды, которые занимаются упорядочением сообщений, а при этом сторч это такие отдельные узлы, и оно больше похоже на какую-нибудь кассандру в этом плане, в плане паттернозаписи. То есть у вас не прилетают байтики, то есть в кавке у вас байтики вылетают в какой-то из брокеров, он их все локально записывает, дальше реплицируются какие-то другие брокеры из него, просто уже, ну, как бы читая его жлоб репликации, и как только там, ну, или от них можно ждать подтверждение, можно не ждать, но в любом случае там всегда по цепочке, или там, ну, даже не по цепочке, там всегда мастер, читающий его логи, и всегда лентности будет зависеть в первую очередь от того, в кого первым мы пишем, ну и да, и всегда это один и тот же диск. Вот, в случае буккипера или лог-девайса вы контактируете со специальным узлом, который координирует упорядочивание записей, вот именно в какой-то конкретный топик и шарф этого топика, он присылает порядковые номера вашему сообщению и записывает их как бы сам к ворумам в нужный набор реплик, при том он, то есть у вас может быть, например, 10 машин обслуживать топик, и он будет выбирать из них там каждый раз рандомные 3, тем самым разбрасывая лентности, разбрасывая нагрузку, то есть с точки зрения вот именно таких вот совсем супер-пупер-мега нагрузок на запись и с точки зрения того, чтобы масштабировать storage, это гораздо более гибкий вариант. Ну и при чтении, соответственно, тоже нужно как бы разбрасывать чтение вообще на всех, читать со всех, там есть всякие разные хитрые оптимизации, как сделать так, чтобы данные, которые на всех узлах при чтении не все подряд посылали, а только, чтобы за каждый кусочек данных только тот один отвечал. Вот, то есть понятно, что data locality при этом несколько теряется, зато получаем гибкость больше. Честно говоря, мне трудно представить юзкейсы для большинства людей, когда вместо кавки вам стоит взять bookkeeper или look device, вот если у вас есть слушатели, придите, просветите меня, потому что мне серьезно тяжело представить, нафига так сложно что-то себе делать, если вы не в Facebook и не в Twitter, и у вас нет таких мегаобъемов, и вам не нужна какая-то мега-единая абстракция для логов. А про какие мегаобъемы мы говорим? Ну смотри, вот у нас кавка справляется спокойно, совершенно вообще не поперхиваясь, 200 гигабайт в час вообще без проблем, и есть система, где мне кажется, я не готов сказать с точностью, мне кажется там типа порядка терабайта в час, наверное, или больше потоки, но вот та система, где терабайт в час, ну опять же, я не уверен, что точное число, я только типа более-менее с потолка беру, там возможно уже был бы смысл в чем-то типа look device, потому что там начинаются как раз эти интересные эффекты, что если у вас кавка просто забита под завязку, то вот то, что у вас в кэше файловой системе, оно читается быстро, но если вам не дай бог нужно произвести историческое чтение, у вас эта фигня может просто встать вся в неприятное положение, и у вас просто вся кавка постоянно начнет тормозить, а вот как раз look device и bookkeeper от этого защищены, потому что у вас, поскольку нагрузка размазывается по многим дискам, вы можете исторические данные читать вообще с тех дисков, с тех машин, которые сейчас активно на них пишут, например, то есть в этом плане очень клево сделано, то есть в кавке в этом случае нужно типа видимо иметь какой-то такой мир, который отстанет, а потом догонится, если вам нужно иногда читать исторические данные, потому что типа основной класс для кавки просто не будет успевать. Вот как-то так, видимо. Но основной юзкейс здесь какой, то есть их не хранить ведь? То есть не сможешь бесконечно хранить такие объемы? По сути дела, что типа хранить больше объем, иметь возможность. Ну то есть как бы у нас сейчас вот тот топик, где 2 гигабайта в час, там наверное около месяца ретеншин, я не знаю, точно нужно об этом спрашивать, но там точно больше 20 дней ретеншин данных. Как бы какой ретеншин на другом топике, я не знаю. Соответственно, я могу себе прекрасно представить ретеншины типа в несколько месяцев, потому что если выкатываешь какую-то фичу, есть какой-то баг, ты его через месяц нашел, ты просто все это перепроцессил и ты молодец, и у тебя ничего не сломалось на самом деле. Вот. Конкретно ребята из Facebook, я хотел забыть сказать, что штука у Facebook конкретно работает на поверхность Db, он там специально затюнин, чтобы быть не совсем безумно неприятным для, ну в плане overhead, но в любом случае там гораздо более хитрая конструкция, чем у Kafka для работы с диском. Я подозреваю, им хватает. То есть я думаю, что те неоптимальности, которым приводят много слов обстракции, они решаются тем, что просто можно прям очень много этих дисковых узлов поставить и все прекрасно. Use cases, которые приводит Facebook, это собственно stream processing pipelines, то есть то же самое, что Kafka, distribution of index updates в больших базах данных, тоже то же самое, что в Kafka, нечто под названием machine learning pipelines, я без понятия, что они под этим имеют в виду, чем это отличается от stream processing. Дальше replication pipelines, то есть опять-таки то же самое, что Kafka, и durable, reliable task use, вот это как раз, мне кажется, кейс, где конкретно Kafka справляется хуже, потому что Kafka, она как бы скорее log file, чем очередь, потому что вот в ней, ну как бы на ней можно сделать очередь, но это не очень удобно, а вот на этом как-то кажется удобнее сделать. Вот. И еще в принципе, ну наверное, у них презентация была, что они складывают всякие, например, могут собственно логи складывать, прям логи-логи, приложений, они там батчатся, всякое такое, то есть я вот даже не очень помню, что у них есть какая-то такая система, то есть у них там типа, они просказали два примера, что типа их систему можно затюнить под очень маленькие latency, а можно затюнить под очень большой throughput и всякие другие конфигурации, то есть там типа, хочешь писать к ворумам, пиши к ворумам, хочешь писать, чтобы была цепочка и меньше нагрузка на сеть, пиши цепочкой, там типа очень много вариантов того, как можно это все затюнить, вот. Это как бы я думаю и минус системы, и плюс. Мне кажется, тема у тебя исчерпана. Ну а с вами мне очень интересно слушать или как бы узнать, зачем вообще используют такие замороченные конструкции, потому что ну мне серьезно, я в теории понимаю, в чем преимущество перед кавкой, но на практике вот мне интересно узнать, кому реально нужна прям вот такая штука вместо кавки и почему. А ты не думаешь, что это на практике был, не знаю, not invented here, мы не хотим заморачиваться со сложностями open source, мы просто сделаем как нам нужно и появлялся один кейс, второй кейс, третий кейс, потом первый кейс исчезал, вот, а как бы фичи оставались. Ну смотри, очень может быть, как минимум, в принципе, очень может быть, как минимум есть две системы, BlockDevice и BookKeeper, которые очень, у них есть различия, но они между собой гораздо больше похожи, чем они похожи на кавку, вот так скажем. И у обоих там high level, есть вот эта идея того, что мы дисковый storage отдираем от нот, которые координируют запись. И значит кому-то это нужно, именно вот в такой конструкции. И эти кто-то сидят в двух разных компаниях, у обоих очень большие объемы данных, больше, чем в среднем по палате, но тем не менее оно существует. И судя по тому, что BookKeeper был open-source, и есть пользователи этой системы, то я думаю, что наверно есть кто-то слушатель, и кто может сказать, что вот, а нафига, ну я закончил тему, если что. Скажи, где блогеры валят и зачем? Саша. Да, тема про то, что видеоблогеры и пользователи твиттера валят с поганых сазов, будучи ими ужасно недовольными. То есть, в частности, вот мой любимый ютуб канал ScanLime переехал, ну я так понимаю в процессе, пока что в бете. Автор подняла себе PeerTube и принесла на него все видосы, и там после двух-трех итераций оптимизации оно достаточно сносно работает, ну то есть прям реально, вот ты заходишь на self-hosted youtube и там все нормально.",
    "result": {
      "query": "LogDevice преимущества по сравнению Kafka"
    }
  }
]