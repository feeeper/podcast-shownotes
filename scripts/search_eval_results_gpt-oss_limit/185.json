[
  {
    "segment_id": "134f9abb-c404-4544-88a3-4bf7e6554a0b",
    "episode_id": "bc53af44-260d-45a7-9fdf-39226f7cf504",
    "episode_number": 185,
    "segment_number": 10,
    "text": "ещё удивляюсь, что-то новостей давно не было. Вот, канал у него совершенно топовый, потому что он очень умный чувак, и он является контрибьютором в программу, называется PulseView, это как раз то, что я описывал про логический анализатор, который вот внешне подключённый, программа тебе на экранчике чего-то рисует, то есть он вот контрибьютор в эту программу и в проект Sigrok, более общий, и у него на канале есть очень много очень крутых видео про Sigrok и про PulseView и про всякое такое, то есть, например, он декодирует USB-шину, то есть и у него есть видео про FPGA и про 3D печать и про что-то ещё, то есть я посмотрел все видео, прям на одном дыхании, я занёс ему на Patreon, потому что это совершенно топовый канал, я его феорически рекомендую. А конкретно вот это видео про HDMI он заснял, потому что на самом деле он хотел, у него есть другое видео, где он тестирует одноплатный компьютер, который ему подарили на Рождество или что-то в этом роде, и ему хотелось записать видео с самого начала, вот как бы с момента загрузки, ты, разумеется, никаким софтом это особо не сделаешь, а на камеру получается как-то не очень красиво, и он вот искал средства записи HDMI-видео, прям вот как бы на уровне железа. А ты, Вань, случайно других их видосов не заценил? Нет. А ты зацени, знаешь, как классно. Погляжу, ты хорошо рассказал. Вот, ну то есть, да, я повторюсь, канал у него совершенно топовый, прям 10 из 10. Что ещё 10 из 10? Это блог Netflix, я думаю, или Брэндона Грэга, впрочем, это примерно одно и то же. Нет, это Netflix, если я не ошибаюсь, вот если я понимаю, о чём речь, это всё-таки сами Netflix, это ребята, которые у них пишут... А, нет, это ещё Грэг, тогда извините, это Ваня, да, я попутал немножко. Ваня, выбери какую-нибудь тему. Да тут куча тем, ну давай начнём с спектра атаки, продолжение спектра атаки. А тогда припомним, что Netflix, это, наверное, ещё не Netflix. Ну я просто знаю, что Netflix и Брэндон Грэг, они топчики, а вот про... Про артстехника, артстехника тоже известные ребята. Продолжай. Извини, Ваня. Хорошо, я просто думал, может, вдруг ты захочешь рассказать. Вышла статья на артстехнике про продолжение всем известной, нашумевшей, молдауной спектр, о том, что вышел новый вид атаки, ну точнее, вышел пейпер, в котором рассказывается про новый вид атаки, который назвали... сейчас я скажу, как его назвали... да как же его назвали... Да что ж такое-то... Бранч Скоп? Да, Бранч Скоп, спасибо большое. Вот, Бранч Скоп, здесь он немножко отличается от Спектр 2, хотя очень на него похож. То есть Спектр 2 и Бранч Скоп оба используют спекулятивное вычисление, но немножко по-разному вытаскивают оттуда информацию. Спектр 2 вытаскивает информацию из... Как там называется... Сейчас, подождите, я сейчас все эти расшифровки найду. А, вот, Бранч Таргет Баффер. Это Спектр 2, а Бранч Скоп вытаскивает информацию из Паттерн Хистори Тейбл. То есть Паттерн Хистори Тейбл это такая вещь, когда у тебя есть предсказание переходов, а его реализуют не в виде, изначально его реализовывали в виде однобитной конструкции, вида вот в предыдущий раз мы скакнули здесь на да или нет. В дальнейшем поняли, что эта вещь, вот подобный однобитовый переход, он не очень хорошо работает, они его сделали двухбитовой. То есть он может иметь четыре значения. Да, однозначно, или да, но менее вероятно. Потом нет, менее вероятно, и нет, однозначно. Когда ты угадываешь правильность перехода, ты двигаешься там соответственно либо вверх все время, и ты можешь пройти по горизонту. Получается, что предсказатель перехода с 4 значениями неплохо прогнозирует, сохраняет предыдущее состояние, неплохо показывает следующие переходы, и при этом фактически вам нужно всего 4 бита, очень дешево. Практически получается так, что он тоже не обнуляется во время спекулятивного вычисления. То есть вы вычислили какую-то ветку вперед, вы знаете, какое было значение, и вы этот переключатель двухбитовый переключили в какое-то новое значение. Соответственно, если вы заранее подготовили этот переключатель, поставили в какое-то одно значение, и в этот спекулятивный режим как-то его переключил, а потом в дальнейшем вы проверили, в каком он состоянии оказался, вы можете понять, по какой ветке пошел спекулятивный режим. То есть объяснение чрезвычайно простое, сделать это довольно сложно, но сами представляете, вот эти два бита, каким образом нужно спекулятивно скакать по этим веткам для того, чтобы определить, а потом считать значение. Но практически получается, что информацию вытягивать можно, а защищаться от этого сложно. То есть пока, ну по крайней мере мне не очень понятно, каким образом будет эта защита сделана, хотя Intel говорит, что мы уже разрабатываем варианты защиты. То есть, иными словами, ждем новых версий процессоров? Только так? Я вот думаю, когда Intel даст ответ, тогда и посмотрим. Мне сложно придумать вариант, когда мы однозначно на уровне только софта сможем это решить. Возможно, они с помощью какого-то микрокода смогут каким-то образом обнулять его значение, но тогда мы получим падение производительности, чего, конечно же, никто не хочет. Может быть, есть какие-то другие варианты, но я про них не знаю. Но в целом очень интересная тема со всеми этими атаками разбираться, потому что это находится на таком интересном стыке производительности, исполнения понимания архитектуры процессоров и одновременно попыткой все это взломать. То есть это... Это надо немножко по-другому повернуть мозг относительно того, когда вы просто пытаетесь понять архитектуру, или относительно того, как вообще работают программы. И здесь это... Ну, мне, по крайней мере, кажется, что это очень любопытно. Ну, примерно как вот на тайне FPGA программировать. Тогда давайте перейдем к Netflix уже. Да, вот. Я, по крайней мере, уверен, что это действительно топ. А вот про артехнику я, честно говоря, не то чтобы много слышал. Реально известный сайт? Это крайне известный ресурс про технологии. У них обычно очень хорошие статьи. Знаешь, там даже они... Что меня в них удивляет, там, не знаю, хоть как макось, казалось бы, новость, боже мой, они про неё могут написать целый редакторский материал с интересными деталями, того, что там слышали о каких-то деталях из-под капота, скажем так. Вот такого рода вещи. И это просто интересный читатель. Это хороший, профессионально сделанный ресурс о технологиях. Но не такой профессиональный, как у Netflix. Ну, у Skyen такой неопрограммистский, он такой, в общем, для технологий. Но Netflix, да, Netflix, он интересен именно нам, для подкаста. Расскажи же. Я расскажу? А почему я расскажу? Это тот, который из новостей. Это как раз там, где я тоже вписался. Собственно, что произошло в тех блоге Netflix, которые внезапно переместился на Medium, мне казалось, раньше у них был в другом месте, хотя я могу ошибаться. Да-да-да, я тоже удивился. Вот, они выпустили библиотеку, которая позволяет, не знаю, как это правильно по-русски сказать, поэтому скажу по-английски, адаптивный лоадшеддинг. Вот, как-то так. Оно же rate limiting. Что, собственно, о чем речь? Представьте, что у вас есть система какого-то массового обслуживания, например, веб-сервер. У всех сейчас есть свой веб-сервер. В нее летят какие-то запросы откуда-то. И если у нее capacity, то есть там многоядерный сервер, на котором запущены или просто много инстансов, или у вас инстанс многотрейдовый, и вот он эти запросы разгребает. Пока он их разгребает быстрее, чем они приходят, все прекрасно. Когда они начинают приходить быстрее, чем он успевает их разгребать, начинает образовываться очередь. Когда образовывается очередь, образовывается latency. Ну, увеличивается latency. Совсем, ну, очередь, она, в общем-то, полезная штука, потому что если у нас, не знаю, какой-то отдельный запрос затупил, то все остальное мы продолжим быстро раскидывать. Это, в общем-то, нормально. Но вот когда нагрузка повышается до такой степени, что мы разбрасывать эти запросы уже не можем, то latency просто растет, растет, растет, растет, очередь копится, копится, пока у нас не взрываются таймауты. И, в общем-то, с этим как борются, ну, просто не дают больше нагрузки на сервер, чем нужно обычно. Ну, и, соответственно, когда нагрузки на система больше не держат, добивают серверами. У этого есть как бы, ну, и если мы работаем в облаке, то серверами добивать можно, ну, просто кнопочку автоскейл покрутить у Амазона там, или на каком облаке вы крутитесь, и вам будет больше инстансов. О-хо-хо-хо, так все прекрасно. Но есть проблема. Пороги нужно сдавать руками. И если у вас там небольшая продакшн, наверное, это не очень большая проблема. Но у Нетфликса большой продакшн. И они задали задачей не искать пороги руками, потому что это трудзатратно, сделали один раз, потом не меняется. А в то время как производительные характеристики производительности системы меняются и с изменением железа, на котором она крутится, и с просто новыми коммитами. И, в общем-то, поставили себе такую задачу, они сделали такой адаптивный редлиметр, который, по большому счету, делает нечто похожее на то, что делает TCP, congestion control. Они прямо так говорят, что, в общем-то, они во многом вдохновившись этим алгоритмом, сделали такую подбиралку лимита, которая, глядя на скопившуюся очередь, этот лимит то увеличивает, то уменьшает, и он в итоге ассоциирует вокруг некого такого фиксированного значения, которое является возможной кпст-системой. Я сейчас не буду вдаваться в формулы, там довольно простая доступная статья, рекомендую пройти по ссылке. И что самое клевое, они библиотеку для Java, которая это делает, и в частности умеет замедлить GFPC, они это заопенсорсили. И, в принципе, если у вас большой продаж, и он у вас с автоскейлингом и прочей такой штукой, и вы пишете на Java, вот для вас Netflix сделал большое добро. Для всех остальных вы можете просто прочитать статью и эту идею поплатить для себя, потому что, в общем-то, это не выглядит как что-то суперсложное. А можно вопрос? Да, конечно. Во-первых, я хотел бы немножечко погрузиться в технические детали, потому что это, на мой взгляд, довольно важно. Ну, и, по крайней мере, я вот до конца не очень понимаю, как оно работает по сравнению с TCP. В смысле, по сравнению? TCP работает на уровне протокола... Прости, на уровне коннекшена отдельного, а этим ребятам нужно не давать больше коннекшенов, если у нас уже случилось насыщение... То есть, вот так как TCP тебе не даёт передать больше пакетов в существующем коннекшене, если у тебя насыщение внутри коннекта, он ничего не знает про другие коннекты. Да. А здесь задача не дать больше коннектов, если у тебя случилось насыщение сервера обслуживая, ну, там даже не коннектор, запросом. То есть, это более высокоуровневая штука, но использующая ту же самую идею. Да. То есть, идея в чём? Они смотрят время, то есть, лейтенансы ответа на гружную систему и ненагруженную систему, и смотрят, что если время начинает возрастать, они уменьшают количество запросов к системе. Простейшая обратная связь. То есть, пока всё нормально, увеличиваем количество, ну, либо разрешаем все, либо увеличиваем количество тех запросов, которые к системе приходят. Как только начинает затыкаться, уменьшаем. И так делаем всё время, в итоге получается, что даже если система постоянно меняется, у нас система будет всегда находиться на грани насыщения, но не переходить, ну, или не сильно переходить эту грань. Я пока всё правильно объясняю? Да. Дальше мы приходим к вопросу, что с каким значением, ну, как бы, минимального лейтенса они сравнивают. И вот здесь у меня возникает вообще непонятка. То есть, они предполагают, что у нас лейтенсы всегда минимальные, пока мы не зайдём на грань, что ли? Какое-то очень сильное упрощение всех систем, существующих в интернете. А смотри, на самом деле им нужно посчитать градиент. Им нужно посчитать направление. Всё так. Им для этого не нужны реальные минимальные лейтенсы. Им достаточно минимального в окне, чтобы понять, куда оно идёт. Ты имеешь в виду, что если лейтенсы начнут увеличиваться, они уменьшают тайм? Они производную считают. Но это тоже глупо. В смысле, почему глупо? Им нужно посчитать производную. Ну, смотри, допустим, у меня... Они считают производную, направление роста функций. Дальше у тебя есть довольно понятная формула, по которой они обновляют лимит. Ну, то есть, я согласен с тобой, что это, как и любая система с фидбэком, это вызывает подозрения. Но они утверждают, что у них зашибись работает на большой нагрузке. Значит, этим они и затюнили. Ну, давай, у меня вот несколько возражений сразу. Во-первых, возражение номер раз. Они говорят, что в целом запросы бывают разные, но если посчитать посреднему, всё ок. У меня вот сразу вопрос такой, вап? Честно говоря, потому что, смотри, как библиотека написана. Вот я понимаю, о чём ты говоришь. Я бы сказал так, что если у тебя есть разные endpoints, прям с разным смыслом, там для них эту метрику нужно считать, конечно же, по отдельности. Дальше. Ну, допустим, ты, ну, например, делаешь... Ну, простейший случай, чтобы и Саша мог поучаствовать, ты делаешь веб-сервис, который отвечает на SQL-запросы. И вот ты говоришь, давайте select будем по одному способу вытаскивать, да, а insert будем, ну, в смысле, как будто бы разные endpoints, да? Смотри, смотри, я сейчас с тобой разговариваю. Я понимаю, куда ты клонишь, я с тобой радикально не согласен, потому что это не то, как работает логика head-to-pashных концов. Обычный head-to-pashный конец выполняет вполне конкретную задачу. Да, какой-то отдельный конец может взять сильным, там, сделать сильно более длинный запрос в базу данных, но это будет outlier. Но вообще у тебя обычно... То есть select, он даёт тебе гораздо большую свободу того, что ты можешь наворачивать, чем отдельный head-to-pashный конец. Ну, то есть ты говоришь о том, что система типа CRUD будет работать хорошо, а всё, что чуть более сложное, и что, например, использует DSL во внутренних запросах, уже на этой штуке работать не будет. Очень может быть, но на самом деле, опять же... Значит ли это, что придётся просто разнести разные запросы по разным идеям? А ты иногда не сможешь это сделать. Мне кажется, всё ещё можно извернуться. Ну, как бы, у нас был DSL внутренний. У тебя внутри DSL, опять же... Вот я тебе объясняю, один и тот же запрос просто на разных кастомерах будет совершенно по-разному себя вести, потому что у кастомеров разные данные. Ну, ты сам сейчас ответил на наш вопрос. Опять же, я не знаю, какой неприблитайка написана, но ты эту метрику можешь считать для разных топоконцов, разных кастомеров, разных ещё чего-то, разных регионов. Как бы мы приходим к тому, что... Во-вторых, это же локальная метрика. Это локальная метрика. Она считается на сервере. То есть, если у тебя на какой-то сервер пришло что-то очень плохое, собственно, даже абстрагируясь с того, что ты можешь зайти по отдельности, или на всякое разное, если у тебя на сервере пришла какая-то неприятная нагрузка, ты немножко там упустил лимиты, это даже, наверное, разумно, что на него будет тратиться меньше новых запросов, которые могут быть нормальными, просто потому что, возможно, ему сейчас тяжелее, чем остальным. Извини. Загрузка пойдёт на другие сервера. Эта штука, в принципе, работает... Предположение о том, что... Сори. Так, давай, ты пока откашляйся, я пока вторую свою заметку скажу. Подобная система, которая даёт отлуп в сервере раньше, чем она даже начнёт обработку этого запроса, предполагает, что у тебя клиент готов к этому. И они сами утверждают... Я в гитхаб у них заходил, там у них в гитхабе немножко по-другому заметка написана, чуть больше деталей. Они говорят, что на уровне сервера мы даём ответ раньше, чем мы начали обработку, соответственно, мы сразу на клиенте видим, что мы обработку не проходили, и, соответственно, клиент должен обеспечивать переподключение к другому серверу. Это даёт вам гарантию того, что, скажем, у вас один сервер из десяти заткнулся каким-то своим причинам, там, жёсткий диск отказал, у него там шнур нагрелся, или ещё какая-нибудь ерунда. Клиент сам переподключится к другому серверу, и это обеспечивает вам жизнеспособность вашего сервера. Ну, тоже такое спорное решение. Ну, здравствуйте. У нас, в принципе, сеть. Скажи мне, ты в последний раз видел нормальную сетевую систему, которая нормально работает, и там нет переподключений. Ну, то есть у тебя разрывы сети — это такая вещь, которая в сети ожидаемая. Нет, мы сейчас говорим с тобой не про разрывы сети, а мы сейчас говорим с тобой про то, что сеть ответила нормально. Но ты с уровня сервера отдаёшь специальный ответ в виду «я сейчас перегружен, ко мне больше не ходи». 502. У тебя в Ход ТП для этого есть специальный код. Вот, то есть как бы ты… Я не уверен, что 502, но, короче, не важно. Просто они при этом утверждают, что это даёт почти стопроцентную гарантию того, что всё будет у вас в таретке. Или 4.2.9, кстати. Ну, смотри, если у тебя Ход ТП, а тут речь про Ход ТП, у тебя прямо в протоколе Ход ТП, прямо вот нижележащий, вне зависимости от приложения, тебе протокол говорит, что у тебя есть такой случай, ты можешь хотеть его обработать. Все абсолютно опишки больших компаний, Twitter, Facebook, ещё, я не знаю, я не совсем на свете работал, но у каждой большой компании, у них есть специальная такая штука, которая скажет, успокойся, остановись, прекрати мне сыпать запросы. И ещё у них есть такая вещь, как экспоненциальный тайм-аут. И больше, то есть ты откроешь спеку на Twitter API, у них прямо сказано, что вот когда мы вам эти коды отдаём, будьте добры, делайте back-off вот по такой-то там, ну там, по-моему, экспоненциально даже написано. Да-да-да. В этой формуле. Я к тому, что любая публичная пипка, на которую предполагается нагрузка, тебе просто в документации будет просить тебя так делать. Так я тебе как раз и говорю, что экспоненциальный back-off и попробуйте переподключиться тут же к другому серверу, они несовместимы между собой. Ну смотри, я сейчас тебе так отвечу, что у тебя, когда ты такое делаешь, ты, наверное, как-то всё настраиваешь, и если у тебя это совсем наружу торчит, ну ты пишешь правильную документацию, если у тебя это просто где-то локально написано, то... Нет, это, конечно, про локальные вещи больше. То у тебя, ну ты делаешь так, как ты считаешь нужным, тем более локальные вещи. Ну не хочешь экспоненциальный back-off, не делаешь экспоненциальный. Хорошо, как ты сделаешь? Как я бы лично сделал? Я бы сделал 2-3 раза обычное подключение, а потом начинай экспоненциальный back-off. Ну то есть там типа... Ну то есть умный клиент. Да, ну в смысле, он... насколько... хттп умный. Уровень умности хттп. Да какой же уровень? Экспоненциальный back-off это не уровень хттп. Скажем так, это хорошо делать в любом клиенте, который уходит в публичный API по хттп. Я все понял. Ну тут я понимаю, почему они сюда идут, просто их желание, а в свои желания они вписывают, во-первых, полную автоматизацию, отсутствие дополнительных действий ручками, защита от реплай-штормов. Ой, не реплай, а как это... повторных запросов реквейр-штормов. Когда у тебя падает один из серверов по каким-то своим причинам, остальные сервера начинают получать очень большой наплыв дополнительных запросов. Они фактически от этого не защищаются. Я с тобой не согласен, мне кажется это довольно прикольная идея. Единственное, что договор раздевался в начале, хочу повторить, что если у вас нету прям там сотни сервисов, которые друг с другом сложно взаимодействуют, и автоскеллинга, вам скорее всего не стоит этим заморачиваться. Но вот я все еще не понимаю твоей претензии, почему это не будет работать. Мне кажется... Это будет работать, просто это не... я возражаю против их мнения, что это однозначно решает любую проблему, которая у них там существует. Они говорят, мы все это поставили и теперь у нас нет проблем совсем. Ну как-то сложно это все. Ну смотри, во-первых, я считаю, что у них достаточно большой продакшн, поэтому скорее всего они не будут крови набрать. То есть они скорее всего наблюдали за системой, прежде чем вообще это писать в open-source. Во-вторых, вот ты говоришь, что им это не поможет, например, с штормом переключения как раз поможет. У тебя в том смысле, что это не будет укладывать остальные сервера, потому что они тоже будут просто лимиты ставить. Соответственно, вместо того, чтобы все клиенты страдали, они просто перестанут принимать трафик, соответственно, включится амазоновский автоскейлинг, поднимет еще серверов, они обслужат тех клиентов, которым дали отпор, и система так или иначе будет как-то продолжать существовать. Возможно, будет гонять много денег на Амазоне. Автоскейлер поможет в данном случае, согласен. Ну то есть у них в принципе проблемы начались с того, что у них есть автоскейлинг, поэтому им нужно вовремя давать отлуп. Ну может быть, согласен. Но самое интересное началось в комментариях. Ты читал комментарии? Если честно, нет. Прибежал возмущенный читатель их блога и написал, что да как вы смеете печатать такое, а именно вставляя цитату «инновации и совершенно новые изобретения», когда вы фактически полностью повторили мой ресерч 2013 года, и более того, вот вам цитаты, как мой ресерч комментировали Netflix инженеры. И вы его фактически полностью перепечатали один к одному. Вот, пожалуйста, вот одно, вот второе, вот третье, то, что у меня использовалось, и то, что используется у вас. И в общем, вы нехорошие бяки-буки. И кстати, а почему-то мою статью никто не лайкает, а вашу статью уже столько-столько залайкали. Вот, расследование. Это звучит как интересная драма, я уверен, ты за ней следил. Ну, дальше я не посмотрел, потому что это довольно свежий комментарий, на него пока никто не ответил.",
    "result": {
      "query": "PulseView логический анализатор Sigrok канал"
    }
  }
]