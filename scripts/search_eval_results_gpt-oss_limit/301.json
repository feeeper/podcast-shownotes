[
  {
    "segment_id": "716f7014-a8a6-48cd-8ae0-5e82254ea95f",
    "episode_id": "16d63b28-de8a-4d5a-9957-d8068a696314",
    "episode_number": 301,
    "segment_number": 2,
    "text": "Там была прекрасная шутка от Бобука о том, что в Яндексе же найдется все. И видимо, что-то не нашлось в Яндексе. И дальше, ну понятно, что будут огромные потери имиджевые для страны, для айти-отрасли. И я думаю, после этих событий просто все так откидывается на несколько, даже десятков лет назад. Никакой речи не может идти про большие компании, которые могут сделать офисы в стране. То есть там были разговоры, что давайте Google сделает офис в Минске с только же классных разработчиков. Ну вот такая политическая ситуация. Вот. Короче, очень надеюсь на изменения. Очень надеюсь на изменения. И я бы хотела... Я давно не была в Белоруссии, около полутора лет. Хотелось бы приехать в страну и почувствовать свободу. Вот. Наверное, менять тему на несколько более, как это, жизнеутверждающую. Есть такой деятель в рамках Ирланды, который, я так понимаю, не является непосредственно участником кор-команды, хотя, может быть, уже и является. Трудно сказать. Костя Сагонас. Он доктор наук. И у него, его студенты периодически что-нибудь занятное... Ой, я неправильно произнес. Константин Сагонас. И его студенты периодически делают что-нибудь интересное. Вот. Не так давно я, кажется, говорил о том, что выходил в Ирланды-23, создали Ирландку-ТП-23. Еще до этого мы обсуждали Ирландку-ТП-22. Но до 22-23 были вынесены некоторые интересные изменения, про которые сейчас вышел припринт-пейпер. И еще несколько пейперов в работе. И вышла статья, как бы такая обзорная в блоге ИрландГорг про то, что они там сделали с... В общем, они сделали новую структуру данных для многопоточной работы. В Ирландии есть такая штука ETS. Если вы никогда с Ирландом не работали, можете думать об этом так, что в Ирландии есть такой встроенный регис. Если, скажем, для большинства типов... То есть у ETS несколько разных типов структур бывает в унтерних. И вот для всяких неупорядочных структур оно уже и так довольно хорошо работало. Там есть только флажок write-concurrency, который, ну, включает-выключает оптимизации для многопоточной работы со структурой данных. И оно уже для многих других типов таблиц давно хорошо работало. Его там докручивали, дооптимизировали, но в целом уже и так неплохо работало. А вот упорядоченные множества, они работали, ну, так, себе. На них всегда, несмотря на любые опции в write-concurrency, true или нетrue, хоть ставь, хоть не ставь, они всегда работали довольно... То есть как бы они всегда были обвешаны одним большим блоком на запись. То есть читать конкуренты там было можно, а вот запись была, ну, так. Тут они придумали очень... И оно внутри было реализовано, как AVL-дерево. Тут они предлагают новую интересную структуру данных, которая, в принципе, ну, судя по их бенчмаркам, во всяком случае, для их использования работает. И она интересна тем, что она адаптивная. То есть вместо того, чтобы как-то там хитро обвешивать локами или делать не блокирующую структуру данных, хотя, судя по паперам, у них там есть не блокирующий вариант, происходит следующее. Берем обычное AVL-дерево. Сверху ставим почти обычный лок. Но к этому локу приделываем специальный каунтер, который мы увеличиваем, когда у нас случается контеншн, и уменьшаем, когда у нас получается его захватить без контеншна. И каждый раз, когда он перешагивает через какое-то значение, каунтер, мы разделяем AVL-дерево на два, на каждое дерево прикрепляем по своему локу с таким же каунтером, а сверху над ними ставим еще такой кусочек узла бинарного дерева, который отвечает за то, как бы нам налево или направо пойти. И так мы можем его наращивать и разделять по мере того, где случается доступ и контеншн. А также, если контеншн долго не случается, счетчик падает ниже какого-то значения, то наоборот мы можем схлопнуть получается AVL-дерево в одно и, соответственно, схлопнуть потом поддерживающую структуру сверху. Это звучит как бы на самом деле проще, гораздо чем это реализуется на C, я уверен, потому что им все это пришлось на C писать в Relang виртуальной машине. В частности, они упоминают, что для того, чтобы не избавляться слишком рано от схлопнутых кусочков, они используют какие-то внутренние механизмы Relang машины, которая трекает, когда какой из потоков реально к чему обращается, и что типа они там полностью провернули плащ, можно выносить мусор. То есть они немножко паразитируют на том, как Relang сделан. То есть, видимо, если вы захотите у себя такое сделать, вам будет сложнее. Но в целом, если вы интересуетесь конкурентными структурами данных или вам интересно, либо иеролонгистам вам интересно, как это работает, можете посмотреть. Эта новость буквально, что была опубликована на этой неделе, поэтому я еще глубоко физически не успел слазить, выглядит интересно. Вот, такая вот новость. Слушай, звучит интересно. Не пишут, они собираются сделать ее частью ОТП? Она уже частью ОТП. А, уже частью ОТП, я пробую сделать. Да, то есть с 22-го релиза уже на ним начали это впиливать. То есть 22-23 они это впилили, только они сейчас описали, плюс там у них в пейперах есть еще future work и всякие другие штуки, которые описаны, но еще не в ОТП, но вроде как думают, что скоро смогут завести. То есть они так постепенно вывозят. Интересно. Звучит как здорово. По крайней мере, вот я смотрю на графики, и графики выглядят офигенно. Ну, то есть тут с точки зрения просто пользователя иеролонговой виртуальной машины, не с точки зрения структуры данных, это интересно, потому что есть довольно важные в экосистеме иеролонго проекты, которые используют именно ordered set и теску для регистрации, например, процессов, и то, что оно теперь быстрее работает, да, да, да. То есть там, конечно, были варианты под чей-то проект, или были альтеративные варианты, которые не использовали упорядоченную структуру, но теперь даже упорядоченную структуру можно так использовать, и это будет хорошо работать, и это здорово. Есть еще парочка ссылок на пейперы разные, на которые они основывались, я смотрю три разных пейпера, надо бы тоже их посмотреть. Я так понимаю, это, собственно, все, в большинстве слов их пейперы, то есть не то, чтобы их все успел смотреть, но очень похоже на то, что это все... Да, да, да, это их. Константинус Сагунос, он во всех автором является. И вот Кжел Винблад, я надеюсь, я правильно произношу, то есть явно Кжел, это его студент, который, собственно, PhD делал, а Константинус, он, видимо, руководитель. То есть еще из того, что студенты Константинуса делали, например, меня могли очень много слышать про как-то про протибои с тестированием, и, собственно, есть две реализации, QuickCheck и Proper. Вот Proper тоже делали студенты Константинуса. Еще там какие-то у него... Я вот сейчас так вот сходу не вспомню, но у него очень-очень всегда раньше, как бы, не знаю, несколько лет назад у него было очень много студентов, которые очень много чего в Erlang проделывали. И это было очень круто. Потом как-то подспало количество вещей, которые публиковались с его участием, и очень приятно также видеть, что они как бы вернулись к тому, чтобы это делать. Для понимания, если у вас есть 32 процесса, которые работают в параллель на одном ETS-хранилище, и они записывают 50% инсертов и 50% делитов, то производительность в операциях в секунду на 32 процессах около нуля для старого OTP-21 и около 15 миллионов для нового. То есть, ну, это прям... Я вообще смотрю на Google Scholar, на самом деле я не правду говорю. У него студенты просто каждый год по нескольким пейперам выпускают. Просто, видимо, далеко не все из этого впиливается в Erlang. Из того, что вы еще могли слышать из того, что есть в Erlang, это, собственно, проект Hype, который... High Performance Erlang. Но его же так и не впилили. Нет, в смысле, он впилен, просто он, типа... Не GIT, а именно просто сам Hype, он там впилен со времен Сарагороха, он просто, типа, до сих пор экспериментальный. Да-да-да, я про то же хотел сказать. Ну, в смысле, как бы ты всегда можешь какой-то отдельный модулем скомпилировать и, типа, в нем вынести логику, которая должна быстро работать, а все остальное без него будет работать. То есть я его так, по-моему, даже когда-то использовал. А также они... Эта команда пыталась же делать, после его же студента пытались GIT делать, но сейчас же есть еще GIT от самих, как бы, от основной команды Erlang, поэтому непонятно, сколько он там пересекается. Там довольно много работ, в принципе, про конкурентность и, там, не знаю, про модуль чекинг, но часть из этого, кстати, не имеет никакого отношения к Erlang просто. То есть там, например, встретили с модулом чекинга Linux kernel read-copy-update. То есть, ну, круто, достаточно свежий папер, ну, 18-го года студенту делали. Просто это не имеет никакого отношения к Erlang. Вот. И, поэтому я давно ничего не слышал. Вот. Я думаю, про Erlang все. Давайте теперь, короче, покекаем с докером. Также на него поплюемся. Как мы очень любим SASS в этом подкасте, так вот, как бы, докер не дает нам изменить свое мнение. Если вы привыкли, что вот вы можете, как бы, на докер какую-то свою имидж запушить, на докерхаб, и, типа, он там будет валяться, ну, как бы, пусть будет. Что мешает, что ли, кому-то. Вот теперь мешает. Докер на бесплатных аккаунтах имиджи, которые давно не пушили и давно не скачивали, будет просто удалять. Поздравствует гниение байтиков. Это, как знаешь, с облаком Mail.ru будет заморожено и очищено. Так то будет заморожено и очищено. Помнишь историю с облаком Mail.ru? Заморожено и очищено. А, да-да-да-да-да-да-да. Все, я просто раз слышал. Слушай, я с самого начала знал, что это будет, и для меня это, поэтому, даже не новость. Ну, как бы, очевидно, что рано или поздно они придут к этой идее. Я никогда не складывал на докерхаб ничего, и... Ну, в смысле, всегда клал докерфайл в репозиторий в нужном, если мне нужно, и собирал на месте, работал как-то локально. В общем, короче, меня в этом плане помешает теперь только гитхаб, потому что, ну, явно, если компания дает вам бесконечное хранилище чего-то бесплатно, значит, вас где-то обманывают. То есть, как бы, гитхаб это настолько сейчас столб всего, и он дает много кому много хранилища бесплатно, где-то замануха, да. Я согласен. Ждем откровения от гитхаба. Скоро мы будем удалять ваши репозитории, если у них не было долго коммитов. Такой драйвер для open-source комьюнити. Коммитить чаще. Морковка сзади, да. Морковка сзади, боже мой. Нет, нет, есть же два способа мотивации. Морковка спереди, морковка сзади. Не слышал раз это? Нет. Понял идею, да, да, да, просто нет. Такой формулировки не слышал. Интересно, вы Светлана Строгников мотивируете. Без комментариев. Даже морковка спереди уже звучит не очень после этого. Правильно, ковид, все клубы закрылись, в том числе BDSM клубы закрылись. Только так остается. Это все еще в IT подкасте, да? Мы ничего не имеем против, но BDSM клубов, они прекрасные. Мы не осуждаем. Ни в коем случае. А если говорить по теме, то где должен был появиться дух Саши и сказать, что все САСы плохи, поэтому от САСов нужно уходить. Я же с этого начал. Я как бы, Саша вещает через меня, я как бы медитирую. Ты, видимо, очень как-то недостаточно его как бы... Сепресс, то есть слово сепресс на русском, это подавляешь его, вот, ты его слишком подавляешь. Измени внутренний Саша, Валера, я перестану тебя подавлять. Боже, можно записывать, прям распаскивать на эти. Так мы уже записываемся. На звонки для телефонов. Я сегодня очень много интересного о вас, ребятах, знаю. Зачем тебе я на звонки для телефонов, Вань? Без комментариев. Как это правильно ответить? Без комментариев. Мы не будем тебя осуждать, Ваня, не волнуйся. Мы очень толерантный подкаст. Мы же теперь все только европейцы здесь остались. Точно, точно. А то Саша не доехал, да? Пошли дальше. Следующая тема моя. Это мы столкнулись на работе и я начал, ну, я заинтересовался этим вопросом, я и до этого этим интересовался. Это довольно большая тема, и она не связана с Кубернетесом, но чаще всего вы ее услышите именно в разрезе Кубернетеса. Когда у вас есть большой кластер с большим количеством машин, и вы на него запускаете кучу маленьких своих программ, окажется, что на самом деле очень сложно разделять ресурсы для ваших маленьких сервисов или приложений, которые работают в Кубернетесе. Скажем, стандартное явление, когда вы запускаете кластер Кубернетеса, я не знаю, на N нодах и на каждом, на N инстанциях, и на каждом инстанции у вас там есть, не знаю, 64 CPU. То есть вы не запускаете на маленьких, смысла нет. Дешевле гораздо с точки зрения оптимизации цены качества, цены производительности, дешевле и правильнее запускать на больших инстанциях. Когда у вас есть 64 CPU в одной машине, а вы хотите запускать на них нормальное количество разумно приложений ваших, каждый из которых, например, потребляет по 2 CPU, по одному CPU, окажется, что на каждой машине у вас запускается дофига всего. И как правильно разграничивать доступ в этом смысле становится таким довольно серьезным вопросом. Потому что с одной стороны вы хотите, чтобы каждый из них работал, с другой стороны, если каждый из них будет слишком сильно использовать ресурсы, то кому-то они явно не достанутся. Это делается с помощью контроля ресурсов в Кубернетесе. Начнем с верхнего уровня. То есть, когда вы определяете работу, у вас в вашем описании вашего сервиса написано, сколько ресурсов он должен использовать. То есть там есть CPU и память. То есть вы можете сказать, распределяя меня на машину, ну, чтобы там было 20 CPU условно. Я очень сильно утрирую. Здесь много деталей, здесь есть квоты, лимиты и так далее. Я ничего не буду говорить. Кто захочет, посмотрит. Я концепцию рассказываю. И получается, что скажем, вот вы запустили там 30, 32 или сколько там должно быть, по 2 CPU. 32 сервиса по 2 ядра каждый использует. И все теоретически должно быть хорошо. Стало лишь 64 в сумме, все работает. Пока они используют меньше, чем 2 CPU, вроде как все нормально. Когда они используют чуть больше, чем 2 CPU, тут начинаются проблемы. С одной точки зрения это можно решить тем, что вы правильно выставляете ресурсы и лимиты. С другой точки зрения, эта проблема проявляется не только тогда, но и тогда, скажем, когда у вас меньше... Это может сильно даже проявляться. Очень сильно я видел это на своих случаях, когда у вас меньше одного CPU надо установить для сервиса. То есть вы говорите ему, я ему дам пол CPU, то есть 500 CPU. Прямо так в кубернете пишите. Я даю ему 500 мили CPU. Очень смешно на самом деле. В чем проявляется проблема? Проблема проявляется в том, что когда вы используете эти лимиты для кубернетеса, это все транслируется в cgroups в ядре. В cgroups в ядре ресурсы отделяются с помощью CFS. CFS называется Completely Fail... В общем, совершенно честный шедулер. Совершенно честный шедулер делает это следующим образом. Он делит все время на чанки по 100 миллисекунд. И потом смотрит в этих чанках. 100 миллисекунд по дефолту. Вы можете изменить его настройками ядра, но по дефолту 100 миллисекунд. Внутри чанка на 100 миллисекунд он смотрит, сколько вы использовали. Если вы использовали чуть больше, он вас зажимает на следующем чанке. Или вы это увидите... Если вы говорите, мне нужен один CPU, я использую один CPU, и вам дается один CPU, вы эту проблему, скорее всего, не увидите. А проблема начинается, когда вы сказали, что, например, мне нужен один CPU, но в моменте, когда пришел большой набор трафика, вы запустили его на двух CPU. И в этом случае в первые 100 миллисекунд, вот этот первый чанк, в первые 100 миллисекунд вы использовали CPU на 200 миллисекунд, потому что вы на двух ядрах работаете одновременно. И ядро в следующие 100 миллисекунд может не дать вам вообще времени. Но я сейчас утрирую, там на самом деле чуть более сложный случай, но вот грубо рассчитывать надо примерно так. То есть 100 миллисекунд вы проработали, следующие 100 миллисекунд вам не дали, потому что вы в прошлый раз перевысили свою квоту. И потом снова на 100 миллисекунд вам снова дают запускать. Если вы все еще продолжаете использовать два CPU, то у вас могут быть просадки. На самом деле там все чуть сложнее, потому что только внутри чанка могут быть ограничения. Но по факту получается, что можно увидеть увеличение лейтенсии любых запросов до 100 миллисекунд. Это дефолтный чанк для шедулера. И получается такая хитрая ситуация, что пришел вам запрос. Вроде простенький маленький запрос, ну не знаю, сложить 2 плюс 2. Вы его выполняете прямо в памяти, ничего никуда не пытаетесь залезть, вы ни в базу данных, ни в какие-то другие сервисы. Вы запишите и отсылаете обратно запрос, но оказывается, что 99-ый персентиль ваших запросов внезапно не пол миллисекунды, как должно быть по-нормальному, а 101 миллисекунды. И вы ничего с этим не сможете сделать, потому что это происходит на уровне ядра, на уровне вашего приложения. Вы изменить это поведение не можете вообще никак. Я прикладываю ссылку на кубернетес.ish, который на самом деле кубернетес-репозиторий, но он не относится к кубернетесу, там просто куча ссылок на разные ресурсы, как это правильно работать, как это правильно попытаться пофиксить. На уровне кубернетеса это все видно в метриках и называется это как троттлинг. То есть CPU вроде как должен был использоваться, но он не использовался, и если вы откроете троттлинг, как он называется, в общем, экспортер метрик, он посылает это как ваш CPU для вашего пода в кубернетесе вроде как должен был выполняться, но не выполнялся, и поэтому вот это на самом деле CPU вам не дали. Если вы будете смотреть, скажем, на метрики, как используют ваш под ресурсы, вы увидите, что вот здесь CPU я использовал на 100%, а вот здесь я не использовал, но вместо этого сильно вырос троттлинг. Это как раз показатель, что вы выходите за доступный вам ресурс. И пофиксить это можно несколькими способами. Первый самый простой способ пофиксить это уменьшить вот эти вот дефолтные чанг с 100 мс, скажем, до 30, до 25. Я видел разные настройки, как по умолчанию делается. Это автоматически означает, что вас точно так же будут троттлить, но просто время, максимальное время, на которое вас затроттлят, оно будет не больше, чем размер чанга, то есть это 25 мс. Это становится уже лучше, то есть вместо 101 мс для того, чтобы сложить 2 плюс 2, вы теперь делаете там 26 мс. И это прям уже намного приятнее, но все равно не решает полностью проблему. Как оказалось, у этого совершенно честного шедулера CFS есть другой режим работы, который основан на не лимитах и квотах, а который основан, называется Shares, и вся эта идея называется CPU Shares. Соответственно, вторая ссылка в шоу-ноутах, это будет ссылка на вики, описание, что такое описание этого шедулера. А третья, это ссылка на Kernel.org, который описывает, как описывает дизайн CFS шедулера с точки зрения вот этого Shares. Как работает в этом случае? Работает в этом случае совершенно по-другому. Строится дерево, красно-черное дерево, автобалансирующееся дерево, со всеми процессами, которые должны запускаться. И вместо того, чтобы прописывать каждому процессу лимиты того, сколько же он может использовать CPU, описывается, какая часть ресурсов данного компьютера с точки зрения CPU может использоваться. Например, если у вас есть два процесса, и вы каждому укажете 5000, и 5000, по умолчанию, там значение 1024. То есть, если у одного 1024, у другого 1024, это значит, что они 50% на 50% делят ресурсы компьютера. Если вы одному оставите 1024, а другому сделаете 1024000, то есть как бы 1.024.000, это значит, что у одного ресурса будет в тысячу раз больше, чем у другого. Посчитайте сами в процентах, сколько это будет. И как это происходит на уровне шедулера? На уровне шедулера в это дерево вписываются значения соотношений, а точнее, там чуть более сложная логика, но по сути мы просто пытаемся максимально точно поддержать это соотношение между всеми процессами, чтобы использование CPU и использование ресурсов продолжалось с помощью... И отношение всех процессов, насколько много они использовали ресурсов, было идеальное соотношение. Соответственно, когда у нас какой-то процесс поработал какую-то единицу времени, либо он сказал, я сейчас ожидаю в I.O., поэтому давайте используйте кто-то другое, либо его насильно выключили, либо еще что-нибудь, ему записывается вот это ядро, подсчитывает сколько CPU процессорного оно дало этому процессу, и в это дерево добавляет в каунтер вот это значение. Соответственно, после этого происходит перебалансировка дерева, и в следующий раз, когда шедулер будет выбирать, какому же процессу дать, он тупо по умолчанию дает всегда в дереве самому левому листу. Самый левый лист – это тот процесс, который меньше всего получил процессорного времени с точки зрения честного определения ресурсов. И получается такая вот интересная штука, что здесь нету лимитов на количество использования CPU, мы не пытаемся точно считать какими-то 100 мс чанками, конечно, там внутри у нас есть минимальный квант, я назвал это квантом, хотя по определению он не называется квантом, хотя мне кажется, логичное название. По умолчанию квант времени – это 0.75 мс, это те промежутки, когда ядро пытается проанализировать, не слишком ли оно много дало этому текущему процессу. Это его тоже можно конфигурировать каким-то образом. И получается интересная штука, что если у вас есть возможность отключить лимиты и включить CPU shares, это становится более честно, у вас исчезают вообще вот эти вот большие задержки по тому, как процессор какой-то определенный отвечает на запросы. Насколько я понимаю, ошибка может быть, мне сейчас тяжело оценить, но мне кажется, что больше одного кванта она все-таки может быть, то есть 10 квантов грубо говоря может быть, но это все равно уже получается 7 мс, а чаще всего намного меньше. И получается, что ресурсы по процессам распределяются более честно, но, к сожалению, это, конечно, возможно только в том случае, возвращаясь на уровень кубернетеса, когда у вас запускаются процессы только вашего, только те, которые вы хотите запускать. Нет, скажем, если вы Amazon, который продает разным заказчикам, вы не можете такого сделать, потому что вы должны точно сказать, я же ему продал 2 CPU, виртуальных CPU, но все равно, поэтому вы не можете с помощью каких-то shares волшебных это сделать. Ну или можете, но это как-то становится очень сложно. Вот такая вот краткая история. Мы с этим коснулись, мы вот сейчас пытаемся включить кубернетес на вот этих CPU shares. Опция консервирования находится в бете все еще кубернетес, но, как вы помните, мы часто рассказываем, кубернетес бета опции, это почти уже можно запускать. Ну, в смысле, что они после бета сразу уходят в general availability и в бетах до сих пор находится большое количество ресурсов, которые давно уже крутятся в продакшене.",
    "result": {
      "query": "производительность ETS concurrent Erlang"
    }
  }
]