[
  {
    "segment_id": "62bb9ec0-d82f-4659-8510-9cdf961e72f2",
    "episode_id": "3e78c7a4-c623-4c6e-af25-0e9d18fbe557",
    "episode_number": 322,
    "segment_number": 7,
    "text": "Наверное, все-таки некоторые авторы это еще и продают как отдельную книгу. Ну, то есть не просто дают ее как lead magnet, а как нечто, что можно продать за деньги. Ну, я про это немного слышал, если честно. Про Джоэля я не знал, что он так сделал. Ну что, может пойдем тогда на юни-тестирование? Расскажи нам самые большие откровения во время написания книги про юни-тестирование, которое ты рассказываешь, слушаешь, читать. Я вам покажу причину. В общем, да, можно обратить по контенту, по основным, по крайней мере вещам. В книге, ну, вообще я считаю, что при юни-тестировании, при написании юни-тестов нужно смотреть на четыре основных параметра, которые показывают, является ли тест хорошим или нет. И, так, если причислять, то первое это защита от багов. Ну, такой очевидный параметр. Второй параметр – это защита от регрессии, от ложных срабатываний, скорее, соряю. Ну вот, третий параметр – это скорость выполнения теста. И четвертый параметр – простота поддержки этого теста. И вот первые два параметра, они складывают из себя такой параметр, который называется точность теста. То есть, первый параметр – это защита от багов. Это то, насколько хорошо тест выявляет какие-то ошибки в вашем коде. Второй параметр при этом – это количество ложных срабатываний. Это тот параметр, который говорит о том, сколько вы получите ложных срабатываний во время работы теста, во время нахождения этим тестом ошибок в вашем коде. Если сложить эту форму, то получается такая формула точности вашего теста, где в числителе будет количество багов, которые этот тест нашел, и знать ли будет количество ложных срабатываний, которые тест выдал во время нахождения этих багов. И оба этих параметра очень важны, потому что если тест у вас не может найти никаких багов, то ценность такого теста стремится к нулю. И наоборот, если тест может найти все баги в вашем коде, но при этом делает это с огромным количеством ложных срабатываний, то ценность такого теста тоже стремится к нулю, потому что здесь у нас уже будет тормиться к бесконечности, и ценность будет равна почти нулю. Мы говорим про тесты вообще или про unit-тесты сейчас? А также, как ты определяешь unit-тест? Сейчас говорим про unit-тесты. Unit-тесты, тут немножко размытые определения у них, но я в книжке их определяю как тест, который следует трем критериям. Первый критерий – это они тестируют одну единицу поведения. Второй критерий – это то, что они делают это быстро. И третий критерий – это то, что они делают это в изоляции от других unit-тестов. И, например, если взять тесты, которые не являются unit-тестами, я их называю интеграционными тестами. То есть, по сути, в автоматизированном тестировании, в тех тестах, которые пишут программисты, бывают только два вида тестов. Первый – это unit-тест, а второй – это интеграционный тест. И n-to-n-тесты, то есть сквозные тесты, это будут те тесты, это будет subset, то есть подможество интеграционных тестов. И если углубиться в это определение более подробно, то чем, например, отличаются два теста, которые работают с базой данных, от тех тестов, которые не работают напрямую с базой данных? Они отличаются тем, что их нельзя запустить в параллель друг с другом, поэтому они не могут запускаться независимо друг от друга. И здесь третий параметр, который говорит о том, что unit-тесты должны выполняться независимо от других тестов, он уже не выполняется. Потому что если вы запустите в параллель два интеграционных теста, которые работают с одной и той же базой данных, то они могут мешать друг другу путем удаления или добавления каких-то данных в эту базу. Ну и плюс второй параметр тоже здесь будет страдать, но здесь это уже более субъективная метрика, то есть насколько быстро выполняется этот тест. Ну я как бы слышу, я в целом соглашаюсь, но я одновременно и не соглашаюсь, потому что те же интеграционные тесты я могу написать так, что они не будут друг другу мешать. И по твоему определению, несмотря на то, что они работают с базой, и может быть создают там каждое по 100 потоков, но я их написал так, что они друг другу не мешают. По твоему определению получается, что они идёне теста. Да, есть ещё первые критерии, это то, что они должны тестировать одну единицу поведения. Такие тесты скорее всего будут тестировать не одну единицу поведения. Тут опять же немножко субъективно, что именно считать одной единицы, но тут можно какие-то общие принципы наработать в проекте. Ну вот под единицей ты имеешь в виду что-то маленькое типа модуля, функции, класса, правильно? Ну тут опять же есть разные школы тестирования, и одна из этих школ, как раз лондонская школа, сводится к тому, что под юнитом, под модулем должен приниматься какой-то небольшой кусок кода. Например, одна функция, либо один класс максимум. И я считаю это неправильным, потому что неважно, какое поведение занимает кода, если тест тестирует только одну единицу поведения, то он всё-таки остаётся юнитестом. То есть в теории это может быть больше одного класса, может быть группа связанных классов. Главное, чтобы этот тест тестировал единственную единицу поведения. Но имеется в виду верное понимание, что эта единица должна быть как бы озлитого аспрексикой. Да, конечно. Но при этом это ещё должна быть единица, которая makes sense для бизнеса. То есть это не должно быть, допустим, единица добавить в кэш такое-то число. Потому что для бизнеса это детали по нитации, для бизнеса это не имеет значения никакого. А вот если я скажу, что... Я проспоряюсь до конца понять твоё видение. Вот у меня есть класс, например. И когда я с ним работаю, он уходит в многопоточность и может быть... Ну, то есть он не уходит в базу ничего такого, но возможно, создает потоки, с которыми этот же класс и будет потом взаимодействовать. Если такое происходит в тесте, это ещё можно отнести к юнитестам или это уже начинается интеграционное тестирование? Прежде чем это, границы размыты, но я бы не стал такой тест относить к юнитестам. Потому что если он начинает работать с какими-то влатильными зависимостими, такими как потоки, базы данных, то такие тесты, на мой взгляд, сразу нужно относить к интеграционным тестам. Я разделяю эту точку зрения. Мне правда очень трудно её нормально обосновать, но с этим исключением, что когда ты создаешь потоки, у тебя начинается сложное взаимодействие между потоками на каналах или мьютоксах или на чём-то. И это уже подразумевает участие лишних сущностей в твоём кодек. Да, тут обосновать эту точку зрения можно путем того, что этот тест проверяет не только логику самого, саму бизнес-логику приложения, но и технические детали, такие вот как многопоточность. Потому что такие вещи лучше разделять. Лучше разделять бизнес-логику, вытаскивать её из многопоточного кода и тестировать только её. А уже многопоточный код, если есть желание протестировать, их нужно тестировать уже интеграционными тестами. Мне понадобится больше одного экземпляра твоей книги, потому что я часто встречаю людей, которые, ну, типа, давайте напишем unit-test, который там типа создает 50 потоков, потом проверяет, что вот тут вот правильно всё проброс. Вот мы здесь замокаем типа контейнер, очередь, что-нибудь, и мы проверим, что в моке действительно вот сюда было положено это значение, которое мы ожидаем. Как бы это будет unit-test. Как бы ты... Очень трудно аргументировать свою точку зрения, не прибегая к грязным приёмам, типа отсылки к авторитету, но чтобы человеку убедить, что он... Не надо так делать, чтобы он напусти к страданиям. Да-да, тут на самом деле смешиваются даже две вещи. Первое – это неправильное использование моков. Второе – это то, что в unit-test используется ещё и тред, тестирует ещё и многопоточность. То есть с многопоточностью тут сами unit-tests должны тестировать только бизнес-логику. Все любые владельни зависимости, которые есть в коде, их нужно выносить из бизнес-логики и тестировать интеграционными тестами. Это первая часть. Вторая часть – работа с моками. Да, это очень часто такое явление, когда в тесте пишут, ну, тестируют допустим какой-то system under-test, какой-то класс допустим, и проверяют, что вот этот класс положил там куда-нибудь в... Не знаю, вызвал метод на каком-то другом классе и проверить... Идёт проверка, что вот был вызван именно такой-то метод с такими-то параметрами и так далее. Это тоже нехорошо, потому что это приводит к хрупкости тестов. Такое использование моков приводит к хрупким тестам. То есть тестам, которые выдают большое количество ложных срабатываний. Потому что при любом рефакторинге такого кода, при изменении допустим взаимодействия между этими двумя классами, у вас тест поломается. Но при этом он поломается не потому, что вы внесли баг в систему, а просто потому, что вы отрефакторили детали имплементации этого кода. А тест у вас был завязан именно на эти детали имплементации. То есть то, как мог выполнить свою цель ваш код, он мог её выполнить разными способами. Но вы в тесте настаиваете именно на одном способе этого выполнения. И когда вы меняете этот способ на другой, то есть рефакторите, у вас тест ломается. И это как раз и будет ложным срабатыванием. И это будет говорить о том, что ваш тест выдаёт слишком много шума при работе. То есть та же формула, про которую я говорил, это сигнал. А я помню, я не назвал, кстати, сигнал. Это шум. То есть вот эта формула, где в числителе у вас количество багов, которые тест находится, называется сигнал. И в знаменателе количество ложных срабатываний, которые тест выдаёт, при этом это называется шум. Вот у вас будет сигнал какое-то количество, какое-то своё количество. И в шуме будет довольно большое значение, которое будет приводить к уменьшению ценностей этого теста. Я не могу сказать, как я поддерживаю то, что сейчас говорит наш гость. Перемотайте на 10 минут и снова прослушайте, потому что это вот прям, знаешь, анаболевшим. Это хорошо. Я хотел бы ремарочку, а потом вопрос. Окей. То есть хорошо, если тест просто упадёт, и ты потратишь время на переписывание теста. Это я, считай, тебе повезло. Потому что, если люди пишут вот эти тесты с многопоточкой и так далее, они, как результат рефакторинга, могут проходить, но иногда падать. И это приводит к билдам, которые падают, типа раз в неделю, а потом у тебя тесты копятся, копятся, копятся, и стабильность билда всё падает, падает, падает. Но никто не понимает, почему. Потому что, ну блин, как бы мы прогоняли тесты на этапе под ревью, как бы всё проходило, а потом мы намержали, намержали, намержали, и у нас теперь билды проходят через раз. Что предлагается потом? Как бы люди придумывают решения. У нас юнит-тесты нестабильные, поэтому, чтобы гарантировать, что мы отловим ошибки... Прости, пожалуйста, я себя чуть-чуть слышу, очень трудно при этом думать. Эхо идёт. Чтобы как бы повысить то, сколько багов находят тесты, а давайте мы наши юнит-тесты будем гонять ночами циклами по тысяче итераций, потому что в одну из тысячи итераций тест точно упадёт. Вот, и как бы, и... Ну это как бы люди всё больше и больше и больше как бы роют себе яму. Просто не надо было писать такой дурацкий тест, который тестирует многопоточность и завязываться на имплементацию конкретно. Да, да, да, да. То есть я был в проекте, где сначала тесты проходили по одному разу, потом собирались все зафейленные тесты, которые были в... Ну, которые упали в этот раз, собирались в кучку и прогонялись ещё раз, на всякий случай. Мало ли они упали по какой-то причине, которая сейчас уже неактуальна. И завершая ремарку, это ещё умножается на то, что ты пишешь на ГО, а в ГО приходят немало людей с бэкграундом Руби, Перла, чего имя такого, и люди просто не очень большой опыт с многопоточностью имеют, а тут они приходят в ГО, где прямо на многопоточность со левой направо начинают писать юнит-тесты на эту многопоточность, понимаешь, и делают ошибки в многопоточности. Это просто как бы бесконечный водоворот боли. Да, это я очень... I feel you, так сказать. Я не хочу ничего плохого сказать об этих людях, они просто не имели много опыта, не ели говна с дедлоками, рейдс-кондишнами, вот со всем этим. Меня просто переполняют эмоции, я прошу прощения. Да, и в книжке я как раз постарался сделать такой фреймворк, настолько объективно, насколько возможно, чтобы дать возможность людям оценивать качество их тестов. Причем я не просто старался говорить, что вот Moki это плохо, нужно их не делать. Я вот, так сказать, подошел, постарался, по крайней мере, подойти издалека, показать, какие именно параметры влияют на качество тестов, это вот защита от багов, от ложных срабатываний и так далее, и как именно Moki ложатся на этот фреймворк. А ложатся Moki таким образом, что они как раз сильно мешают второму параметру, то есть второй параметр это у нас количество ложных срабатываний. Потому что Moki, если пользоваться ими неправильно, то они приводят к слишком большому количеству ложных срабатываний, и это в свою очередь уменьшают качество теста, точнее ценность теста уменьшают, и приводят к таким ситуациям, как вот ты сейчас описал, и то, что у нас тоже было. И это приводит, то, что ты описал с многопоточницей, это называется Flickering Tests, это очень похожая ситуация на ложные срабатывания, то есть это по сути есть ложные срабатывания, но причины у них не рефакторинг, а просто инфраструктурные зависимости, которые ведут себя нестабильно. А потом ты приезжаешь на GitHub Actions, который на каком-нибудь тормозном инстанте гоняется. Да, да, да. И это как раз называется тест Flickering, когда тест падает из-за того, что с инфраструктурой что-то не так. Когда тест выдаёт ложные срабатывания из-за рефакторинга, это уже то, о чём я говорил в книге. Тест Flickering влияет на четвёртый параметр, это Maintainability, то есть то, насколько этот тест легко поддерживать. А простота рефакторинга, то есть если тест падает после рефакторинга, это уже второй параметр, когда тест приводит к большому количеству шума, и не является стокем к рефакторингу. Какой-то ещё вопрос был? Да, у меня был вопрос, я хотел задать про эту формулу. Ты довольно много на неё ссылаешься, но при этом я как человек, читающий подобную литературу, я всегда на подобных формулах немножко спотыкаюсь при чтении. Потому что автор вводит формулу, которая новая для читателей, при этом у неё должны быть очень веские доказательства, что формула не такая. Например, если ты делишь что-то на что-то и не объясняешь, почему это именно в первой степени в числителе и в первой степени в знаменателе, это фактически ты с неба схватил какое-то значение и решил его использовать. Почему, скажем, не вести в знаменатель в квадрате, что была квадратичная зависимость и так далее. То есть какие-то более строгие доказательства для введения формулы мне лично всегда хотелось бы видеть. Насколько это было тебе очевидно, вот эта формула, насколько эта формула была хорошо принята при проверке книги, при ревью и так далее, вот мне хотелось бы именно формулу затронуть. Да, формула имеет в виду сигнал к шуму, то есть количество багов. Тут, к сожалению, программирование это не точная наука, не как математика. И формулы в программировании, по крайней мере те, которые касаются таких более высокоруних вещей, такие как вот эта формула, они являются такими точными формулами, которые можно там вставить куда-нибудь в функцию, потом посчитать насколько являются этими сценами. Это скорее такой фреймворк, то есть ментальная модель, которая помогает лучше понять, как оценивать ценность тестов. То есть оценивать, оценка сама будет тоже субъективная, но при этом этот фреймворк помогает скоординировать эту оценку у разных людей так, чтобы она была чуть ближе друг к другу. А то здесь как раз формула, она скорее, как раз эта формула, она скорее просто опирается на довольно известную формулу, которая называется signal-to-noise ratio. Это вот в обработке сигналов довольно популярная. Радейках, радейках, SNR, наше все. Да, да, то есть я так сказать отталкивался от этой довольно хорошей, довольно известной формулы. На самом деле конечно формула должна была выглядеть как сигнал в числителе, в заменателе должен быть не просто шум, потому что если шум равен нулю, то у нас ценность будет бесконечность. В заменателе должна быть сигнал плюс шум. То есть если у нас сигнал равен ни единице, то есть тест может найти все баги и при этом не выдает никакого шума, то ценность равна тоже единице, то есть максимум. Если уже шум какой-то там допустим больше единицы, то ценность будет стремиться к нулю постепенно. Вот, ну опять же это не является какой-то точной формулой, это просто такой больше для наглядности, чтобы лучше понимать как именно оценивать тесты в проекте. Есть еще понятие сенад, отношение сигнала к шуму и искажению. Signal to noise and distortion. Может быть это будет лучшей аналогией к тому, что ты ищешь. Ну, посмотри. Может быть да. Возьми на вооружение, если подойдет. А как по поводу, что по поводу review, то есть вот именно за формулы и их формулировку цеплялись как-нибудь? Я не думаю нет. Я думаю люди там понимали примерно, что это все-таки не точная формула. Я в книге об этом писал, что это не является какой-то прям такой точной наукой. Вот. И, ну хотя был да один комментарий там, где я писал про, не конкретно про эту формулу, там про график тоже, который скорее более для наглядности, который я приводил, где я приводил график того, как проект развивается с тестами и без тестов. Ну вот тут да и на графике было примерно так, что без тестов у вас сложность проекта уходит в небеса, так сказать по экспоненте. С тестами у вас сложность написания проекта с тестами в начале является больше, чем проекта без тестов, но постепенно кривая увеличивается с меньшей скоростью, чем кривая проекта без тестов. Вот. То есть в какой-то момент они пересекаются и в какой-то момент проект с тестами начинает оправдывать себя по сравнению с проектом без тестов. Вот. И комментарий был такой, что, ну вот, на каких данных вы это делаете такой вывод? Вот. Ну тут опять же данных никаких нету. Тут во-первых, да, только можно приводить опыт там свой и людей, которых, о которых ты знаешь, и проектов, которых ты знаешь. И плюс еще, как я сказал, все-таки программирование это не quantifiable вещь. То есть ее нельзя вот так вот численно оценить. Тут больше нужно полагаться в большинстве таких случаев приходится полагаться на больше качественную оценку, а не количественную. Ты знаешь, мы по-моему разбирали здесь в подкасте какую-то статью, когда ребята анализировали проекты на гитхабе по соотношению тестов и покрытию тестов к коду и делали отсюда какие-то выводы. По-моему, можно какие-то количественные оценки сделать не именно на фоне того, как точная формула, но какие-то статистические какие-то вести. Ну, вида, например, количества новых ошибок к старому коду, который давно не менялся в зависимости от того, сколько там было тестов. Какие-то можно сделать такие метрики, но я говорю, таких очень мало будет. Метрика покрытия кода это одна из таких метрик, но даже у нее такие есть limitations, то есть ограничения. То есть не всегда ее можно применять. К примеру, сама метрика, если она является хорошим показателем того, что у вас в проекте что-то не так, но она является плохим показателем того, что в проекте все хорошо. То есть, еще раз, если метрика это покрытие кода говорит о том, что в проекте слишком мало покрытия, то это верный признак того, что в проекте недостаточно тестов. То есть это является хорошим признаком того, что в проекте не все в порядке. Но при этом обратно неверно. Если даже метрика покрытия тестами показывает даже 100%, то это еще не означает, что тесты в проекте являются хорошего качества. Это просто означает, что каждый из этих тестов, точнее хотя бы один из этих тестов, выполняет все строчки в проекте. При этом качество самих тестов может быть каким угодно. Поэтому сама метрика покрытия тестами является плохой метрикой о том, она не говорит о том, насколько хорош этот проект. Она только может сказать, насколько это покрытие, этот проект, плох в плане юнитестирования. То есть можно написать, например, тест, где вы запускаете код какой-то, но при этом не делаете никаких ассершенов. И там все exception, то есть исключения, если они есть, то проглатываете в кем-нибудь try-catch. И такой тест будет проходить всегда и будет показывать на такой тест, что у вас покрытие 100%, но при этом такой тест не имеет никакого смысла. Это, конечно, такой утерянный кейс, но можно, допустим, скипануть, пропустить какой-то ассерт релевантный, то есть у вас код выполнится, но при этом сами результаты его выполнения вы забудете проверить. Все-таки вещи тоже бывают. Можно вопрос к эксперту. У меня давным-давно был коллега, он где-то подсмотрел практику, называется матричные тесты, может, слышал. Смысл в чем? У тебя есть какой-то один тест, но у него могут быть параметры. У тебя, допустим, это даже не unit test, да? Ты вызываешь одну функцию с двумя параметрами, потом другую функцию с такими-то еще параметрами, один из которых результат первой функции, например, и ты ожидаешь, что она возвращает. И вот это отношение, типа первый вход, второй вход, третий вход и результат общий, это табличка. Ты не представляешь, да? И его идея в чем? Что ты берешь этот первый вход, второй, третий, результат, записываешь в виде матрички и запускаешь цикл. Что ты думаешь про такие тесты? Да, это хороший подход. Это называется параметризированный тест, когда вы пишете не просто один тест, где вы хардкодите там, допустим, входные значения и проверяете выходные значения, тоже хардкодите ожидаемый результат. Вот эти выходные значения и выходное значение, их можно, так сказать, заэкстрактить и передавать на вход вашему тесту. И вот, например, C sharp и в других языках тоже это есть. Там прямо можно в атрибуте передать те значения, которые передаются на вход. И одно из них будет возвращаемым значением, то есть ожидаемым результатом. И можно так сделать, что тест будет вызывать какую-то функцию с теми параметрами, которые мы передали на вход этому тесту, и проверять результат, сходя из того, что тоже передано этому тесту, как входной параметр. У меня немножко другой опыт. Может быть, он, на счастье, связан с, смотри выше, про многопоточность. Но что происходит на практике? Нужно представить себе реальные условия. Там команда, человек 10, разрабатывает этот код, он меняется, меняются условия. И у тебя рано или поздно в таком тесте появляется ИВчик. А потом второй ИВчик. И очень быстро... Просто, короче, бить людей за ИВчики и все. Просто берешь такой дрын и бьешь их. Я вот к чему. Я склоняюсь к мысли, что отдельный тест должен быть отдельным куском кода. Если куски кода повторяются почему-то, ну хорошо, вынеси в отдельную функцию, переиспользуй. Но вот эти матричные тесты у меня в сознании отложилось как ругательство. Саша, смотри, ты же знаешь, я очень большой фанат property-based тестов. Но они для всего... Во-первых, не везде есть эти фреймворки. Во-вторых, это вообще не всегда смысленно. Если свойства достаточно простые, и тебе просто хочется проверить парсер на эйчкейсах, у тебя реально код теста довольно простой. И в этом случае тебе нужно тесту скормить набор заранее готовых типов входов, и ты примерно знаешь, что должен быть на выходе. Это идеальный пример такого матричного теста. Ты говоришь про юнит-тестирование, а я начал с того, что тест не юнитовый. Ну, в смысле, я пример взял, потому что он просто для понимания. Но я могу представить такое же для интеграционного теста. Опять же, у нас есть какая-то система. Скорее всего, она какая-то не очень... Не знаю. Короче, она принимает на вход строку какую-то и исполняет бизнес-кейс. Послушай, пожалуйста, почему я начал с того, что тест не юнитовый? Потому что в реальности все намного сложнее. Например, ты шлешь запросы в систему. И у тебя был, например, запрос регистрация, которая возвращает окей, если там email валидный, не окей, если email невалидный, и не окей, если пароль слишком короткий. И потом кто-то добавил к этой регистрации какой-то еще запрос с похожим паттерном. А потом у тебя поменялись бизнес-тревоги, и регистрация распалась на два запроса. Понимаешь? И тебе же не писать данного теста. Ты торопишься, у тебя дедлайны, у тебя спринт заканчивается. Ты добавляешь туда маленький ИВ. Ты понимаешь, к чему это в пределе приходит? Я понимаю, но такой тест не надо было написать как матчный. Матчный тест – это инструмент. Это знаешь, зачем отдавать сожжем в адских топках, не знаю, из программирования. Я JavaScript обычно очень люблю. Ты, Саша, был раньше главным аргументатором, что плохие человек и инструменты – они плохие, они нехорошие. Матчный тест – это один из инструментов. Смотри, я все-таки за разделение. То есть property-based-тесты у меня в голове, по крайней мере, все-таки ассоциируются в первую очередь с unit-тестированием. Да, можно написать property-based, который там чуть ли не системный. Но в первую очередь они у меня как бы ассоциируются с unit-тестами с чистой функцией или классом, который имеет стейт, но он несложный, мы че-то таким. Когда мы переходим на следующий уровень, на интеграционные тесты, вот тут я топлю за подход, что надо их отделить на отдельные тесты. Я на этой неделе писал property-based-тест на ассинхронную очередь запросов в Spark. Все зависит от той фантазии. Что за тест был? Мне нужно было проверить, у нас там кастомный переписыватель плана, мне нужно было проверить, что на всех таких входах вся эта конструкция не разваливается. То есть, у меня свойство конструкция не разваливается. Это простое свойство, оно не меняется от входов. И матричные тесты, и property-based-тесты. Там важно не то интеграционный unit, там важно то, что у тебя проверяется какое-то одно свойство. А не то, что у нас во время регистрации, если у нас регистрация это сложный flow, то на него нужно покрывать разные flow, их нужно покрывать разными тесткейсами. А если у нас реально одно свойство, но оно должно выполняться на множестве входов, то мне кажется, вот это как раз хороший способ применить property-based или матричное. И, кстати, у меня property-based-тест довольно такой, я property-based-тест тащил именно потому, что мне нужно было довольно в сратых входах генерировать. То есть, там в принципе можно было бы матричные тесты сделать, если у меня было чуть менее LA. Но в принципе, можно свойства сложнее формулировать. У меня есть еще похожий тоже вокруг Spark-кейс, когда я писал коннектор к очень интерпреттной штуке, она умеет фильтр принимать. Я, например, генерировал VAR-условие для Spark-а, и там свойство было такое, что, значит, короче, если VAR-условие без push-down-а должны возвращать такой же результат, как если мой коннектор производит push-down этих VAR-условий фильтра этой системы. Это тоже довольно такой системный тест, и тут нетривиальное свойство, надо сказать. И я не представляю, как такое можно решить, типа куча кейсов. То есть, мне тогда по тескейсу писать на каждую возможную пермутацию того, что там случится внутри VAR-конструкции в Spark-кейс запросе, это было бы странно. А так я могу нагенерировать что-то в соответствии с тем, что нагенерирует мне Property-Base Framework. И если бы у меня не было Property-Base Framework в скале, я бы писал матричный тест. Да, здесь на самом деле затронули несколько хороших интересных тем. Давайте сначала быстренько пройдемся по IF-у. Да, IF-ы в тестах – это зло. То есть, если есть IF-ы, то нужно такой тест разбивать на два теста, и чтобы в них не было IF-ов. Потому что если у вас появляются в тесте IF-ы, то такой тест становится сложно поддерживать, и в таком тесте резко возрастает вероятность внесения багов. То есть, у вас будут тесты не в самом коде, а в тестах, которые покрывают этот код. И этого нужно избегать. Ну и плюс такие тесты покрывают несколько сразу use-кейсов, что тоже неправильно. Для этого у вас есть сами тесты, чтобы вы это делали. При этом, я не думаю, что это идет в разрез использованию матричных тестов, то есть параметризированных тестов. Потому что если у вас появляется IF-ы, то разнесите такой тест на два, и в каждых этих двух тестах будет иметь свой набор параметров. То есть, это будут такие две матрицы, два параметризированных теста. Но мой коллега из Далека у прошлого приводит контраргумент, но у тебя же дублирование кода. Это не инструмент для избавления дублирования кода. У тебя дублирование кода вынеси в отдельную функцию, чувак. Я не согласен. Мне в будущем придется поменять поведение. Мне придется пять тестов поменять. Пять вместо одного. Вынеси в общую функцию helper. Тут по поводу дублирования кода в тестах, это тоже еще один хороший топик, интересный. Тут нельзя подходить к этому так же, как к дублированию кода в обычном продакшн коде. Потому что в тестах у вас главное это читаемость этих тестов. При этом, если вы будете выносить какие-то общие куски из тестов в, там, не знаю, в, допустим, в конструктор класса, где вы будете инцелизировать, допустим, тесты, и потом в самих тестах уже будут только те куски, которые отличаются друг от друга. Это, наоборот, будет снижать читаемость тестов, потому что у вас не будет перед глазами всего контекста, всего контекста того теста, который выполняется на данный момент. Тут можно сделать немножко там по-другому. Тут можно вынести, допустим, в приватные методы, вынести инцелизацию начальных значений. При этом они сделают так, чтобы у вас они параметризировались, то есть чтобы перед глазами было, допустим, создание кастомера. Когда вы создаете кастомера с таким-то параметром, вы знаете, что вы его создаете именно с такими-то параметром, как именно он создается, вам уже неважно. Главное — это понимать, какие периметры есть у этого кастомера. И потом уже в тесте его использовать. При этом если это инцелизация этого кастомера вынесена, допустим, где-нибудь в конструктор, то есть она не видна и поэтому теряется контекст. То есть тут нужно немножко по-другому подходить к дублированию кода в тестах и иметь вот это вот в виду. По поводу... Да, по поводу Ифа сказали, что нужно просто разносить данные тесты, то что они будут дублировать какие-то там вещи, но ничего страшного, потому что дублирование кода тестов — это не так страшно, как дублирование кода в продакшн-коде. По поводу property-based-тестинга. Если проверяется property на то, что там упала система или нет, то да, тут как бы неважно, какие unit-тесты, не unit-тесты, и неважно, как именно они написаны, главное, что там, допустим, exception у вас не было, то есть система не упала. Тут property-based, да, думаю, подходит. Но вообще, в общем случае, я думаю, property-based-тестинг подходит только для unit-тестов, если вам нужно спротестировать property, которая не, ну так сказать, зависит не только от, так скажем, которая зависит от входных значений, вот, и при этом имеет какое-то выходное значение, потому что вот такой стандартный пример property-based-тестинга — это проверка реверсии листа, когда вы подаёте на вход какой-то лист и на вход функции reverse. Делайте reverse два раза, и на выходе у вас получается такой же список, который был подан на вход. И при этом неважно, какой список вы подаёте этой функции reverse, у вас на выходе должен получиться ровно такой же список. Вот, и это очень хорошее такое property у этой функции reverse, когда два раза, если вы его сделаете, получается исходный список. Вот, то есть такие property по-другому, кроме как вот unit-тестами, проверить не получится, потому что если у вас появляются какие-то side-эффекты в системе, то там, ну, по крайней мере, по моему опыту довольно сложно их проверять property-based-тестингами. Вот, и вообще в целом есть три подхода к unit-тестированию. Первый — это, я его в книжке назвал communication-based-testing, это, по сути, функциональное программирование, когда у вас есть на вход какие-то параметры, и всё, что делает функции или там метод, это возвращает какое-то значение. Она не меняет никакое своё состояние, не меняет состояние каких-то внешних зависимости, она только возвращает весь результат операции, это, по сути, вот возвращение значения из этой функции. Это вот, так, я его назвал, он называется output-based-testing. Я его назвал случайно communication-based-testing другой. Вот, первый — это output-based-testing, это проверка чисто функциональных методов, то есть тех методов, которые являются чистыми с точки зрения функционального программирования. Второй способ — это state-based-testing, когда вы проверяете состояние системы после того, как вы запустили какой-то метод на нём. Вот, и третий — это communication-based, это когда вы проверяете, что ваш метод, ваш там класс или объект вызвал какой-то метод на другом объекте либо классе, то есть с помощью моков тех же самым. И property-based-testing хорошо ложится на output-based-testing. Я решительно не согласен с этим утверждением. Так, есть какой-то контрпример, кроме вот этого, которое вы только что говорили? Ну, смотри, ты прав, что нужно, короче, что сложно применить property-based, если у нас, короче, всё неконтролируемо ходит и делает неконтролируемые side-effects. Однако у нас side-effects и state могут как бы быть контролируемыми. И на самом деле, если мы там смотреть фреймворки, из которых вообще это всё пошло, тот же самый QuickCheck, например, их Enterprise-версия QuickCheck'а, ну, просто я работал в компании, в которую приходили ребята из, как это называлось, компании с QuickCheck, я даже забыл, и пытались нам продать, собственно, Enterprise на QuickCheck, и показывали некоторые примеры. У них на самом деле, конкретно даже для Erlang, у них были такое расширение к фреймворку, которые как раз таки позволяли тебе делать, как это, invariant на state actor. То есть, тебе property-based генерирует какой-то поток входов, возможно, конкурентный, и ты накладываешь на как бы actor, который под тестом invariant, и типа ни после какого сообщения invariant не должен поменяться. Здесь есть важный момент, это actor, в который сообщения приходят, ну, как бы они упорядочиваются в виртуальной машине и приходят в каком-то порядке. И понятно, что invariant проверяется, как бы, после каждого шага. Понятно, что это как бы система, которая позволяет себя так инструментировать. То есть, в случае какого-то более общего, например, если мы говорим про state какого-то приложения, которое взаимодействует с базы данных, мы, например, можем говорить о state в базе данных, которая происходит после каждой транзакции, и у нас, поскольку транзакции, мы, в принципе, можем их, вообще говоря, откатить, и не как бы, даже больше того, каждый экземпляр теста может бежать в свои собственные транзакции, и в транзакциях могут быть под транзакции. Ты уверен, что в тесте хочешь ходить в базу и смотреть, что в ней? Вот смотри, если у тебя внутри теста, как бы весь тест летит в одной, как бы, обрамляющей транзакции, то даже если в ней есть под транзакции, то два теста изменения друг друга видеть не будут. Я имею в виду просто как проверку в тесте, что содержимое базы такое. Ты уверен, что хочешь это проверять? Смотря что ты и зачем тестируешь. Если ты тестируешь движок запросов, то это может быть что-то, что ты можешь хотеть проверять. Понятно дело, что в бизнес-логе кто-то проверять, скорее всего, не очень будешь хотеть. Но если ты пишешь какую-то обмотку вокруг базы данных, которая должна делать определенные вещи с ней, то да, потому что у тебя поведение системы это изменение в базе данных нужным образом. Но ты понимаешь, что я в голове держу более типичный сценарий, типа там микросервис или что-то? Ну, я просто говорю про базу данных в данном случае, потому что мне легко привести этот пример, что у нас стоит контролируется, потому что у нас есть транзакции, понятно, что... Или там акторы, у них стоит контролируется, потому что это актерная система. И то и другое достаточно хорошо поддаются инструментации. Ну, если ты понимаешь, что там, не знаю, какой-то произвольный код, ну, без какой-то, не знаю, ручки, за которые можно подергать, так будет очень сложно тестировать. Но вот если ручки добавлены, то его, значит, становится возможно так тестировать. То есть, стейт-бейст тесты тестировать делать можно, но для этого нужен способ как бы контролировать шаги изменения состояния и возможность его откатывать. Ну, смотри, вот то, что ты писал первое, то, что проверяются инварианты у актора, это по сути разновидность того первого самого варианта, где вы проверяли в проекте, что система не падала после каких-то входных значений. Потому что здесь никакие property не проверяются, кроме того, что invariant соблюдается. А invariant соблюдается, это property самого, так сказать... Нет, этот invariant нужно заходить как часть своего теста. Ну, как там же... В чем выражается то, что invariant не упал? В том же exception и не было какого-то... Нет, это invariant, короче, просто берешь, у тебя вот такая функция, в которую тебе стейт-актор обходит, ты в этой функции выполняешь произвольный код и говоришь, он типа стейт похож на то, что я хочу, или не похож. А что есть похожее или не похожее? Ну, смотри, например, у тебя в акторе есть какая-то целочисленная применная, и у тебя invariant, эта целочисленная применная, никогда не принимает, скажем, значение 0 или меньше. Не в каких условиях. Это может быть invariant. Этот invariant сам по себе в акторе, он может быть никак не закожен. Ну, понятно, что такой invariant легко заходить в акторе, но это может быть какой-то очень сложный invariant, который ты просто не будешь хотеть на каждое сообщение проверять и там ассертивать. Да, я к тому, что вот такой invariant не зависит... У него нет выходных каких-то значений, которые зависят от входных значений. И поэтому это property, по сути, является разновидностью property того, что система не падает, потому что мы просто проверяем, что состояние этой системы изменилось в каких-то там рамках положенных, допустим, стало не отрицательно, либо не стало больше 100, к примеру. Я понимаю о чем-то, там, по-моему, такое тоже можно было делать, я не хочу соврать, но там, по-моему, можно было сделать и на invariant в духе, что если когда-то пришло такое-то сообщение, то стейти должно быть там, типа, такое-то, соответственно, такое-то условие должно выполняться просто и так. Ну, C-sharp вообще тоже есть аналог чека, и там тоже можно проверять стейт. То есть в целом property-based, он будет работать со стейтом, я просто к тому, что это довольно геморройное занятие. Я согласен с этим, то есть как бы, чтобы твою точку зрения поддержать, была такая компания BASHO, которая делала такой баз данных реак, баз данных сейчас теперь целиком открыто, и у них тесты на много что были написаны как раз на QuickCheck. И несмотря на то, что у них была парочка тестов, по-моему, которые были про стейт, они старались писать код так, что у них был такой модуль логики, который на входное какое-то, не знаю, действие, говорил исполняющим модулю, то есть он выдавал такую последовательность действий, что делать. И вот они тестировали при помощи QuickCheck в первую очередь эти модули, которые выдавал, что делать, и он был очень интенсивно QuickCheck-ом протестирован, что на любое изменение состояния выдавалась правильная реакция. Но эта правильная реакция, это по сути структура данных, и очень легко проверить, что она правильная, и легко проверить, не нужно никуда стейта, никуда лазить. Это, кстати, очень такой распространенный, хороший подход по трансформации тестов из стейт-бейст тестов в аутпут-бейст тесты, которые потом очень легко проверить. То есть когда вы не напрямую меняете состояние, а сначала возвращаете инструкции по изменению состояния, вы не надо менять уже эти инструкции, а потом уже сама логика по накатыванию этих инструкций на состояние, она настолько простая, что ее тестировать уже не обязательно ее не нужно. Да, именно. То есть ее можно потестировать, но это гораздо проще делается. Отдельно сложную логику планирования, отделять ее и тестировать отдельно от логики исполнения, это очень здорово работает. И вот, собственно, в реаке этим пользовались. Да, действительно, гораздо проще написать на аутпут свойства, чем на стейт. Да, и вот, кстати, очень много подходов, в том числе рефакторинга тестов, они заключаются в том, чтобы отделить все сайд-эффекты, вынести из бизнес-логики вообще все, что можно. Это многопоточность, работу с базой данных, работу с сайд-эффектами, если это возможно, не всегда это возможно. То есть как можно больше вынести все это не чистоты, так сказать, из доменной бизнес-логики, так чтобы протестировать саму бизнес-логику, было настолько просто, что оно можно было даже QuickChack'ом сделать. Потому что такие тесты, даже если это не QuickChack, а просто обычные тесты, либо матричные, проматеризированные тесты, то все равно такие тесты будут очень просто написать и поддерживать, потому что никакого состояния проверять их не нужно, нужно просто передать какое-то значение на вход и проверить, что выходное значение равно тому,",
    "result": {
      "query": "продажа книг как lead magnet"
    }
  }
]