[
  {
    "segment_id": "4e3fb4e3-2fbb-48e3-89c4-c81e80266083",
    "episode_id": "a6ab2f6d-1d59-4562-839a-f81f1b8a2ec0",
    "episode_number": 412,
    "segment_number": 15,
    "text": "И таким образом, эту уязвимость как бы можно митигировать, но по факту, все стандартные реализации SSH-серверов, они ведут себя так, как ожидается по протоколам. И на самом деле, это становится еще чуть более беспокоительным, если подумать о том, что не так уж и сложно просканировать все IPv4 пространство и перепробовать эти ключи. Это не то чтобы быстрый процесс, но там с доступом к облакам и всем таком, это можно там за несколько сотен долларов, вообще говоря, проделать. И у тебя будет одна карта IPv4 адресов и GitHub username, в которой имеют доступ. И вот это уже начинает немножечко беспокоить. IPv6 такой трюк, конечно же, не прокатит, потому что слишком много IPv6 адресов в природе, но большая часть интересных серверов в интернете, они имеют IPv4 адрес. Вот такая тема. Это все. Но и теоретически это не то чтобы прям вот какая-то кошмарная уязвимость, вроде там, Hardbleed или еще чего-то, но немножечко беспокоит. А ну еще кого-нибудь беспокоит, или только меня? Ну, мне было интересно это услышать, потому что когда ты это первый раз приносил, я как-то это не прочитал так. Спасибо, что зашел. Вот, уважаемые слушатели, видите, мы не полно развернули тему, и можно прийти и развернуть ее полнее. Что еще можно развернуть вполне, эта тема в закладке нет для пересказа. Во-первых, у меня коллега Ян, Ян Недведский, по-моему, у него фамилия, написал инструмент для посгресса, для трассировки локов. В посгрессе есть два типа локов, так называемые lightweight локи и heavyweight локи. Heavyweight — это когда вы вот прям таблицу лочите. Lightweight — это то, что пользователи обычно не видны, это, например, лок на страницу в sharetbuffers. И вот этот инструмент позволяет трейсить, взять освобождение этих локов, потом собирать статистиков, на каких локах, сколько мы ждали, они в посгрессе проименованы, и по тому, на каких локах ты сколько висел, под какой нагрузкой, ты можешь сделать правдоподобные догадки о том, почему ты висишь, или, может быть, найти battle neck в посгрессе и попытаться его устранить. Для этого был использован BPF, который мы нежно любим. И конкретно для... у BPF, как в нашем случае, конечно, известно, у него много трейс... как это сказать... источников трейспойнтов. И для heavyweight локов используется Uprobs. Uprobs это фактически... я сразу скажу, я как бы нифига не эксперт в BPF, это мое дежурнанское понимание, что ты, имея отладочные символы и исполняемый файл, можешь определить, где в этом исполняемом файле примерно находятся те или иные функции, и возможно даже определить смысл аргументов этих функций. И с помощью какой-то офигенной магии, которая мне неизвестна, понять, что если я вот в бинарнике зашел вот в это место, значит, произошел вызов этой функции. Если я вернулся с того-то места, то значит, у меня эта функция вернула результат. И Uprobs он как раз... то есть Uprobs не работает как-то специально с генитированным бинарником. Uprobs работает в принципе с любым бинарником, для которого у тебя есть отладочные символы. И с одной стороны, это хорошо, это удобно, у тебя фактически с любым бинарем в твоем продакшене будет это работать. Проблем в том, что в релизных билдах у тебя используется оптимизация, и как результат функции могут инлайниться, это могут как-то схлопываться, передаваться через регистры, через которые их трудно отслеживать, поэтому Uprobs в общем случае не совсем надежно. Поэтому это будет хорошо работать только на дебаг билдах, без оптимизации. Для light weight локов в пасгрессе есть то, что называется USDT. User Defined Trace Points. Изначально это штука для Detrace, но теперь ее поддерживает и BPF и BPF Trace. BPF Trace это фактически аналог Detrace для Linux. И идея как раз заключается в том, чтобы специальным образом инструментировать бинарник, но его для этого нужно собрать специальными флагами, у него чуть-чуть прощает производительность, но бинарник будет специальным образом инструментирован, чтобы как раз инструмент трассировки мог трассировать то, что тебе интересно. И вот для light weight локов такая инструментация есть. Вот этот инструмент использует, поэтому он более надежен и трассировка light weight локов будет работать даже на релизных билдах, в продакшне надежно и стабильно. Проект находится на гитхабе. Звучит как полезный инструмент, чтобы с ним ознакомиться. И вот для меня он был особенно интересен как источник идеи для Apache, в частности для heavy weight локов можно запилить USDT, чтобы их трассировать даже в релизных билдах. Я был немножко другим занят на ближайшие пару дней, поэтому пока этим вопросом плотно не занимался. Но планирую заняться. Вопрос о возражении в комментариях. Кто использует BPF на Prod'е или иначе Alex? Я его сам не использую. Я не удивлюсь, если какая-то наша инфраструктура на нем построена, но это все настолько далеко от меня, что к счастью мне не нужно это знать. Ну, вообще Почему к счастью? Это как интересная тема. Это интересная тема. У меня просто много других интересных тем, про которые мне нужно знать. И я рад, что мне не нужно знать про еще одну. Это не только интересная тема, но еще и инструмент, который реально позволяет тебе на Prod'е диагностировать причину проблемы. У господина Брэнда на Грэга аж две книги, ну там две новых книги, я не считаю там старые его книги, про то, как отлаживать Prod вот этими инструментами. Я не знаю, насколько это публичная информация, поэтому много я об этом говорить не могу, но у нас есть такая штука, как Continuous Profiling, которая постоянно собирает профили со всех тасков в Prod'е. И это очень удобно. На чем она именно построена, я не знаю, но очевидно overhead у нее достаточно низкий, чтобы она работала. Я понял. Используются какие-то другие инструменты. А что еще у меня в коротких темах, это доклады серии Advanced Database Systems 2020 года курс читает Энди Павла в этот раз. Извините, перебью, я немножко отходил от микрофона. Я почти уверен, что я никогда не видел Google Stack, поэтому не знаю, на чем у них Continuous Profiling построен, но если посмотреть на компании, которые это пытаются продавать вне Google, типа Polar Signals, это построено на EBPF. Теперь можно? Да, извини. Курс читает Энди Павла, и это будет не совсем пересказ, потому что, сейчас поймете почему, доклад про Scheduling говорит нам о том, что как из дерева, из плана запроса, понять, что делать и как исполнять, и желательно еще чтобы поддерживать распраллеливание запросов, и мне, честно говоря, было интереснее всего про распраллеливание запросов, но этого как раз было не так много. Идет долгий handwaving на тему Tread vs процессы, SMP vs NUM, вот эти вот рассуждения. Интересно, мне показался вопрос, вот смотрите, мы работаем на NUM-архитектуре. У нас для каждого ядра, точнее не так, у нас для N-ого количества ядер есть память физическая, которая к этим ядрам близка, а другой кусок памяти он для другого сокета и ближе к другим ядрам, на NUM-архитектуре как противоположность SMP. Внимание, вопрос. Работая на NUM-а, я вызываю молок. Ну, и для определенности я вот на таком-то ядре. Я на таком-то ядре вызываю молок. На линуксе для определенности. Где будет выделена память? Ну, имеется в виду, что мы же на NUM-а, и у нас вот есть память, которая поближе к нам, память, которая ближе к другому сокету. Как операционная система решает, где выделить память при вызове молока? Ну, молок, он сам по себе не обязательно же память у операционной системы просит, он может из уже выделенной выделить тебя, правильно? Возможно... Допустим... Да, прости, допустим, мы знаем, что мы его вообще первый раз вызываем, и у нас нет свободной освобожденной памяти. Ну, тогда это, соответственно, запрос пойдет в ядро. Ядро, наверное, знает, на каком сокете мы выполняемся, и может попробовать выделить память, которая поближе. Я не знаю, достаточно ли Linux умный, чтобы так делать, но, кажется, противоподобны гипотезы. Это хороший ответ, и мой ответ звучал бы так же, но я посмотрел доклад, и занимательный ответ. Когда ты вызываешь молок и уходишь в ядро, ядро выделяет память нигде, она ее не выделяет. Она в виртуальном пространстве процесса помечает определенные страницы, что вот здесь надо сделать вид, что мы когда-то освободили память, выделили память, но реально выделение физической памяти не происходит, а происходит оно в момент обращения к памяти, происходит прерывание, ты проваливаешься в ядро, и тогда ядро уже выделяет реальную физическую память. И это одна из причин, почему в Linux работает, по-моему, это называется overcommitment, или как-то так. То есть ты можешь выделить, ядро может сделать вид, что оно выделило себе больше памяти, чем у нее реально физически есть, при условии, что ты к этой памяти не будешь обращаться. Это не связано с вопингом. Такой вот занимательный факт. И в момент обращения к этой памяти, физически она действительно будет выделена на том сокете, на той памяти, которая ближе к тому сокету, на котором ты исполняешься, согласно этому докладчику. Мне это показалось очень интересным, я когда-то очень давно вроде как помнил, что ядро Linux на самом деле не выделяет память, но естественно я про это забыл. Это не что-то, что в повседневной жизни часто пригодится. Дальше они пытаются рассказать про параллельное распараллельно запросов, но я это воспринял как такой непонятный хендвейвинг, что-то, давайте сделаем воркеров, а давайте у них будет кью тасок, а давайте у нас будет джобстилинг, на случай, если у нас таски кончились, а другого воркера они не кончились. Но все равно, если ты реально будешь чем-то таким заниматься, тебе нужно подумать, вот у меня 15 джойнов и вот у меня, допустим, узлы в плане исполнения запросов я распараллелил, но мне же нужно не просто выполнить какую-то таску и забыть про нее, мне нужно делать свою часть под запрос, прокидывать ее куда-то наверх, как-то синхронизироваться, потому что узел выше меня, он тоже работает в параллель в виде отдельной таски, и все это намного сложнее, чем показано в этом докладе, поэтому мне эта часть, она меня не очень впечатлила. Ну и да, все это выглядит как все еще достаточно кровоточащий край индустрии, где никто не знает как правильно, поэтому все делают как им кажется правильно, и вот такого прям единого подхода вроде как нет. Интересным показалась часть про SQL сервер, сообщается, что в Microsoft на каком-то этапе сделали отдельную операционную систему, называется SQL OS, это специальная операционная система, чтобы на ней крутить СУБД. Была когда-то идея, я забыл как она называется, про микроядры или как это называлось, Валер, когда ты вот Ирланг на железе запускаешь? Unicernal. Unicernal, спасибо. Вот как Unicernal, только для СУБД. И из забавных решений, которые они приняли, что вот ты как разработчик СУБД, ты должен по всему своему коду рассовывать yield, потому что ты же не на настоящей операционной системе, ты вот на особенной, и переключения контекста нет, у тебя Unicernal не отнимает контекст, и поэтому тебе нужно сказать, что вот в этом месте я закончил свой кусок работы, и ядро такое думает, ага, а сколько твоя таска исполнялась, а сколько исполнялись другие таски по времени, может быть пора поисполнять другие, а может быть не пора, тогда ты из yield вернешься дальше в код и продолжишь исполняться. Итак, в SQL Server напихали много-много yield'ов. А дальше из забавного, что как будто бы вот эта идея с SQL OS, она не то чтобы сильно полетела, но зато Microsoft смог портировать SQL Server на Linux, потому что они все равно переписывали под интерфейс, вроде как похоже на POSIX, а потом подумали, о чем бы на Linux они не запустятся. И вроде как где-то в интернетах есть статья про то, как портировали SQL Server на Linux, я не читал, мне было не настолько интересно, но она как будто бы где-то есть. Ну вот такой пересказ. Мне, мне эта лекция повторюсь, не то чтобы сильно понравилась, хотя в пару моментов было прикольно, вопросов, возражений, комментариев. Ну и ладно, пойдем тогда по темам наших слушателей. По темам наших слушателей. Пойдем, пойдем по темам очевоевых две.",
    "result": {
      "query": "SSH уязвимость сканирование IPv4 ключей"
    }
  }
]