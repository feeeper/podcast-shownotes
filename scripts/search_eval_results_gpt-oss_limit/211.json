[
  {
    "segment_id": "e520ee04-0418-482b-a273-45bf494259af",
    "episode_id": "bbfdab1c-5506-4b45-a1bd-0ee1a3be9be7",
    "episode_number": 211,
    "segment_number": 6,
    "text": "Вот, я на самом деле хотел обсудить, даже не столько... Я просто для тех, кто испытывал сложности с непониманием услышанного, посыл такой, что если добавлять такие ручки, то развитие скедулера остановится. То есть не будем их добавлять, давайте лучше развивать скедулер, как-то так. Вот, я на самом деле хотел сказать, я почему-то тему притащил даже не столько ради обзора, сколько ради того, чтобы в очередной раз напомнить слушателям, что в 2018 году вам скорее всего не нужны никакие базы больше, кроме Postgres. И вот если раньше вам не нужны были... То есть вначале Postgres сделал ненужным документ, ну всякие там современные JSON-базы, потому что с появлением JSON-B стало все клево. Потому что, ну, во-первых, достаточно трендов... Транзакции можно индексировать, можно хранить, можно достаточно удобно запрашивать. Довольно клевая поддержка. И там сейчас уже достаточно несложно настраивать в какой-нибудь стулон или патроне, поэтому с фейлори у Postgres тоже все нормально. Единственное, что не очень шардируется. Но с другой стороны, тут уже 2018, и как бы шардировать... Ну, вот у меня... Я периодически хвастаюсь тем, что вот на нашем проекте очень большой размер данных, то есть там порядка 10 терабайт. И это вылезает в один ящик с репликами, но оно не партиционируется. И вот у нас пока нет необходимости партиционировать. То есть оно, может быть, когда-нибудь появится, но если ее нет у нас, значит, я думаю, что типа 99,9% приложений могут жить на Postgres без партиционирования. А еще есть Clickhouse. А вот я как раз хотел сказать, что с этим релизом Postgres, наша жопа становится ближе к тому, чтобы Clickhouse тоже был не нужен. А вот и скажи. Давай, ну переубеди. В Clickhouse есть уникальные фичи, которых нет в Postgres. Их можно сделать на триггерах, но в Clickhouse они уже есть. Например? Например, у них есть разные движки таблиц. По дефолту у них есть вот merge3 движок, туда просто можешь складывать данные. Подожди, подожди, подожди, какой merge3, какие движки таблиц? Зачем? Дело в том, что... Зачем ты этого хочешь? То, о чем ты говоришь, это не фича. Это, ну, есть такая возможность, но она не решает бизнес-проблему. Какую бизнес-проблему решает Clickhouse, которая не решается в Postgres? Ну вот смотри. У нас идет поток данных о том, что делают клиенты. Это все складывается в одну большую табличку. Искать по ней не очень удобно. Я создаю... Почему? Данных много. И что? У меня запрос отрабатывает одну секунду на поиск чего-то. Одна секунда – это долго, потому что я ее показываю клиенту в real-time, хочу показывать. Поэтому я создаю другую таблицу, на которой создаю Materialize.py, и данные из первой таблицы агрегируются и идут в другую таблицу. А все новые данные, мне не надо рефрешить оригинальную таблицу, они попадут автоматически уже и в другие таблицы, которые объявлены как Materialize.py. Это раз. Второй момент. Прости, я не понял, чем это отличается от Materialize.py в Postgres? В Postgres тебе нужно руками Materialize.py обновлять. Или я совсем дупой? Нет? Я думаю, это не совсем правда. Значит, я отстал от жизни. Нет, Materialize.py обновляется руками, все верно. И в защиту кликхауса у него все-таки сильно другая модель данных, которая позволяет некоторые классы запросов делать сильно быстрее. Это колоночный сторидж. В Postgres он не такой. Добавив действительно кастомные табличные движки, можно сделать что-то подобное к кликхаусу, но до этого очень далеко. То есть кликхаус, по сути, он во многих местах нужен. Я не хочу с этим спорить. Мы используем как для аналитики. Например, еще один пример. Идет поток данных, пользователь смотрит видео. Каждые 5 секунд шлется ping к каким-то там ID-шникам видео. Главная таблица принимает это, как большой garbage. И потом эти же данные идут под таблицу, где автоматически агрегируются. В одной таблице складываются все эти пинги, и мы знаем, сколько времени юзер посмотрел это видео. В другой таблице агрегируется по-другому, и мы можем построить хит-мэп, какая часть видео была наиболее популярна. Это да, это можно все строить SQL-запросами уже потом, используя данные главной таблицы, но все это будет занимать время. Тут же можно заранее преагрегировать, сложить в нужные места и уже пользователю с минимальными запросами показывать нужные ему данные. На самом деле, с правдивостью ради, я хочу сказать, что я не хочу спорить с тем, что кликхаус – это клевый и удобный продукт. На самом деле, я точно знаю место, не знаю, в Аджесте, где, если бы исторически не было Postgres, можно было бы, наверное, сделать кликхаус, но в то же время, если посмотреть доклад, у нас, например, был Крис Трэверс, мой коллега на P2ConFru, и еще в паре мест рассказывал, можно послушать его доклад про Postgres, типа, по-моему, 20 терабайт дальше или 10 терабайт and beyond, я не помню. По-моему, 10 терабайт and beyond. Вот, он там немножко рассказывает про то, как у нас это все сделано на Postgres, то есть мой посыл в том, что Postgres настолько мощная и крутая система, что, ну, типа, кликхаус там немножко больше из коробки, но на Postgres можно накрутить все, что ты рассказываешь, и даже лучше, да, Postgres не колоночный, поэтому есть особенности. Можно делать его чуть более колоночным с кастомными типами данных, можно взять сразу колоночный базу данных и не трахать себе мозг, но мой изначальный посыл был на самом деле не про это, а про то, что если вы не какой-то супер-мега большой бизнес, а просто начинаете свой какой-то маленький проект, сейчас прекрасное время для того, чтобы вообще не ломать голову, просто взять Postgres и он, типа, пока у вас не появится, не знаю, первые тысяча, две, три, четыре, десять клиентов, не ломать себе голову. Еще заметка, две даже. Если ты маленький бизнес, используя Amazon, там есть колоночный баз данных, плати чисто за количество данных или запроса, второй момент по мне… Не согласен, решительно не согласен. Дорого, у тебя еще клиентов нет, Amazon это реально дорого. Если ты стартап, которому дали пару миллионов, есть куда тратить деньги, но это ладно. Второй момент, ощущения от кликхауса мои, мне кажется, это горящий танк, потому что момент, пока я интегрировал кликхаус, успело смениться пяток версий этого кликхауса, они ввели новый синтакс из-за создания таблиц, не знаю, старые они выкинут или нет, и все, что я написал, мне, похоже, скоро придется апгрейдить. И документация не поспевает за фичами, которые они имплементируют. Это весело. Вот, а в Postgres все стабильно. Не, как бы, то есть я, наверное, сейчас, если бы делал что-то совсем аналитическое, наверное, бы смотрел или на кликхаус, или на дрилл, или на... в крайнем случае, обмазать что-нибудь с парком. То есть, ну, в принципе, это все, наверное, такие совсем аналитические штуки. С другой стороны, как бы, ну вот, зачем? Зачем? Сколько нужен данных, чтобы... То есть вот у тебя в кликхаусе сколько данных, Артур? 300 гигабайт. Вот, серьезно, 300 гигабайт – это такой объем, который можно положить просто в память. И построив хороший индекс, то есть тут не нужен колоночный стороч на 300 гигабайт. Это данные, которые спокойно влезают в раму одного сервера, ты в него можешь засунуть жирный Postgres, в котором все в раме, с немножко индексами, и все будет быстро работать. Особенно, когда на единственном Postgres... Цена за Amazon. Цена за Amazon? Да. Инстанс, на котором кликхаус сейчас крутится, он, не знаю, стоит 60 евро в месяц, 70. Два кликхауса уже 140. И это все равно дешево. Если возьмешь 300 гигабайт, 400 инстанс, он будет стоить там более штуки. А со ценами то, что мы используем в железо, все-таки смотрят. Ну, кстати, в этом плане, окей, а если вместо кликхауса вообще взять Redshift, будет не дешевле? С Redshift проблема в том, что он облачный. И я, например, не понимаю, как разрабатывать сервис, как тестировать сервис, где у тебя сервер, он не у тебя, он где-то. Как запустить интеграционные тесты? Тут тогда надо разрабатывать слишком много для DevOpsов. А у нас их всего ничего. Надо отметить, что когда сравнивали цены, это неправильное сравнение, потому что ты не используешь кликхаус как основную базу. У нас основная база Postgres, и там мы храним данные, а кликхаус только для аналитики. До этого у нас был, он сейчас есть, Elasticsearch, но Elasticsearch это печаль. Например... Мы поделились более, да? Да, я сказал, данных немного, 300 гигабайт. У меня заняло 2 дня сдампить эти данные просто на машину. В текстовом виде, я полагаю? В JSON. Я не понимаю, кликхаус 20 секунд занимает сдампить все данные. Да, там NVM диски, крутая машина, да. В Elasticsearch это была боль. Позволь спросить, если у вас основная база Postgres, и учитывая, что в Postgres есть полный текстовый поиск, зачем вам Elasticsearch вообще? В Elasticsearch... У нас 2 Elasticsearch, 3 Elasticsearch, 4. Ой, уже забыл. 3. 1 Elasticsearch для полного текстового поиска, потому что, когда мы начинали этот проект, в Postgres полный текстовый поиск еще нормально не работал. Это был 2013 год. Ну, ладно, там все это работает, там как-то ищут. Ищут, и там запросы выглядят страшно. Следующий Elasticsearch это для сбора лодов, потому что у нас система Greylock 2. Ну, как бы это из коробки, вроде как работает, не трогает. Иногда отваливаются, логи потеряны, ну и хрен с ними. Да и опять же, туда редко кто ходит. Был Elasticsearch, у нас сейчас есть, это для аналитики, куда складывали данные о том, кто чего сделал, посмотрел, куда кликнул, что скачал. Можно было строить агрегации, потом показывать красивые графики клиенту. Все это красиво работало, но начало тормозить. Можно было скейлить этот Elasticsearch, но так как мне пришлось эти все запросы писать, я знаю теперь, как их писать, но я очень не люблю их писать. Это не скейл. И писать аналитику на нескейле, это какая-то боль. Опять же, классическая фича, в Elasticsearch нет джоинов. Получалось так, что данные, как бы у вас их все еще есть где-то, чтобы их получить, тебе нужно твоим приложением несколько раз туда сходить, взять, и уже заджоинить у тебя в приложении.",
    "result": {
      "query": "Postgres vs ClickHouse для аналитики"
    }
  }
]