[
  {
    "segment_id": "52c56a9f-5405-4519-9b26-167365cba47b",
    "episode_id": "ba71f585-43bf-4010-a721-59d195e8c75c",
    "episode_number": 216,
    "segment_number": 5,
    "text": "Я читал The Morning Paper, а не сам пейпер. Извините, меня не осилил. Но я хотел, скорее, даже не столько пейпера обсудить, сколько вообще, наверное, воспользоваться этим как поводом к обсуждению, потому что, скажем так, я не помню, чтобы хоть у той компании, в которой я работал, был абсолютно идеально сделан фейловер DC или вообще в некоторых компаниях его не было совсем никак. То есть для начала поднимите руки, кто вообще делал многодатацентровые штуки в своей компании сам как-нибудь? Ну вот я могу сказать, я работал в Яндексе, мы там делали в IT такую штуку. Вот. В IT мог работать междатацентрово, соответственно, на алгоритме консенсуса, который основан на рафте, соответственно, если датацентрово... В одном регионе? В разных регионах. То есть там был один сервис, который в пяти разных регионах, и, соответственно, сервис блокировок и медной информации очень похожий на Zookeeper. Идеологически было примерно так. И, соответственно, ему были не страшны выпадения целиком датацентра. И был еще другой сервис, в смысле, тот же самый в IT, там уже были таблицы развернуты, он как раз сервис... этот кластер высокой доступности, там уже больше было серверов, и, соответственно, он работал в трех разных, ну, в нескольких разных регионах, и, соответственно, ему тоже были не страшны выпадения любого полностью. То есть и этом не требовало ручной какой-то... ну, специальных ручных действий, все происходило автоматически. И оно так и действительно происходило. А YT это Яндекс.Танк, верно? Не верно. YT это YT, оно никак не расшифровывается, но такое... Мы внутри себя говорили IT, а исторически, возможно, это какие-то слухи, что это Яндекс.Тейбл. То есть вначале для таблиц по типу мы продюс, а потом просто YT уже никто не расшифровывал, понятно, о чем это речь, и, в принципе, на хабре есть статья по этому поводу. Я, наверное, ссылку пришлю про описание этого продукта в YT конкретно, что он может... Собственно, в принципе, мы его называли IT. У нас был в продакшене Amazon и наш собственный дата-центр. Это считаются мульти-дата-центры. А на сколько были геораспределенные штуки? Они были все в Калифорнии, но в разных местах. Валер, если ты сейчас говоришь, то, скорее всего, в Mute. Я говорю, почти 4, потому что дата-центров много, но на самом деле, поскольку латенция не очень большая, то ну как бы так, ну почти. С одной стороны, да. А с другой стороны, ты же не с проблемами латенции хочешь бороться. На самом деле. А вот тут как бы со всеми проблемами, потому что если у тебя нет геораспределенности, то вот, ну, блин, это уже будет спойлер к пейперу. Собственно, да, о чем пейпер? Вот у вас есть, например, задача такая взять и преддатный фейловер дата-центра, из него просто вывести трафик. И вот это внезапно не самая тривиальная задача, потому что ну, как его, в основном нам нужно аккуратненько унести трафик с каждого отдельного сервиса, который у вас есть, один за другим, так, чтобы ничего не сломалось. Вот разрешение зависимости, задача разрешения зависимости, мне кажется, от геораспределенности зависит, ну, не очень. А вот задача, как сказать, задача... Ну, можно я, кстати, расскажу немного про проблематику, зачем вообще это нужно. То есть, в принципе, если проектировать систему изначально правильно, то даже в этом случае необходимо последовательное переключение дата-центров. Если у нас сервис, у которого строу-отпут не такой большой, то в принципе это не нужно делать. Но когда у нас огромное количество серверов, огромное количество сервисов, сервисы сложные, они могут быть стейтлес, стейтфул, причем стейтлес, они могут требовать координации, выбора лидера и прочее. То есть, не просто стейтфул сервисы, а стейтфул с какой-то сложной логикой. И плюс у нас еще гоняются огромные потоки данных, то просто так взять чик и переключить с одного дата-центра все на другое, это может привести к тому, что очень многое станет раком, потому что многие сервисы зависят от других. Когда мы начинаем квэрить эти сервисы, они какое-то время будут недоступны в момент переключения, эти сервисы становятся тоже недоступны и вот так вот все это разрастается и все может в принципе лечь при счастливом стечении обстоятельств, так сказать. Поэтому очень важно именно плавно перенести на грунт. Можно сделать, взять там какие-нибудь свитчи, переключать и все, но как правило это может привести к серьезным последствиям, потому что перенаправление трафика, выбор лидера, это такой очень тонкий момент и если делать это плавно, то гораздо менее заэффектит всю систему и пользователи вообще в принципе могут этого не заметить. То есть именно важно как корректно и плавно лечить с одного дата-центра на другое. Вот задача в этом состоит. Передаю слово дальше. В общем вот да. И в Facebook они писали свой подход к этому делу. У них есть фреймворк под названием Maelstorm, который я так понимаю, я не до конца вокнул описание на маэлстейпер. Маэлстром. Да, Маэлстром. Он, я так понимаю, втыкается и в само приложение и еще является какой-то штукой вокруг и еще является там такая система с как бы таким фидбэк-клубом и для сервисов, для каждого пишется такой специальный рандбук, который описывает как его переключать. Их там разные бывают типы сервисов в плане того насколько у них там стейт. То есть есть вообще без стейта. Самое простое то есть в приложении например есть какая-нибудь штука, может быть какая-то штука типа там soft state, когда его не хочется совсем здесь потерять и он должен там типа накопиться и типа там трафик должен для одного пользователя в одно и то же место приходить. То есть типичный пример это сервера, которые держат сессию чата, например. И наконец у нас есть вещи типа баз данных, где есть прям hard state, которые на дисках и там их нужно как-то аккуратно запромокить в зависимости от того, что там за баз данных. Вот. Ну и плюс еще некоторые сервисы нужно прям совсем конкретно кастомизировано реплицировать, если это какая-то кастомная супер мега система. Вот. Соответственно нужно ну да и сервисы друг от друга могут зависеть. То есть их там нужно в правильном порядке переключать. Ну вот и собственно ребята, ну то есть Facebook делает две вещи. Во-первых, да, они все это вот формализуют при помощи вот этого механизма рандбуков. Они каждый рандбук, когда он запускается они его мониторят. Они там типа, если мониторинг замечает, что что-то не работает, они его просто перестают эмигрировать и типа альертят админов. Если что-то работает хорошо, они будут пытаться быстрее это делать. Ну возникает вопрос, как вообще проверять, что рандбуки будут работать, когда все сломается. В общем, тут подход похожий на Netflix. Они, ну конечно, у них не хаосманки, но в общем они периодически некоторые вещи рандомно дрейнят и плюс раз встал, они дрейнят дата-центр куда-нибудь. Вот, таким образом они убеждают, что у них нет никаких проблем. Ну именно в самих рандбуках, что они типа более-менее рабочие. Вот, мне в принципе импонирует подход проверять, что у вас фейловер вообще фейловер, а не что-то. Но с другой стороны, тут как-то чистый записанного подхода, трудно сказать типа, вау, какая крутая штука, которая решит все проблемы. Потому что, ну, они не описали каких-то, на мой взгляд, они не описали каких-то супер сказочных открытий. Это просто система, которая не тривиальная, ее нужно написать, ее нужно заимплементировать. И вот я такого вообще не знаю даже, из чего это строить, чтобы не самому, а с чего-то начать. У кого-то есть идеи, из чего начать такое строить? Я вижу большую ошибку в попытке построить что-то абстрактное, техническо решающее нечто, когда задача еще не сформулирована. Потому что, ну, в простейшем случае, я в каждом дата-центре поднимаю по Postgres, говорю, что один из них мастер, делаю репликацию, пишу всегда на один мастер, да, это медленный, это latency. Но это, пожалуйста, вот у тебя решение, которое там, если один DCEU паул, нет проблем, запромоутили мастера в другом. Тут начинаются сразу оговорки, да, что ой, а вот мне, пожалуйста, такой же, но без latency. Поэтому очень важно отталкиваться от задачи. Ну, задача drainage traffic, как бы там задача очень хорошо описана в самом пейпере, как ни странно, или там в самой статье. То есть хочется сделать просто такое же, но из готовых инструментов. Ну, я один из возможных вариантов описал. Ну, нет, подожди, это не такое же. Это максимум такой легкий частный случай для одной базы данных, даже без собственного приложения писал. То есть у них же прикол в том, что они вначале перебросят, я не помню в каком там порядке, но я подозреваю, что вначале зафейловывают базу данных, а потом вэб-приложение. Нет, они описывают это в ранбуках для каждого приложения. Ну да, да, что в какую очередь перебрасывать, да, это правда. То есть мне очень понравился подход тем, что они, во-первых, изобретают велосипед. То есть разработчик сервиса сам должен знать и показывать в каком порядке тебе нужно работать с проблемой. И поэтому здесь у них написана просто библиотека стандартных случаев вида перенести шарду, перенести, переключить лоад-балансер и прочие вещи, а разработчик сервиса просто использует эти библиотеки для того, чтобы настроить правильный ранбук для данной системы. Я считаю, нехрен писать приложения, которые имеют прям hard-hard стейт. Ну, это существенно облегчит перенос трафика из одного доца в другой. Ну, ты же понимаешь, ты сейчас по сути говоришь, нехрен решать бизнес-задачу, да? Например. Нет, почему? Просто есть задачи, которые требуют низкой летции, например, соответственно, данные хочется положить как можно ближе, локально, желательно в том же дата-центре или на той же наде. И таких задач очень много. Поэтому говорит, что а давайте будем делать все по фен-шу, то есть данные отдельно, сервисы отдельно, и сервис выносится стейтлес. Даже оказывается, что стейтлес сервис, если там высокая нагрузка, их надо шардировать. Соответственно, если мы их шардируем, возникает такая конфигурация, но этот шард у нас получается уже очистить структура, то есть каждый шард нельзя взять и какой-то шард просто убрать. Тогда частично будет деградация сервиса. Получается, что все зависит от многих обстоятельств, и говорить, что есть решение для всех вещей, на мой взгляд, не очень полномерно. Ну и к тому же, нужно понимать, что некоторые системы разрабатывались, имеют иногда легоси, то есть разрабатывались без учета каких-то факторов, и поэтому, если система может помочь преодолеть какие-то сложности, это очень замечательно, я считаю. Полностью согласен, не надо искать серебряных вуль, и напомню, народную мудрость, если вы можете не писать распределенную систему, не пишите. И все-таки мне интересен вопрос, вот у нас уже есть, например... У меня такая компания, Диакий бизнес... Подожди, подожди, мне очень нравится вот этот переход, но все-таки, как бы два человека сказали, надо отталкиваться от задач, не надо искать универсальное решение, и ты такой, нет, ну а все-таки? Я не ищу универсальное решение, мой вопрос был, из чего это строить? Какие вы тулы знаете, хоть как-то похожие на описанное? Не предложение, типа давайте в постгрызе фейловерим, а предложение, как фейловерить всю систему. Я знаю потрясающий тул, он находится в такой уютненькой емкости, которая у людей между ушек. Вот, если ему... Саш, ты трогаешь? Ну, блин, Валер, ну потому что нужно брать конкретный бизнес с конкретными требованиями и решать задачу. Я понимаю вопрос Валера, Валер, да я поясню, может быть, Саша. Саш, смотри, у Валеры сейчас беда, ты чувствуешь у него по голосу, что у него это пригорает, ему нужно сделать то же самое, и он понимает, что он не сможет один, а ему, скорее всего, одному дали эту задачу, тягаться с тысячей пользователей, с тысячей разработчиков Фейсбука, которым тоже дали ту же самую задачу, и они ее делали полгода. Ему это надо сделать за одну неделю и одному. И поэтому он спрашивает, соответственно, помогите мне, пожалуйста, подскажите, как мне сделать то же самое, но только из готовых компонентов. Подскажите мне хотя бы ссылки на готовые компоненты. Валер, простой ответ, нет ссылок на готовые компоненты, такие задачи решает только Фейсбук, Гугл и еще пять компаний. Ну и теперь повезло вам тоже. К счастью, мне так не надо, и делать это не мне, но скажем так, если бы мне это пришлось делать, я не знаю, с какого конца хвататься. Возвращаясь к Пейперу, они выделяли, что мне понравилось, во-первых, они выделили некоторый класс одинаковых по сути сервисов. Сейчас я открою, может быть, даже скажу. Это сервисы, которые вообще не имеют стейта, сервисы, которые имеют стики стейт. В качестве примера это здесь мессенджеры. То есть, когда ты пишешь в мессенджер, у тебя есть коннект с каким-то пользователем, и этот коннект, он получается стейтфул, потому что он накапливает, я не знаю, там кэш, список отосланных, не отосланных и прочее. Есть отдельный класс, который называется Replication. Это привязки, как там, есть небольшой стейт, но нет привязки к каким-то отдельным хастам, насколько я помню. И есть полный стейтфул, когда у тебя, в общем, это самый жесткий стейтфул, какой есть. Соответственно, они разделяют по-разному, показывают по-разному, как делать ранбуки для этого, как немножко отличаются внутри семантика. Они показывают, каким образом каждый ранбук состоит из маленьких тасков, и соответственно, эти таски могут иметь зависимость между собой, и необходимо у них поэтому, вот этот run time, вот этого maelstrom, он состоит из нескольких частей, в качестве одной из частей у них есть решатель. Решатель водоворота, если так до конца перевести. Он определяет зависимость между этими тасками, и запускает, соответственно, таски, проверяет, как они запущены, и во время подготовки ранбука каждый разработчик, соответственно, определяет, в том числе и обратную связь, то есть какие-то метрики, которые показывают, правильно ли выполнен данный таск. И вот это самая интересная штука, как мне показалось, то есть представьте себе ситуацию. Вы разрабатываете сервис, и вы пишете ранбук. Насколько часто меняется сервис, пока вы разрабатываете? А очень часто. И постоянно должен меняться ранбук. Это как документация, которую никто никогда не проверяет, но в данном случае она должна всегда работать, потому что у вас это единственный способ оградить пользователя от проблем. И они пришли к отличному выводу, они пришли к выводу, что у них должна быть система, которая запускает эти ранбуки постоянно и проверяет, насколько они правильно работают. То есть если бы документацию можно было проверить и семантику и сказать, что сейчас какая-то фигня написана, и у тебя она не работает, было бы тоже идеально. И вот они так запускают, запускают, и, соответственно, через метрики, которые определяет сам пользователь, они определяют, что в настоящий момент времени у нас шарда еще не переместилась, подождите, выполнять все остальные вещи, а вот сейчас у нас шарда переместилась, или таск заполнился, выполнился. А, и, кстати, у них система с помощью 75-го персентиля выполнения именно данного таска на данной задаче проверяет, что у тебя зависла данная таска или не зависла, и если зависла, она, соответственно, создаёт какой-то алерт. Ну, короче, мне очень понравилось, как они все это написали, но я понимаю, что это огромный объем работы. Вот просто прям очень большой. И я не думаю, что найдутся хоть какие-то полуготовые компоненты, скорее всего, большую часть всего придется писать с нуля, и я не знаю, мне страшно было браться за такую задачу. Валера? Валера сидит и боится. Переход. За какую задачу еще страшно браться? Это за задачу написать постмортум для гитхаба. Я бы по-другому переходил, на самом деле. Мне кажется, а у кого нет мелистрёма, тот вот так фейловерится. Кто не тестирует свои ранмуки на фейловер каждый день, того фейловер не работает. У гитхаба фейловер сработал, но давайте очень специфичным образом. Значит, сейчас у нас опять продолжается рубрика про распределённую систему. В прошлый раз мы говорили про exactly once, я рассказывал. Сейчас мы поговорим про гитхаб и то, что с ним случилось. В принципе, тема, на мой взгляд, очень интересная. Вообще, постмортум и анализ инцидента это очень классная вещь. Те компании, которые предоставляют анализ инцидентов, они молодцы. Они предоставляют пользователям прозрачность и пользователи больше доверяют инженерам компаний и вообще продукту как-то колобок. Это полезно и для разработчиков распределённых систем, потому что они на основе этого опыта могут понять, что вот так делать не стоит или если сделать так, то, возможно, такие вещи, поэтому стоит сделать что-то дополнительное. Дополнительное, что предотвратит это в будущем. То есть это, в принципе, полезно для всех и для самой компании. То есть когда инженеры спят о том, что они делали внутри себя. То есть они внутри себя понимают, что вот с нами произошло то-то-то и мы хотим сделать то-то-то. Это очень хорошо дисциплинирует. И вообще, если немного в сторону, вот есть такая шутка-не шутка, что умные учатся на своих ошибках, а мудрые на ошибках чужих. Так вот, инцидент анализис — это ошибки чужих, но которым имеет смысл учиться. Ну и плюс всегда интересно посмотреть, как же вообще всё устроено. Потому что, как правило, невозможно рассказать какие-то технические детали без раскрытия информации о том, как устроена система или её части. Ну вот, давайте перейдём и посмотрим, что же случилось относительно недавно. Я сначала кратко опишу последовательность, ну, важные моменты, которые были в инциденте. А потом мы разберём, на мой взгляд, какие-то моменты, которые требуют наиболее преступного внимания. Значит, всё началось с чего? Началось с того, что свитч, который связывал два дата-центра, топология следующая, что есть два дата-центра, кстати, в статье про инцидент там очень хорошая картинка, поэтому имеет смысл взять треть, потому что на словах её сложно описать, но идея в чём? Есть два дата-центра в них, расположены серверы MySQL. MySQL реплицируется, используя мастер-слоевый механизм.",
    "result": {
      "query": "маилстром фейловер дата центров"
    }
  }
]