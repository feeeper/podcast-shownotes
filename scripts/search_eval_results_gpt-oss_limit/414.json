[
  {
    "segment_id": "c88d92c8-7696-4aed-bef9-8a37c7ca50cb",
    "episode_id": "a124de6f-7f57-46c2-beb9-91b6d0dcd8a4",
    "episode_number": 414,
    "segment_number": 8,
    "text": "А еще я посмотрел доклад из серии Advanced Database Systems в этот раз про компиляцию и генерацию запросов. Это будет очень короткий пересказ, потому что пересказывать есть не так много чего. Здесь у нас по большому счету такой хендвейвинг, что вот если запросы их интерпретировать, вот как бы наивно парсить, то есть получить планы исполнения, вот потом его как в интерпретаторе крутить, то это медленно. У нас нас интересует как писать in memory базу данных, поэтому скорость критична, поэтому запросы по-любому надо компилировать. Дальше говорится, что многие используют LLVM, но если кто-то пишет базу данных по JVM, там и JVM байт-кода используют с последующей джет компиляцией. Все говорят, что поддерживать джет компиляции запросов это лютый головняк, и так говорят инженеры IBM и MMSQL, и собственно те, кто работали над Peloton. И действительно ценным в этом докладе есть несколько слайдов с графичками, то есть насколько реально джет компиляции ускоряет запросы, если мы не учитываем само время компиляции запроса, мы можем попытаться им пренебречь в предположении, что мы кэшвируем скомпилированные запросы. Вот тогда можно получить, но на самом деле здесь достаточно большой разброс до 100 раз, но сто это вот прям оценка сверху. Если брать там числа поскромнее, но все равно во многие разы компиляция при помощи LLVM в данном случае повышает скорость исполнения запросов по сравнению с интерпретацией. Хотя если подумать, вот график, который смотрел немножко стороненький, потому что здесь запросы исполняются секунды, то есть речь видимо про... А, и здесь написано, что это TPC-H, поэтому понятно, что на LTP-нагрузках у тебя будет не такой большой выигрыш, потому что запрос может исполняться 2 милисекунды, не знаю, 5 милисекунд, 10, но все равно ты будешь дольше его компилировать или дольше искать в кэше вот этот скомпилированный запрос, чем реально его исполнять. Но это не значит, что для любых LTP-запросов это бесполезно. Вот еще говорится про время компиляции, как оно зависит от флагов оптимизации. Если компилировать без оптимизации, то... Ну, тут на некотором железе Intel Core 2.0. На самом деле, я думаю, это не суть важно, потому что вряд ли компиляция распараллеливается в этом бейчмарке. Компиляция запроса занимает 120-200 милисекунд, если без оптимизации, а с оптимизациями до 600 милисекунд, ну, смотря что за запрос. Что намекает нам, что компиляция штука не бесплатная, по крайней мере конкретно в случае с использованием бэкэнда, вроде GCC или LLVM. И наконец, докладчик делится такой мыслью, что хороший подход, он говорит про некоторую систему, которую они разрабатывали после Peloton. Честно говоря, я не уверен, что именно имеется в виду. Ну, видимо, NoisePage. Возможно, но неизвестно, использовали они то, о чем он говорит, потом в итоге в NoisePage. Но они... Давай так, Энди говорит, что хороший подход выглядит так. Ты берешь план исполнения запроса и транслируешь его в байт-код, притом этот байт-код, он твой внутренний байт-код, твоей суббд, это не LLVM байт-код или что-то такое. И когда ты получил этот байт-код, ты делаешь следующее, ты сразу начинаешь его интерпретировать, не ожидая ничего. И пока ты его интерпретируешь, ты параллельно запускаешь thread, который этот байт-код конвертирует в байт-код LLVM и уже компилирует, оптимизирует. И тогда в следующий раз, когда ты придешь с этим же запросом, но, возможно, с другими параметрами, то есть запрос, он нормируется, из него выбрасываются все параметры, которые ты в него можешь подсунуть. Когда ты в следующий раз придешь с этим запросом, ты увидишь, что в кэшеке уже есть обосконфилированная версия, и вместо того, чтобы его еще раз конфилировать, просто возьмешь готовую инструкцию. Вот, в общем-то, и весь доклад. Комментарии, вопросы, дополнения. Ну а поскольку вопросов нет, я передаю слово обратно Ивану, который, как пизде, отошел налить себе чайку. Поэтому мы перейдем к темам наших слушателей. Темы наших слушателей. Первую тему, которую мне нашел Телеграм.",
    "result": {
      "query": "Компиляция запросов in‑memory БД производительность"
    }
  }
]